

## Papers for 2024-10-29

| Title | Authors | Summary |
|-------|---------|---------|
| Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation (Read more on [arXiv](https://arxiv.org/abs/2410.18565) or [HuggingFace](https://huggingface.co/papers/2410.18565))| Remek, adgw, djstrong, lflis, chrisociepa | This research aimed to develop a high-performing Polish language model. The authors adapted the Mistral 7B v0.1 model and further pre-trained it on a curated dataset of Polish and English texts, incorporating techniques like Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate.  Evaluation on the Open PL LLM Leaderboard showed a 9 percentage point improvement over Mistral-7B-v0.1 on the RAG Reader task. This implies that adapting and further training existing multilingual models can significantly improve performance for specific languages. The paper does not detail the exact composition of the training dataset (sizes of Polish vs. English portions, etc.) and the rationale behind the chosen weights for the Weighted Instruction Cross-Entropy Loss.   Follow-up questions:  1. What were the specific data cleaning and quality assessment procedures used for the Polish portion of the training dataset, and how did they contribute to the observed performance gains? 2.  Could the authors provide further details on the distribution of weights assigned to the instruction-response pairs in the Weighted Instruction Cross-Entropy Loss and explain how these specific values were determined? 3. What is the detailed split between instruction data from OpenHermes-2.5, orca-math, and the manually generated instruction data in the post-training dataset?  |
| AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant (Read more on [arXiv](https://arxiv.org/abs/2410.18603) or [HuggingFace](https://huggingface.co/papers/2410.18603))| Fangzhi Xu, Qiushi Sun, Zhuohang Dang, Minnan Luo, Chengyou Jia | This research aimed to develop a scalable platform for integrating heterogeneous agents to automate computer operating system tasks.  The key methodology involved creating AgentStore, a platform with an AgentPool of specialized agents, an AgentEnroll protocol for adding new agents, and a MetaAgent using an AgentToken strategy to manage and select agents for task execution.  On the OSWorld benchmark, AgentStore achieved a 23.85% success rate, more than doubling the previous best system's performance (11.21%).  This implies that for AI practitioners, integrating specialized agents significantly enhances agent systems in both generalization and specialization for complex, open-ended computer tasks. The paper does not provide details about the training data or the agent integration protocol, stating they will be available when the project is open-sourced.   Follow-up questions:  1. What is the specific architecture of the MetaAgent, including details about its multimodal processing capabilities and how it integrates the system state information? 2. Can you elaborate on the agent integration protocol, specifically the format and content of the document developers need to provide during AgentEnroll? 3. How does the automated process with self-instruct generate diverse and consistent training data for AgentToken, and what mechanisms prevent hallucination or irrelevant data generation during this process?  |
| GPT-4o System Card (Read more on [arXiv](https://arxiv.org/abs/2410.21276) or [HuggingFace](https://huggingface.co/papers/2410.21276))| Adam Perelman, Adam P. Goucher, Adam Lerer, Aaron Hurst, OpenAI | a) This system card analyzes GPT-40, an omni-modal AI model, assessing its capabilities, limitations, and safety implications, with a focus on speech-to-speech interactions.  b) Evaluations include external red teaming across diverse languages and demographics, converting existing text-based evaluations to audio using text-to-speech, and Preparedness Framework assessments for cybersecurity, bio-threats, persuasion, and model autonomy.  c) GPT-40â€™s voice output classifier achieved 96% precision and 100% recall in English for detecting deviations from authorized voices.  d)  AI practitioners should be aware of the potential for misuse of voice generation capabilities, the residual risk of unintentional voice generation despite mitigations, and the potential for disparate performance across accents and languages, necessitating further research and mitigation development.   Follow-up questions:  1. What specific techniques were used in post-training to align the voice model to ideal completions and prevent unauthorized voice generation? 2. How does GPT-40's performance on non-English languages compare to its performance on English across other modalities besides text, such as image and video understanding? 3. What are the limitations of the current evaluations, especially concerning the use of TTS for converting text-based evaluations to audio, and how can future evaluations be improved to address these limitations?  |
| Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction (Read more on [arXiv](https://arxiv.org/abs/2410.21169) or [HuggingFace](https://huggingface.co/papers/2410.21169))| Zhengren Wang, Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Qintong Zhang | This paper surveys document parsing techniques for extracting structured information from various document formats. The authors review both modular pipeline systems, comprised of layout analysis, content extraction, and relation integration stages, and end-to-end approaches using vision-language models (VLMs).  The survey consolidates commonly used datasets, like PubLayNet for layout analysis and ICDAR for OCR, and associated evaluation metrics, including IoU for layout analysis and character error rate for text recognition.  While lacking quantitative comparisons between the modular and VLM approaches, the authors highlight the emerging trend of unified frameworks and universal OCR paradigms exemplified by models like GOT, which achieved performance improvements on complex charts and non-traditional content.  This suggests that VLMs offer a promising path towards more general and efficient document parsing solutions.  Follow-up Questions:  1.  Given the limitations discussed for both modular systems and VLMs, what specific strategies (e.g., architectural changes, training techniques) could be most effective for improving the performance of VLMs on high-density text and complex table structures found in document images? 2.  What are the comparative computational resource requirements (training time, memory, inference speed) of modular systems and end-to-end VLM approaches for document parsing, and how do these impact practical deployment considerations? 3.  While GOT demonstrates a promising universal OCR approach, how effectively does it generalize to diverse document types and languages beyond the datasets mentioned in the paper, and what further research is needed to assess its real-world applicability across different domains?  |
| LongReward: Improving Long-context Large Language Models with AI Feedback (Read more on [arXiv](https://arxiv.org/abs/2410.21252) or [HuggingFace](https://huggingface.co/papers/2410.21252))| Zhenyu Hou, Shulin Cao, Xin Lv, Zhongni Hou, Jiajie Zhang | a) The research aims to improve the performance of long-context large language models (LLMs), addressing the issue of compromised quality in LLM-synthesized training data.  b) The proposed method, LongReward, uses an off-the-shelf LLM to provide rewards for model responses based on helpfulness, logicality, faithfulness, and completeness, combined with the Direct Preference Optimization (DPO) reinforcement learning algorithm.  c) Experiments showed that DPO models using LongReward outperformed supervised fine-tuning (SFT) models on long-context tasks by 4.9% and 5.5% for Llama-3.1-8B and GLM-4-9B, respectively.  d) LongReward provides a practical method for aligning long-context LLMs with human preferences, enabling AI practitioners to train models with improved long-context capabilities and reduced hallucinations.   Follow-up questions:  1.  What is the computational cost of using LongReward, particularly with respect to the number of API calls to the judge LLM, and how can this be optimized for practical deployment? 2.  How does the choice of the "off-the-shelf" LLM used as the judge in LongReward affect the performance and biases of the final trained long-context LLM? 3.  Could LongReward be adapted for other RL algorithms beyond DPO, and what might be the potential benefits or drawbacks of such adaptations?  |
| DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation (Read more on [arXiv](https://arxiv.org/abs/2410.18666) or [HuggingFace](https://huggingface.co/papers/2410.18666))| Xiaotian Han, Huaibo Huang, Xiaoqiang Zhou, Yuang Ai, Ye27 | This research aims to improve real-world image restoration (IR) by addressing dataset limitations and developing a high-capacity model.  The authors introduce GenIR, a privacy-preserving data pipeline using text-to-image diffusion models and multimodal large language models to generate a synthetic dataset of one million high-quality images. They then present DreamClear, a Diffusion Transformer-based IR model incorporating degradation priors via a Mixture of Adaptive Modulator (MoAM).  On the LSDIR-Val benchmark, DreamClear achieves a 0.3836 LPIPS score.  This work offers practitioners a method for creating large-scale, privacy-safe IR datasets and a high-performing model leveraging diffusion and degradation priors.  Follow-up questions:  1. What are the specific architectural details and hyperparameters of the routing network (R) within the MoAM module, and how were these determined? 2. While the paper mentions model distillation and quantization as potential solutions for improving inference speed, are there any specific experiments or preliminary results demonstrating the effectiveness of these methods on DreamClear? 3. Could the GenIR pipeline be adapted for other vision tasks beyond image restoration, and what modifications might be necessary for such adaptations?  |
| MarDini: Masked Autoregressive Diffusion for Video Generation at Scale (Read more on [arXiv](https://arxiv.org/abs/2410.20280) or [HuggingFace](https://huggingface.co/papers/2410.20280))| Yanping Xie, Mengmeng Xu, Zijian Zhou, Shikun Liu, Haozhe Liu | a) The research aimed to develop a scalable and efficient video generation model that combines the flexibility of masked autoregressive (MAR) modeling with the stability of diffusion models (DMs).  b) MarDini uses an asymmetric architecture with a MAR planning model operating on low-resolution inputs to generate planning signals, and a lightweight DM generating high-resolution frames conditioned on these signals and unmasked frames.  A progressive training strategy with increasing task difficulty (from video interpolation to image-to-video generation) and resolution was employed.  c) MarDini-L/T achieved an FVD score of 117.13 on the DAVIS-7 video interpolation benchmark, surpassing previous methods.  The paper does not explicitly report results for image-to-video generation on VBench without motion score guidance.  d) AI practitioners can leverage MarDini's architecture and training strategy to develop efficient and scalable video generation models trained from scratch without relying on generative image pre-training, enabling the creation of long-term video interpolations, video expansions, and image-to-video animations using a single model.  The paper does not provide sufficient detail to assess general image-to-video generation performance compared to state-of-the-art, only reporting a subset of the evaluated VBench metrics.  Follow-up Questions:  1.  Could you elaborate on the specific implementation details of the "Identity Attention" mechanism and quantify its impact on training stability across different model sizes and resolutions? 2. How does MarDiniâ€™s performance on standard image-to-video generation tasks (with full motion score guidance) compare to state-of-the-art models on VBench? The paper references improved "physical principles" but doesn't quantify this, and it only compares MarDini to other methods on a subset of VBenchâ€™s metrics.  3.  What are the limitations of the current progressive training scheme, and how can it be further optimized for even greater scalability and efficiency in terms of both training time and resource utilization?  |
| A Survey of Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.20011) or [HuggingFace](https://huggingface.co/papers/2410.20011))| Samyadeep Basu, Yu Xia, Ryan Aponte, Xuan Shen, Chien Van Nguyen | a) This survey aims to provide a comprehensive overview of Small Language Models (SLMs), focusing on their architectures, training techniques, and model compression methods. b) The authors propose a novel taxonomy categorizing SLM optimization methods based on the techniques used (pre-processing, training, post-processing) and the constraints addressed (inference compute, training time, etc.).   c) MobileBERT achieved a 4.3x size reduction and a 5.5x speedup compared to the base version of BERT. d)  AI practitioners can utilize this taxonomy and the survey's summary of existing techniques to select appropriate methods for developing and deploying SLMs under specific resource constraints.  Follow-up questions:  1.  While the survey mentions trade-offs between optimization goals, are there any quantitative analyses or specific examples that illustrate these trade-offs (e.g., memory-efficient training vs. inference speed)? 2.  The paper mentions neural architecture search (NAS) for SLMs.  Are there recommended NAS methods or tools specifically suited for the scale and characteristics of SLMs, and how do they compare in terms of computational cost and effectiveness? 3. How does data privacy for small language models compare to data privacy for large language models with the same underlying architecture, i.e. is privacy "easier" with small language models because less data is available to analyze for extraction of personal or protected data?  |
| GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation (Read more on [arXiv](https://arxiv.org/abs/2410.20474) or [HuggingFace](https://huggingface.co/papers/2410.20474))| Minhyuk Sung, Taehoon Yoon, Phillip Y. Lee | a) This research aims to develop a training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT) that allows for precise control over object placement within user-specified bounding boxes.  b) The proposed method, GrounDiT, employs a two-stage denoising process: a global update based on cross-attention map alignment with bounding boxes and a local update involving the cultivation and transplantation of noisy image patches, leveraging DiT's "semantic sharing" property.  c) On the HRS benchmark, GrounDiT achieves 45.01% spatial accuracy, a +14.87% improvement over the previous state-of-the-art training-free method (R&B).  d) AI practitioners can use GrounDiT to enhance user controllability in text-to-image generation with DiT models by achieving fine-grained spatial grounding without model retraining.  This enables more precise object placement and layout control for various applications like image editing and compositional image generation.    Follow-up questions:  1.  The paper mentions increased computational cost due to separate object branches.  How does this cost scale with the number of bounding boxes, and what are the practical implications for real-time applications? 2.  Could the semantic sharing property be exploited for other tasks beyond spatial grounding, such as style transfer or controlled image manipulation within specific regions? 3. While the paper focuses on PixArt-Î±, how adaptable is GrounDiT to other DiT architectures, and what modifications might be necessary for optimal performance?  |
| COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training (Read more on [arXiv](https://arxiv.org/abs/2410.19313) or [HuggingFace](https://huggingface.co/papers/2410.19313))| Kurt Keutzer, Yao Lu, Ligeng Zhu, Han Cai, Haocheng Xi | a) The paper investigates reducing the memory footprint of FP8 training for large language and vision-language models, specifically targeting optimizer states and activations which are often kept in higher precision in existing FP8 training frameworks.  b) COAT (Compressing Optimizer States and Activations for FP8 Training) introduces Dynamic Range Expansion for optimizer states and Mixed-Granularity Activation Quantization, combining per-tensor and per-group quantization.  c) COAT achieved a 1.54x reduction in end-to-end training memory compared to BF16 and a 1.43x speedup on Llama-7B, 13B, and 30B models, while maintaining nearly lossless performance across various tasks.  d) AI practitioners can utilize COAT to enable full-parameter training of larger models on fewer GPUs or double batch sizes in distributed settings, facilitating more efficient large-scale model training.  This improved memory efficiency translates directly into larger batch sizes and potentially longer context lengths, both beneficial for training larger models.  Follow-Up Questions:  1. How does COAT's Dynamic Range Expansion handle potential overflow or underflow issues, particularly with second-order momentum which the paper mentions is sensitive to quantization?  2.  The paper mentions per-group quantization for activations of non-linear layers - what specific group sizes were found to be optimal for different model architectures and how sensitive is the performance to these group size choices?  3. What is the impact of COAT on inference latency, and how easily can models trained with COAT be deployed for inference with existing FP8 inference solutions?  |
| Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines (Read more on [arXiv](https://arxiv.org/abs/2410.21220) or [HuggingFace](https://huggingface.co/papers/2410.21220))| Xiangyu Yue, Xiaohan Ding, Yiyuan Zhang, Zhixin Zhang | a) The paper aims to improve the generalization ability of Vision-Language Models (VLMs) to handle unseen images and novel concepts by integrating them with web search agents.  b) The proposed Vision Search Assistant framework uses a three-step process: 1) Visual Content Formulation to extract object-level descriptions and correlations from images using a VLM. 2) Web Knowledge Search, an iterative algorithm using an LLM as a planning agent to generate sub-questions and a searching agent to retrieve and summarize web information. 3) Collaborative Generation, combining visual content, user prompt, and web knowledge to generate the final answer using the VLM.  c) In closed-set evaluations on the LLaVA-W benchmark, Vision Search Assistant achieved an overall score of 84.9%, a +6.4% improvement over the baseline LLaVA 1.6-7B model.  d) AI practitioners can leverage this framework to build more robust and adaptable VLMs capable of handling real-world, open-domain scenarios requiring up-to-date information and complex reasoning about visual content.  The ability to integrate real-time information access through a web search significantly enhances VLM performance, particularly in reasoning tasks.  Follow-up questions:  1.  What are the computational costs and latency implications of the iterative Web Knowledge Search process, particularly for complex images requiring multiple iterations? 2.  How robust is the system to noisy or irrelevant web search results, and what mechanisms are in place to ensure the quality and reliability of the retrieved information? 3.  Could the Visual Content Formulation stage benefit from more advanced scene graph generation techniques to better capture relationships between objects beyond simple co-occurrence in captions?  |
| LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior (Read more on [arXiv](https://arxiv.org/abs/2410.21264) or [HuggingFace](https://huggingface.co/papers/2410.21264))| Abhinav Shrivastava, Hao Chen, Yixuan Ren, Saksham Suri, Hanyu Wang | a) The paper aims to develop a video tokenizer optimized for autoregressive (AR) generative models, addressing limitations of existing patchwise tokenizers in capturing holistic representations and efficiently aligning with AR generation.  b) LARP employs holistic tokenization using learned queries, a stochastic vector quantizer (SVQ), and a lightweight AR transformer as a training-time prior model to structure the latent space for AR generation.  c) On the UCF101 class-conditional video generation benchmark, LARP achieved a state-of-the-art FrÃ©chet Video Distance (FVD) score of 57.  d) AI practitioners can utilize LARP to improve the quality and efficiency of AR video generation, potentially enabling the development of more sophisticated and scalable video generation models.  The paper's emphasis on aligning the latent space with the generative process is impactful, suggesting a potential pathway for enhancing AR model performance in various visual domains.   Follow-up questions:  1. How does the computational cost of LARP, including the training-time prior model, compare to existing video tokenizers, particularly during inference?  2.  Could the holistic tokenization approach of LARP be adapted for other AR tasks beyond video generation, such as video captioning or action recognition?  3. The paper mentions using a Llama-like transformer as the AR generative model.  What specific architecture and hyperparameters were used, and how were they chosen?  |
| Fast Best-of-N Decoding via Speculative Rejection (Read more on [arXiv](https://arxiv.org/abs/2410.20290) or [HuggingFace](https://huggingface.co/papers/2410.20290))| Jiahao Qiu, Huitao Yang, Ruiqi Zhang, Momin Haider, Hanshi Sun | a) The research aims to develop a more computationally efficient inference-time alignment algorithm for Large Language Models (LLMs) that achieves comparable performance to Best-of-N decoding with large N.  b) The proposed Speculative Rejection algorithm begins with a large initial batch size and iteratively prunes lower-scoring partial utterances based on a reward model, dynamically reducing computational cost.  c)  Using Llama-3-8B with the RM-Mistral-7B reward model on the AlpacaFarm dataset, Speculative Rejection achieved a reward score comparable to Best-of-N with N between 1920 and 3840, requiring 16-32x fewer GPUs.  d) AI practitioners can utilize Speculative Rejection to significantly reduce the computational resources needed for inference-time alignment of LLMs, enabling the use of higher effective N values on single accelerators, potentially improving alignment effectiveness.  e) The paper notes that different combinations of LLMs and reward models vary in reward score improvement, and the relation between this variance and LLM or reward model properties is not fully explored.   Follow-up questions:  1. How does the choice of rejection rate (Î±) affect the trade-off between computational cost and final reward score across different LLM architectures and reward model complexities?  2. Could the performance of Speculative Rejection be further improved by incorporating prompt-dependent adaptive rejection rates or by using reward models trained as value functions?  3.  Are there other metrics beyond reward score, such as diversity or fairness, that could be incorporated into the rejection criteria for Speculative Rejection?  |
| Neural Fields in Robotics: A Survey (Read more on [arXiv](https://arxiv.org/abs/2410.20220) or [HuggingFace](https://huggingface.co/papers/2410.20220))| Abhinav Valada, Nick Heppert, Yen-Chen Lin, Mauro Comi, Muhammad Zubair Irshad | a) This survey paper reviews the applications of Neural Fields (NFs) across various robotics domains, analyzing their benefits and limitations. b) The authors categorize and analyze over 200 research papers on NFs in robotics, focusing on core frameworks like Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting, and their use in pose estimation, manipulation, navigation, physics simulation, and autonomous driving. c) The paper shows a rapid growth in NF robotics publications, increasing from 6 publications comprising 10% of total NF publications in 2021 to 73 publications making up 22% in 2023.   d) The survey provides AI practitioners with a comprehensive overview of existing NF techniques in robotics, highlighting their strengths and weaknesses in different applications, aiding in informed selection and development of future NF-based robotic systems.   Follow-up questions:  1. Given the computational intensity of NFs, what specific optimization strategies are most promising for deploying them in real-time robotic applications on resource-constrained hardware? 2. What are the most effective methods for integrating semantic information, like that from foundation models, into NF representations to improve generalization and enable higher-level reasoning capabilities in robots? 3.  How can NFs be effectively combined with physics simulators to create physically realistic training environments for robots, and what are the main challenges in ensuring successful sim-to-real transfer of learned policies?  |
| Language Models And A Second Opinion Use Case: The Pocket Professional (Read more on [arXiv](https://arxiv.org/abs/2410.20636) or [HuggingFace](https://huggingface.co/papers/2410.20636))| David Noever | This research investigated the effectiveness of Large Language Models (LLMs) as second opinion tools in complex medical and legal scenarios.  The study analyzed LLM performance on 183 challenging medical cases from Medscape and 21 Supreme Court cases, comparing responses to crowd-sourced physician and published judicial decisions, respectively.  Foundational LLMs achieved >81% accuracy on straightforward medical cases but only 43% accuracy on complex medical cases, compared to consensus human expert answers.  This disparity suggests that while LLMs excel in information retrieval and structured scenarios, they currently struggle with the nuanced reasoning required for complex, real-world problem-solving. The paper doesn't specify details of the LLM prompting or fine-tuning strategies used.   Follow-up questions:  1. What specific prompting strategies were employed to elicit detailed reasoning and alternative diagnoses from the LLMs, and how did prompt engineering influence performance, particularly in ambiguous cases? 2. How did the inclusion of visual data (for the subset of cases with imaging) affect LLM performance across different models, and were there specific image processing or multimodal fusion techniques employed to integrate this information? 3. What specific metrics beyond accuracy, such as F1-score, precision, and recall, were used to evaluate LLM performance, especially in cases with multiple viable diagnoses?  |
| Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2406.10615) or [HuggingFace](https://huggingface.co/papers/2406.10615))| Yang Gao, Jiacheng You, Yingdong Hu, Tong Zhang | a) This research aims to improve sample efficiency in robotic manipulation by leveraging the inductive bias of action locality, which posits that robot actions are primarily influenced by the target object and its local environment.  b) The authors introduce SGRv2, an imitation learning framework built upon the Semantic-Geometric Representation (SGR) that incorporates action locality through an encoder-decoder architecture, relative target position prediction, point-wise weighting, and dense supervision.  c)  SGRv2 achieves a 53.2% average success rate on 26 RLBench tasks using only 5 demonstrations, outperforming the RVT baseline on 23 of these tasks and demonstrating improved sample efficiency.  d) AI practitioners can utilize the principles of action locality and the SGRv2 framework to develop more sample-efficient robotic manipulation models, reducing the reliance on large demonstration datasets which are costly to acquire. The most impactful finding is the significant improvement in sample efficiency,  directly addressing the practical challenge of limited real-world robotic data.  Follow-up questions:  1. How does the computational cost of SGRv2 compare to other methods like RVT and PerAct, especially considering the use of point-wise predictions and weighted averaging?  2. Could the concept of action locality and the techniques employed in SGRv2 be generalized to other robotic tasks beyond manipulation, such as navigation or multi-agent scenarios?  3. While the paper demonstrates robustness to visual distractors, how robust is SGRv2 to variations in the physical properties of the environment, such as changes in friction or object weight?  |
