

## Papers for 2024-10-31

| Title | Authors | Summary |
|-------|---------|---------|
| CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation (Read more on [arXiv](https://arxiv.org/abs/2410.23090) or [HuggingFace](https://huggingface.co/papers/2410.23090))| Hongjin Qian, Ziliang Zhao, Kelong Mao, dongguanting, ariya2357 | CORAL is a new benchmark for evaluating multi-turn conversational Retrieval-Augmented Generation (RAG) systems. The research aimed to create a benchmark dataset for evaluating the performance of RAG systems in multi-turn conversational settings.  The key methodology involved automatically converting English Wikipedia pages into 8,000 multi-turn, information-seeking conversations using four different conversation flow sampling strategies and large language models.  Qwen2.5-1.5B-SFT achieved the highest retrieval score, outperforming commercial closed-source LLMs with 23.1 MRR.  This benchmark enables AI practitioners to rigorously evaluate and improve multi-turn conversational RAG systems, facilitating the development of more robust and knowledge-grounded conversational AI agents.  |
| A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks (Read more on [arXiv](https://arxiv.org/abs/2410.22391) or [HuggingFace](https://huggingface.co/papers/2410.22391))| Korbinian PÃ¶ppel, Maximilian Beck, Vihang Patil, Thomas Adler, Thomas Schmied | Here's a concise summary of the AI research paper following your strict guidelines:  i) This paper investigates the suitability of modern recurrent architectures, particularly xLSTM, for building large action models (LAMs) to achieve fast inference in robotics.  ii) The main objective was to test the hypothesis that modern recurrent models are better suited for LAMs than Transformers regarding training and inference speed.  iii)  The researchers developed a Large Recurrent Action Model (LRAM) using xLSTM and trained it on a large-scale multi-domain dataset (894M transitions from 432 tasks) using a supervised learning setting similar to Decision Transformer.  iv) Experiments showed that xLSTM-based LRAMs outperformed Transformers in terms of both performance and speed across 432 tasks; specifically, on the 206M parameter models, xLSTM achieved better performance than Transformers and the inference time was significantly lower, with significantly reduced latency compared to Transformer-based models across different context lengths.  v)  The most impactful finding, the superior inference speed of xLSTM-based LRAMs, suggests that modern recurrent architectures offer a compelling alternative to Transformers for real-time robotic applications requiring fast inference.  The paper lacks information regarding the specific hardware used for the comparison of speed/latency.  |
| Stealing User Prompts from Mixture of Experts (Read more on [arXiv](https://arxiv.org/abs/2410.22884) or [HuggingFace](https://huggingface.co/papers/2410.22884))| Nicholas Carlini, Jamie Hayes, Ilia Shumailov, Itay Yona | This paper demonstrates a novel attack exploiting architectural flaws in Mixture-of-Experts (MoE) LLMs to extract user prompts.  The research aimed to determine if an adversary could exploit Expert-Choice-Routing (ECR) in MoE models to disclose a victim's prompt when batched together.  The attack manipulated expert routing within a two-layer Mixtral model using crafted adversarial batches, triggering the ECR tie-breaker to leak information. In their evaluation, 99.9% (4833/4838) of the secret tokens across a test set of 1000 common English words were successfully recovered.  This vulnerability highlights the critical need for AI practitioners to consider prompt security and batch independence during the design and deployment of MoE-based LLMs.  |
| AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels (Read more on [arXiv](https://arxiv.org/abs/2410.20050) or [HuggingFace](https://huggingface.co/papers/2410.20050))| Xiao Zhou, Xiangxu Zhang, Lei Li, zl101 | This paper introduces SL-HyDE, a self-learning framework for zero-shot medical information retrieval.  The research aims to develop an effective dense retrieval system for medical information without requiring relevance-labeled training data.  The key methodology involves a self-learning framework that iteratively refines a large language model (LLM) for generating hypothetical documents and a dense retrieval model for document ranking.  SL-HyDE improved NDCG@10 by an average of 4.9% across ten datasets compared to HyDE (Qwen2 as generator + BGE as retriever).  This improvement suggests that AI practitioners can leverage SL-HyDE to develop more accurate medical information retrieval systems without the need for expensive and time-consuming manual annotation of relevance data.  |
| TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters (Read more on [arXiv](https://arxiv.org/abs/2410.23168) or [HuggingFace](https://huggingface.co/papers/2410.23168))| Jan Eric Lenssen, Yongqin Xian, Muhammad Ferjad Naeem, Yue Fan, Haiyang Wang | TokenFormer introduces a fully attention-based architecture for scaling transformer models.  The research aims to address the high computational cost of scaling transformers, which traditionally requires retraining from scratch when architectural changes are made.  The core methodology replaces linear projections in transformers with a token-parameter attention layer, treating model parameters as tokens that interact with input tokens via attention.  Scaling TokenFormer from 124M to 1.4B parameters incrementally achieves a perplexity of 11.77, comparable to a transformer trained from scratch at 1.4B parameters but at significantly reduced training cost.  This allows AI practitioners to scale transformer models more efficiently by reusing pre-trained models and avoiding computationally expensive retraining from scratch.  |
