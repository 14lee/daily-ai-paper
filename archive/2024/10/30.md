

## Papers for 2024-10-30

| Title | Authors | Summary |
|-------|---------|---------|
| CLEAR: Character Unlearning in Textual and Visual Modalities (Read more on [arXiv](https://arxiv.org/abs/2410.18057) or [HuggingFace](https://huggingface.co/papers/2410.18057))| Denis Bobkov, Boris Mikheev, Alexey Zhavoronkin, Dmitrii Korzh, therem | This research aims to evaluate machine unlearning (MU) techniques in multimodal large language models (MLLMs).  The authors introduce CLEAR, a synthetic dataset of fictitious individuals with associated images and text, and evaluate 10 adapted MU methods across textual, visual, and multimodal setups using metrics like ROUGE-L, probability score, truth ratio, and forget quality.  In multimodal unlearning on the CLEAR dataset using the LLaVa model, the SCRUB method maintained a retain metric of approximately 0.48 while achieving a forget metric of 0.36.  This suggests that current state-of-the-art unlearning algorithms struggle with multimodal setups, demonstrating the need for new approaches specifically designed for MLLMs. The paper also indicates that  L1 regularization on LoRA adapter weights can mitigate catastrophic forgetting.  Follow-up questions:  1.  How does the performance of the evaluated MU methods on the synthetic CLEAR dataset compare to performance on real-world multimodal datasets, and what modifications might be necessary for practical application? 2.  What is the computational cost of applying L1 regularization on LoRA weights during unlearning, and how does this impact the feasibility of applying this technique to larger MLLMs? 3.  Given the observed challenges in multimodal unlearning, what specific research directions might be most promising for developing more effective MMU algorithms, such as exploring alternative regularization techniques or novel architectural modifications?  |
| AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions (Read more on [arXiv](https://arxiv.org/abs/2410.20424) or [HuggingFace](https://huggingface.co/papers/2410.20424))| Qianbo Zang, Ziming Li, zhangysk, Liam-Liu, aaabiao | This paper aims to develop AutoKaggle, a framework for autonomously completing Kaggle data science competitions using tabular data.  The framework utilizes a phase-based workflow with five specialized agents (Reader, Planner, Developer, Reviewer, and Summarizer) combined with iterative debugging, unit testing, and a machine learning tools library.  In evaluations across eight Kaggle competitions, AutoKaggle achieved a valid submission rate of 0.83 using the GPT-40 model. This indicates the potential for multi-agent systems to automate complex data science workflows, achieving near-human-level performance. The paper does not explicitly state the performance metrics of the individual agents, which makes it difficult to assess their respective contributions.   Follow-up questions:  1.  Could the authors elaborate on the specific roles and interactions of each agent within the multi-agent system, and provide quantitative measures of their individual performance or contribution to the overall system performance? 2. How does the performance of AutoKaggle vary across different types of Kaggle competitions (e.g., classification vs. regression, different dataset sizes)? Are there certain competition characteristics where it performs particularly well or poorly, and why? 3.  What are the limitations of the current machine learning tools library, and what future extensions or improvements are planned to enhance its capabilities and address the observed debugging challenges related to feature engineering tools?  |
| SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization (Read more on [arXiv](https://arxiv.org/abs/2410.21411) or [HuggingFace](https://huggingface.co/papers/2410.21411))| Chuang Gan, Donglai Wei, Jiawei Zhou, zmeng0116, EthanTaylor | a) **Objective:** To develop a zero-shot social relation recognition framework that addresses the limitations of existing end-to-end models in terms of generalizability and interpretability.  b) **Methodology:** SocialGPT, a modular framework, utilizes Vision Foundation Models (VFMs) to convert images into textual social stories and Large Language Models (LLMs) with a structured prompt (SocialPrompt) for text-based reasoning.  Greedy Segment Prompt Optimization (GSPO) automatically tunes the SocialPrompt using gradient information at the segment level.  c) **Results:** SocialGPT with Vicuna-13B and GSPO achieved 69.23% accuracy on the PIPA dataset, exceeding the prior state-of-the-art TRGAT by 1.4%.  d) **Implication:**  AI practitioners can leverage SocialGPT as a strong zero-shot baseline for social relation recognition, utilizing the power of pre-trained VFMs and LLMs while benefiting from GSPO for automatic prompt optimization and enhanced performance. The paper mentions additional benefits of interpretability of results and generalization to novel image styles but does not provide supporting quantitative details.  **Follow-up Questions:**  1. How does the performance of GSPO compare to other prompt optimization methods on social relation recognition tasks, particularly those not relying on segment-level optimization?  2.  What are the computational costs and time complexities of GSPO, particularly concerning the number of segments and candidate prompts?  3. The paper claims generalization to novel image styles. What is the quantifiable performance on these styles (e.g. sketch, cartoon) compared to existing models and in what domains or use cases are these improvements most significant?  |
| OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization (Read more on [arXiv](https://arxiv.org/abs/2410.19609) or [HuggingFace](https://huggingface.co/papers/2410.19609))| Hongming Zhang, Wenhao Yu, Kaixin Ma, Wenlin Yao, Hongliang He | This research aims to develop an open-source, multimodal web agent capable of improving its performance through iterative real-world exploration and feedback.  The methodology involves imitation learning from a GPT-40-based agent, followed by cycles of self-exploration, GPT-40 feedback, and optimization using the Idefics2-8b-instruct LMM.  On the WebVoyager test set, the agent's task success rate increased from 19.9% after imitation learning to 25.8% after three optimization cycles. This suggests that iterative optimization with real-world feedback can improve open-source, multimodal web agent performance. The paper does not detail the computation resources or time required for training or optimization.   Follow-up Questions:  1. What were the specific hyperparameter settings used for fine-tuning Idefics2-8b-instruct during both the imitation learning and iterative optimization phases?  2.  How does the performance of OpenWebVoyager compare to closed-source multimodal models like GPT-4V on more complex web navigation tasks not included in the evaluated datasets? 3. What is the breakdown of successes and failures attributed to visual understanding versus textual understanding limitations within the agent?  |
| Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning (Read more on [arXiv](https://arxiv.org/abs/2410.22304) or [HuggingFace](https://huggingface.co/papers/2410.22304))| Paul Mineiro, ydeng9 | a) This research aims to improve the quality of reasoning traces generated by Large Language Models (LLMs) for mathematical problem-solving.  b) The proposed method uses an online learning Flow comprising multiple LLMs that collaboratively construct solutions, trained via Direct Preference Optimization (DPO) with rollouts.  c) Using flow-generated reasoning traces for Supervised Fine-Tuning (SFT) led to an accuracy of 71.3% on GSM8K and 27.8% on MATH for Llama-3-8B-Instruct, outperforming SFT with self-generated and ground-truth traces.  d) AI practitioners can use online-learned multi-agent Flows to generate superior reasoning traces for LLM fine-tuning, leading to improved performance in complex reasoning tasks. The paper highlights the impact of flow-generated reasoning traces for improving single-model SFT performance in math problem-solving, offering a new approach to enhance LLM reasoning capabilities.  Follow-up questions:  1. What are the computational resource requirements (e.g., GPU hours, memory) for training the flow and performing SFT with the proposed method compared to baseline methods? 2.  How does the chunk size parameter affect the performance and training efficiency of the Flow, and what strategies can be used for optimizing this parameter? 3. Could this approach be generalized to other reasoning tasks beyond mathematics, such as commonsense reasoning or logical deduction?  |
| ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference (Read more on [arXiv](https://arxiv.org/abs/2410.21465) or [HuggingFace](https://huggingface.co/papers/2410.21465))| Ningxin Zheng, Size Zheng, Wenlei Bao, Li-Wen Chang, preminstrel | a) The research aimed to improve the throughput of long-context large language model (LLM) inference, which is hampered by the growing memory footprint and access needs of the key-value (KV) cache.  b) SHADOWKV, a proposed system, stores a low-rank representation of the pre-Rotary Position Embedding (pre-RoPE) key cache on the GPU, offloads the value cache to the CPU, and employs a chunk-based approximation method with outlier detection for sparse attention during decoding.  c)  On an A100 GPU, SHADOWKV achieved up to a 3.04× throughput increase for Llama-3.1-8B with a batch size of 122K context length samples, surpassing the theoretical throughput of an infinite batch size under the assumption of infinite GPU memory.  d) AI practitioners can leverage SHADOWKV to significantly improve the serving efficiency of long-context LLMs without substantial accuracy degradation by reducing the KV cache's memory footprint and optimizing sparse attention mechanisms.   Follow-up questions:  1. What are the practical considerations and potential trade-offs involved in implementing the low-rank approximation and value offloading strategy for different hardware configurations (e.g., systems with limited CPU memory or varying PCIe bandwidth)?  2.  How does SHADOWKV's chunk-based KV selection method compare to other sparse attention techniques in terms of computational complexity and robustness to different LLM architectures and tasks? 3.  Is the code publicly available, and what level of technical expertise is required to integrate SHADOWKV into existing LLM serving pipelines?  |
| Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset (Read more on [arXiv](https://arxiv.org/abs/2410.22325) or [HuggingFace](https://huggingface.co/papers/2410.22325))| Yongyuan Liang, Huanyu Li, Tao Huang, Yifei Sun, Guangqi Jiang | This research investigates whether manipulation-centric visual representations improve robot learning.  The authors propose Manipulation Centric Representation (MCR), which pre-trains a visual encoder on the DROID robotic dataset and incorporates dynamics information (robot actions and proprioceptive states) via a novel contrastive loss, an action prediction loss, and a time contrastive loss.  Across four simulated robotic manipulation domains, MCR outperforms the strongest baseline by 14.8% in terms of average success rate. The most impactful finding is the strong correlation between “manipulation centricity,” the representation’s ability to focus on manipulation-relevant regions, and downstream task performance. This implies that AI practitioners can improve robot learning efficiency by designing representations that prioritize manipulation-relevant information.  Follow-up questions:  1. How does the choice of pre-trained backbone architecture (ResNet vs. ViT) influence the effectiveness of MCR and its manipulation centricity? 2.  Could MCR be adapted for other robotic tasks beyond manipulation, such as navigation or grasping, and if so, how might the pre-training objectives need to be modified? 3. What are the limitations of using Grad-CAM to measure manipulation centricity, and are there alternative, potentially more robust methods for evaluating this characteristic?  |
| Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2410.21845) or [HuggingFace](https://huggingface.co/papers/2410.21845))| Sergey Levine, Jeffrey Wu, charlesxu0124, jianlanluo | a) This research aims to develop a reinforcement learning (RL) system for vision-based robotic manipulation capable of acquiring diverse dexterous skills in real-world settings.  b) The system, HIL-SERL, uses a sample-efficient off-policy RL algorithm (RLPD) with a pretrained visual backbone, incorporates human demonstrations and corrections, and employs a sparse reward function based on a trained binary classifier.  c) HIL-SERL achieves a 100% success rate on nearly all evaluated tasks within 1 to 2.5 hours of real-world training, representing an average 101% improvement in success rate and 1.8x faster cycle time compared to imitation learning baselines trained with an equivalent amount of human data.  d) The results indicate that carefully designed RL systems can enable real-world acquisition of complex vision-based manipulation policies within practical training times, exceeding imitation learning and potentially unlocking wider application of robots in complex manipulation tasks.  The most impactful finding is the high success rate achieved in short training times, highlighting the potential of RL for real-world robotics applications previously considered infeasible.   Follow-up questions:  1.  How does the system's performance vary with different pretrained visual backbones, and are there ways to optimize backbone selection for specific manipulation tasks?  2.  What are the limitations of the current human correction interface (SpaceMouse), and how could more intuitive and efficient interfaces enhance performance and broaden the range of correctible errors?  3.  While the paper mentions the lack of extensive randomization and tests in unstructured environments, how could these be incorporated into future research to validate the generalizability and deployability of HIL-SERL in real-world scenarios?  |
