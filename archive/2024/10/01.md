

## Papers for 2024-10-01

| Title | Authors | Summary |
|-------|---------|---------|
| MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning (Read more on [arXiv](https://arxiv.org/abs/2409.20566) or [HuggingFace](https://huggingface.co/papers/2409.20566))| nm-w, pdufter, zhegan27, fly6464, haotiz | a) This research aimed to improve multimodal large language model (MLLM) performance in text-rich image understanding, visual referring and grounding, and multi-image reasoning after pre-training.  b) The researchers adopted a data-centric approach, focusing on continual pre-training with high-resolution OCR data, an optimized visual instruction-tuning data mixture for supervised fine-tuning (SFT), and dynamic image splitting for high-resolution image comprehension.   c) MM1.5-30B significantly improved performance over its predecessor MM1-30B on tasks such as MathVista (increasing the score from 39.4 to 55.6), DocVQA (from 75.8 to 91.4), and InfoVQA (from 47.3 to 67.3). d)  The paper demonstrates the importance of careful data curation and training strategies for improving MLLM performance, even at smaller scales, providing valuable guidance for practitioners developing and fine-tuning MLLMs.  The impact of text-only pre-training data on MLLM performance, and how the proportion of such data in pre-training affects the efficiency of transfer learning to SFT is an impactful finding, suggesting that optimization of pre-training data is crucial for effective SFT.  Follow-up Questions:  1. The paper mentions the use of in-house synthetic caption data that outperformed public datasets in some settings. Could the authors elaborate on the specific methodology used for generating these in-house captions, including the models, data sources, and any filtering or quality control mechanisms employed? 2. Given the findings on the impact of image resolution in continual pre-training, are there recommendations for optimal resolution ranges for different MLLM scales, considering the trade-off between performance and computational cost? 3. What specific techniques were used for optimizing the "optimized visual instruction-tuning data mixture" mentioned for SFT, and how was the final mixture composition determined?  More specifically, how do you decide when the model is overfitting to the data?  |
| DiaSynth -- Synthetic Dialogue Generation Framework (Read more on [arXiv](https://arxiv.org/abs/2409.19020) or [HuggingFace](https://huggingface.co/papers/2409.19020))| Eng Siong Chng, Tushar Pranav, AlexWuuuu, SkAndMl | a) The paper addresses the scarcity of high-quality, large-scale, domain-specific dialogue datasets for training dialogue systems.  b) DiaSynth, a synthetic dialogue generation framework, uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to generate dialogues based on user-provided topics, dynamically generated subtopics and personas, and specified conversational characteristics.  c)  Fine-tuning pretrained language models on synthetic data generated by DiaSynth resulted in a performance improvement of 16.47% compared to base models on a dialogue summarization task using LLaMA-3 as the LLM backbone.  d)  DiaSynth offers AI practitioners a scalable and cost-effective method for generating synthetic dialogue data for training dialogue systems, especially in domains with limited existing data. The results indicate that synthetic data from moderate-sized open-source LLMs can be a viable alternative to scarce or costly real-world data.   Follow-up questions:  1.  The paper mentions differing performance across LLMs (LLaMA-3, GPT-4) based on dialogue structure (formal vs. informal).  Could further analysis elucidate the specific factors within these structures that influence LLM performance and inform optimal LLM selection for specific application domains?  2. While the paper demonstrates effectiveness in summarization, how does DiaSynth-generated data perform in other downstream tasks relevant to dialogue systems, such as intent detection, slot filling, or sentiment analysis?  3. What are the computational resource requirements and associated costs of using DiaSynth to generate large synthetic datasets, particularly when employing larger LLMs or generating data for diverse domains?  |
| Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2409.18943) or [HuggingFace](https://huggingface.co/papers/2409.18943))| yuelin bai, Ziqiang Liu, Yunshui Li, Lei Zhang, Jiaming Li | a) The research investigated the ability of Large Language Models (LLMs) to generate responses of specified lengths, introducing the Target Length Generation Task (TLG).  b)  A model-agnostic method named RULER, utilizing Meta Length Tokens (MLTs), was proposed and tested on several LLMs. RULER adds an MLT, indicating the desired length, to the input and trains LLMs end-to-end on a dataset augmented with MLTs.  c) RULER improved the Flexible Match (FM) score, a measure of adherence to the target length range, by an average of 29.57 across all tested models and length levels.  d) AI practitioners can use RULER to improve the control over output length in LLMs, enhancing their ability to adhere to specific length constraints in diverse applications.  The paper does not address potential effects of RULER on other LLM performance metrics beyond those related to length control, nor its computational efficiency.  Follow-up questions:  1. How does the performance of RULER vary with different training dataset sizes and compositions, particularly with respect to the distribution of target lengths?  2. What is the computational overhead of incorporating RULER, both during training and inference, compared to standard LLM usage?  3. Does RULER impact other performance metrics of the LLMs, such as factual accuracy, reasoning ability, or toxicity of generated text?  |
| Hyper-Connections (Read more on [arXiv](https://arxiv.org/abs/2409.19606) or [HuggingFace](https://huggingface.co/papers/2409.19606))| banggu, YunyaoMao, Taoer, hongzhihuang, mathfinder | a) This research explores hyper-connections as a learnable alternative to residual connections in neural networks, aiming to address limitations like the seesaw effect between gradient vanishing and representation collapse.  b) Hyper-connections introduce learnable depth and width connections within layers, allowing the network to adjust connection strength and dynamically rearrange layers; a dynamic variant (DHC) conditions these connections on the input.  c)  In large language model pre-training, a model with DHC and an expansion rate of 4 (OLMOE-1B-7B-DHCÃ—4) converged 1.8 times faster and showed a 6-point improvement on ARC-Challenge accuracy compared to a residual connection baseline after training on 500 billion tokens.  d) AI practitioners can utilize hyper-connections as a potential drop-in replacement for residual connections, offering potential performance gains and faster convergence, particularly in large language models.  The paper also suggests potential applicability in computer vision tasks, but the provided results are limited.  Follow-up questions:  1. What is the computational overhead of hyper-connections compared to standard residual connections during both training and inference, especially for very deep networks? 2.  How robust are the performance improvements of hyper-connections across different model architectures, datasets, and hyperparameter settings beyond those tested in the paper, particularly in vision tasks where less experimentation is presented? 3. The paper mentions that hyper-connections can learn to rearrange layers.  Can further details be provided on how this rearrangement is analyzed and its specific impact on model behavior?  |
| UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2409.20551) or [HuggingFace](https://huggingface.co/papers/2409.20551))| Ce Hao, Zhengkai Jiang, Xibin Yuan, Qiaojun Yu, SiyuanH | This research aims to improve robotic manipulation by creating a unified representation of affordances for both tools and articulated objects. The researchers developed UniAff, a multimodal large language model (MLLM) fine-tuned on a synthetic dataset of 1500 objects with labeled part-level 6D poses, manipulation types, and affordances.  UniAff achieved a 56.9% improvement in IOU for detecting functional affordances of tools compared to ManipVQA. This work provides a new model and dataset for object-centric robotic manipulation, potentially improving the generalization of robotic manipulation tasks. It is unclear how the synthetic dataset generation generalizes to the real world or the computational cost of UniAff.  Follow-up questions:  1. What are the specific architectural details of the Mixed Visual Encoder used in UniAff, and how were the different visual encoders (CLIP, DINOv2, Q-Former) combined?  2.  What is the breakdown of the 19 articulated object categories and 12 tool categories in the synthetic dataset, and what are the specific real-world datasets used to create the synthetic data? 3. How does UniAff perform in real-world settings on a broader range of tasks and objects not represented in the current experimental setup?  |
| Cottention: Linear Transformers With Cosine Attention (Read more on [arXiv](https://arxiv.org/abs/2409.18747) or [HuggingFace](https://huggingface.co/papers/2409.18747))| Eric C. Larson, TrevorDohm, gmongaras | a) This paper introduces Cottention, a novel attention mechanism designed to address the quadratic memory complexity of softmax attention in transformers.  b) Cottention replaces the softmax operation with cosine similarity and rearranges the attention equation to achieve linear memory complexity with respect to sequence length.  A custom CUDA kernel was developed for efficient computation, and a learned scalar parameter was introduced to stabilize training.  c)  On the GLUE benchmark, a BERT model using Cottention achieved an average score of 81.8, compared to 83.1 for the softmax baseline.  d) Cottention offers AI practitioners a more memory-efficient alternative to softmax attention, enabling the processing of longer sequences without significant performance degradation, as demonstrated by comparable results on the GLUE benchmark and perplexity on GPT-J language modelling tasks.  The paper notes theoretical linear memory complexity with respect to sequence length but acknowledges a discrepancy between theoretical and observed memory usage related to input dimensionality, warranting further investigation.  Follow-up Questions:  1.  The paper mentions a discrepancy between the theoretical and empirical memory usage with respect to input dimensionality. What further investigations could be conducted to explain this discrepancy and potentially optimize memory usage further?  2.  The custom CUDA kernel for Cottention is mentioned but not detailed extensively. What specific optimization strategies were employed in the kernel design, and how do they contribute to the efficiency gains observed?  3. How does the training time and computational cost of Cottention compare to Softmax and other linear attention methods, considering both the forward and backward passes, particularly for very long sequences?  |
| Image Copy Detection for Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2409.19952) or [HuggingFace](https://huggingface.co/papers/2409.19952))| Yi Yang, Zhentao Tan, Yifan Sun, WenhaoWang | a) The paper investigates how to detect content replication generated by diffusion models, introducing the task of Image Copy Detection for Diffusion Models (ICDiff).  b)  A new dataset, Diffusion-Replication (D-Rep), containing 40,000 image-replica pairs with six annotated replication levels, was created using Stable Diffusion V1.5 and LAION-Aesthetics V2 images. A novel method, PDF-Embedding, which converts replication levels to probability density functions and uses a set of learned vectors for each image, was proposed.  c) PDF-Embedding outperformed protocol-driven methods and non-PDF methods on the D-Rep test set, achieving 56.3% in Pearson Correlation Coefficient (PCC) and 25.6% in Relative Deviation (RD) using an exponential PDF.  d)  AI practitioners developing diffusion models should consider integrating ICDiff methods like PDF-Embedding to assess and mitigate potential copyright infringement or unwanted replication of training data in generated images. The replication ratios of several well-known diffusion models against a large-scale gallery were found to range from 10% to 20%, indicating a significant practical need for such detection.   Follow-up questions:  1. How does the computational cost and performance of PDF-Embedding scale with larger image databases and with more recent, higher-resolution diffusion models beyond Stable Diffusion V1.5?  2. Could the PDF-Embedding method be adapted or improved for detecting partial image replication, as opposed to full-image replication, within diffusion model outputs?  3. How robust is PDF-Embedding to adversarial attacks designed to evade copy detection in generated images?  |
| Can Models Learn Skill Composition from Examples? (Read more on [arXiv](https://arxiv.org/abs/2409.19808) or [HuggingFace](https://huggingface.co/papers/2409.19808))| Sanjeev Arora, Anirudh Goyal, Simran Kaur, Haoyu Zhao, dingliyu | This research investigates whether fine-tuning can improve compositional generalization in LLMs, specifically their ability to combine language skills in novel ways.  The study fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 on a dataset generated by GPT-4, consisting of text samples exhibiting combinations of 1, 2, or 3 language skills.  Results showed that fine-tuning on these examples improved the models' ability to compose up to 5 held-out skills, with LLaMA-2-13B-Chat's success rate for composing 3 held-out skills increasing from 4% to 37%. This suggests that models can learn a "meta-skill" of composition, generalizing beyond specific skill combinations seen during training.  AI practitioners can leverage this finding by incorporating skill-rich (potentially synthetic) text data into training to improve the compositional capabilities of LLMs.   Follow-up Questions:  1. What is the impact of varying the size and diversity of the training dataset (beyond the current 13,957 samples) on the compositional generalization performance?  2.  How does this fine-tuning approach compare to other methods for improving compositional generalization, such as curriculum learning or specific architectural modifications?  3.  Beyond the SKILL-MIX evaluation, how can this improved compositional ability be effectively applied to more complex, real-world NLP tasks, and what are the potential limitations in such applications?  |
| Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code (Read more on [arXiv](https://arxiv.org/abs/2409.19715) or [HuggingFace](https://huggingface.co/papers/2409.19715))| Dongjin Kang, Yongho Song, Seungjun Moon, Taeyoon Kwon, Hyungjoo Chae | a) The research aims to improve open-source natural language feedback models for code editing by creating a reinforcement learning environment that better aligns feedback with code improvement. b) The authors developed COFFEE-GYM, comprising the COFFEE dataset of human code edits with pairwise feedback annotations and COFFEEEVAL, a unit-test-driven reward function, used with PPO and DPO reinforcement learning algorithms. c) Feedback models trained with COFFEE-GYM achieved a 13.4% improvement in Pass@1 accuracy on both HumanEvalFix and COFFEE-TEST compared to a baseline DeepSeekCoder-7B model without feedback. d) AI practitioners can utilize COFFEE-GYM and COFFEEEVAL to train open-source feedback models that generate helpful feedback for code editing, achieving performance comparable to closed-source models like GPT-4.  The paper highlights the importance of pairwise feedback data and robust reward models in training effective feedback systems.  Follow-up questions:  1.  The paper mentions limitations regarding the scope of editing being focused on correctness, not efficiency or readability.  How could COFFEE-GYM be extended to incorporate these additional aspects of code quality into the feedback and reward models? 2.  How robust is COFFEEEVAL to the specific choice of code editor model used? Could using a weaker or stronger editor significantly impact the learned feedback model?  Are there experiments or analyses planned to address this potential dependency? 3.  While the paper demonstrates improved performance on specific benchmarks, how well does this generalize to real-world code editing scenarios in diverse programming languages and codebases beyond competitive programming and the provided test sets?  |
| IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding (Read more on [arXiv](https://arxiv.org/abs/2409.19627) or [HuggingFace](https://huggingface.co/papers/2409.19627))| Jianzong Wang, Jing Xiao, zhangxulong, Pechola | a) This paper aims to develop a robust neural audio watermarking model with efficient localization capabilities, addressing the limitations of existing methods regarding capacity, imperceptibility, and locating efficiency. b) The authors propose IDEAW, which employs a dual-stage invertible neural network (INN) to separately embed a locating code and a watermark message into the audio, along with a balance block to mitigate the asymmetry introduced by the attack layer during robustness training. c) IDEAW achieves higher capacity and comparable robustness under various attacks compared to baseline methods, demonstrating a signal-to-noise ratio (SNR) of 35.41 dB and accuracy of 99.44% when embedding a 56-bit payload (46-bit message + 10-bit locating code).  The proposed dual-embedding strategy reduces localization time overhead by approximately 40-50% compared to existing methods. d)  AI practitioners working on audio security and copyright protection can utilize IDEAW for robust and efficient watermark embedding and extraction, improving localization speed significantly compared to traditional approaches.  Follow-up questions:  1.  How does the performance of IDEAW vary across different audio genres and lengths, beyond the speech and music datasets used in the evaluation? 2. What is the computational complexity of IDEAW's embedding and extraction processes, and how does it scale with increasing audio length or watermark payload size? 3. Could the dual-embedding strategy be extended to other watermarking domains, such as image or video, using similar invertible network architectures?  |
