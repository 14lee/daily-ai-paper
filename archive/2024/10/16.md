

## Papers for 2024-10-16

| Title | Authors | Summary |
|-------|---------|---------|
| MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation (Read more on [arXiv](https://arxiv.org/abs/2410.11779) or [HuggingFace](https://huggingface.co/papers/2410.11779))| Haoming Xu, Bozhong Tian, Xiang Chen, Chenxi Wang, Ningyu | a) This research investigates the mechanism of hallucinations in Multimodal Large Language Models (MLLMs) and proposes a mitigation method. b) The authors analyze MLLM behavior through object probing, probability analysis across transformer layers, and early exit experiments, then introduce Dynamic Correction Decoding with preCeding-Layer Knowledge (DeCo). DeCo dynamically selects preceding layers with higher ground truth token confidence and integrates their knowledge into the final layer output logits.  c) DeCo reduces hallucination rates on the CHAIR benchmark by an average of 10.8% compared to baselines across various MLLMs and decoding strategies. d)  AI practitioners can use DeCo as a training-free decoding method to mitigate hallucinations in MLLMs during inference, potentially improving the reliability of generated content in image captioning and VQA tasks.  This is particularly relevant for applications where factual accuracy is critical.   Follow-up questions:  1. How does DeCo's performance compare to existing training-based hallucination mitigation methods in terms of both accuracy and computational cost? 2.  Can DeCo be effectively combined with other decoding strategies or post-processing methods for further hallucination reduction? 3. What are the limitations of DeCo in handling other types of hallucinations beyond object hallucinations, such as incorrect attribute assignment or relationship descriptions?  |
| MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.11710) or [HuggingFace](https://huggingface.co/papers/2410.11710))| Xiaoshuai Song, Jiaheng Liu, Zekun Wang, Yanan Wu, Pei Wang | a) This research aimed to create a benchmark for evaluating Large Language Model (LLM) performance on diverse real-world tool-use tasks.   b) The authors developed MTU-Bench, consisting of MTU-Instruct (a training dataset derived from existing dialogue datasets and synthesized tool calls) and MTU-Eval (an automatic evaluation framework with fine-grained metrics).   c)  Their fine-tuned model, MTU-LLaMA, achieved a tool selection accuracy of 92.31% on single-turn, single-tool tasks in the normal test set.   d) AI practitioners can use MTU-Bench to more comprehensively evaluate and improve the tool-use capabilities of LLMs, particularly in complex multi-turn and multi-tool scenarios. The demonstrated superior performance of MTU-LLaMA across multiple settings indicates its potential for more robust tool integration in real-world applications.   Follow-up questions:  1. How does the performance of MTU-LLaMA compare to other state-of-the-art tool-learning models on benchmarks beyond MTU-Bench? 2. What specific types of errors are most prevalent in the hard test set, and how can these insights guide future model development to improve robustness? 3.  Could the automated data synthesis pipeline be adapted for other types of tasks beyond tool use, such as code generation or reasoning?  |
| LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.09342) or [HuggingFace](https://huggingface.co/papers/2410.09342))| Yu Chao, Xinyi Chen, Chong Li, Zihan Zhou, shuo-hf | a) The research aims to improve long-text processing in Large Language Models (LLMs) by mitigating the loss of long-range information when using divide-and-conquer strategies.  b) The proposed LLM×MapReduce framework employs a three-stage process (map, collapse, reduce) augmented by a structured information protocol and in-context confidence calibration.  c) On the InfiniteBench benchmark, LLM×MapReduce achieved an average score of 68.66%, outperforming closed-source models like GPT-4 (57.34%) and other open-source models.  d)  AI practitioners can utilize this training-free method to extend the effective context window of LLMs, enhancing performance on tasks requiring the comprehension of long sequences without needing extensive computational resources or retraining.  The significant performance improvement over existing methods makes LLM×MapReduce a viable solution for long-text applications.  Follow-up questions:  1. What are the specific prompt engineering techniques used in each stage (map, collapse, reduce) of LLM×MapReduce, and how can these be adapted for different downstream tasks?  2.  How does the computational cost of LLM×MapReduce, including the multiple inference calls, compare to the cost of training LLMs with extended context windows using methods like LongLoRA or adjusting RoPE frequencies?  What are the tradeoffs?  |
| SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI (Read more on [arXiv](https://arxiv.org/abs/2410.11096) or [HuggingFace](https://huggingface.co/papers/2410.11096))| Wenbo Guo, Yuheng Tang, Zhun Wang, Yuzhou Nie, yuyangy | a) The research aims to develop a comprehensive platform for evaluating the security risks of code generation AI models in both insecure code generation and facilitation of cyberattacks.  b) SECCODEPLT utilizes a two-stage data creation pipeline involving expert-crafted seed examples and automated mutation for insecure code evaluation, alongside a real-world attack environment with dynamic metrics for cyberattack helpfulness assessment.  They compared their benchmark with CYBERSECEVAL using LLM-based judgement on prompt security relevance and faithfulness.  c) SECCODEPLT achieved near 100% in both security relevance and prompt faithfulness, while CYBERSECEVAL scored 67.81% and 42% respectively. When testing against SOTA models, GPT-4 performed best in secure coding, with a 52% secure code rate on instruction generation without security policies, though still demonstrating a need for improvement.  d) AI practitioners developing or deploying code generation models should leverage SECCODEPLT for more robust security risk assessments and prioritize safety alignment strategies to mitigate the risks of generating insecure code and facilitating cyberattacks.  It is unclear whether human verification was used on the automatically generated data used in the large-scale data generation process.  Follow-up questions:  1. How does the performance of the rule-based detection compare to the dynamic detection methods in identifying insecure code generated by the models on SECCODEPLT?  Does the paper report on the false positive/negative rates?  2.  What are the specific details of the attack environment construction, and how scalable is it for evaluating different types of attacks beyond the ones presented in the paper?  3. What specific mitigation strategies, beyond general safety alignment, can be derived from the SECCODEPLT findings for improving the security of code generation models?  |
| LVD-2M: A Long-take Video Dataset with Temporally Dense Captions (Read more on [arXiv](https://arxiv.org/abs/2410.10816) or [HuggingFace](https://huggingface.co/papers/2410.10816))| Zhijie Lin, Daquan Zhou, Yuqing Wang, XihuiLiu, YuuTennYi | a) The research aimed to create a high-quality dataset of long videos with dense captions to facilitate the training of long-form video generation models.  b) The authors developed a pipeline involving automated video filtering (using scene cut detection, optical flow, and multi-modal large language models) and a hierarchical captioning approach (using image grids and large language models).  c) The resulting LVD-2M dataset contains 2 million long-take videos (over 10 seconds each) with temporally dense captions, achieving a long-take video ratio of 86.8% based on human evaluation.  d) AI practitioners working on video generation can utilize LVD-2M to fine-tune models for generating longer, more dynamic, and semantically consistent videos, potentially improving metrics like dynamic degree and object class recognition as measured by VBench. The paper notes limitations in dataset size and potential for misuse of generated videos, which practitioners should consider.  Follow-up questions:  1. What specific technical details were used in the hierarchical captioning pipeline with LLaVA and Claude3-Haiku, including prompt engineering and parameter settings?  How were inconsistencies or hallucinations in the generated captions addressed?  2.  While the paper mentions fine-tuning on a 7B LM-based video generation model and a 1.8B parameter diffusion-based I2V model, what are the computational requirements for fine-tuning these models on LVD-2M, and how can these resources be optimized for practical use by AI practitioners?  3.  How can the filtering process be further refined to eliminate subtle jump cuts, which were identified as a major remaining challenge, potentially utilizing more advanced scene change detection algorithms or incorporating visual coherence metrics?  |
| What Matters in Transformers? Not All Attention is Needed (Read more on [arXiv](https://arxiv.org/abs/2406.15786) or [HuggingFace](https://huggingface.co/papers/2406.15786))| Zheyu Shen, Guoheng Sun, Shwai He, charleslipku | a) This paper investigates the redundancy of different modules (Blocks, MLP layers, Attention layers) within Transformer-based large language models (LLMs).  b) The authors use a similarity-based metric to assess module redundancy and propose techniques like "Attention Drop" and "Joint Layer Drop" to prune redundant layers.  c)  Dropping 50% of the Attention layers in Llama-2-70B resulted in a 48.4% speedup with only a 2.4% performance drop.  d)  AI practitioners can significantly improve the efficiency of LLMs, particularly regarding inference speed and memory usage (KV-cache), by strategically pruning redundant Attention layers, often without substantial performance degradation.   Follow-up Questions:  1. How does the proposed "Joint Layer Drop" method compare with other structured pruning techniques, such as filter pruning or layer-wise magnitude pruning, in terms of performance-efficiency trade-off on different LLM architectures and sizes?  2.  Could the "Attention Drop" method be adapted for efficient training of large language models, given that the paper demonstrates consistent redundancy in attention layers throughout the training process?  3. What are the potential implications of this work for hardware design, particularly considering the reduction in KV-cache memory usage achieved by pruning attention layers?  |
| Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts (Read more on [arXiv](https://arxiv.org/abs/2410.10626) or [HuggingFace](https://huggingface.co/papers/2410.10626))| Yuping Zheng, Nuo Chen, Juhao Liang, Xidong Wang, Guorui Zheng | a) This research aims to develop a multilingual medical Large Language Model (LLM) accessible in numerous languages, addressing data scarcity challenges, particularly for low-resource languages.  b) The researchers construct a multilingual medical dataset, analyze LLM information flow using a circuits-based routing analysis within a Mixture of Experts (MoE) framework, and introduce the concept of "language family experts" to scale the model to 50 languages efficiently.  c) The 2B parameter Apollo-MoE model achieved 54.8% accuracy on a 12-language medical benchmark and 44.9% accuracy on a 38 low-resource language benchmark.  d) AI practitioners can leverage the "language family experts" approach within a Post-MoE architecture to scale multilingual LLMs efficiently without proportionally increasing parameters, facilitating the development of language-inclusive medical AI applications.  The most impactful finding is the “Spread Out in the End” phenomenon observed in the information flow circuits, which directly led to the development of Post-MoE architecture applying MoE only in later layers and improving low-resource language performance without additional training.  Follow-up questions:  1. How does the performance of Apollo-MoE compare to existing state-of-the-art multilingual LLMs in zero-shot or few-shot settings across different medical tasks beyond the presented benchmarks?  2. What specific linguistic features are used to define the language families, and how was the effectiveness of this grouping validated for the MoE routing?  3. What are the computational resource requirements (e.g., GPU memory, training time) for different Apollo-MoE model sizes, and how do they scale with the number of languages?  |
| GS^3: Efficient Relighting with Triple Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2410.11419) or [HuggingFace](https://huggingface.co/papers/2410.11419))| Xiang Feng, Fan Pei, Yixin Zeng, Zoubin Bi, NCJ | a) This research aims to develop a real-time, high-quality novel lighting-and-view synthesis method from multi-view point-lit images.  b) The approach utilizes a spatial and angular Gaussian-based representation with a triple splatting process: angular Gaussian splatting for appearance, shadow splatting for self-shadowing, and Gaussian splatting for combining these with residual effects predicted by an MLP.  The representation is optimized end-to-end by minimizing the difference between rendered and input photographs.  c) The method achieves a rendering speed of over 90 frames per second on a single commodity GPU and a training time of 40-70 minutes.   d) AI practitioners can leverage this approach for efficient and high-quality relighting of complex objects and scenes, potentially impacting applications like virtual reality, augmented reality, and visual effects.  The paper demonstrates successful reconstruction of a wide range of challenging appearance characteristics like anisotropic reflectance.    Follow-up questions:  1.  The paper mentions the possibility of using separate sets of angular Gaussians for each spatial Gaussian if sufficient input data is available.  Could more details be provided on the trade-off between quality and computational cost when using this approach? How much improvement in quality is observed in practice? 2. What specific hardware configuration constitutes the "single commodity GPU" referenced for the 90fps rendering speed?  How does performance scale with the number of spatial and angular Gaussians? 3.  What are the limitations of the current shadow splatting method, and what alternative approaches could be explored to improve shadow quality in cases where it is not as crisp as desired?  |
| Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free (Read more on [arXiv](https://arxiv.org/abs/2410.10814) or [HuggingFace](https://huggingface.co/papers/2410.10814))| Ziyue Li, zhoutianyi | a) This research investigates whether the routing weights (RW) in Mixture-of-Experts (MoE) LLMs can function as effective embedding models without further training.  b) The study analyzes RW in comparison to hidden state (HS) embeddings, proposing a combined embedding method called MoE Embedding (MOEE) that concatenates or performs a weighted sum of similarities calculated from RW and HS embeddings.  c)  MOEE (sum), using a weighted sum of similarities from RW and HS, achieved a 22.45% improvement over HS on the DeepSeekMoE-16B model in the Massive Text Embedding Benchmark (MTEB), averaging across all tasks without prompts.  d)  AI practitioners can leverage the readily available RW in MoE LLMs as effective embedding models without the computational expense of further training or fine-tuning, enhancing performance in various downstream tasks like semantic textual similarity and classification.   Follow-up questions:  1. How does the performance of MOEE compare to other state-of-the-art embedding methods that *do* require training, especially considering the trade-off between computational cost and accuracy?  2. What are the specific implementation details for calculating the weighted sum in MOEE (sum), including the choice of weighting factor (α) and similarity metric, and how can these be optimized for different downstream tasks?  3. Could the observed complementarity between RW and HS embeddings be leveraged for other applications beyond embedding, such as model interpretability or knowledge distillation?  |
| SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2410.09754) or [HuggingFace](https://huggingface.co/papers/2410.09754))| Jun Jet Tai, Hyunseung Kim, Donghu Kim, Hojoon Lee, godnpeter | This research investigates whether incorporating a simplicity bias into network architecture enables effective parameter scaling in deep reinforcement learning (RL).  The authors introduce SimBa, a novel RL network architecture combining running statistics normalization, a residual feedforward block, and post-layer normalization.  Experiments across various RL algorithms and 51 continuous control tasks show SimBa consistently improves sample efficiency.  Specifically, SimBa with Soft Actor-Critic (SAC) matches or surpasses state-of-the-art methods on the DMC, MyoSuite, and HumanoidBench benchmarks, achieving an average return of 706 points on the DMC Hard benchmark.  This suggests that, for RL practitioners, simply modifying network architecture to SimBa can improve performance and scalability without computationally expensive add-ons like self-supervised objectives or planning.   Follow-up questions:  1. How does SimBa's performance compare to other architecture scaling methods like BroNet or SpectralNet when using algorithms besides SAC, such as TD7 or DreamerV3, given the paper's focus on SAC? 2. The paper mentions SimBa's effectiveness in high-dimensional input spaces.  What is the threshold where SimBa's benefits become particularly significant compared to a standard MLP, and how does this relate to the choice of environment? 3.  While the paper analyzes plasticity, it doesn't explicitly connect it to the generalization capabilities of the learned policies.  Are there further investigations planned or insights available on how SimBa's impact on plasticity affects generalization in dynamic RL environments?  |
| Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices (Read more on [arXiv](https://arxiv.org/abs/2410.11795) or [HuggingFace](https://huggingface.co/papers/2410.11795))| Liangliang Zhao, Guoli Jia, Yuzhu Zhang, Zhiyuan Ma, iseesaw | a) This survey paper aims to comprehensively review advancements in efficient diffusion models (DMs) covering architectural designs, training, inference, and deployment to facilitate broader understanding and application.  b) The authors organize existing literature into a taxonomy of six categories: principles, architecture, training/fine-tuning, sampling/inference, deployment, and applications, analyzing and comparing the performance of various efficient DM techniques.  The survey also compares different approaches such as U-Net, Transformer, and SSM-based backbones.  c) The survey presents various techniques to improve DM efficiency, including SnapFusion which reduced mobile text-to-image generation time to under 2 seconds on an iPhone 14 Pro.  It lacks specific quantitative benchmarks comparing the different architectural designs and training methods mentioned.  d) AI practitioners can use this survey as a roadmap to understand the core principles and practical strategies for developing and deploying efficient DMs across various tasks like image/video generation and editing, 3D synthesis, and medical/bioinformatics applications.  The survey's organization can guide practitioners in selecting appropriate efficient DM techniques based on task requirements.  Follow-up questions:  1.  Could you provide a more detailed comparative analysis of the different network backbones (U-Net, Transformer, SSM, RWKV, etc.) in terms of computational cost, memory footprint, and performance trade-offs for specific tasks like high-resolution image synthesis and long video generation? 2.  The survey mentions the scalability dilemma of DMs compared to LLMs. What are the current most promising research directions to overcome this limitation and enable the emergence of powerful capabilities in DMs similar to those observed in large language models? 3.  What are the best practices for deploying and optimizing DM inference in resource-constrained environments, particularly for real-time applications on mobile and web platforms?  Can the survey provide more detailed guidance or examples?  |
| Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2410.08001) or [HuggingFace](https://huggingface.co/papers/2410.08001))| Jia Zeng, Jisong Cai, Li Chen, Hongyang Li, qwbu | a) The paper aims to develop a synergistic dual-system framework, RoboDual, to improve robotic manipulation by combining the generalization capabilities of a large-scale pre-trained generalist policy (OpenVLA) with the efficiency and adaptability of a specialist policy. b) RoboDual uses a diffusion transformer-based specialist policy conditioned on multimodal sensory inputs and outputs (latent representations and discretized actions) from the generalist policy. The generalist and specialist are trained separately with potentially different datasets. c) RoboDual achieved a 12% performance improvement on CALVIN and a 20% increase over the most competitive baseline in a real-world setting across a range of manipulation tasks. It also maintained strong performance with only 5% of demonstration data and enabled a 3.8x higher control frequency compared to the generalist alone. d) AI practitioners can leverage RoboDual to efficiently deploy large VLA models for real-world robotic manipulation tasks by combining them with lightweight and adaptable specialist models. The dual-system approach can potentially improve performance, efficiency, and adaptability in data-constrained environments.  Follow-up questions:  1. How does the performance of RoboDual vary across different VLA architectures as the generalist policy? Are there specific VLA characteristics that are more conducive to synergistic integration with a specialist? 2. What are the tradeoffs between using a multi-task versus a single-task trained specialist policy in RoboDual, specifically in terms of performance, data efficiency, and computational cost? 3.  Could the current fixed inference ratio between generalist and specialist be replaced with an adaptive mechanism that dynamically adjusts the frequency based on task complexity or environment dynamics?  |
| Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt (Read more on [arXiv](https://arxiv.org/abs/2410.09745) or [HuggingFace](https://huggingface.co/papers/2410.09745))| Tatsunori Mori, Chengguang Gan | a) The research investigated the Mutual Reinforcement Effect (MRE), examining whether word-level and text-level information in text classification tasks mutually enhance performance. b) The authors conducted fine-tuning experiments with a novel input-output format on 21 MRE mixed datasets using LLaMA3-8B, and applied word-level information as a knowledgeable verbalizer in few-shot text classification using T5-base.  c)  In 16 out of 18 sub-datasets, knowledgeable verbalizers constructed with word-level information outperformed the original method in text classification, with improved F1 scores on sentiment analysis datasets. It's unclear what "original method" refers to specifically. d)  AI practitioners can leverage word-level information, such as entities and sentiment polarity, to improve the performance of text classification models, particularly in sentiment analysis and few-shot learning scenarios.   Follow-up questions:  1. What is the precise construction method of the "original KV" used as a baseline in the knowledgeable verbalizer experiments? How were the label-related high-frequency words chosen and utilized?   2. Could the authors provide more details on the pre-processing steps and the specific configurations of OpenPrompt utilized for the knowledgeable verbalizer experiments? This would allow replication of these results. 3. What specific metrics beyond F1-score (e.g., precision, recall) were observed in the knowledgeable verbalizer experiment, and how did they vary across different datasets and languages?  |
| Towards Natural Image Matting in the Wild via Real-Scenario Prior (Read more on [arXiv](https://arxiv.org/abs/2410.06593) or [HuggingFace](https://huggingface.co/papers/2410.06593))| Qianru Sun, Hao Zhang, Peng-Tao Jiang, Yu Liang, XiaRho | This research aims to improve interactive image matting, specifically using bounding boxes as input, by addressing limitations of existing methods relying on synthetic data and frozen segmentation models.  The authors introduce a new dataset, COCO-Matting, derived from COCO and featuring 38,251 human instance-level alpha mattes in complex natural scenes, and propose the Semantic Enhanced Matting (SEMat) framework.  SEMat incorporates a feature-aligned transformer and matte-aligned decoder within a modified SAM architecture and uses regularization and trimap losses during training. On the HIM2K dataset, the HQ-SAM-based SEMat achieved a 9.4% relative improvement in Mean Absolute Difference compared to the previous state-of-the-art, SmartMat.  This research provides AI practitioners with a new dataset and model architecture for enhanced interactive matting in real-world scenarios.   Follow-up questions:  1.  Given the computational cost of training SEMat, are there strategies for efficient fine-tuning or adaptation to specific downstream tasks with limited resources? 2.  The paper mentions limitations regarding SAM's performance on rare objects. How does this limitation specifically translate to SEMat's performance, and are there mitigation strategies, such as data augmentation or few-shot learning techniques, to address this? 3.  How does the performance of SEMat compare to other interactive segmentation models besides SAM when adapted for matting using the proposed COCO-Matting dataset and training framework?  |
