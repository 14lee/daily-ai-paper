

## Papers for 2024-10-17

| Title | Authors | Summary |
|-------|---------|---------|
| HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks (Read more on [arXiv](https://arxiv.org/abs/2410.12381) or [HuggingFace](https://huggingface.co/papers/2410.12381))| Xiao Li, Guancheng Lin, Huiyu Bai, Linquan Wu, zfj1998 | a) The paper investigates the visual understanding and reasoning abilities of Large Multimodal Models (LMMs) in coding tasks that require visual context.  b) The researchers created HumanEval-V, a benchmark of 108 Python coding tasks adapted from existing problems and requiring LMMs to generate code solutions based on images and function signatures, evaluated using pass@k metrics.  c) State-of-the-art LMMs performed below expectations, with even proprietary models like GPT-4o achieving only 13% pass@1 on HumanEval-V.  d)  AI practitioners developing LMMs should focus on improving models' visual understanding and reasoning as well as coding proficiencies, as current models demonstrate significant weaknesses in integrating these skills.  e) The paper notes a consistent performance degradation in open-weight LMMs compared to their language-only decoder counterparts on coding benchmarks, highlighting a need for further improvement in multimodal training strategies.   Follow-up questions:  1.  The paper mentions "hallucination errors" due to overfitting. Could the authors elaborate on the specific types of hallucinations observed and how they relate to the adaptation process used in creating HumanEval-V?  2.  Given the limited improvement from zero-shot Chain-of-Thought prompting, what other reasoning or prompting techniques could be explored to better assist LMMs in solving these visual coding tasks?  3. What specific architectural changes or training strategies could be implemented to address the performance degradation observed in open-weight LMMs compared to their decoder counterparts on coding tasks?  |
| VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI (Read more on [arXiv](https://arxiv.org/abs/2410.11623) or [HuggingFace](https://huggingface.co/papers/2410.11623))| Sicheng Zhou, Yangyang Yu, Kechen Fang, yetian, SijieCheng | a) The research assesses the capabilities of Multi-modal Large Language Models (MLLMs) in understanding egocentric videos for application in Embodied AI tasks.  b) A new benchmark, VidEgoThink, was created with four interrelated tasks: video question-answering, hierarchy planning, visual grounding, and reward modeling; data was generated using Ego4D and GPT-40, then filtered by human annotators; and 14 MLLMs across three categories (API-based, open-source image-based, and open-source video-based) were evaluated.  c)  MLLMs performed poorly across all tasks, with the best average accuracy on video question-answering reaching only 32.82% across all dimensions.  d) The findings indicate current MLLMs require significant improvement for effective application in first-person scenarios in Embodied AI, particularly in understanding temporal dynamics and generating actionable outputs, despite having certain potential for advancement.   Follow-up Questions:  1.  Given the poor performance on temporal reasoning tasks, what specific architectural modifications or training strategies could be explored to improve MLLMs' ability to understand action sequences and temporal relations in egocentric videos?  2.  The paper mentions an automatic data generation pipeline; it would be useful to know more specific details of this pipeline.  Could the authors elaborate on the specific prompts used for GPT-40 and the filtering criteria employed by the human annotators to improve replicability and allow further exploration of this data generation approach?  3. The paper briefly mentions future work on developing egocentric foundation models for robotics.  What specific robotic tasks are the authors envisioning these models being applied to, and what are the key challenges they anticipate in adapting VidEgoThink or similar benchmarks for evaluating these specialized models?  |
| The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio (Read more on [arXiv](https://arxiv.org/abs/2410.12787) or [HuggingFace](https://huggingface.co/papers/2410.12787))| Hang Zhang, Yang Zhou, Yun Xing, Sicong Leng, ClownRat | a) This paper investigates the causes and prevalence of hallucinations in Large Multimodal Models (LMMs) processing language, visual, and audio data.  b) A new benchmark called "The Curse of Multi-Modalities" (CMM) was created, using object/event-level probing questions in a binary classification framework to evaluate LMM performance across various multimodal contexts and hallucination subcategories.  c)  LMMs exhibit significant vulnerabilities to Audio-Language (AL) hallucinations, with Gemini-1.5-pro achieving only a 14.5% Hallucination Resistance (HR) score in this category.  d)  AI practitioners should prioritize addressing spurious inter-modality correlations, especially those involving audio, and mitigate the overreliance on unimodal priors when developing and deploying LMMs. The specific training strategies mentioned (balanced multi-modal training data, advanced cross-modal fusion, mitigating linguistic priors, and refined safety alignment) could be beneficial.   Follow-up Questions:  1.  The paper highlights the limited availability of visual-audio-language datasets as a potential reason for stronger AL correlations.  Are there recommended strategies or resources for constructing or augmenting such datasets to improve AL hallucination resistance?  2.  Could the authors elaborate on the specific implementation details of the "dynamic fusion strategies" mentioned as a potential improvement for cross-modal fusion? What are some promising architectures or approaches for achieving more context-aware modality integration?  3.  The paper identifies varying response tendencies in different LMMs (overconfidence vs. excessive caution).  Are there specific evaluation metrics or techniques beyond PA and HR that could be used to better characterize and compare these tendencies, enabling a more nuanced understanding of their impact on downstream tasks?  |
| Revealing the Barriers of Language Agents in Planning (Read more on [arXiv](https://arxiv.org/abs/2410.12409) or [HuggingFace](https://huggingface.co/papers/2410.12409))| Kai Zhang, Siyu Yuan, jiangjiechen, kexunz, hsaest | This paper investigates why language agents struggle with planning tasks.  Permutation Feature Importance (PFI) analysis of constraint and question components within prompts was used. The results show that constraints have a limited role, and the influence of the question decreases with increasing planning horizon; OpenAI's 01 model achieves only 15.6% on the TravelPlanner benchmark.  This implies that current memory updating strategies for language agents, while offering some improvements, resemble "shortcut learning" and do not fully address the core issues of constraint integration and long-horizon goal maintenance.    Follow up questions:  1. How does the PFI analysis method account for the variability in the natural language generation process of LLMs across different prompts and trials? 2. How can the insights regarding the limitations of episodic and parametric memory updating inform the development of more effective memory mechanisms for language agents specifically aimed at improving planning performance? 3.  Can the observed weakness in constraint handling be addressed by incorporating symbolic planning techniques within the LLM framework for agent planning?  |
| DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception (Read more on [arXiv](https://arxiv.org/abs/2410.12628) or [HuggingFace](https://huggingface.co/papers/2410.12628))| Conghui He, Bin Wang, Hengrui Kang, Zhiyuan Zhao | a) The research aims to improve the speed and accuracy of Document Layout Analysis (DLA) by addressing the trade-off between multimodal and unimodal methods.  b) The authors introduce DocLayout-YOLO, which uses a synthetic dataset (DocSynth-300K) generated by their Mesh-candidate BestFit algorithm and integrates a Global-to-Local Controllable Receptive Module (GL-CRM) within a YOLOv10 architecture.  c) DocLayout-YOLO achieved 78.8% mAP on the DocStructBench dataset with an inference speed of 85.5 frames per second (FPS).  d)  AI practitioners can leverage DocLayout-YOLO for real-time, accurate DLA in applications such as document parsing, information retrieval, and knowledge extraction, benefiting from its improved speed and accuracy compared to previous methods.   Follow-Up Questions:  1. What are the details of the GL-CRM's integration with the YOLOv10 architecture, and how does this module specifically contribute to the improved handling of multi-scale elements?  2. While the paper mentions that DocSynth-300K offers improved diversity, what are the limitations of this synthetic dataset, particularly when dealing with extremely complex or unusual document layouts not well-represented in the training data?  3.  Can the Mesh-candidate BestFit algorithm be adapted for other layout generation tasks beyond document layout analysis, such as webpage layout or UI design?  |
| Exploring Model Kinship for Merging Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.12613) or [HuggingFace](https://huggingface.co/papers/2410.12613))| Huajun Chen, Shumin Deng, Ningyu Zhang, Yunzhi Yao, Yedi Hu | a) This research investigates whether a metric called "model kinship" (similarity between LLMs based on weight differences from a base model) can guide and improve the performance of iterative LLM merging.  b) The researchers analyzed open-source LLMs using Pearson Correlation, Cosine Similarity, and Euclidean Distance to calculate model kinship, correlating it with merging performance gains and examining its behavior across different merging stages. They also proposed a "Top-k Greedy Merging with Model Kinship" strategy that incorporates kinship into model selection for merging.  c)  A statistically significant correlation was found between the absolute value of merge gain and model kinship. Using the kinship-guided merging strategy, the researchers achieved an average task performance of 69.13 across six tasks, compared to 68.72 using a standard greedy strategy. It is unclear why the results focus on absolute merge gain rather than merge gain itself, and the choice and impact of merging six specific tasks is also not explained.  d) AI practitioners can utilize model kinship to guide model selection during iterative merging, potentially escaping local optima and achieving higher performance gains on multi-task learning benchmarks. Using model kinship also offers potential as an early stopping criterion in iterative merging, improving resource efficiency.   Follow-up questions:  1.  How does the choice of the base model affect the calculation and interpretation of model kinship, and what are best practices for base model selection? 2.  Beyond the six tasks used in this study, how does model kinship generalize to broader sets of tasks or different task domains, and what are the limitations of its applicability? 3.  Can the concept of model kinship be extended to guide other LLM combination techniques beyond simple weight averaging, such as knowledge distillation or parameter fusion?  |
| Large Language Model Evaluation via Matrix Nuclear-Norm (Read more on [arXiv](https://arxiv.org/abs/2410.10672) or [HuggingFace](https://huggingface.co/papers/2410.10672))| Yi Chang, Yahan Li, WhiteCatY, xiatingyu | This research aimed to develop a more computationally efficient metric for evaluating information compression and redundancy reduction in Large Language Models (LLMs).  The researchers proposed using the Matrix Nuclear-Norm, approximated by the L1,2-norm, as a computationally less expensive alternative to Matrix Entropy.  Results showed the Matrix Nuclear-Norm achieved speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model with increasing sizes from 111M to 6.7B parameters. This improvement allows AI practitioners to more efficiently evaluate LLMs, especially as model sizes continue to scale, making the Matrix Nuclear-Norm a potentially practical choice for assessing compression capabilities. The paper does not definitively state whether Matrix Nuclear-Norm and Matrix Entropy yield comparable evaluation accuracy despite the stated claim of "comparable accuracy".  Follow-up questions:  1.  While the paper demonstrates computational efficiency gains, how does the Matrix Nuclear-Norm's correlation with downstream task performance compare to Matrix Entropy's? 2.  The paper mentions anomalies in Matrix Nuclear-Norm values for certain model sizes (2.7B and 13B). What are the potential underlying reasons for these anomalies and how might they affect the metric's reliability in evaluating these specific models? 3. How sensitive is the Matrix Nuclear-Norm to the choice of L1,2-norm approximation, and are there alternative approximations that might improve its accuracy or stability further?  |
| ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs (Read more on [arXiv](https://arxiv.org/abs/2410.12405) or [HuggingFace](https://huggingface.co/papers/2410.12405))| Dahua Lin, Xinyu Fang, KennyUTC, zsytony, JingmingZ | a) The research aimed to evaluate and understand prompt sensitivity in large language models (LLMs) at the instance level.  b) ProSA, a framework incorporating the PromptSensiScore (PSS) metric and leveraging decoding confidence, was developed.  c)  Results across multiple datasets and models revealed variations in prompt sensitivity, with Llama3-70B-Instruct exhibiting the highest robustness and Qwen1.5-14B-Chat demonstrating the most serious prompt sensitivity on the MATH dataset. d)  Higher model confidence correlated with increased prompt robustness, suggesting prompt sensitivity reflects the model's decoding logic. This finding provides a new metric for evaluating LLM robustness and emphasizes the importance of considering prompt engineering and selection strategies in development and applications.    Follow-up Questions:  1. How does the ProSA framework compare with existing methods for evaluating prompt sensitivity in terms of computational cost and insights provided? 2.  Could the decoding confidence be used as a signal for automated prompt optimization or selection? 3. How does the observed correlation between model size and prompt sensitivity vary across different model architectures (e.g., decoder-only vs. encoder-decoder)?   |
| ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2410.08584) or [HuggingFace](https://huggingface.co/papers/2410.08584))| Wenqi Shao, Jing Liu, Feng Chen, Yefei He, kpzhang996 | a) The research aims to improve the efficiency of Large Vision-Language Models (LVLMs) by addressing computational bottlenecks in the prefill phase and memory bottlenecks in the decoding phase.  b) ZipVL employs a dynamic, layer-wise adaptive ratio assignment for important tokens based on attention score distribution, combined with token-level sparse attention in the prefill phase and mixed-precision KV cache quantization in the decoding phase.  c)  Experiments demonstrate a 2.6× speedup in the prefill phase and a 50.0% reduction in GPU memory usage on the LongVA-7B model for the Video-MME benchmark, with a 0.2% accuracy reduction.  d)  AI practitioners can leverage ZipVL to significantly improve the inference speed and reduce the memory footprint of LVLMs, facilitating their deployment in resource-constrained environments.  The dynamic ratio assignment, in particular, offers a more robust and adaptive approach compared to fixed sparsity methods.    Follow-up Questions:  1. What are the specific implementation details regarding the integration of ZipVL with different fast attention mechanisms besides FlashAttention?  2. How does the performance of ZipVL scale with increasing video lengths or image resolutions, particularly with regards to the trade-off between computational cost and accuracy?  3.  Could the dynamic ratio allocation strategy be further improved by incorporating factors beyond attention scores, such as textual context or visual saliency?  |
| Improving Long-Text Alignment for Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2410.11817) or [HuggingFace](https://huggingface.co/papers/2410.11817))| Chongxuan Li, Zehan Wang, Tianyu Pang, Chao Du, luping-liu | a) This research addresses the challenge of aligning text-to-image (T2I) diffusion models with long, complex text prompts, which often exceed the token limits of standard encoders like CLIP and result in incomplete or inaccurate image generation.  b) The authors propose LongAlign, combining segment-level encoding, which divides long text into segments and processes them individually, with a decomposed preference optimization method that fine-tunes diffusion models using a reweighted combination of text-relevant and text-irrelevant preference scores derived from a modified CLIP-based model.  c) The fine-tuned Stable Diffusion (SD) v1.5 model, after 20 hours of training using LongAlign on 6 A100 GPUs, achieves a FID score of 19.63 on a 5k image dataset, outperforming baseline foundation models like PixArt-a and Kandinsky v2.2 in long-text alignment.  d) AI practitioners can leverage LongAlign to improve the fidelity of T2I generation from detailed text prompts by overcoming input length limitations and enhancing alignment between text and generated images. The decomposition of preference scores during fine-tuning helps mitigate overfitting, a common issue in reward-based optimization of diffusion models.   Follow-up questions:  1. What are the specific implementation details for merging the segment embeddings in LongAlign, especially regarding the choice of concatenation versus other aggregation methods, and how does this impact the computational complexity?  2. How does the reweighting factor *w* in the gradient-reweight reward fine-tuning affect the trade-off between text alignment and visual quality (e.g., aesthetics, photorealism), and is there a systematic method for determining the optimal *w* value for different datasets and models?  3.  How robust is LongAlign to variations in text segmentation strategies (e.g., sentence-level versus semantic chunk-level segmentation), and what preprocessing steps are necessary to ensure consistent performance across diverse text formats and domains?  |
| Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models (Read more on [arXiv](https://arxiv.org/abs/2410.11081) or [HuggingFace](https://huggingface.co/papers/2410.11081))| Yang Song, Cheng Lu | a) This research aims to improve the training stability and scalability of continuous-time consistency models (CMs) for fast generative sampling.  b) The authors introduce TrigFlow, a simplified theoretical framework unifying diffusion and CM formulations, alongside improved network architecture, time-conditioning, and training objectives incorporating tangent normalization and adaptive weighting.  They also enhance Jacobian-vector product computation for Flash Attention to improve training efficiency.  c) The resulting simplified CMs (sCMs) achieved a 2-step FID score of 1.88 on ImageNet 512x512 with 1.5 billion parameters, narrowing the gap to state-of-the-art diffusion models to within 10%.  d) AI practitioners can leverage these stabilized and scalable continuous-time CMs for high-quality image generation with significantly reduced sampling compute compared to traditional diffusion models.  The simplification provided by TrigFlow could also make CMs more accessible for development and analysis.   Follow-up questions:  1. Could the TrigFlow framework be adapted for other data modalities beyond images, such as audio or 3D models, and what modifications might be necessary?  2. What are the practical memory and compute requirements for training sCMs at the reported scale, and how do they compare to training comparable diffusion models?  3. How sensitive are the sCM results to the hyperparameters introduced for tangent normalization and adaptive weighting, and are there recommended starting points for tuning these on new datasets?  |
| Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL (Read more on [arXiv](https://arxiv.org/abs/2410.12491) or [HuggingFace](https://huggingface.co/papers/2410.12491))| Sonali Parbhoo, Arjun Jagota, Jared Joselowitz, skrishna | This research investigated whether Inverse Reinforcement Learning (IRL) can recover the reward functions underlying the training of Large Language Models (LLMs) fine-tuned with Reinforcement Learning from Human Feedback (RLHF).  The researchers applied a Max-Margin IRL algorithm to extract reward models from toxicity-aligned LLMs of varying sizes (70M and 410M parameters), trained on a subset of the Jigsaw toxicity dataset. The extracted reward model for the 70M parameter LLM achieved 80.40% accuracy in predicting human preferences on a held-out test set.  This indicates that, at least for smaller models and specific tasks, IRL can extract reward models that capture key aspects of the original RLHF objective, which has implications for interpretability and potential vulnerability analysis. The paper mentions challenges with the non-identifiability of reward functions and potential scalability issues for larger LLMs but does not fully elaborate on mitigations or solutions.  Follow-up questions:  1. How does the performance of the proposed Max-Margin IRL method compare to other IRL techniques, such as Max-Entropy or adversarial IRL, in extracting reward models from RLHF-trained LLMs, especially for larger models and more complex reward structures?  2.  What specific mitigation strategies are proposed to address the non-identifiability of the recovered reward functions, and how do these impact the reliability and interpretability of the extracted models for practical applications like debugging or bias detection?  3. Given the potential for misuse of extracted reward models, what concrete recommendations would the researchers offer for responsible disclosure and use of these models within the broader AI community?  |
| Neural Metamorphosis (Read more on [arXiv](https://arxiv.org/abs/2410.11878) or [HuggingFace](https://huggingface.co/papers/2410.11878))| Xinchao Wang, Xingyi Yang | This paper aims to create self-morphable neural networks adaptable to various sizes without retraining. The key methodology involves training a neural implicit function (INR) as a hypernetwork to learn the continuous weight manifold of neural networks, incorporating strategies for intra- and cross-network smoothness.  On CIFAR10 image classification, the proposed method, NeuMeta, achieved 91.76% accuracy with a full-sized ResNet20 and 89.56% accuracy at a 75% compression rate, often outperforming individually trained models at smaller sizes. This implies that AI practitioners could potentially achieve significant model compression without retraining or substantial performance loss.  Follow-up questions:  1. How does the computational cost of using the INR to generate weights compare to the cost of fine-tuning a pruned model or training a smaller model from scratch, especially for very large networks? 2.  The paper mentions limitations in the INR's representational ability for complex tasks like segmentation; how might these limitations be addressed to improve performance on such tasks at higher compression rates? 3. Could NeuMeta be extended to enable dynamic morphing of network architectures during inference based on resource availability or input characteristics?  |
| WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation (Read more on [arXiv](https://arxiv.org/abs/2410.12722) or [HuggingFace](https://huggingface.co/papers/2410.12722))| Juan Carlos Climent Pardo, Yingya Li, Siena Placino, João Matos, shanchen | a) The research aimed to create and evaluate a multilingual, multimodal benchmark dataset to assess vision-language models (VLMs) in healthcare question answering (QA).  b)  Researchers collected multiple-choice medical exam questions from Brazil, Israel, Japan, and Spain, pairing them with images and validating English translations. They then evaluated the performance of 10 open and closed-source VLMs with and without image input, using accuracy as the metric, and calculated Cohen's kappa for cross-linguistic consistency.  c)  GPT4o achieved the highest accuracy across most datasets, but only reached 58% accuracy on the Hebrew version of the Israeli dataset.  d)  The results indicate a need for improvement in VLMs' ability to handle diverse languages, especially those underrepresented in training data, as demonstrated by lower performance in non-Roman alphabet languages like Hebrew.  The impact of image input varied significantly across model families, with Gemini models showing the largest performance gains.  Follow-up questions:  1. What specific pre-training datasets were used for the evaluated VLMs, and what is their representation of different languages and medical concepts?  2.  How does the performance of the VLMs on this multiple-choice dataset compare to their performance on other medical QA tasks, such as free-text generation or information retrieval?  3. Beyond accuracy and Cohen's Kappa, what other metrics (e.g., calibration, robustness, fairness) would be relevant to evaluate VLMs in this context, and were they examined in the research?  |
| OMCAT: Omni Context Aware Transformer (Read more on [arXiv](https://arxiv.org/abs/2410.12109) or [HuggingFace](https://huggingface.co/papers/2410.12109))| Andrew Tao, Rafael Valle, Matthieu Le, Karan Sapra, goarushi27 | a) This research aims to improve cross-modal temporal understanding in multimodal Large Language Models (LLMs), particularly the ability to correlate events across audio and video streams.  b) The authors introduce a new dataset, OCTAV (Omni Context and Temporal Audio Video), designed to capture event transitions across audio and video, and a new model, OMCAT (Omni Context Aware Transformer), which leverages Rotary Time Embeddings (ROTE) for enhanced temporal grounding.  OMCAT is trained using a three-stage pipeline: feature alignment, instruction tuning, and OCTAV-specific training.  c) OMCAT achieves state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks, outperforming existing models by a substantial margin on the OCTAV benchmark (19.0% Recall@1 IoU 0.7 on OCTAV-ST-ActivityNet for OMCAT vs 1.57% for GroundingGPT). It also shows competitive results in zero-shot settings.  d) AI practitioners can leverage OMCAT and the OCTAV dataset to develop more robust multimodal applications requiring fine-grained temporal understanding, such as video analysis, content creation, and interactive media. The improved performance on time-anchored tasks directly enhances the ability of LLMs to understand and generate temporally consistent responses in multimodal contexts.   Follow-up questions:  1.  What are the computational costs and scalability implications of ROTE compared to other temporal embedding methods, especially when applied to longer videos or higher-resolution data? 2.  How does the performance of OMCAT degrade with noisier or more ambiguous audio-visual data, which is common in real-world scenarios not represented in the artificially constructed OCTAV dataset?  3.  Can the ROTE embeddings be effectively generalized to other multimodal tasks beyond audio-visual understanding, such as integrating text, images, and sensor data with time dependencies?  |
| Tracking Universal Features Through Fine-Tuning and Model Merging (Read more on [arXiv](https://arxiv.org/abs/2410.12391) or [HuggingFace](https://huggingface.co/papers/2410.12391))| Desmond Elliott, nilq | a) This research investigates how features in one-layer Transformer language models evolve (emerge, disappear, persist) during fine-tuning to new domains and model merging via spherical linear interpolation. b) The study uses small-scale Mistral-like Transformers trained on English text and programming code (Python and Lua), with feature extraction performed using sparse autoencoders analyzing MLP activations. c) Few features persist across fine-tuning and merging, though persistent features often correspond to generic text properties like punctuation and formatting (e.g., a variable assignment feature maintained an average 85.1% cross-correlation across models).   d)  AI practitioners can leverage these findings to understand feature dynamics when adapting existing models for new domains or tasks using fine-tuning and merging techniques. The low feature persistence suggests that substantial feature change is expected when applying these techniques, and monitoring/analysis of these changes may be crucial.   Follow-up Questions:  1. How do the findings generalize to larger, more complex Transformer models used in real-world applications? 2. Are there alternative merging techniques or hyperparameter settings that could improve feature retention during merging? 3.  Could controlling or manipulating these evolving features during fine-tuning and merging lead to more robust and adaptable models?  |
| DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities (Read more on [arXiv](https://arxiv.org/abs/2410.07722) or [HuggingFace](https://huggingface.co/papers/2410.07722))| Jeff Dalton, Iain Mackie, Sean MacAvaney, Shubham Chatterjee, Thong Nguyen | This paper investigates whether incorporating entities into learned sparse retrieval (LSR) improves its effectiveness. The researchers introduce a Dynamic Vocabulary (DyVo) head, which uses entity embeddings and an entity retrieval component to generate entity weights, merged with word piece weights to create joint representations.  On the CODEC dataset, DyVo with GPT-4 generated entity candidates achieves an nDCG@10 of 56.46, compared to 52.61 for LSR without entities. This implies that augmenting LSR with dynamically retrieved entities can improve retrieval effectiveness, especially in entity-rich datasets. AI practitioners working with LSR can use the DyVo head to expand vocabularies with entities from external knowledge bases, potentially increasing performance.  Follow-up questions:  1. What is the computational overhead of the entity retrieval component, especially at scale with large knowledge bases? 2. How robust is the method to different entity embedding sources, and how can embedding quality be efficiently evaluated within this framework? 3. What strategies could be employed to further reduce the dependence on computationally expensive large language models for candidate generation during training and inference?  |
