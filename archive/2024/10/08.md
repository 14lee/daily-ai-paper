

## Papers for 2024-10-08

| Title | Authors | Summary |
|-------|---------|---------|
| Differential Transformer (Read more on [arXiv](https://arxiv.org/abs/2410.05258) or [HuggingFace](https://huggingface.co/papers/2410.05258))| Li Dong, thegenerality, sunyt32, yuqxia, ytz20 | This research addresses the problem of Transformers over-attending to irrelevant context in attention mechanisms.  The authors propose a Differential Transformer (DIFF Transformer) using a differential attention mechanism that calculates attention scores as the difference between two softmax attention maps.  Results on language modeling tasks show DIFF Transformer outperforms standard Transformer models, requiring only 65% of the model size or training tokens to achieve comparable performance.  For in-context learning on the TREC dataset, DIFF Transformer improved average accuracy by 5.2% to 21.6% compared to the standard Transformer. This architecture allows AI practitioners to train more efficient and performant large language models.  Here are some follow-up questions an AI practitioner might have:  1. What is the computational overhead of the differential attention mechanism compared to standard softmax attention, particularly with different FlashAttention implementations? 2.  How does the performance of DIFF Transformer compare to other attention-mechanism modifications designed to address similar issues of focusing on irrelevant context, and what are the tradeoffs? 3.  Beyond language modeling, how does the differential attention mechanism perform on other downstream tasks that heavily rely on attention, such as machine translation or image captioning?  |
| LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations (Read more on [arXiv](https://arxiv.org/abs/2410.02707) or [HuggingFace](https://huggingface.co/papers/2410.02707))| Roi Reichart, Zorik Gekhman, belinkov, tokeron, hadasor | This research investigated how large language models (LLMs) encode and represent errors, termed "hallucinations," within their internal activations.  The study employed probing classifiers trained on intermediate LLM representations to predict error presence and type, alongside an analysis of repeated sampling of LLM-generated answers. Probing classifiers trained on the activations of *exact answer tokens* achieved significantly higher error detection performance (AUC of 0.85 on TriviaQA with Mistral-7b-instruct) compared to methods using other tokens.  However, these probing classifiers did not generalize well across datasets representing different tasks, suggesting skill-specific truthfulness encoding.  The study highlights a potential disconnect between LLMs' internal representations and external behavior, where the model may internally encode the correct answer but consistently generate an incorrect one.  A clear quantitative finding comparing probe-based answer selection accuracy vs. greedy decoding across different error types is not presented in a consolidated manner, making direct comparison difficult.   Follow-up questions from an AI practitioner:  1.  Could the "skill-specific" nature of truthfulness encoding be mitigated by multi-task training of the probing classifier, and if so, how would performance compare to single-task training on diverse datasets? 2.  Given the observed discrepancy between internal encoding and external behavior, what specific modifications to the decoding process or model architecture could potentially improve the alignment and reduce erroneous outputs? 3.  How does the performance of exact answer token probing compare to other state-of-the-art error detection methods across a broader range of LLM architectures and sizes, including larger models not tested in this study?  |
| VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide (Read more on [arXiv](https://arxiv.org/abs/2410.04364) or [HuggingFace](https://huggingface.co/papers/2410.04364))| Jong Chul Ye, geonyoung-park, bryanswkim, DHCAI | a) The research aims to improve the temporal consistency of pre-trained text-to-video (T2V) diffusion models without requiring additional training or fine-tuning.  b) VideoGuide interpolates denoised samples from a "guiding" pre-trained VDM (which can be the same as the sampling VDM or a different one) into the denoising process of the main "sampling" VDM during the initial sampling steps.  c)  When applied to AnimateDiff, VideoGuide achieved the best performance across all evaluated metrics, including a subject consistency score of 0.9614, exceeding the base AnimateDiff score of 0.9183.    d) VideoGuide offers AI practitioners a computationally efficient method to enhance the temporal quality of existing T2V diffusion models by leveraging other pre-trained models, potentially combining the strengths of different models without requiring retraining. The paper implies, but does not explicitly state, whether this technique preserves unique features of the sampling VDM, such as controllability.   Follow-up Questions:  1.  How does the choice of the guiding VDM affect the specific aspects of the generated video, such as style, motion, and text coherence, and what strategies can be used for selecting the most effective guiding model for a given task?  2. The paper focuses on 16-frame videos. How does VideoGuide scale with longer video generation and what modifications, if any, are required to maintain performance and computational efficiency?  |
| FAN: Fourier Analysis Networks (Read more on [arXiv](https://arxiv.org/abs/2410.02675) or [HuggingFace](https://huggingface.co/papers/2410.02675))| Yongding Tao, Ge Li, Jingjingxu, zkcpku, dongyh | This research investigates how to enable neural networks to effectively model periodicity.  The authors propose Fourier Analysis Networks (FAN), which integrate Fourier Series into the network architecture to explicitly encode periodic patterns.  On symbolic formula representation tasks, FAN consistently outperforms baselines like MLP, KAN, and Transformer as the number of parameters increases. For example, on the task of representing *f(x) = J₀(20x)*, FAN achieves significantly lower test RMSE than other baselines across various parameter sizes. This suggests that AI practitioners can leverage FAN to improve model performance, particularly in domains involving periodic or quasi-periodic data, such as time series analysis and symbolic computation, by replacing standard MLP layers with FAN layers. It is unclear how the comparative parameter and FLOP counts in Table 1 are calculated.  Follow-up questions:  1.  How does the performance of FAN scale with the complexity of the periodic functions being modeled, and what are the practical limitations in terms of computational cost? 2.  Are there specific types of periodic or quasi-periodic data where FAN offers the most significant advantages over other architectures, and are there any scenarios where it might be less suitable?  3.  How robust is FAN to noise in periodic data, and what techniques could be used to further enhance its robustness?  |
| Presto! Distilling Steps and Layers for Accelerating Music Generation (Read more on [arXiv](https://arxiv.org/abs/2410.05167) or [HuggingFace](https://huggingface.co/papers/2410.05167))| Jonah Casebeer, Ge Zhu, Njb, tberg12, ZacharyNovack | a) The research aims to accelerate inference in diffusion-based text-to-music (TTM) models by reducing sampling steps and computational cost per step.  b) The authors develop Presto, a dual-faceted distillation approach comprising: Presto-S (step distillation using GAN-based distribution matching), Presto-L (layer distillation with variance preservation and budget awareness), and Presto-LS (combined layer-step distillation).  c) Presto-LS achieves a 10-18x speedup compared to the base model, resulting in a latency of 230/435ms for generating 32-second mono/stereo audio at 44.1kHz on an A100 40GB GPU, while also improving diversity (higher recall) compared to Presto-S.  d) AI practitioners working on real-time or interactive music generation applications can leverage Presto-LS to significantly reduce inference latency without substantial quality loss, potentially enabling new interactive experiences. The paper focuses exclusively on offline generation, and its applicability to real-time or streaming generation remains unclear.   Follow-up questions:  1.  How does Presto-LS perform on longer music pieces (e.g., > 1 minute), and how does the latency scale with duration? 2.  Could the variance preservation technique used in Presto-L be generalized to other diffusion-based generative models beyond music, such as text-to-image or text-to-video? 3. What are the memory and compute requirements for training and deploying the different Presto models (S, L, LS)?  |
| Named Clinical Entity Recognition Benchmark (Read more on [arXiv](https://arxiv.org/abs/2410.05046) or [HuggingFace](https://huggingface.co/papers/2410.05046))| Clément Christophe, Tathagata Raha, Muhammad Umar Salman, Marco AF Pimentel, Wadood M Abdul | a) The research aims to establish a standardized benchmark for evaluating Named Clinical Entity Recognition (NER) models in the clinical domain.  b) The benchmark employs a curated collection of publicly available clinical datasets with entities standardized using the OMOP Common Data Model, along with token-based and span-based evaluation metrics (precision, recall, and F1-score) in different averaging modes (Micro and Macro).  Both exact and partial matching strategies are also incorporated.  c) GLiNER-based architectures achieve higher F1-scores (78.25% for condition entities using span-based macro-averaged scores) compared to decoder-only (LLM) models on the clinical NER task.  d) AI practitioners developing clinical NER systems should consider using GLiNER-based models for superior performance compared to decoder-only architectures, particularly for token-level classification tasks where accurate extraction of span information is critical.  Follow-up questions:  1.  Given the performance advantage of GLiNER models over traditional LLMs, what specific adaptations or fine-tuning strategies were used for the GLiNER models included in this benchmark to optimize their performance on the clinical NER task?  2. The paper mentions the issue of label imbalance in clinical datasets. How does this label imbalance affect the evaluation metrics reported, and were any techniques used to mitigate the impact of this imbalance on model training or evaluation?    |
| OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction (Read more on [arXiv](https://arxiv.org/abs/2410.04932) or [HuggingFace](https://huggingface.co/papers/2410.04932))| Xu Yan, Weichao Qiu, bingbl, Evenc, lilelife | a) The research aims to achieve spatial control with instance-level customization in image generation using multi-modal instructions (text and image references) associated with user-defined masks.  b) OmniBooth introduces a "latent control signal" (lc), a high-dimensional spatial feature integrating spatial, textual, and image conditions.  Text embeddings are "painted" into lc, while image embeddings undergo "spatial warping" before integration. A modified ControlNet framework aligns lc with latent image features.  c) On the MS COCO val2017 dataset, OmniBooth achieved a FID score of 17.8, outperforming InstanceDiffusion (FID 23.9) and ControlNet (FID 20.3).  The paper doesn't clarify how the "synthetic COCO val-set" used for evaluation was generated.  d) AI practitioners can leverage OmniBooth to develop image generation models offering users fine-grained control over instance placement and attributes via multi-modal instructions, surpassing the limitations of global prompts or single-modality control. The improved FID score suggests potential for higher quality and more controllable image synthesis.   Follow-up questions:  1. Could you elaborate on the creation of the "synthetic COCO val-set" used for evaluation?  Specifically, how were instance masks and captions generated, and how does this synthetic set relate to the original COCO val2017 set?  2. What are the computational costs (e.g., training time, inference speed) associated with OmniBooth compared to baseline models like ControlNet and InstanceDiffusion?  3. How does the proposed "spatial warping" method handle instances whose reference images significantly differ in aspect ratio or pose from the target mask region?  Does this lead to distortions or artifacts in the generated images?  |
| TLDR: Token-Level Detective Reward Model for Large Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.04734) or [HuggingFace](https://huggingface.co/papers/2410.04734))| Rui Wang, Tong Xiao, tbpangolin, pzzhang, deqing | a) The research aimed to develop a token-level reward model (TLDR) for multimodal large language models (VLMs) to improve interpretability and granularity compared to traditional binary reward models.  b) TLDR uses a perturbation-based method to generate synthetic hard negatives and token-level labels to train the model, leveraging a pretrained VLM (PaliGemma-3B-Mix-448) and a linear reward model head applied to each token.  c) TLDR achieves 98.6% token-level accuracy and can speed up human annotation by 3 times when correcting synthetic captions.  A correlation of 0.892 (p=0.006) was found between the log of the hallucination rate and MMMU score.  d) TLDR provides AI practitioners with a tool for enhanced self-correction in VLMs, more effective hallucination detection, and faster data annotation for vision-language tasks.  Follow-up questions:  1. How does the performance of TLDR scale with larger VLMs and datasets, particularly with more complex and nuanced visual scenes? 2. Can TLDR be adapted for other multimodal tasks beyond image captioning and VQA, such as visual question generation or image retrieval? 3. What are the computational resource requirements for training and deploying TLDR, and how might these impact practical application in resource-constrained settings?  |
| UniMuMo: Unified Text, Music and Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2410.04534) or [HuggingFace](https://huggingface.co/papers/2410.04534))| Yutong Zhang, Kun Su, Han Yang, auspicious3000, Jiaben | a) This research aimed to create a unified model, UniMuMo, capable of generating music, motion, and text in arbitrary combinations conditioned on inputs from any of these modalities.  b) The key methodology involved aligning unpaired music and motion data based on rhythmic patterns, encoding music and motion into a joint token space using a shared codebook, and training a transformer decoder with a novel music-motion parallel generation scheme. A T5 decoder is then fine-tuned for captioning.  c) UniMuMo achieved competitive results on unidirectional generation benchmarks, for example, achieving a CLAP similarity score of 0.29 on text-to-music generation when trained on data containing vocals.  The paper does not provide clear comparisons on combined generation tasks (e.g., text and music to motion).  d) This work provides AI practitioners with a unified framework for multimodal content generation involving music, motion, and text, potentially streamlining development and deployment compared to using separate models for each task.  The impact on real-world combined generation tasks is unclear due to the lack of reported results on such scenarios.   Follow-up questions:  1.  What are the quantitative results of UniMuMo on multi-conditional generation tasks like text-and-music-to-motion or music-and-text-to-motion, as shown in Figure 1, since these seem to be the major contribution differentiating it from other methods? 2. Could the authors provide further insights into the limitations of the rhythmic pattern alignment technique and its potential impact on generating motions for music with complex and varying rhythms? 3. Can the proposed framework be extended to other modalities beyond music, motion, and text, such as image or video?  |
| LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2410.02884) or [HuggingFace](https://huggingface.co/papers/2410.02884))| Tong Che, Jingdi Lei, schrodingers-tiger, jwu323, qq8933 | This research aims to improve large language model (LLM) performance on complex mathematical reasoning, particularly at the Olympiad level.  The LLaMA-Berry framework utilizes Self-Refine applied to Monte Carlo Tree Search (SR-MCTS) for solution path optimization and a Pairwise Preference Reward Model (PPRM) with Enhanced Borda Count (EBC) for solution evaluation. On the AIME2024 benchmark, the success rate increased from 2/30 (baseline LLaMA-3.1-8B-Instruct) to 8/30 using LLaMA-Berry.  This suggests that LLaMA-Berry can enhance LLM reasoning ability on difficult benchmarks without additional training, potentially reducing the need for extensive labeled data in complex mathematical problem-solving.  Follow-up questions:  1. How does the computational cost of SR-MCTS and PPRM with EBC scale with increasing model size and problem complexity, and what are the practical implications for deployment? 2.  What is the performance of LLaMA-Berry with different LLMs other than the ones mentioned in the ablation study, especially with larger parameter models and close-source ones? 3. Could the pairwise comparison approach of PPRM be adapted to other domains beyond mathematical reasoning, such as code generation or theorem proving, and what modifications would be required?  |
| MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2410.04698) or [HuggingFace](https://huggingface.co/papers/2410.04698))| cxiong, lunshi, hendrydong, yuhuixu, demolei | This research aims to evaluate the long-context mathematical reasoning abilities of LLMs.  The authors developed MATHHAY, an automated benchmark containing 673 mathematical reasoning questions across various topics and difficulty levels, paired with relevant and irrelevant documents forming "haystacks" of 32K-128K tokens.  Evaluation involved both exact match and LLM (GPT-40) judging. Gemini-1.5-Pro-002 achieved the highest overall performance, reaching only 51.26% accuracy at 128K tokens.  This result highlights the significant need for improvement in LLMs' long-context mathematical reasoning capabilities, which is crucial for real-world applications involving complex numerical analysis.  Follow-up questions:  1.  How does the performance of the LLM judge (GPT-40) compare across different question difficulty levels (single-step vs. multi-step) and document placements (First, Middle, Last)?   2.  What specific error analysis was performed to understand the types of mistakes LLMs made on MATHHAY, beyond overall accuracy?   3.  What are the specific criteria used by the GPT-40 LLM judge to determine the correctness of an answer when an exact match is not found?  |
| TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles (Read more on [arXiv](https://arxiv.org/abs/2410.05262) or [HuggingFace](https://huggingface.co/papers/2410.05262))| siminniu, fan2goa1, WinfredShi, Ki-Seki, Duguce | This research aimed to evaluate the reasoning abilities of Large Language Models (LLMs) in dynamic contexts.  The researchers created TurtleBench, a dataset of 1,532 yes/no questions derived from user interactions with an online "Turtle Soup Puzzle" game, and evaluated nine LLMs using 0-shot and 2-shot prompting.  Claude-3.5-Sonnet and GPT-40 achieved the highest overall accuracy, exceeding 87%, in the zero-shot setting. OpenAI's o1 series models performed significantly worse than expected. The paper suggests that relying solely on latent Chain-of-Thought, as observed in the o1 models, may not be sufficient for complex reasoning tasks and that excessive CoT length can introduce noise.   Follow-up questions:  1.  Given the observed performance disparity between OpenAI's o1 models and other leading LLMs like Claude-3.5-Sonnet and GPT-40 on TurtleBench, what specific architectural or training differences might contribute to this discrepancy?  2.  How does the dynamic nature of the TurtleBench dataset, with its real-time collection of user guesses, prevent data contamination and model cheating compared to static benchmarks, and how can this methodology be applied to other reasoning tasks beyond yes/no puzzles?  3.  The paper mentions a cost analysis for different LLMs, but what are the trade-offs in terms of cost and performance when choosing between commercially available LLMs (like Claude and GPT) versus open-source models (like Llama) for reasoning tasks, considering the findings of this research on TurtleBench?  |
| MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion (Read more on [arXiv](https://arxiv.org/abs/2410.03825) or [HuggingFace](https://huggingface.co/papers/2410.03825))| fcole, trevordarrell, hurjunhwa, irwinherrmann, Junyi42 | a) The research aims to directly estimate dynamic scene geometry from monocular video, addressing challenges in traditional multi-stage approaches.  b) The approach, Motion DUSt3R (MonST3R), adapts the DUSt3R pointmap representation for dynamic scenes by estimating per-timestep pointmaps and aligning them based on static scene elements. It leverages fine-tuning on a combination of synthetic and real-world datasets with depth and pose annotations and introduces optimizations for video-specific tasks like global point cloud alignment and confident static region identification.  c) On the Sintel dataset for video depth estimation, MonST3R achieves an absolute relative error of 0.335 and a percentage of inlier points (δ < 1.25) of 58.5%.  It demonstrates competitive performance on camera pose estimation and promising qualitative results for feed-forward 4D reconstruction.  The paper doesn't clearly define metrics used for 4D reconstruction.  d) MonST3R offers AI practitioners a faster, potentially more robust alternative to traditional optimization-based methods for estimating geometry from dynamic scenes. This is particularly relevant for applications like robotics, augmented reality, and 3D scene understanding.   Follow-up questions:  1.  The paper mentions challenges with handling dynamic camera intrinsics in practice despite the theoretical capability. Could the authors elaborate on the specific nature of these challenges and the manual constraints required? 2.  What are the specific quantitative metrics used to evaluate the 4D reconstruction results, and how does MonST3R compare against other state-of-the-art methods on these metrics? 3.  What are the computational requirements (memory and runtime) for applying MonST3R to longer videos and higher resolutions compared to the reported experiments?  |
| Autonomous Character-Scene Interaction Synthesis from Text Instruction (Read more on [arXiv](https://arxiv.org/abs/2410.03187) or [HuggingFace](https://huggingface.co/papers/2410.03187))| thuhsy, YixinChen, awfuact, milleret, jnnan | This research investigates synthesizing multi-stage human-scene interactions (HSIs) directly from text instructions and goal locations.  The authors propose a framework using an autoregressive diffusion model to generate motion segments, incorporating scene representations and a scheduler for autonomous stage transitions.  Quantitative results demonstrate improved motion synthesis over existing methods, achieving a 0.907 F1 score for interactive motion synthesis. The introduced LINGO dataset (16 hours of motion capture data in various indoor scenes) facilitates training models for complex, language-guided HSI generation.  This work provides a unified approach to HSI synthesis, enabling more realistic and autonomous character animation in 3D environments.  However, the paper does not fully describe the architecture of the autonomous scheduler, limiting a full understanding of its functionality.  Follow-up questions:  1.  Can you provide more details on the architecture and training process of the autonomous scheduler? 2.  How does the model handle ambiguous or poorly written text instructions?  What error handling mechanisms are in place? 3.  What are the limitations of the LINGO dataset, particularly regarding the diversity and realism of the interactions?  |
| Grounding Language in Multi-Perspective Referential Communication (Read more on [arXiv](https://arxiv.org/abs/2410.03959) or [HuggingFace](https://huggingface.co/papers/2410.03959))| alsuhr, mao1207, ZinengTang | This research investigates how differing visual perspectives affect the success of referential communication between embodied agents.  The authors created a dataset of human-written referring expressions in a 3D environment and evaluated various vision-language models as speakers and listeners, including GPT-40, LLaVA-1.5, Ferret, and Groma.  Fine-grained model Ferret achieved the highest accuracy in comprehending human-written referring expressions at 69.2%, but all models significantly underperformed compared to human-human communication (87.6% success rate).  Fine-tuning LLaVA-1.5 with a preference-based learning approach using data from interactions improved its performance to 69.3% communicative success with human listeners, surpassing GPT-40. This implies that learning from interaction data holds significant potential for enhancing referential communication models, even outperforming stronger pre-trained models.   Follow-up questions:  1. Could the preference-based learning approach be extended to incorporate multi-turn dialogue where clarification requests are allowed, and how would that impact performance? 2. How do the different referential strategies observed in human vs. model-generated expressions affect listener comprehension, and could explicitly training models on these strategies further improve performance? 3. How robust is the fine-tuned LLaVA-1.5 model to different 3D environments and object types not present in the ScanNet++ dataset used for training and evaluation?  |
