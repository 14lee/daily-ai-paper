

## Papers for 2024-10-03

| Title | Authors | Summary |
|-------|---------|---------|
| From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (Read more on [arXiv](https://arxiv.org/abs/2410.01215) or [HuggingFace](https://huggingface.co/papers/2410.01215))| Xiaodong Gu, Chengcheng Wan, Songsong Wang, YerbaPage | This research addresses the problem of low pass rates in LLM-generated code due to subtle errors.  The authors introduce MGDebugger, which uses a hierarchical, bottom-up debugging strategy, decomposing code into subfunctions and debugging them recursively with LLM-simulated execution and automatically generated test cases.  Experiments on HumanEval show MGDebugger improves accuracy by 17.7% over seed generations when using DeepSeek-Coder-V2-Lite (16B).  This implies that AI practitioners can significantly improve the correctness of LLM-generated code by adopting hierarchical debugging strategies rather than treating programs as monolithic units. The paper states MGDebugger achieves a 97.6% repair success rate on HumanEval-Fix using DeepSeek-Coder-V2-Lite (16B); however, it doesn't clarify the baseline repair success rate for this dataset/model combination, making it difficult to assess the relative improvement.  Follow-up questions:  1.  How does MGDebugger's performance compare to traditional symbolic execution or program analysis techniques for debugging, especially in terms of scalability and handling complex codebases? 2.  What are the computational resource requirements (e.g., memory, time) of MGDebugger compared to other LLM-based debugging methods, and how do they scale with code size and complexity? 3.  Could the hierarchical decomposition strategy be automated further, and what are the potential challenges in applying it to real-world codebases with complex dependencies and interactions between modules?  |
| Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis (Read more on [arXiv](https://arxiv.org/abs/2409.20059) or [HuggingFace](https://huggingface.co/papers/2409.20059))| nunonmg, PierreColombo, CelineH, emmanuelmalherbe, hgissbkh | a) This paper investigates the effects of preference-based alignment, particularly Contrastive Preference Optimization (CPO), on the quality of Large Language Model (LLM)-based translations.  b) The researchers conducted experiments fine-tuning an LLM translation model with CPO and Supervised Fine-Tuning (SFT), using various quality metrics (xCOMET-QE, CometKiwi, chrF) for alignment and evaluation, with both multi-system and mono-system candidate generation approaches.  c) CPO consistently outperformed SFT on high-quality data when aligning with neural metrics like xCOMET-QE, sometimes significantly increasing scores on the alignment metric (e.g.,  +2.75 for xCOMET-QE in en-xx translations with a multi-system approach). However, it also introduced adverse effects between neural and lexical metrics, and exhibited sensitivity to the chosen candidate systems.  d) AI practitioners aligning LLMs for translation should carefully consider the choice of candidate generation systems and potential trade-offs between optimizing neural versus lexical metrics when employing CPO.  The instability of CPO across different downstream metrics warrants caution.  The mono-system approach offers more control and may mitigate some of these issues while achieving comparable alignment effectiveness.  This improved control stems from being able to fine-tune the choice of candidate option quality with greater precision in the mono-system setting.   Follow-up questions:  1. How does the computational cost of generating multiple candidates in the mono-system approach compare to the cost of accessing and using multiple external systems in the multi-system approach?  2.  Could the instability of CPO be addressed by exploring different values for the Î² hyperparameter or by modifying the training procedure (e.g., different optimizers, learning rate schedules)?  3. What are the practical implications of the adverse metric effects between neural and lexical metrics for real-world translation applications, where both types of metrics are often considered important?  |
| LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks (Read more on [arXiv](https://arxiv.org/abs/2410.01744) or [HuggingFace](https://huggingface.co/papers/2410.01744))| Zhihan Zhang, Tianqing Fang, Mengzhao Jia, kaixinm, wyu1 | This research aimed to develop a multimodal large language model (MLLM) capable of handling text-rich, multi-image tasks.  The researchers curated a one-million-instance instruction-tuning dataset (LEOPARD-INSTRUCT) and implemented an adaptive high-resolution multi-image encoding module based on pixel shuffling.  LEOPARD-Idefics2, a variant trained on this dataset, outperformed the previous best-performing open-source MLLM on text-rich multi-image benchmarks by an average of 9.61 points.  This suggests that LEOPARD and its associated dataset are valuable resources for developing MLLMs specialized in complex, text-rich, multi-image scenarios.  The paper doesn't explicitly state the metric used for the +9.61 point improvement, though it does mention average normalized levenshtein similarity and accuracy in Table 3, making it difficult to understand precisely what this improvement represents.   Follow-up questions:  1. What specific metric (e.g., accuracy, F1-score, etc.) was used to calculate the +9.61 point improvement on the multi-image text-rich benchmarks, and on which specific subset of benchmarks was this average calculated? 2. What is the computational cost (e.g., GPU hours, FLOPs) of training LEOPARD compared to baseline models, and how does the adaptive high-resolution encoding module impact inference time? 3. Can the adaptive high-resolution encoding module be effectively applied to other visual encoders besides SigLIP-SO-400M, and are there plans to release the LEOPARD-INSTRUCT dataset publicly?  |
| ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2410.01731) or [HuggingFace](https://huggingface.co/papers/2410.01731))| galchechik, cohenor, yuvalalaluf, adihaviv, rinong | a) This research aims to improve text-to-image generation quality by automatically tailoring workflows to individual user prompts.  b) The authors propose two LLM-based approaches: ComfyGen-IC uses an LLM with a pre-computed table of flows and scores for prompt categories to select flows, while ComfyGen-FT fine-tunes an LLM to predict flows based on prompts and target scores.  Both leverage ComfyUI, representing workflows as JSON.  c) ComfyGen-FT outperforms baseline models and generic workflows on both human preference and prompt alignment benchmarks, achieving a 0.61 overall score on GenEval compared to 0.59 for the best baseline.  d)  This work indicates that AI practitioners can improve text-to-image generation quality by moving beyond fixed models or generic workflows and adopting prompt-adaptive workflow generation techniques.  Specifically, fine-tuning LLMs to predict workflows based on both prompts and target scores shows promise for enhanced performance.  Follow-up questions:  1. What are the computational costs and scalability challenges associated with training and deploying ComfyGen-FT, particularly for large datasets and complex workflows?  2. How does the performance of ComfyGen-FT vary across different LLM architectures and sizes, and what are the trade-offs between performance and computational resources?  3. Can the proposed framework be extended to other generative tasks beyond text-to-image generation, such as image editing or video generation, and what adaptations would be necessary?  |
| Not All LLM Reasoners Are Created Equal (Read more on [arXiv](https://arxiv.org/abs/2410.01748) or [HuggingFace](https://huggingface.co/papers/2410.01748))| Aaron Courville, Daniel Toyama, Alessandro Sordoni, agarwl, arianhosseini | This research investigates the depth of grade-school math (GSM) problem-solving and reasoning capabilities of LLMs.  The study evaluates LLM performance on Compositional GSM, a new dataset derived from GSM8K, requiring models to solve chained math problems where the answer to the first question is a variable in the second. Results reveal a significant reasoning gap, defined as the performance difference between solving compositional pairs and individual questions; for example, the smaller, more cost-efficient GPT-40 mini exhibits a 14.2% reasoning gap on compositional GSM despite high accuracy on GSM8K.  This implies that instruction-tuning, while effective for single-step problem-solving, does not necessarily translate to improved multi-hop reasoning, and high scores on standard benchmarks may mask deficiencies in compositional reasoning abilities, a critical insight for AI practitioners developing and applying such models.   Follow-up Questions:  1.  What specific modifications were made to the GSM8K problems to create the Compositional GSM dataset, and how might these modifications differentially impact various LLM architectures or training paradigms? 2.  Given the observed overfitting during finetuning on GSM8K, what alternative training strategies could be explored to improve compositional reasoning without sacrificing generalization performance on other tasks? 3. Could the study's findings about the reasoning gap in cost-efficient models be extrapolated to other problem domains beyond grade-school math, and if so, what are the implications for real-world AI applications where resource constraints are a major factor?  |
| 3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection (Read more on [arXiv](https://arxiv.org/abs/2410.01647) or [HuggingFace](https://huggingface.co/papers/2410.01647))| Dan Xu, Yuanliang, YangCaoCS | a) The paper aims to introduce 3D Gaussian Splatting (3DGS) for 3D object detection, addressing the challenges of ambiguous spatial distribution and excessive background blobs encountered when adapting 3DGS to this task.  b) The authors propose a novel method called 3DGS-DET, incorporating two key strategies: 2D Boundary Guidance, which utilizes object boundaries from posed images to train the 3DGS model, and Box-Focused Sampling, which constructs 3D object probability spaces based on 2D bounding boxes for probabilistic sampling of Gaussian blobs.  c) On the ScanNet dataset, 3DGS-DET achieves a mean Average Precision (mAP) of 59.9 at an Intersection over Union (IoU) threshold of 0.25, surpassing the baseline 3DGS pipeline by 5.6 points.  d) AI practitioners can leverage the proposed 3DGS-DET method to achieve improved performance in 3D object detection tasks by utilizing the explicit and efficient representation offered by 3DGS, enhanced with boundary and sampling strategies.  The paper specifically notes that other detectors can potentially use the enhanced 3DGS representations.  Follow-up questions:  1. Could the performance of 3DGS-DET be further improved by jointly training the 3DGS representation and the detection network, rather than training them sequentially?  2. How does the computational cost of Boundary Guidance and Box-Focused Sampling compare to other 3D object detection methods, particularly those based on point clouds or voxels?  3.  The paper mentions using CAGroup3D and FCAF3D as detectors.  Could the specific detector choice significantly impact the results observed?  Would other detectors trained on point clouds yield similar improvements from using the 3DGS representations?  |
| HelpSteer2-Preference: Complementing Ratings with Preferences (Read more on [arXiv](https://arxiv.org/abs/2410.01257) or [HuggingFace](https://huggingface.co/papers/2410.01257))| okuchaiev, gshennvm, trias702, odelalleau, alexwb | a) This paper investigates whether Bradley-Terry style or Regression style reward models are more effective for aligning language models to instructions, and explores combining both approaches.  b) The authors collect preference annotations and justifications alongside existing ratings in the HelpSteer2 dataset, enabling a head-to-head comparison of both reward modeling styles. They also experiment with a novel combined approach, initializing a Scaled Bradley-Terry model with a Helpfulness-Only SteerLM Regression model, and further refining it with ExPO.  c) The combined reward model (Scaled BT + EXPO) achieves 94.1% on RewardBench, outperforming over 140 other reward models as of October 1, 2024.  d)  AI practitioners can leverage this combined reward model and the HelpSteer2-Preference dataset for training more accurate reward models, especially for RLHF, and potentially improve the performance of language models at following instructions.  Follow-up questions:  1. How does the performance of the combined reward model (Scaled BT + EXPO) vary across different RewardBench categories (Chat, Chat-Hard, Safety, Reasoning), and what are the potential reasons for such variations?  2. What are the computational resource requirements (e.g., memory, FLOPs) for inference with the combined reward model compared to individual Bradley-Terry or Regression models?  3. What specific techniques were used for pre-processing the preference justifications, and how did those pre-processing steps impact the performance of Pairwise Justifier models?  |
| RATIONALYST: Pre-training Process-Supervision for Improving Reasoning (Read more on [arXiv](https://arxiv.org/abs/2410.01044) or [HuggingFace](https://huggingface.co/papers/2410.01044))| Guoxuan Wang, danyaljj, ChuyuLiu, ylu610, Dongwei | a) The research aims to improve the reasoning capabilities of Large Language Models (LLMs) by addressing the issue of incomplete reasoning chains with implicit rationales.  b) The proposed method, RATIONALYST, involves extracting implicit rationales from unlabeled text (The Pile) and reasoning datasets (GSM8K and ECQA), training a model to predict these rationales, and using the predicted rationales to provide process-supervision during LLM inference.  c) Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on seven representative reasoning benchmarks, including mathematical, commonsense, scientific, and logical reasoning datasets.  d) AI practitioners can use RATIONALYST to enhance the reasoning performance and interpretability of LLMs across various tasks by incorporating a process-supervision mechanism based on implicit rationales extracted from readily available unlabeled data.  The improved interpretability is particularly important for debugging and gaining deeper insights into LLM's reasoning process.    Follow-up Questions:  1. How does the performance of RATIONALYST scale with larger base LLMs (e.g., LLaMa-3-70B) or more powerful rationale extractors (e.g., GPT-4)? 2. What are the computational costs and infrastructure requirements associated with extracting and filtering rationales from large datasets like The Pile, and how can these be optimized? 3. Could RATIONALYST be adapted for specific domains or tasks by training it on a curated dataset of domain-specific rationales, and how would this impact its performance and generalizability?   |
| Quantifying Generalization Complexity for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.01769) or [HuggingFace](https://huggingface.co/papers/2410.01769))| maxtiktok, Nrain, zhuokai, Xulianghuang, luohy | This research investigates how task complexity and model size affect the generalization ability of Large Language Models (LLMs).  The study uses SCYLLA, a dynamic benchmark generating in-distribution and out-of-distribution data for 20 tasks across varying complexities.  Results reveal a "generalization valley," where the performance gap between in-distribution and out-of-distribution data is non-monotonic, peaking at a "critical complexity" that shifts rightward with increasing model size.  Specifically, LLaMA-3.1-405B achieved near-perfect generalization scores (0.997 and 0.996) on O(N) and O([N, NÂ²]) tasks, respectively. This suggests that scaling LLM size improves generalization, delaying but not eliminating over-reliance on memorization at higher task complexities.   Follow-up questions:  1. How does the specific distribution of OOD data generation in SCYLLA affect the observed generalization valley, and how would these results compare if alternative OOD sampling strategies were employed?  2. Given the implicit reasoning observed in models like o1-mini, what further analysis could be conducted to better understand and potentially leverage these capabilities in downstream tasks or model development?  3.  Could the performance of specialized LLMs (e.g., Qwen2.5-Math-7B) at higher complexities be improved by utilizing multi-stage prompting that decomposes complex tasks into sub-tasks within their expertise range?  |
| EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis (Read more on [arXiv](https://arxiv.org/abs/2410.01804) or [HuggingFace](https://huggingface.co/papers/2410.01804))| George Kopanas, Alexander Mai, xharlie, dorverbin, phedman | a) The research aims to develop a real-time, differentiable, emission-only volume rendering method that addresses the limitations of existing techniques like 3D Gaussian Splatting (3DGS), particularly "popping" artifacts.  b) The proposed method, Exact Volumetric Ellipsoid Rendering (EVER), represents the scene as a collection of constant-density ellipsoids and uses ray tracing to compute the volume rendering integral exactly.  This allows for the inclusion of effects like defocus blur and fisheye lens distortion.  c) EVER achieves a framerate of 30 FPS at 720p resolution on an NVIDIA RTX4090 on the challenging Zip-NeRF dataset and achieves a lower LPIPS score (0.368) compared to existing real-time methods like 3DGS (0.418) and StopThePop (0.411).  d) AI practitioners working on novel view synthesis can use EVER to generate high-quality, pop-free renderings in real-time, enabling applications that require fast and consistent 3D scene representations. The paper does not state the impact on memory usage, nor quantify inference time on hardware other than an NVIDIA RTX4090.  Follow-up questions:  1.  How does the memory footprint of EVER compare to 3DGS, particularly when scaling to even higher resolution or more complex scenes? 2.  Could the constant density assumption of EVER be relaxed to allow for more complex density variations within individual primitives, and how would that impact performance and quality? 3. What is the performance (FPS and quality metrics) of EVER on other commonly used GPUs, besides the NVIDIA RTX 4090 mentioned in the paper?  |
| E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding (Read more on [arXiv](https://arxiv.org/abs/2409.18111) or [HuggingFace](https://huggingface.co/papers/2409.18111))| Ying Shan, Yang Wu, Zhongang Qi, Zongyang Ma, Ye Liu | a) This research addresses the lack of fine-grained event-level and diverse task assessment in current video-language understanding benchmarks, aiming to create a more comprehensive evaluation for Video Large Language Models (Video-LLMs).  b)  The authors introduce E.T. Bench, a benchmark with 7.3K samples across 12 tasks and 8 domains, focusing on event-level and time-sensitive understanding of long videos. They also propose E.T. Chat, a novel Video-LLM using embedding matching for timestamp prediction, and E.T. Instruct 164K, a dedicated instruction-tuning dataset.  c) State-of-the-art Video-LLMs struggle with E.T. Bench, especially on grounding and dense captioning tasks, while E.T. Chat achieves state-of-the-art performance among open-source models, with a 38.4% Accref (averaged accuracy on referring tasks) on E.T. Bench.  d)  AI practitioners developing Video-LLMs should consider incorporating finer-grained temporal understanding and multi-event scenarios in training data and model design, prioritizing both spatial and temporal reasoning capabilities for improved performance on complex video understanding tasks. The paper notes potential data leakage in benchmark evaluation due to overlap with existing datasets used for model training, which might affect the validity of zero-shot evaluation.    Follow-up questions:  1.  Given the limitations of discrete token prediction for timestamps, what other alternative approaches besides embedding matching could be explored for improving temporal understanding in Video-LLMs?  2. How can the E.T. Bench benchmark be improved to mitigate the potential data leakage issue mentioned in the paper and ensure a more robust evaluation of Video-LLMs in zero-shot settings?  3. What specific architectural modifications in E.T. Chat contribute to its superior performance on grounding and dense captioning tasks compared to other state-of-the-art open-source Video-LLMs?  |
| Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling (Read more on [arXiv](https://arxiv.org/abs/2410.01440) or [HuggingFace](https://huggingface.co/papers/2410.01440))| Jiazhong Yu, Cao Sheng, Fei Li, feifeiobama, ljh0104 | a) The research aims to improve closed-loop long-horizon robotic planning in LLMs by addressing limitations like unidirectional dependency and lack of error correction.  b) The paper proposes "equilibrium sequence modeling," formulating self-refinement as a fixed-point problem solved through iterative refinement and utilizing a nested equilibrium solving process to incorporate environmental feedback efficiently.  An experience memory and world model complement the planner.  c) Evaluated on VirtualHome-Env, the method achieved a success rate improvement of up to 19% with error correction compared to not using error correction. It shows superior scaling for inference computation.  d) This provides AI practitioners a supervised learning approach to train self-refining LLM planners for robotics without needing complex reinforcement learning or process supervision, potentially leading to more robust and efficient long-horizon task completion.  Follow-up questions:  1. What are the specific architectural details of the world model used, and how does its performance compare to more complex world models that simulate environmental states rather than just feedback?  2. How does the proposed method's computational cost during training and inference scale with increasing model size and task complexity compared to alternative approaches like Tree-Planner or SELF-REFINE?  3.  The paper mentions failure scenarios like hallucination and lack of history awareness. What specific mitigation strategies, beyond the mentioned reasoning techniques, could be explored to address these limitations?  |
| HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration (Read more on [arXiv](https://arxiv.org/abs/2410.01723) or [HuggingFace](https://huggingface.co/papers/2410.01723))| Xinjie Zhang, Jing Liu, Ruihao Gong, Zining Wang, Yushi Huang | a) **Objective:** To accelerate the inference speed of Diffusion Transformers (DiTs) for image generation tasks by mitigating discrepancies between training and inference in learning-based feature caching methods.  b) **Methodology:** HarmoniCa framework, employing Step-Wise Denoising Training (SDT) to align training with the full denoising trajectory and Image Error Proxy-Guided Objective (IEPO) to incorporate final image error into training.  c) **Results:**  HarmoniCa achieved a 1.52x speedup and an FID of 27.61 for PIXART-Î± 256Ã256 with a 20-step DPM-Solver++, compared to an FID of 27.68 for the non-accelerated model.  d) **Implication:** AI practitioners can leverage HarmoniCa to significantly reduce inference latency in DiT models without substantial performance degradation, improving practical deployment for high-resolution image generation tasks.  This is particularly relevant to generative AI application developers.   **Follow-Up Questions:**  1. How does the performance of HarmoniCa scale with even larger DiT models and higher resolutions beyond those tested in the paper (e.g., greater than 2048x2048)? 2.  Could the proxy mechanism in IEPO be further refined to more accurately represent final image error, potentially leading to further performance gains? 3. What is the memory footprint of HarmoniCa during inference, and how does it compare to other acceleration techniques like pruning or quantization, particularly for resource-constrained environments?  |
| Selective Aggregation for Low-Rank Adaptation in Federated Learning (Read more on [arXiv](https://arxiv.org/abs/2410.01463) or [HuggingFace](https://huggingface.co/papers/2410.01463))| Huijie Fan, Liangqiong-QU, yanranw1, stevezs, gpx333 | a) This paper investigates how to effectively aggregate Low-Rank Adaptation (LoRA) matrices in Federated Learning (FL) for improved performance on downstream tasks.  b) The authors introduce Federated Share-A LoRA (FedSA-LORA), where both A and B matrices of the LoRA update are trainable during local training, but only the A matrices (responsible for general knowledge) are aggregated on the server. This method is then generalized to other LoRA variants (rsLoRA and VeRA).  c)  On the GLUE benchmark's RTE task with a severe non-IID data distribution, FedSA-LoRA achieved 90.20% accuracy, outperforming standard LORA (88.80%) and FFA-LoRA (88.83%).  d) AI practitioners can use FedSA-LoRA to efficiently fine-tune large language models in federated learning settings, especially with non-IID data, by reducing communication overhead and improving performance compared to existing methods. The impactful finding, that A matrices capture general knowledge while B matrices learn client-specific knowledge, allows for more targeted aggregation and better generalization across clients.   Follow-up questions:  1. How does the performance of FedSA-LoRA scale with the number of clients and the heterogeneity of the data distribution in more complex real-world scenarios beyond the presented experiments?  2.  What are the computational and memory overheads of FedSA-LoRA compared to other PEFT methods in federated settings, particularly for very large language models?  3.  How robust is FedSA-LoRA to malicious client behavior, and what mitigation strategies could be implemented to enhance its security in adversarial federated learning environments?  |
