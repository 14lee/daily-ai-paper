

## Papers for 2024-10-11

| Title | Authors | Summary |
|-------|---------|---------|
| MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code (Read more on [arXiv](https://arxiv.org/abs/2410.08196) or [HuggingFace](https://huggingface.co/papers/2410.08196))| juntingpan, shiwk20, Houxing, scikkk, AJZhou | a) This research aimed to improve large language models' (LLMs) mathematical reasoning abilities through continued pretraining on a dataset enriched with code and associated reasoning steps.  b) The researchers curated a 19.2B-token dataset, MathCode-Pile, consisting of math-related web data, code using mathematical packages, textbooks, synthetic data, and importantly, model-generated code with corresponding natural language reasoning steps extracted from mathematical texts.  LLMs were then pretrained on MathCode-Pile.  c) MathCoder2-Llama-3-8B, trained with MathCode-Pile, achieved 4-shot accuracies of 38.4% on MATH and 69.9% on GSM8K, demonstrating improvements of 17.0% and 15.1% respectively over the baseline Llama-3 model trained without MathCode-Pile's model-translated code and reasoning steps data.  d) AI practitioners can leverage MathCode-Pile and the method for generating code paired with reasoning steps to enhance the mathematical capabilities of LLMs, especially for tasks requiring tool-integrated reasoning.  The open-sourcing of the code and data facilitates reproducibility and further research.   Follow-up questions:  1. How does the performance of MathCoder2 compare to other state-of-the-art models on more complex mathematical reasoning tasks beyond the five benchmark datasets used in the study?  2. What are the computational resource requirements for pretraining with MathCode-Pile, and how scalable is the proposed method for larger model sizes or datasets?  3.  Could the performance improvement seen with the paired code and reasoning steps be further enhanced by different data generation strategies, such as incorporating diverse reasoning paths or error analysis?  |
| PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs (Read more on [arXiv](https://arxiv.org/abs/2410.05265) or [HuggingFace](https://huggingface.co/papers/2410.05265))| Yi Bin, Jiahao Wang, Yi Liu, wqshao126, ChenMnZ | a) The research aims to improve the efficiency of Large Language Model (LLM) quantization, specifically addressing the challenge of token-wise outliers that hinder per-tensor static quantization.  b) PrefixQuant prefixes high-frequency outlier tokens and the [BOS] token in the KV cache, thereby preventing their generation during inference and enabling effective per-tensor static quantization.  Block-wise fine-tuning is also used to further refine the quantization parameters.  c) On a W4A4KV4 (4-bit weight, activation, and KV cache) quantized Llama-3-8B model, PrefixQuant achieved a 7.43 WikiText2 perplexity and 71.08% average accuracy on five common-sense reasoning tasks, outperforming previous dynamic quantization methods.  d) AI practitioners can utilize PrefixQuant to achieve faster and more memory-efficient LLM deployment through its per-tensor static quantization approach, exceeding the performance of existing dynamic quantization techniques without retraining. The paper specifically highlights increased inference speeds compared to previous approaches.   Follow-up questions:  1. How does the performance of PrefixQuant scale with different model sizes and architectures beyond those tested in the paper?  2.  What are the specific memory savings achieved by PrefixQuant compared to dynamic quantization methods and FP16 models across different hardware platforms?  3. The paper mentions isolating outlier tokens improving training stability.  Are there quantitative measures of this increased stability (e.g., variance of loss during training), and how significant is this improvement compared to existing quantization-aware training methods?   |
| MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents (Read more on [arXiv](https://arxiv.org/abs/2410.03450) or [HuggingFace](https://huggingface.co/papers/2410.03450))| Zongqing Lu, Xinru Xu, tellarin, yuejunpengpku | a) This research aims to improve embodied agent performance by developing a more effective multimodal trajectory retriever that prioritizes task relevance over surface-level similarity.  b) The proposed method, MLLM As ReTriever (MART), uses interactive learning to fine-tune an MLLM retriever with preference pairs based on trajectory effectiveness, incorporating a Trajectory Abstraction mechanism to condense trajectory information.  c) In experiments across AI2-THOR and LEGENT environments, MART significantly outperformed baseline methods, achieving a 10% higher success rate on unseen tasks in AI2-THOR.  d) AI practitioners can leverage MART to improve embodied agent performance in unseen environments and complex, long-horizon tasks by fine-tuning an MLLM as a task-aware retriever rather than relying solely on similarity-based retrieval.   Follow-up questions:  1. How does the computational cost of fine-tuning the MLLM retriever with preference pairs scale with the size of the expert trajectory memory?  2.  Could the Trajectory Abstraction mechanism be further improved by incorporating reinforcement learning to dynamically select the most relevant milestones based on the current task and environment?  3. How robust is MART to noisy or incomplete trajectory data, and what strategies could be employed to mitigate the impact of such data on retriever performance?  |
| DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models (Read more on [arXiv](https://arxiv.org/abs/2410.08207) or [HuggingFace](https://huggingface.co/papers/2410.08207))| akashsri, FelixXu, quandao10, ligongh, AristHe | a) This paper addresses the challenge of controlled content editing in discrete diffusion models, including multinomial diffusion and masked generative models.  b) The authors introduce DICE (Discrete Inversion for Controllable Editing), a novel inversion algorithm that records noise sequences and masking patterns during the reverse diffusion process, enabling accurate reconstruction and flexible editing without predefined masks or attention manipulation.  c) Experiments on image and text modalities show DICE achieves superior performance; on the PIE-Bench dataset, DICE+Paella achieved a structure distance of 11.34×10⁻³, outperforming masked inpainting and continuous diffusion models.  d) DICE provides AI practitioners with a new technique for fine-grained manipulation of discrete data, such as text and image tokens, by enabling precise inversion and controlled editing with discrete diffusion models. The improved structural preservation and editing capabilities demonstrated by DICE on images and text represent a significant advancement for applications like text-guided image editing and sentiment modification in text.    Follow-up questions:  1.  How does the computational cost of DICE compare to existing methods like DDIM inversion or masked inpainting, particularly for high-resolution images or long text sequences?  2.  The paper mentions hyperparameters τ, λ₁, and λ₂. What is the impact of these hyperparameters on editing performance, and are there recommended strategies or guidelines for tuning them for different tasks and datasets?  3. Could DICE be extended or adapted to work with other types of discrete data beyond text and images, such as audio or time series data represented as discrete tokens?  |
| Benchmarking Agentic Workflow Generation (Read more on [arXiv](https://arxiv.org/abs/2410.07869) or [HuggingFace](https://huggingface.co/papers/2410.07869))| Ningyu, xiaoyuehanbin, consultantQ, Runnaning, GoooDte | a) This research introduces WORFBENCH, a benchmark for evaluating Large Language Model (LLM) agents' ability to generate workflows, addressing limitations in existing frameworks.  b)  WORFBENCH includes diverse scenarios, complex graph workflow structures, and a rigorous evaluation protocol called WORFEVAL based on subsequence and subgraph matching algorithms.  c)  Evaluation across various LLMs revealed a significant performance gap between linear and graph planning, with GPT-4 achieving only 52.47% on graph workflow generation.  d) For AI practitioners, this highlights the need to improve LLM agents' graph planning capabilities, potentially through integrating world knowledge or world models, as this significantly impacts their effectiveness in complex, real-world scenarios. The gap between sequence and graph planning capabilities emphasizes that current LLMs struggle with generating more complex, parallel workflows, even with strong language understanding.   Follow-up Questions:  1.  Could providing LLMs with explicit training data on graph structures, beyond simply relying on implicit learning from sequential data, improve graph workflow generation performance? 2. What specific strategies for integrating world knowledge or world models would be most effective in addressing the observed limitations in graph planning? 3. How can the insights from WORFBENCH be applied to improve the design and development of workflow-based LLM applications in specific domains like robotics or software automation?  |
| Agent S: An Open Agentic Framework that Uses Computers Like a Human (Read more on [arXiv](https://arxiv.org/abs/2410.08164) or [HuggingFace](https://huggingface.co/papers/2410.08164))| Shuyu Gan, Saaket Agashe, xw-eric, jc-y42, Jiuzhouh | a) The research aimed to develop an agentic framework enabling autonomous interaction with computers through a Graphical User Interface (GUI) to automate complex tasks.  b) Agent S integrates experience-augmented hierarchical planning, continual memory updates, and an Agent-Computer Interface (ACI) tailored for Multimodal Large Language Models (MLLMs).  c) On the OSWorld benchmark, Agent S achieved a 20.58% overall success rate, a substantial improvement over the baseline's 11.21% and a new state-of-the-art result.  d) AI practitioners can leverage Agent S to build GUI agents capable of complex task automation, particularly in "Daily" and "Professional" computer task categories, where significant performance gains were observed. The high success rate improvement directly impacts the feasibility of deploying autonomous GUI agents for practical applications.   Follow-up questions:  1. What are the specific primitive actions included in the constrained action space of the ACI, and how are they chosen to balance expressiveness and safety for MLLM-based GUI agents?  2.  Given the observed error analysis focusing on planning and grounding, what future work is planned to address these bottlenecks and further improve Agent S's reliability, specifically in terms of reducing repetitive actions caused by grounding errors?  3. How does the continual learning process adapt to evolving software interfaces or application updates, and what mechanisms ensure the ongoing relevance and effectiveness of the learned experiences stored in the narrative and episodic memories?  |
| Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow (Read more on [arXiv](https://arxiv.org/abs/2410.07303) or [HuggingFace](https://huggingface.co/papers/2410.07303))| Ling Yang, hsli-cuhk, Edify-Kd2024, DrinkingCoder, wangfuyun | a) The paper investigates the core factors contributing to the effectiveness of rectified flow for accelerating diffusion model generation and explores its generalization to broader diffusion model variants.  b) The authors propose Rectified Diffusion, which retrains a pre-trained diffusion model using pre-computed noise-sample pairs, eliminating the need for flow-matching and v-prediction used in rectified flow. They also introduce Rectified Diffusion (Phased), which enforces local first-order linearity of the ODE path within segmented time steps, and utilize consistency distillation for low-step generation enhancement.  c) Rectified Diffusion achieves a 1-step FID score of 27.26 on the COCO-2017 validation set compared to 47.91 for Rectified Flow, demonstrating faster training and superior performance.  d) AI practitioners can leverage Rectified Diffusion to simplify the training process and improve the performance of accelerated diffusion models without model conversion to flow-matching forms, potentially enabling faster and higher quality generation for various applications. The most impactful finding is that paired noise-sample retraining is the crucial element, not ODE path straightness, expanding the applicability of rectified diffusion to wider diffusion model types.   Follow-up questions:  1. How does the performance of Rectified Diffusion scale with different model architectures and datasets beyond Stable Diffusion and COCO?  2. What are the practical considerations and limitations when implementing the phased approach for real-world applications with varying computational constraints?  3. How does the choice of consistency distillation technique impact the final performance, and are there alternative distillation methods that could further improve low-step generation quality?  |
| Intriguing Properties of Large Language and Vision Models (Read more on [arXiv](https://arxiv.org/abs/2410.04751) or [HuggingFace](https://huggingface.co/papers/2410.04751))| Ho-Jin Choi, yechan99, mkmiracle, kobiso, passing2961 | This research investigates the perceptual and cognitive properties of Large Language and Vision Models (LLVMs), particularly how they process and interpret visual information. The study evaluates LLaVA-series models on 10 benchmarks, including MMVP, MathVista, and AI2D, using methods such as permutation of visual patch tokens, occlusion of image regions, and use of synthetic images.  Results show that LLVMs exhibit permutation invariance with minimal performance drop (e.g., <1% average drop for LLaVA 1.5 across 10 benchmarks after shuffling visual patch tokens) and robustness to occlusion, even solving some math problems with limited visual input. This implies that LLVMs process images globally rather than relying heavily on localized pixel information. For AI practitioners, this suggests that optimization efforts should focus on enhancing global image understanding and cross-modal alignment rather than solely on pixel-level processing.  Here are some follow-up questions an AI practitioner might ask:  1.  Given the observed permutation invariance, could architectural modifications that explicitly encourage local feature attention improve performance on tasks requiring detailed visual understanding, such as MMVP or fine-grained image classification? 2.  How can the observed trade-off between complex cognitive reasoning abilities and basic visual recognition capabilities (catastrophic forgetting) be mitigated during the fine-tuning process of LLVMs? 3.  How can we design more complex and interactive evaluation benchmarks to better assess the performance and generalization capabilities of LLVMs in real-world scenarios that necessitate multi-turn interactions and personalized responses?  |
| Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2410.06508) or [HuggingFace](https://huggingface.co/papers/2410.06508))| Ye Tian, haitaominlp, Pluie1503, freesunshine0316, russwang | a) This research aims to improve the reasoning capabilities of Large Language Models (LLMs) by more effectively distilling behaviors learned through Monte Carlo Tree Search (MCTS).  b) The proposed ALPHALLM-CPL framework uses stepwise trajectory pair extraction from MCTS and curriculum preference learning (CPL) to train LLMs.  CPL dynamically adjusts the training sequence of trajectory pairs, prioritizing those most critical for learning.  c) On the GSM8K benchmark, ALPHALLM-CPL improved the performance of LLaMA2-7B from 14.6 to 36.5, a 150% increase.  d) AI practitioners can leverage ALPHALLM-CPL to significantly enhance the mathematical reasoning abilities of LLMs using MCTS without needing extensive external data or stronger models, offering a path toward more autonomous LLM improvement.  Follow-up questions:  1. What is the computational cost of generating the stepwise trajectory pairs and implementing the curriculum preference learning compared to existing MCTS distillation methods?  2. How does the performance of ALPHALLM-CPL vary with different values of the margin  'τ' and balance rate 'α' used in trajectory pair extraction and curriculum preference learning, respectively?  What guidelines are there for tuning these hyperparameters?  |
| Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality (Read more on [arXiv](https://arxiv.org/abs/2410.05210) or [HuggingFace](https://huggingface.co/papers/2410.05210))| Junmo Kim, In So Kweon, Dong-Jin Kim, Jae Won Cho, ytaek-oh | This research aimed to improve the compositional reasoning of Vision-Language Models (VLMs) while maintaining their performance on standard multi-modal tasks.  The researchers developed Fine-grained Selective Calibrated CLIP (FSC-CLIP), which incorporates local hard negative loss based on patch-token alignments and selective calibrated regularization to mitigate the negative impact of hard negative training. FSC-CLIP, when fine-tuned on a 100K subset of LAION-COCO, achieved a compositionality score of 53.5 and a zero-shot classification score of 55.9, nearly matching the pre-trained CLIP's zero-shot performance. This suggests that FSC-CLIP allows for significant improvements in compositional reasoning without sacrificing performance on other crucial VLM tasks, offering a more balanced and robust model for AI practitioners.  It is unclear if this method extends beyond fine-tuning to pre-training, or whether it is directly applicable to other similar architectures or models besides CLIP.  Follow-up questions:  1.  How does the computational cost of FSC-CLIP during training and inference compare to existing fine-tuning methods like DAC-LLM or NegCLIP, especially with larger datasets and models? 2. Could the authors elaborate on the limitations of using short captions, and provide concrete examples of the complex contextual nuances and longer-range dependencies in detailed descriptions that current VLMs struggle with?  What future research directions are suggested for addressing these challenges?  |
| SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe (Read more on [arXiv](https://arxiv.org/abs/2410.05248) or [HuggingFace](https://huggingface.co/papers/2410.05248))| Sanqiang Zhao, Marzyeh Ghassemi, wzhouad, szhang42, YuxinXiao | This paper investigates improving large language model (LLM) instruction-tuning performance without relying on curated datasets.  The authors propose SFTMix, which leverages training dynamics to split a dataset into confident and unconfident subsets and applies a Mixup-based regularization during instruction tuning.  Results on MT-Bench and AlpacaEval-2 show that SFTMix outperforms the next-token prediction (NTP) baseline, with Llama-3.1-8B achieving a 4.5825 overall score on MT-Bench with SFTMix versus 4.3625 with NTP.  This implies that AI practitioners can potentially improve LLM instruction-tuning performance and generalization on downstream tasks by incorporating the SFTMix recipe without requiring costly dataset curation.  The paper does not specify the precise algorithm for assigning data points to confident/unconfident splits based on the perplexity calculations.  Follow-up questions:  1.  What is the specific algorithm used to assign data points to the "confident" and "unconfident" subsets based on the calculated Conf(Vᵢ|Xᵢ) values?  Is it a simple threshold, or a more complex clustering approach? 2.  How does the computational cost of calculating the training dynamics and performing the Mixup regularization compare to the computational savings from using less curated data?  Is there a net benefit in terms of resource usage? 3.  How does SFTMix perform with very large LLMs and datasets where calculating perplexity over the entire training set for multiple checkpoints becomes significantly more expensive?  Are there strategies for efficient approximation or scaling in such scenarios?  |
| Progressive Autoregressive Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2410.08151) or [HuggingFace](https://huggingface.co/papers/2410.08151))| Hao Tan, Zhan Xu, smebliu, YicongHong, desaix | a) The research aims to extend the temporal capacity of video diffusion models, which are currently limited to short video generation due to computational constraints during training.  b) The authors propose progressive autoregressive video diffusion models, assigning progressively increasing noise levels to latent frames within the attention window during denoising, enabling autoregressive generation of extended video sequences.  This method involves finetuning existing video diffusion models on a modified noise schedule and applying a specific autoregressive sampling procedure.  c)  On a long video generation task (60 seconds, 1440 frames), their best performing model (PA-M) achieved an average dynamic degree score of 0.8, substantially outperforming other baselines while maintaining competitive scores on other metrics like aesthetic and imaging quality.  It is unclear how the number of training steps differed between PA-M and other models.  d) AI practitioners can leverage this progressive denoising technique to generate significantly longer, high-quality videos using existing video diffusion model architectures, potentially reducing the need for computationally expensive training of entirely new long-video models.  The paper implies this progressive denoising method can be applied to different video diffusion architectures, but only demonstrates it on transformer-based architectures.  Follow-up questions:  1.  Could the performance gains of progressive autoregressive denoising be further enhanced by exploring alternative noise scheduling strategies beyond the linear schedule used in this research?  2.  How does the computational cost of finetuning a pre-trained video diffusion model with progressive noise levels compare to the computational cost of training a new model specifically designed for long-video generation?  3. The paper mentions chunk-by-chunk processing as being crucial. How does chunk size impact long-video generation quality and computational cost, and is there an optimal chunk size for different model architectures?  |
| GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2410.06154) or [HuggingFace](https://huggingface.co/papers/2410.06154))| aquila147, mdorkenw, paulgavrikov, sivand, kevinmzy | This research explores using Large Language Models (LLMs) to optimize prompts for Vision-Language Models (VLMs), aiming to improve VLM performance on downstream vision tasks like image classification.  The key methodology, GLOV, involves a meta-prompting LLM with task descriptions and ranked in-context examples, coupled with embedding space guidance to steer prompt generation.  Results show GLOV improves zero-shot CLIP accuracy on ImageNet by up to 15.0% and LLaVa accuracy by up to 57.5%.  This implies AI practitioners can leverage LLMs to automatically discover highly effective prompts for VLMs, significantly boosting performance without gradient-based training or fine-tuning.  Follow-up questions:  1. What are the computational resource requirements (e.g., GPU memory, runtime) for running GLOV, especially with larger datasets and VLMs? 2. How sensitive is GLOV's performance to the choice of LLM and its hyperparameters (e.g., number of optimization steps, guidance scaling factor)? 3. How does the performance of GLOV-generated prompts compare to fine-tuning VLMs on downstream tasks in few-shot settings?  |
| Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System (Read more on [arXiv](https://arxiv.org/abs/2410.08115) or [HuggingFace](https://huggingface.co/papers/2410.08115))| Cheng Yang, Chen Qian, Jiarui Yuan, zibuyu9, weizechen | a) The research aimed to develop a training framework for Large Language Model (LLM)-based Multi-Agent Systems (MAS) that enhances communication efficiency and task effectiveness.  b) OPTIMA, the proposed framework, uses an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability, incorporating techniques like Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Monte Carlo Tree Search (MCTS).  c) OPTIMA achieved up to a 2.8x performance gain with less than 10% of the tokens compared to Multi-Agent Debate (MAD) on tasks requiring heavy information exchange.  d) OPTIMA enables more efficient use of inference compute, potentially leading to better inference-time scaling laws, which AI practitioners can leverage for performance gains without additional model training.  OPTIMA's demonstrated ability to significantly reduce token usage while improving performance is directly applicable to improving the computational efficiency of deployed LLM-based MAS.  Follow-up questions:  1. How does OPTIMA's MCTS-inspired DPO data generation compare to alternative data generation methods for multi-agent DPO in terms of computational cost and resulting data quality?  2.  Could the observed improvements in inference scaling laws be further amplified by combining OPTIMA with more advanced answer aggregation techniques like weighted voting?  3. What are the limitations of OPTIMA's current implementation, and what future research directions could address these limitations (e.g., scaling to larger models, more complex multi-agent scenarios)?  |
| Emergent properties with repeated examples (Read more on [arXiv](https://arxiv.org/abs/2410.07041) or [HuggingFace](https://huggingface.co/papers/2410.07041))| François Charton, Knykny | a) The research investigates the impact of training example repetition on transformer performance in mathematical tasks, challenging the prevailing assumption that maximizing distinct training examples is always optimal. b) The study uses algorithmically generated datasets for greatest common divisor (GCD), modular multiplication, and matrix eigenvalue calculation, controlling repetition frequency and employing two-set training (repeating a random subset more frequently). c) For GCD, with a training budget of 600 million examples and a data budget of 100 million, two-set training with a repeated subset of 50,000 examples (repeated 3000 times) achieved 69 correctly predicted GCDs, outperforming single-set training which achieved 27.  d)  AI practitioners should consider training set size (distinct examples) as a hyperparameter and explore the potential of two-set training, where repeating a small random subset more frequently can improve performance and learning speed.  The paper lacks information on the computational costs of two-set training compared to standard practices.   Follow-up questions:  1. How does the computational cost of two-set training, including storage and processing overhead from increased repetition, compare to standard single-epoch training with a larger dataset?  2. How does two-set training perform in comparison to curriculum learning approaches using specifically curated example subsets for repetition?  3. What is the relationship between the optimal repetition frequency and dataset characteristics like size and task complexity in a two-set training paradigm?  |
| Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations (Read more on [arXiv](https://arxiv.org/abs/2410.08049) or [HuggingFace](https://huggingface.co/papers/2410.08049))| xyyue, DingXiaoH, Yiyuan | This paper investigates whether large-kernel ConvNets can offer universal modeling capabilities similar to Vision Transformers (ViTs) with reduced complexity.  The authors propose UniRepLKNet, a novel ConvNet architecture based on a set of design principles for large kernels, emphasizing depth-wise convolutions, identity shortcuts, and dilated small kernel re-parameterization.  UniRepLKNet achieves 88.0% ImageNet top-1 accuracy and demonstrates strong performance across modalities like audio (98.5% accuracy on Speech Commands V2), video, and time-series forecasting. This suggests that large-kernel ConvNets provide a viable, efficient alternative to transformers for diverse AI tasks.  Follow-up questions:  1.  The paper mentions modality-specific preprocessing to transform data into 3D embedding maps.  Could the authors elaborate on the specific preprocessing steps used for each modality beyond the brief descriptions provided?  This information would be crucial for replicating the results and applying the architecture to new modalities. 2.  What are the memory and computational requirements of UniRepLKNet compared to ViTs and other state-of-the-art models on downstream tasks beyond ImageNet classification?  More detailed comparisons would help assess the practical advantages of UniRepLKNet for resource-constrained applications. 3. How does the performance of UniRepLKNet change with varying kernel sizes in different stages, and what guidelines can be derived for selecting optimal kernel sizes based on specific task characteristics?  Deeper analysis of kernel size influence could lead to more fine-grained architectural optimization.  |
| MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2410.07707) or [HuggingFace](https://huggingface.co/papers/2410.07707))| ztz1989, jiahao97, Free1unch, Rosetta-Leong, RuijieZhu | a) The paper aims to improve dynamic scene reconstruction quality and robustness by incorporating explicit motion priors into deformable 3D Gaussian Splatting (3DGS).  b) MotionGS, the proposed framework, decouples optical flow into camera and motion flow, using the latter to guide 3D Gaussian deformation. It also incorporates a camera pose refinement module that alternately optimizes 3D Gaussians and camera poses.  c)  On the NeRF-DS dataset, MotionGS achieves a mean PSNR of 24.54, outperforming the baseline method (Deformable 3DGS) which achieved 23.61.  d) AI practitioners can use MotionGS to reconstruct dynamic scenes from monocular video with improved quality and robustness compared to existing deformable 3DGS methods, especially in scenarios involving complex or rapid motion.  The CUDA-based implementation of the Gaussian flow and camera pose optimization allows for efficient training and rendering.   Follow-up questions:  1. Could the optical flow decoupling module be adapted or improved for scenes where segmentation masks for dynamic objects are not readily available or easily obtained?  2.  How does the computational cost of the motion flow extraction and camera pose refinement impact real-time rendering performance, and what are the potential optimization strategies to mitigate this?  3. How sensitive is MotionGS to the accuracy of the initial camera poses provided by COLMAP, and are there alternative initialization strategies that could further improve robustness in challenging scenarios?  |
