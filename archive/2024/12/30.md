

## Papers for 2024-12-30

| Title | Authors | Summary |
|-------|---------|---------|
| HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs (Read more on [arXiv](https://arxiv.org/abs/2412.18925) or [HuggingFace](https://huggingface.co/papers/2412.18925))| Wanlong Liu, Xidong Wang, Ke Ji, Zhenyang Cai, Junying Chen | Here is a concise summary of the research paper "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs":  i)  The paper introduces HuatuoGPT-o1, a medical large language model (LLM) designed to enhance complex reasoning in the medical domain using verifiable medical problems and a two-stage training approach. ii)  The main research objective is to develop an LLM capable of performing complex medical reasoning verifiable through objective ground-truth answers. iii) The key methodology involves a two-stage approach: (1) using a verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, and (2) applying reinforcement learning (RL) with verifier-based rewards to enhance reasoning. iv) The primary result is that the 70B parameter version of HuatuoGPT-o1 outperformed other open-source general and medical-specific LLMs across multiple medical benchmarks, achieving an average score of 73.4. v) The principal implication for AI practitioners is that using verifiable problems and a two-stage training process (fine-tuning with complex reasoning trajectories followed by RL with verifier feedback) can significantly enhance the complex reasoning abilities of LLMs in specialized domains like medicine.  |
| Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models (Read more on [arXiv](https://arxiv.org/abs/2412.18605) or [HuggingFace](https://huggingface.co/papers/2412.18605))| Hengshuang Zhao, Chao Du, Tianyu Pang, Ziang Zhang, Zehan Wang | Here is a concise summary of the research paper "Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models":  **i) Summary:** This paper introduces Orient Anything, a novel model for estimating the 3D orientation of objects in single- and free-view images by learning from a dataset of rendered 3D models.  **ii) Main research question or objective:** How can a robust and generalizable model be developed to accurately estimate object orientation in images, overcoming the scarcity of labeled training data?  **iii) Key methodology:** A pipeline was developed to annotate the front face of 3D objects and render 2 million images from random views; the model is trained to predict 3D orientation by fitting probability distributions of three angles, incorporating strategies for synthetic-to-real transfer.  **iv) Primary results:** Orient Anything achieves state-of-the-art accuracy in orientation estimation on both rendered and real images; specifically, it achieved 73.94% accuracy in predicting the azimuth of objects in rendered images.  **v) Principal implication for AI practitioners:** AI practitioners can leverage Orient Anything as a foundational tool for tasks requiring accurate object orientation estimation, such as enhancing spatial reasoning in vision-language models and improving the generation of images with specific object poses.  |
| Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment (Read more on [arXiv](https://arxiv.org/abs/2412.19326) or [HuggingFace](https://huggingface.co/papers/2412.19326))| Kunchang Li, Chenting Wang, Yinan He, Zhilin Li, Ziang Yan | Here is a concise summary of the research paper "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment":  i) This paper introduces Task Preference Optimization (TPO), a novel method to enhance multimodal large language models (MLLMs) by aligning them with fine-grained visual tasks. ii) The main research objective is to improve MLLMs' fine-grained visual understanding and performance on specific visual tasks without compromising their general multimodal capabilities. iii) The key methodology is the use of differentiable task preferences derived from visual tasks, learnable task tokens, and multi-task co-training of task-specific heads with the MLLM. iv) The primary result is that TPO improves the performance of VideoChat and LLaVA on multimodal benchmarks, achieving an overall 14.6% improvement in multimodal performance compared to baseline models. v) For AI practitioners, TPO provides a scalable method to enhance MLLMs with specialized visual perception skills, enabling the development of more robust and versatile multimodal AI systems.  |
| The Superposition of Diffusion Models Using the Itô Density Estimator (Read more on [arXiv](https://arxiv.org/abs/2412.17762) or [HuggingFace](https://huggingface.co/papers/2412.17762))| Kirill Neklyudov, Alexander Tong, Avishek Joey Bose, Lazar Atanackovic, Marta Skreta | Here is a concise summary of the AI research paper:  i) **Summary:** The paper introduces SUPERDIFF, a novel framework for combining pre-trained diffusion models during inference using a scalable Itô density estimator.  ii) **Main research question/objective:** Can multiple pre-trained diffusion models be combined solely at inference in a theoretically sound and efficient manner?  iii) **Key methodology:** SUPERDIFF leverages a new Itô density estimator for the log-likelihood of the diffusion SDE to enable superposition, combining models through an automated re-weighting scheme during inference.  iv) **Primary results:** SUPERDIFF outperforms individual models on CIFAR-10, with a Feature Likelihood Divergence (FLD) of 5.33 ± 0.05 compared to 7.51 ± 0.11 for the best single model, and enables effective prompt-based image editing and de novo protein structure design.  v) **Principal implication for AI practitioners:** AI practitioners can use SUPERDIFF to combine multiple pre-trained diffusion models without retraining, enabling efficient generation, improved performance, and novel applications like concept interpolation and protein design.  |
| From Elements to Design: A Layered Approach for Automatic Graphic Design Composition (Read more on [arXiv](https://arxiv.org/abs/2412.19712) or [HuggingFace](https://huggingface.co/papers/2412.19712))| Ji Li, Ting Liu, Danqing Huang, Shizhao Sun, Jiawei Lin | Here's a concise summary of the research paper:  i) **Summary:** This paper introduces LaDeCo, a novel framework for automatic graphic design composition from multimodal elements using a layered approach. ii) **Main research question/objective:** How to automatically compose multimodal graphic elements into a cohesive and aesthetically pleasing design. iii) **Key methodology:** LaDeCo employs a layer planning module using GPT-4o to categorize elements and a layered design composition process that uses fine-tuned Large Multimodal Models (LMMs) to predict element attributes layer-by-layer, incorporating rendered images of previous layers as context. iv) **Primary results:** LaDeCo significantly outperforms baseline models in design composition, achieving an overall LLaVA-OV score of 8.08 compared to 5.34 for FlexDM and 6.53 for GPT-4o on the design composition task. v) **Principal implication for AI practitioners:** AI practitioners can leverage LaDeCo's layered approach and LMMs to build more effective and efficient automatic graphic design systems, enabling applications such as resolution adjustment, element filling, and design variation.  |
| Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging (Read more on [arXiv](https://arxiv.org/abs/2412.19512) or [HuggingFace](https://huggingface.co/papers/2412.19512))| Shang-Tse Chen, Saurav Sahay, Shachi H Kumar, Hsuan Su, farnhua | Here is a concise summary of the research paper, strictly following your guidelines:  i)  This paper proposes a method to mitigate safety degradation in fine-tuned large language models (LLMs) by merging the weights of pre- and post-fine-tuned models. ii) The main research question is how to improve downstream task performance while preserving safety in LLMs without relying on additional safety data. iii) The key methodology used is a two-step approach: fine-tuning the base model on a downstream task, then merging the base model with the fine-tuned model via weight interpolation. iv) The primary result shows that merging the models significantly reduces the Attack Success Rate (ASR) across various downstream tasks; for instance, on the medical assistance task, the ASR is reduced by over 30%. v) For AI practitioners, this method offers a practical solution for adapting safety-aligned LLMs to downstream tasks while preserving their inherent safety features without requiring additional safety data.  |
| SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images (Read more on [arXiv](https://arxiv.org/abs/2412.17606) or [HuggingFace](https://huggingface.co/papers/2412.17606))| Yoshitaka Ushiku, Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, Risa Shinoda | Here's a concise summary of the research paper "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images," strictly adhering to your guidelines:  i)  **Summary:** The paper introduces SBS Figures, a synthetic dataset for pre-training figure-based question-answering models, generated through a novel stage-by-stage pipeline. ii) **Main research question/objective:** The main objective is to develop a method for creating a large-scale, diverse, synthetic figure QA dataset to improve the performance of figure QA models. iii) **Key methodology:** A three-stage pipeline was used: (1) generate visualization target data, (2) render figures via Python code, and (3) generate QA pairs using LLMs, all progressively transforming seed data. iv) **Primary results:** Pre-training with SBS Figures improved the average accuracy on the ChartQA dataset by 6.42 points for the Pix2Struct model. v) **Principal implication for AI practitioners:** AI practitioners can use the SBS Figures dataset and pipeline to pre-train and fine-tune their models, enhancing performance on figure QA tasks without the need for manual annotation.  |
| VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2412.19645) or [HuggingFace](https://huggingface.co/papers/2412.19645))| Junfu Pu, Zhongang Qi, Xiaodong Cun, Yong Zhang, Tao Wu | Here is a concise summary of the research paper "VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models":  i) **Summary:** VideoMaker is a framework for zero-shot customized video generation that leverages the inherent capabilities of video diffusion models (VDMs) for subject feature extraction and injection without requiring additional modules. ii) **Main research question/objective:** Can VDMs be utilized to extract and inject subject features for customized video generation without the need for external modules or extensive retraining? iii) **Key methodology:** The method uses the VDM itself to extract fine-grained subject features from a reference image and injects these features using a modified spatial self-attention mechanism within the VDM, along with a Guidance Information Recognition Loss. iv) **Primary results:** VideoMaker outperformed existing methods in customized human video generation, achieving a Face Similarity score of 0.8047 compared to the next best result of 0.7323 from ID-Animator. v) **Principal implication for AI practitioners:** AI practitioners can achieve high-quality, zero-shot customized video generation by fine-tuning the pre-trained VDM to activate the inherent force of video diffusion model, offering a more efficient alternative to existing methods that rely on external modules.  |
