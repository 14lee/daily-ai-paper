

## Papers for 2024-12-02

| Title | Authors | Summary |
|-------|---------|---------|
| On Domain-Specific Post-Training for Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.19930) or [HuggingFace](https://huggingface.co/papers/2411.19930))| Xintong Zhang, doubling, edward2021, buaahsh, daixuancheng | This paper investigates domain-specific post-training for adapting general Multimodal Large Language Models (MLLMs) to specialized domains like biomedicine and food.  The research aims to improve MLLM performance in specific domains through data synthesis and a novel single-stage training pipeline.  A visual instruction synthesizer generates domain-specific tasks from image-caption pairs, filtered by a consistency check, and used for single-stage training alongside image captioning data.  AdaMLLM, the resulting adapted MLLM, outperformed general MLLMs across various domain-specific tasks, with a 58.3% average performance on biomedical tasks using PMC-Raw image-caption data and single-stage training. This research provides AI practitioners with a method for efficiently adapting pre-trained MLLMs to specialized domains using readily available image-caption datasets, enabling enhanced performance on domain-specific downstream tasks.  |
| Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS (Read more on [arXiv](https://arxiv.org/abs/2411.18478) or [HuggingFace](https://huggingface.co/papers/2411.18478))| Zengqi Wen, Feihu Che, Shuai Zhang, fmk345, Jinyang23 | HiAR-ICL enhances in-context learning for complex reasoning tasks by focusing on high-level thinking patterns rather than specific examples.  The research aims to improve LLM performance on complex reasoning tasks by shifting from example-based in-context learning to a paradigm based on abstract thinking patterns.  The core methodology uses Monte Carlo Tree Search (MCTS) to explore reasoning paths and construct “thought cards” representing these patterns, which are then selected based on a cognitive complexity metric.  HiAR-ICL achieves 79.6% accuracy on the MATH benchmark using Qwen2.5-7B-Instruct, outperforming GPT-40 (76.6%) and Claude 3.5 (71.1%). This implies AI practitioners can leverage high-level reasoning patterns and MCTS to enhance the performance and generalization of LLMs, especially smaller models, on complex reasoning tasks without extensive demonstration engineering.  |
| Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2411.19108) or [HuggingFace](https://huggingface.co/papers/2411.19108))| MoonQiu, weilllllls, Jeff-Wang, StevenZhang, LiewFeng | TeaCache accelerates video diffusion model inference by selectively caching intermediate model outputs.  The research aimed to improve the inference speed of diffusion-based video generation models without compromising visual quality.  The method estimates output differences using timestep embedding modulated noisy inputs and a rescaling strategy based on polynomial fitting to determine caching schedules.  Experiments showed up to a 4.41x speedup on Open-Sora-Plan with a negligible -0.07% VBench score degradation. This training-free caching strategy offers AI practitioners a way to substantially reduce the computational cost of deploying state-of-the-art video diffusion models.  |
| DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding (Read more on [arXiv](https://arxiv.org/abs/2411.19527) or [HuggingFace](https://huggingface.co/papers/2411.19527))| Mingu Kang, Minseo Kim, Jisoo Kim, junwann, whwjdqls99 | DisCoRD decodes discrete motion tokens into continuous motion using rectified flow to enhance naturalness while preserving faithfulness to conditioning signals.  The research aimed to address the limitations of existing discrete and continuous human motion generation methods, specifically under-reconstruction and frame-wise noise in discrete methods, and cross-modal mapping ambiguity in continuous methods.  The core methodology involves training a rectified flow model conditioned on frame-wise features extracted from discrete motion tokens, enabling iterative refinement in continuous space.  On HumanML3D, DisCoRD achieved a Fréchet Inception Distance (FID) of 0.032, surpassing existing discrete methods in naturalness.  This provides AI practitioners with a method to generate more realistic and faithful human motion from discrete representations, applicable to various motion generation tasks such as text-to-motion and music-to-dance generation.  |
| Puzzle: Distillation-Based NAS for Inference-Optimized LLMs (Read more on [arXiv](https://arxiv.org/abs/2411.19146) or [HuggingFace](https://huggingface.co/papers/2411.19146))| nav4, nailon-nvidia, talor-abr, tomer-nv, abercovich | Puzzle is a framework for accelerating LLM inference on specific hardware while preserving model capabilities.  The research aimed to optimize large language model architectures for efficient inference on specific hardware while maintaining accuracy. The methodology involved decomposed neural architecture search (NAS) using blockwise local knowledge distillation (BLD), mixed-integer programming for constraint optimization, and global knowledge distillation (GKD). The derived model, Nemotron-51B, achieved a 2.17x inference throughput speedup on a single NVIDIA H100 GPU compared to its parent model, Llama-3.1-70B-Instruct, while preserving 98.4% of its capabilities. This provides AI practitioners with access to state-of-the-art language models optimized for efficient deployment with minimal accuracy trade-offs, enabling wider adoption across various applications and hardware.  |
| Trajectory Attention for Fine-grained Video Motion Control (Read more on [arXiv](https://arxiv.org/abs/2411.19324) or [HuggingFace](https://huggingface.co/papers/2411.19324))| Xingang-Pan, Jianlou, PKUWilliamYang, Vicky0522, zeqixiao | This paper introduces trajectory attention for precise camera motion control in video generation.  The research aims to improve the precision and consistency of camera motion control in generated videos, addressing limitations of existing methods that struggle with temporal coherence or rely on implicit control mechanisms. The core methodology involves modeling trajectory attention as an auxiliary branch alongside traditional temporal attention in video diffusion models, allowing explicit injection of trajectory information while maintaining the model's generative capabilities.  Experiments on camera motion control for images show the method achieves an Absolute Trajectory Error (ATE) of 0.0396 meters on 25-frame sequences.  This provides AI practitioners with a plug-and-play module for enhanced camera motion control in video diffusion models, improving the precision and consistency of generated video motion, particularly valuable for tasks requiring fine-grained control over camera movement.  |
| Video Depth without Video Models (Read more on [arXiv](https://arxiv.org/abs/2411.19189) or [HuggingFace](https://huggingface.co/papers/2411.19189))| toshas, PeterTor, peterjohnson, dnarnhofer, Bingxin | RollingDepth estimates temporally consistent video depth using a modified single-image latent diffusion model (LDM).  The research aimed to develop accurate and temporally stable video depth estimation without computationally expensive video diffusion models. The key methodology involved adapting a single-image LDM (Marigold) to process short video snippets, incorporating cross-frame self-attention and a robust, optimization-based global alignment algorithm. RollingDepth achieved a 9.6% absolute mean relative error on the PointOdyssey dataset, outperforming existing video and single-image depth models.  This implies that AI practitioners can leverage modified single-image LDMs for efficient and accurate video depth estimation, avoiding the computational burden of dedicated video models.  |
| AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos (Read more on [arXiv](https://arxiv.org/abs/2411.19950) or [HuggingFace](https://huggingface.co/papers/2411.19950))| bys0318, AlbertHuyb, lshmouse, thuzhaowang, hyz317 | AlphaTablets is a novel 3D plane representation for reconstructing planar surfaces from monocular videos.  The research aimed to develop a more accurate and generalizable method for 3D planar reconstruction from monocular video input.  The core methodology involved representing 3D planes as rectangles with alpha channels (AlphaTablets), differentiable rasterization for rendering, and a bottom-up pipeline incorporating optimization and a merging scheme. On the ScanNet dataset, the method achieved a 0.456 F-score for 3D geometry reconstruction, outperforming existing methods. This new representation and pipeline offer AI practitioners a more effective and flexible way to reconstruct and edit 3D planar structures from monocular videos, potentially improving applications in scene understanding, robotics, and mixed reality.  |
| Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing (Read more on [arXiv](https://arxiv.org/abs/2411.19460) or [HuggingFace](https://huggingface.co/papers/2411.19460))| Hyunjun Kim, dwightro, arkimjh, lakelee | Video-Ma²mba is a novel large multimodal model designed for efficient long-form video understanding.  The research aimed to address the challenge of quadratic memory and computational demands of transformer-based models when processing long video sequences.  The key methodology involved replacing the transformer backbone with the linear-complexity Mamba-2 architecture and introducing Multi-Axis Gradient Checkpointing (MA-GC) for memory efficiency.  Video-Ma²mba achieved a 4.1% improvement on the Video-MME benchmark compared to a 16-frame limited baseline. This implies that AI practitioners can leverage MA-GC within the Mamba-2 framework to process long video sequences (up to 2 hours at 1 FPS on a single GPU) more efficiently than transformer-based models, potentially improving performance in video understanding tasks by capturing more complete temporal information.  |
| AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2411.18673) or [HuggingFace](https://huggingface.co/papers/2411.18673))| willi-menapace, aliaksandr-siarohin, guochengqian, universome, sherwinbahmani | AC3D analyzes and improves 3D camera control within pre-trained video diffusion transformers.  The research aims to enable precise 3D camera manipulation in video diffusion models without sacrificing video quality.  The key methodology involves analyzing motion spectral volumes, linearly probing internal model representations for camera pose knowledge, and curating a dataset of dynamic videos with static cameras.  Results show an 18% improvement in video fidelity (FVD) and 25% improvement in camera steering accuracy compared to the closest baseline.  AI practitioners can leverage these insights to develop more precise and efficient camera control mechanisms for text-to-video generation and related applications by understanding how to condition camera pose within video diffusion transformer architectures and tailor training data to enhance scene dynamism while preserving camera control fidelity.  |
| FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion (Read more on [arXiv](https://arxiv.org/abs/2411.18552) or [HuggingFace](https://huggingface.co/papers/2411.18552))| Xiatian Zhu, Hai X. Pham, Isma Hadji, Adrian Bulat, Haosen Yang | FAM diffusion introduces two novel modules to improve high-resolution image generation with pre-trained latent diffusion models. The objective is to enable high-resolution image generation without retraining, addressing issues like object repetition and inconsistent local textures seen when upscaling. The key methodology involves a Frequency Modulation (FM) module, operating in the Fourier domain to enhance global structure consistency, and an Attention Modulation (AM) module to improve local texture consistency. FAM diffusion achieves state-of-the-art performance, demonstrating a CLIP score of 32.33 at 4x upscaling with SDXL, and significantly reducing latency compared to patch-based methods. This allows AI practitioners to generate high-quality, high-resolution images from pre-trained models without computationally expensive retraining or significant latency overheads.  |
| LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification (Read more on [arXiv](https://arxiv.org/abs/2411.19638) or [HuggingFace](https://huggingface.co/papers/2411.19638))| nljubesi, TajaKuzman | This paper proposes a teacher-student framework using LLMs for multilingual news topic classification without manual annotation.  The research aims to develop accurate and computationally efficient multilingual IPTC news topic classifiers for languages lacking annotated training data.  The methodology employs GPT-40 to automatically annotate news articles in four languages, creating a training dataset for fine-tuning an XLM-ROBERTa student model. The XLM-ROBERTa model, trained on 15,000 automatically labeled instances, achieves a macro-F1 score of 0.746. This demonstrates the feasibility of using LLM-generated labels to train smaller, more efficient models for multilingual text classification, enabling AI practitioners to build robust classifiers for low-resource languages without extensive manual annotation efforts.  |
