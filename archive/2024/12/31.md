

## Papers for 2024-12-31

| Title | Authors | Summary |
|-------|---------|---------|
| Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization (Read more on [arXiv](https://arxiv.org/abs/2412.18525) or [HuggingFace](https://huggingface.co/papers/2412.18525))| Tao Yuan, Yuxin Song, Yifan Sun, Xiu-Shen Wei, axxkaya | The research introduces Explanatory Instructions, a novel approach for defining computer vision tasks through linguistic descriptions, to improve zero-shot generalization in vision-language models. The main research objective is to enable vision-language models to genuinely understand and generalize to unseen vision tasks by using detailed linguistic transformations from input to output images. The key methodology involves creating a dataset (DECVT) with 12 million "image input → explanatory instruction → output" triplets and training an auto-regressive-based vision-language model (AR-based VLM) on this dataset. The primary results show that the trained model achieved instruction-level zero-shot capabilities and demonstrated promising vision task-level zero-shot generalization, with the model achieving a 20.69 F1 score on the Canny-to-Image task using unseen instructions. The principal implication for AI practitioners is that Explanatory Instructions can enhance the adaptability of vision-language models, allowing them to perform unseen tasks without task-specific fine-tuning, although the paper notes that the model's task-level zero-shot ability is still limited and unstable.  |
| On the Compositional Generalization of Multimodal LLMs for Medical Imaging (Read more on [arXiv](https://arxiv.org/abs/2412.20070) or [HuggingFace](https://huggingface.co/papers/2412.20070))| Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | This paper investigates compositional generalization (CG) in multimodal large language models (MLLMs) for medical imaging analysis. The main research question is whether MLLMs can leverage CG to understand unseen medical images by recombining learned elements (Modality, Anatomical area, and Task). The key methodology involved constructing a dataset called Med-MAT from 106 medical datasets, defining image elements by MAT-Triplet, and conducting experiments to assess model performance on unseen combinations. A primary result is that MLLMs trained on combinations sharing the same MAT-Triplet demonstrated successful generalization, with the model achieving 91% accuracy on the X-ray, Brain dataset when trained on combinations like CT, Brain(State) and X-ray, Bones. The principal implication for AI practitioners is that CG can be used by MLLMs for medical imaging analysis, which is a way to understand unseen medical images and improve generalization in multi-task training scenarios involving medical image data.  |
| Efficiently Serving LLM Reasoning Programs with Certaindex (Read more on [arXiv](https://arxiv.org/abs/2412.20993) or [HuggingFace](https://huggingface.co/papers/2412.20993))| Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | Dynasor is a system designed to optimize inference-time compute for large language model (LLM) reasoning queries. The main research question is how to effectively schedule and allocate inference compute for LLM reasoning programs that generate multiple outputs for a single query. The key methodology is using "certaindex," a proxy for statistical reasoning progress based on model certainty, to dynamically guide compute allocation and co-adapt scheduling with reasoning progress. Dynasor reduces compute by up to 50% in batch processing and sustains 3.3 times higher query rates or 4.7 times tighter latency SLOs in online serving compared to existing systems. The principal implication for AI practitioners is that using certaindex to dynamically allocate resources for LLM reasoning tasks can significantly improve efficiency and meet latency targets without sacrificing accuracy.  |
| TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2412.21037) or [HuggingFace](https://huggingface.co/papers/2412.21037))| Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | TangoFlux is a text-to-audio model that uses flow matching and CLAP-Ranked Preference Optimization for fast and high-quality audio generation. The main research objective is to develop an efficient text-to-audio (TTA) model that addresses the challenges of controllability and preference alignment in audio generation. The key methodology involves a rectified flow-based model trained with CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference pairs using a CLAP model as a proxy reward model. Primary results show that TangoFlux achieves a CLAP score of 0.480 and an FD score of 75.1 in 3.7 seconds using 50 steps, outperforming other models in objective evaluations and aligning well with human preferences. The principal implication for AI practitioners is that TangoFlux provides a highly efficient and effective solution for generating high-quality, text-aligned audio, making it a valuable tool for practical applications where inference speed and audio quality are critical.  |
| Edicho: Consistent Image Editing in the Wild (Read more on [arXiv](https://arxiv.org/abs/2412.21079) or [HuggingFace](https://huggingface.co/papers/2412.21079))| Ceyuan Yang, Qiuyu Wang, Yinghao Xu, Hao Ouyang, Qingyan Bai | Edicho is a training-free method for consistent image editing across multiple in-the-wild images. The main research objective is to achieve consistent edits across diverse images without requiring paired training data or optimization. The key methodology involves using explicit image correspondence to guide the self-attention mechanism and classifier-free guidance during the denoising process of diffusion models. Primary results demonstrate that Edicho achieves a text alignment score of 0.3228 and an editing consistency score of 0.9355 in global editing tasks, outperforming other methods. For AI practitioners, Edicho offers a plug-and-play solution for consistent image editing that can be integrated with existing diffusion-based editing models, enabling applications like generating coherent visual narratives and maintaining characteristics in marketing materials.  |
| Bringing Objects to Life: 4D generation from 3D objects (Read more on [arXiv](https://arxiv.org/abs/2412.20422) or [HuggingFace](https://huggingface.co/papers/2412.20422))| Gal Chechik, Dvir Samuel, Ori Malca, Ohad Rahamim | 3to4D generates 4D content from static 3D objects and text prompts. The main research question is how to generate 4D content (dynamic 3D objects) from user-provided 3D assets and text prompts while maintaining the object's identity. The key methodology involves first converting a 3D mesh into a static 4D Neural Radiance Field (NeRF), then animating it using an Image-to-Video diffusion model guided by text, employing incremental viewpoint selection and masked Score Distillation Sampling (SDS) loss for improved motion realism. The primary results show that 3to4D outperforms baseline methods, achieving a threefold improvement in identity preservation measured using LPIPS scores (15.0 ±0.1 for 3to4D vs 44.3 ± 0.2 for the next best method). The principal implication for AI practitioners is that 3to4D provides a more effective method for generating customized 4D content from existing 3D models compared to adapting existing text-to-4D or image-to-4D methods.  |
| HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation (Read more on [arXiv](https://arxiv.org/abs/2412.21199) or [HuggingFace](https://huggingface.co/papers/2412.21199))| Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | The paper introduces HumanEval Pro and MBPP Pro, benchmarks for evaluating large language models (LLMs) on self-invoking code generation tasks. The main research objective is to assess LLMs' ability to solve a base problem and then utilize that solution to address a more complex, related problem. The key methodology involves generating new, more challenging versions of existing benchmarks (HumanEval and MBPP) using Deepseek-V2.5, then manually reviewing and refining them. The primary result is that most LLMs experience a significant performance drop on self-invoking tasks compared to traditional code generation; for instance, the o1-mini model achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. The principal implication for AI practitioners is that current LLMs, while proficient in isolated code generation, struggle with tasks requiring progressive reasoning and self-invoking code, highlighting a need for further research in this area.  |
| Facilitating large language model Russian adaptation with Learned Embedding Propagation (Read more on [arXiv](https://arxiv.org/abs/2412.21140) or [HuggingFace](https://huggingface.co/papers/2412.21140))| Daniil Chernyshev, RefalMachine | This paper introduces Learned Embedding Propagation (LEP) as a cost-effective method for adapting large language models (LLMs) to new languages, specifically Russian, while preserving original model knowledge. The main research objective is to address the limitations of language adaptation posed by restricted access to high-quality instruction-tuning data. The key methodology involves training new token embeddings and propagating them to an instruction-tuned LLM using linear transformations derived from parameter decomposition, bypassing the need for full instruction-tuning. The primary results show that LEP applied to LLaMa-3-8B and Mistral-7B achieves competitive performance with OpenChat 3.5, with the LEP-Extended model achieving a Micro-Avg score of 0.632 after calibration. The principal implication for AI practitioners is that LEP offers a viable alternative to traditional language-specific instruction-tuning, reducing costs associated with language adaptation while maintaining or surpassing performance benchmarks.  |
| Training Software Engineering Agents and Verifiers with SWE-Gym (Read more on [arXiv](https://arxiv.org/abs/2412.21139) or [HuggingFace](https://huggingface.co/papers/2412.21139))| Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | SWE-Gym is a new benchmark for training software engineering agents that can solve real-world GitHub issues. The main research objective is to create an environment for training and evaluating language-model-based software engineering agents using real-world Python tasks. The key methodology involves constructing SWE-Gym, containing 2,438 Python tasks with executable runtime environments, unit tests, and natural language task specifications, and using it to train agents via policy improvement algorithms like rejection sampling, fine-tuning and inference-time scaling through verifiers. The primary result is that fine-tuned models achieved up to 19% absolute gains in resolve rate on SWE-Bench Verified and Lite test sets. The principal implication for AI practitioners is that SWE-Gym enables the development of more capable software engineering agents by providing a realistic and scalable training environment with executable feedback.  |
| OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System (Read more on [arXiv](https://arxiv.org/abs/2412.20005) or [HuggingFace](https://huggingface.co/papers/2412.20005))| Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | OneKE is a dockerized system for knowledge extraction that uses LLM-based agents and a configurable knowledge base. The main research objective is to develop a comprehensive system for knowledge extraction that can handle diverse data types, complex schemas, and improve through error debugging. The key methodology involves using three agents (Schema Agent, Extraction Agent, and Reflection Agent) with a configurable knowledge base consisting of a Schema Repository and Case Repository to support schema analysis, knowledge extraction, and error handling. The primary results show that the Case Retrieval method improved performance on both CrossNER and NYT-11-HRL datasets, with F1 scores increasing from approximately 40 to over 60 on CrossNER when using the LLaMA-3-8B-Instruct model. The principal implication for AI practitioners is that OneKE provides a flexible framework for knowledge extraction tasks without requiring model fine-tuning, allowing for easier adaptation to various domains and data formats, although it's unclear how performance compares to other fine-tuned methods.  |
