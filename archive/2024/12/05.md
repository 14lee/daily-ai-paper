

## Papers for 2024-12-05

| Title | Authors | Summary |
|-------|---------|---------|
| SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance (Read more on [arXiv](https://arxiv.org/abs/2412.02687) or [HuggingFace](https://huggingface.co/papers/2412.02687))| Khoi Nguyen, anhttran1111, termanteus, aengusng, viettmab | SNOOPI enhances one-step text-to-image diffusion model training stability and control via novel guidance techniques.  The research aimed to address the instability of Variational Score Distillation (VSD) across different architectures and the lack of negative prompt guidance in one-step diffusion models. The authors introduced Proper Guidance - SwiftBrush (PG-SB), which utilizes a random guidance scale during training, and Negative-Away Steer Attention (NASA), which integrates negative prompts during inference via cross-attention manipulation.  Integrating PG-SB and NASA with a PixArt-a backbone achieved a Human Preference Score v2 (HPSv2) of 31.08. This offers AI practitioners a more stable and controllable method for developing efficient one-step text-to-image diffusion models with enhanced image quality and adherence to both positive and negative prompts.  |
| Imagine360: Immersive 360 Video Generation from Perspective Anchor (Read more on [arXiv](https://arxiv.org/abs/2412.03552) or [HuggingFace](https://huggingface.co/papers/2412.03552))| liuziwei7, guoyww, mimihe, tongwu2020, jingtan | Imagine360 generates immersive 360° videos from standard perspective videos.  The research aimed to develop a framework for transforming perspective videos into 360° equirectangular videos.  The core methodology involved a dual-branch video denoising structure with antipodal masking and elevation-aware design, trained on a combined dataset of WEB360 and a newly collected YouTube dataset. Imagine360 achieved a VQA score of 0.8672, outperforming comparison methods like 360DVD and Follow-Your-Canvas.  This provides AI practitioners with a new tool for generating high-quality 360° videos from readily available perspective video data, facilitating easier creation of immersive content.  |
| Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion (Read more on [arXiv](https://arxiv.org/abs/2412.03515) or [HuggingFace](https://huggingface.co/papers/2412.03515))| An Zhao, slysun, haoranxu, mengcy, SYZhang0805 | ScoreLiDAR, a novel distillation method, accelerates 3D LiDAR scene completion using diffusion models.  The research aimed to improve the speed of diffusion-based 3D LiDAR scene completion while maintaining high quality. The method uses Variational Score Distillation (VSD) adapted for 3D data and incorporates a novel Structural Loss to preserve geometric details.  On the SemanticKITTI dataset, ScoreLiDAR achieved a 5x speedup, reducing completion time from 30.55 seconds to 5.37 seconds per frame while improving Chamfer Distance by 8%. This allows AI practitioners to utilize diffusion models for real-time or near real-time 3D LiDAR scene completion in applications like autonomous driving where fast processing is crucial.  |
| PaliGemma 2: A Family of Versatile VLMs for Transfer (Read more on [arXiv](https://arxiv.org/abs/2412.03555) or [HuggingFace](https://huggingface.co/papers/2412.03555))| mjlm, AlexeyG, yonatanbitton, dkeysers, mitsch | Here's a summary of the AI research paper following your strict guidelines:  i) **1-line summary:** PaliGemma 2, a family of versatile vision-language models (VLMs), was developed and evaluated on a broad range of transfer tasks, demonstrating improved performance over its predecessor.  ii) **Main research question/objective:** To investigate the impact of model size and resolution on VLM transfer performance and expand the breadth of transfer tasks beyond those in the original PaliGemma.  iii) **Key methodology:**  A family of VLMs was created by combining the SigLIP-So400m vision encoder with various Gemma 2 language models (2B, 9B, and 27B), trained at three resolutions (224px², 448px², 896px²) using a three-stage training process.  These models were then fine-tuned on a wide array of transfer tasks including several new tasks such as table and molecular structure recognition.  iv) **Primary results:**  PaliGemma 2 achieved state-of-the-art results on many transfer tasks; for example, on ICDAR'15 Incidental and Total-Text, it outperformed the previous state-of-the-art in text detection and recognition (HTS) achieving F1 scores of 75.9 and 74.2, respectively.  v) **Principal implication for AI practitioners:**  The release of PaliGemma 2 as open-weight models provides a resource for fine-tuning on various tasks, offering valuable insights into the impact of model scaling on transfer learning and state-of-the-art performance in several domains. The extensive analysis of model size and resolution's effects on numerous tasks provides a valuable resource for model design choices in VLM development.  The specific quantitative results on numerous benchmarks allow for direct comparison with existing models and informed decision-making in selecting appropriate models for various applications.  |
| TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2412.03069) or [HuggingFace](https://huggingface.co/papers/2412.03069))| sweetrabor, gaozong, xuwang, liqingzju, leo1117 | TokenFlow is a novel unified image tokenizer designed to bridge the gap between multimodal understanding and generation.  The central research question is whether a single image tokenizer can derive representations suitable for both multimodal understanding and generation.  The key methodology involves a dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining alignment via shared index mapping, enabling simultaneous access to both feature types.  In multimodal understanding benchmarks, TokenFlow surpasses LLaVA-1.5 13B by 7.2% average improvement, marking the first time discrete visual input outperforms this baseline. This improvement significantly impacts AI practitioners by providing a more efficient and performant approach to unify image representations for both understanding and generation tasks within a single framework.  |
| Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2412.00493) or [HuggingFace](https://huggingface.co/papers/2412.00493))| asdfg80, slvjul, zd11024 | Video-3D LLM enhances 3D scene understanding by incorporating 3D positional information into video representations.  The research aimed to develop a generalist model for various 3D scene understanding tasks, addressing the limitations of current MLLMs in handling 3D spatial information.  The authors developed Video-3D LLM, which leverages a pre-trained Video LLM and integrates 3D position encodings derived from depth images into video features, along with a maximum coverage sampling strategy for efficient frame selection.  The model achieved state-of-the-art performance on benchmarks like ScanRefer (58.1% Acc@0.25), Scan2Cap (41.3 BLEU-4@0.5IoU), ScanQA (30.1% EM), and SQA3D (58.6% EM).  AI practitioners can utilize this approach to enhance performance in applications requiring 3D spatial reasoning, such as robotics, 3D visual grounding, and question answering. The improvement in accuracy on ScanRefer, by incorporating 3D positional data, highlights the practical benefit for developing more robust 3D scene understanding applications.  |
| NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images (Read more on [arXiv](https://arxiv.org/abs/2412.03517) or [HuggingFace](https://huggingface.co/papers/2412.03517))| Chengwh, bluestyle97, Yw22, ZyZcuhk, l-li | NVComposer synthesizes novel views from multiple sparse and unposed images without requiring external alignment.  The objective is to generate novel views at specified target camera poses from unposed conditional images without explicit pose estimation or pre-reconstruction.  The approach uses an image-pose dual-stream diffusion model to generate views and implicitly predict poses, combined with a geometry-aware feature alignment adapter distilling geometric priors from a pre-trained dense stereo model.  On the RealEstate10K dataset, NVComposer achieves a PSNR of 22.55 with four input views, outperforming comparison methods. This provides AI practitioners with a more robust and accessible method for generative novel view synthesis, eliminating the need for potentially unstable external alignment pre-processing.  |
| VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.19103) or [HuggingFace](https://huggingface.co/papers/2411.19103))| SunYoung Park, Daeyoung Kim, kimyoungjune, hojunssss | VARCO-VISION is a novel open-source, Korean-English bilingual vision-language model (VLM).  The research aimed to develop a high-performing bilingual VLM and accompanying Korean evaluation benchmarks.  The authors employed a four-stage training strategy involving feature alignment pre-training, basic and advanced supervised fine-tuning, and preference optimization using translated and human-validated datasets.  VARCO-VISION-14B achieved 82.21% accuracy on the K-MMBench benchmark, outperforming similarly sized open-source models.  This release provides AI practitioners with a powerful tool for developing Korean-focused multimodal applications and resources for further research in bilingual VLM training and evaluation.  |
| CleanDIFT: Diffusion Features without Noise (Read more on [arXiv](https://arxiv.org/abs/2412.03439) or [HuggingFace](https://huggingface.co/papers/2412.03439))| Björn Ommer, FrankFundel, kolja-b, stefan-baumann, kliyer | CleanDIFT is a novel method for extracting noise-free, timestep-independent features from pre-trained diffusion models.  The research aimed to improve the quality and efficiency of diffusion feature extraction by eliminating the need for adding noise to input images. The methodology involved fine-tuning a trainable copy of a diffusion model on clean images while aligning its internal representations with the timestep-dependent features of the original model using projection heads and a cosine similarity loss.  On the SPair-71k dataset for zero-shot unsupervised semantic correspondence, CleanDIFT improved PCKbbox accuracy by 1.86 percentage points compared to standard diffusion features. AI practitioners can use CleanDIFT to extract superior, noise-free features from diffusion models more efficiently, eliminating the need for noise or timestep ensembling for various downstream tasks like semantic correspondence, depth estimation, and semantic segmentation.  |
| MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation (Read more on [arXiv](https://arxiv.org/abs/2412.03558) or [HuggingFace](https://huggingface.co/papers/2412.03558))| zouzx, yhyang-myron, XingqiaoAn, bennyguo, huanngzh | MIDI generates compositional 3D scenes from single images by extending pretrained image-to-3D object generation models to multi-instance diffusion. The objective is to generate multiple spatially correlated 3D instances with accurate relationships from a single image. MIDI employs a novel multi-instance attention mechanism within a denoising transformer, trained on scene-level and single-object data, to model cross-instance interactions and spatial coherence directly during 3D generation. On the BlendSwap dataset, MIDI achieves a scene-level Chamfer Distance of 0.077 and F-Score of 78.21, outperforming other single-image 3D scene generation methods. AI practitioners can use MIDI to create coherent and high-fidelity 3D scenes from single images, potentially impacting applications like 3D content creation and scene understanding.  |
| One Shot, One Talk: Whole-body Talking Avatar from a Single Image (Read more on [arXiv](https://arxiv.org/abs/2412.01106) or [HuggingFace](https://huggingface.co/papers/2412.01106))| Boyang Guo, Leipeng Hu, JuyongZhang, YudongGuo, xiangjun-xj | This paper introduces a method for creating animatable, expressive, whole-body talking avatars from a single image.  The objective is to reconstruct a 3D talking avatar from a single image that can be animated with realistic gestures and expressions.  The method uses pose-guided image-to-video diffusion models to generate pseudo-labels and trains a coupled 3D Gaussian Splatting (3DGS)-mesh hybrid avatar representation with several regularizations.  On a self-driven motion reenactment task, the method achieved a peak signal-to-noise ratio (PSNR) of 29.31, outperforming comparison methods. This provides AI practitioners with a new technique to create realistic and controllable talking avatars from limited input data, potentially impacting applications in virtual reality, augmented reality, and telepresence.  |
| Mimir: Improving Video Diffusion Models for Precise Text Understanding (Read more on [arXiv](https://arxiv.org/abs/2412.03085) or [HuggingFace](https://huggingface.co/papers/2412.03085))| Dandan Zheng, Kecheng Zheng, Yutong Feng, Shuai Tan, BiaoGong | Mimir is a novel text-to-video generation framework that enhances text comprehension in video diffusion models.  The research aims to address the limited text understanding of current video diffusion models, especially when processing short captions or complex motions, by integrating the capabilities of large language models (LLMs).  The key methodology involves a "token fuser" that harmonizes the outputs of text encoders and decoder-only LLMs, enabling the model to leverage both learned video priors and advanced text comprehension of LLMs.  Mimir achieves 97.68% on Background Consistency in the VBench benchmark, outperforming all other compared models.  This implies that AI practitioners can utilize Mimir’s architecture to improve video generation quality and text comprehension, particularly for short, complex prompts.  |
| Weighted-Reward Preference Optimization for Implicit Model Fusion (Read more on [arXiv](https://arxiv.org/abs/2412.03187) or [HuggingFace](https://huggingface.co/papers/2412.03187))| Xiaojun Quan, Tianyuan Shi, Longguang Zhong, Fanqi Wan, Ziyi Yang | The paper introduces Weighted-Reward Preference Optimization (WRPO) for fusing heterogeneous large language models (LLMs).  The research aims to improve the capabilities of a target LLM by implicitly learning from multiple robust open-source LLMs without vocabulary alignment or distribution merging.  WRPO uses a progressive adaptation strategy and weighted reward mechanism within a preference optimization framework, mitigating distributional deviations between source and target LLMs.  When applied to LLaMA3-8B-Instruct, WRPO achieves a 55.9% length-controlled win rate against GPT-4-Preview-1106 on AlpacaEval-2. This provides AI practitioners with a more efficient and effective method for integrating strengths from various LLMs into a single model, potentially outperforming larger, computationally expensive ensembles.  |
| NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training (Read more on [arXiv](https://arxiv.org/abs/2412.02030) or [HuggingFace](https://huggingface.co/papers/2412.02030))| Yi-Zhe Song, Kai Zou, Hmrishav Bandyopadhyay, ChenDY | NitroFusion introduces a dynamic adversarial training framework for high-fidelity single-step text-to-image diffusion.  The objective is to improve the quality of single-step diffusion models, which typically suffer from quality degradation compared to multi-step models, while maintaining speed advantages.  The key methodology involves a dynamic discriminator pool with specialized and periodically refreshed discriminator heads, employing multi-scale and dual-objective (conditional/unconditional) GAN training.  NitroFusion achieves an Aesthetic Score of 5.92 and an Image Reward of 0.991 on the COCO-5k validation dataset, exceeding its 8-step teacher model in these metrics. This offers AI practitioners a single model capable of both rapid generation and high-fidelity image synthesis, dynamically adjustable through bottom-up refinement with 1-4 denoising steps.  |
