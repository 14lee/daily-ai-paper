

## Papers for 2024-12-17

| Title | Authors | Summary |
|-------|---------|---------|
| RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation (Read more on [arXiv](https://arxiv.org/abs/2412.11919) or [HuggingFace](https://huggingface.co/papers/2412.11919))| douzc, Benen2024, wuyongkang, jinjiajie, lixiaoxi45 | Here is a concise summary of the research paper "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation" based on the provided guidelines:  i) **Summary:** RetroLLM is a unified framework that integrates retrieval and generation into a single process, enabling large language models (LLMs) to directly generate fine-grained evidence from a corpus during the generation process using constrained decoding. ii) **Main Research Question/Objective:** How to address the limitations of existing retrieval-augmented generation (RAG) methods, such as the need for separate retrievers, redundant input tokens, and the lack of joint optimization of retrieval and generation. iii) **Key Methodology:** The authors propose hierarchical FM-Index constraints and a forward-looking constrained decoding strategy to guide the LLM in generating corpus-constrained clues and relevant evidence. iv) **Primary Results:** RetroLLM outperforms RAG methods across both in-domain and out-of-domain tasks; for example, RetroLLM achieves an accuracy of 61.6% on the NQ dataset, compared to 52.4% for the Naive RAG method. v) **Principal Implication for AI Practitioners:** AI practitioners can leverage RetroLLM to develop more efficient and accurate RAG systems by eliminating the need for separate retrievers and enabling joint optimization of retrieval and generation, leading to improved performance in knowledge-intensive tasks.  |
| Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models (Read more on [arXiv](https://arxiv.org/abs/2412.09645) or [HuggingFace](https://huggingface.co/papers/2412.09645))| Yu Qiao, liuziwei7, Ziqi, shulin16, Fan-s | Here is a concise summary of the research paper "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models":  i)  The paper introduces Evaluation Agent, a framework for efficiently evaluating visual generative models using dynamic, multi-round assessments tailored to user-specified criteria. ii)  The main research objective is to develop an evaluation framework that overcomes the limitations of existing methods by efficiently assessing visual generative models' capabilities based on user needs and providing detailed, interpretable results. iii)  The key methodology employs Large Language Model (LLM)-based agents in a two-stage process: a proposal stage for planning and prompt generation, and an execution stage for sampling and evaluating visual content using an extensible toolkit. iv)  The primary result is that Evaluation Agent reduces evaluation time to 10% of traditional methods while achieving comparable accuracy to standard benchmarks like VBench and T2I-CompBench. v)  The principal implication for AI practitioners is that they can leverage Evaluation Agent to conduct faster, more flexible, and user-specific evaluations of visual generative models, facilitating more targeted development and refinement.  |
| BrushEdit: All-In-One Image Inpainting and Editing (Read more on [arXiv](https://arxiv.org/abs/2412.10316) or [HuggingFace](https://huggingface.co/papers/2412.10316))| yshan2u, ZyZcuhk, juxuan27, BianYx, Yw22 | Here is a concise summary of the BrushEdit research paper, strictly adhering to your guidelines:  i)  BrushEdit is a novel framework for inpainting-based, instruction-guided image editing that integrates multimodal large language models (MLLMs) and a dual-branch image inpainting model. ii) The main research objective is to develop a new image editing paradigm that overcomes challenges related to inference efficiency, scalable data curation, editability, and controllability in existing methods. iii) The key methodology involves a four-step process: editing category classification, primary editing object identification, acquisition of editing mask and target caption via MLLMs and detection models, and image inpainting using a dual-branch model (BrushNet). iv) Primary results demonstrate that BrushEdit achieves superior performance across seven metrics, including a PSNR score of 32.16 for background preservation in edited images, which is the best result compared to other methods. v) The principal implication for AI practitioners is that BrushEdit provides a user-friendly, free-form, multi-turn interactive framework for instruction-based image editing, enabling more precise control and superior editing quality without the need for extensive training.  |
| ColorFlow: Retrieval-Augmented Image Sequence Colorization (Read more on [arXiv](https://arxiv.org/abs/2412.11815) or [HuggingFace](https://huggingface.co/papers/2412.11815))| Yong Liu, yshan2u, ZyZcuhk, juxuan27, JunhaoZhuang | Here is a concise summary of the research paper "ColorFlow: Retrieval-Augmented Image Sequence Colorization":  i)  The paper introduces ColorFlow, a novel three-stage diffusion-based framework for reference-based colorization of black-and-white image sequences that preserves object and character identity. ii) The main research objective is to develop a method for automatic image sequence colorization that maintains color consistency and identity preservation across frames, using a pool of color reference images. iii) The key methodology involves a three-stage pipeline: Retrieval-Augmented Pipeline (RAP) for extracting relevant color patches, In-context Colorization Pipeline (ICP) for performing colorization with a two-branch design using a self-attention mechanism, and Guided Super-Resolution Pipeline (GSRP) for upsampling to high-resolution images. iv) ColorFlow outperforms existing models across multiple metrics, achieving over 37% reduction in FID score compared to state-of-the-art colorization models. v) For AI practitioners, ColorFlow offers a robust framework for high-quality, reference-based image sequence colorization, setting a new standard with the potential for direct industrial application in fields such as manga and animation production.  |
| Byte Latent Transformer: Patches Scale Better Than Tokens (Read more on [arXiv](https://arxiv.org/abs/2412.09871) or [HuggingFace](https://huggingface.co/papers/2412.09871))| spermwhale, Chunting, marg33, benjamin-mlr, artidoro | Here's a concise summary of the AI research paper "Byte Latent Transformer: Patches Scale Better Than Tokens":  i) **Summary:** This paper introduces the Byte Latent Transformer (BLT), a new byte-level language model architecture that dynamically groups bytes into patches to improve efficiency and robustness compared to tokenization-based models. ii) **Main research question/objective:** How can a byte-level language model be designed to match the performance of tokenization-based models at scale while improving inference efficiency and robustness? iii) **Key methodology:** BLT uses a dynamic, learnable method for grouping bytes into patches based on next-byte entropy and a new model architecture that mixes byte and patch information processed by local and global transformer blocks. iv) **Primary results:** BLT models match training FLOP-controlled performance of Llama 3 up to 8B parameters and achieve up to 50% inference FLOP savings; a BLT-Entropy model outperforms the Llama 3 tokenizer-based model on 4 out of 7 tasks while trained on the same amount of data. v) **Principal implication for AI practitioners:** BLT demonstrates that dynamically allocating compute based on input complexity via patching can lead to more efficient and robust language models, offering a viable alternative to tokenization-based models.  |
| Causal Diffusion Transformers for Generative Modeling (Read more on [arXiv](https://arxiv.org/abs/2412.12095) or [HuggingFace](https://huggingface.co/papers/2412.12095))| Haoqi Fan, Shi Guan, Deyao Zh, Chaorui Deng, Andy1621 | Here's a concise summary of the research paper "Causal Diffusion Transformers for Generative Modeling":  i) **Summary:** This paper introduces CausalFusion, a decoder-only transformer that unifies autoregressive (AR) and diffusion models for generative modeling by factorizing data across both sequential tokens and diffusion noise levels. ii) **Main research question or objective:**  How can sequential factorization be introduced to a diffusion model to improve its performance and enable a smooth transition between AR and diffusion generation modes? iii) **Key methodology:** The authors propose a dual-factorization approach in a decoder-only transformer that processes data across sequential tokens and diffusion noise levels, with adjustable AR and diffusion steps, and introduce a generalized causal attention mechanism. iv) **Primary results:** CausalFusion achieves state-of-the-art results on the ImageNet class-conditional generation benchmark; for instance, CausalFusion-XL achieves a FID-50k score of 1.77 on 256x256 images with classifier-free guidance. v) **Principal implication for AI practitioners:** AI practitioners can leverage CausalFusion as a powerful and versatile generative modeling framework that combines the strengths of AR and diffusion models, offering improved performance and flexibility for tasks like image generation, multimodal modeling, and zero-shot image manipulation.  |
| Smaller Language Models Are Better Instruction Evolvers (Read more on [arXiv](https://arxiv.org/abs/2412.11231) or [HuggingFace](https://huggingface.co/papers/2412.11231))| Hua Zhou, Yaqi Zhang, Lulu Zhao, dongguanting, Chaox72 | Here is a concise summary of the research paper "Smaller Language Models Are Better Instruction Evolvers":  i) **Summary:** This study investigates the efficacy of smaller language models (SLMs) in evolving instructions for large language models (LLMs) compared to larger models, challenging the notion that larger models inherently possess superior instruction evolution capabilities. ii) **Main research question/objective:** Do SLMs outperform LLMs in evolving instructions, and if so, why? iii) **Key methodology:** The authors conducted experiments across three instruction evolution scenarios (Evol-Instruct, AutoIF, and Auto Evol-Instruct) using SLMs and LLMs from the Llama-3 and Qwen-2 families and evaluated performance on various benchmarks, including IFEval and FollowBench. iv) **Primary results:** SLMs can synthesize more effective and diverse instructions than LLMs; specifically, on the FollowBench benchmark, SLM-evolved instructions (SLM-INST) achieved nearly a 10% improvement over Llama-3-8B and Llama-3.1-8B when supervised by Llama-3.1-70B-Instruct. v) **Principal implication for AI practitioners:** AI practitioners can leverage SLMs to generate more complex and diverse instructions for instruction tuning, potentially leading to more capable LLMs while using fewer computational resources.  |
| IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations (Read more on [arXiv](https://arxiv.org/abs/2412.12083) or [HuggingFace](https://huggingface.co/papers/2412.12083))| Jiaqiwang, Dubhe-zmc, jingtan, tongwu2020, lizb6626 | Here is a concise summary of the research paper "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations":  **i) Summary** IDArb is a diffusion-based model for intrinsic decomposition of an arbitrary number of images under varying illuminations, achieving multi-view consistency and disentangling intrinsic components from lighting effects.  **ii) Main research question or objective** The main objective is to develop a model that can perform accurate and multi-view consistent intrinsic decomposition (surface normals, albedo, roughness, metallic) on an arbitrary number of images captured under varying, unconstrained illuminations.  **iii) Key methodology used** The proposed method, IDArb, utilizes a diffusion-based model with a cross-view, cross-component attention module and an illumination-augmented, view-adaptive training strategy, trained on a new dataset (ARB-Objaverse) containing 5.7M multi-view RGB images.  **iv) Primary results** IDArb outperforms state-of-the-art methods in intrinsic decomposition, achieving a PSNR of 33.62 for albedo estimation in multi-view settings.  **v) Principal implication for AI practitioners** IDArb provides a unified solution for inverse rendering across different input regimes, offering AI practitioners a robust method for generating accurate intrinsic components from arbitrary image sets, directly applicable in tasks like relighting, photometric stereo, and 3D reconstruction.  |
| SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2412.11605) or [HuggingFace](https://huggingface.co/papers/2412.11605))| howang, yuxiaod, lrxl, wangcunxiang, CCCCCC | Here's a summary of the paper "SPAR: SELF-PLAY WITH TREE-SEARCH REFINEMENT TO IMPROVE INSTRUCTION-FOLLOWING IN LARGE LANGUAGE MODELS" following your guidelines:  i) **Summary:** This paper introduces SPAR, a self-play framework that uses tree-search refinement to improve instruction-following in large language models (LLMs) by creating better preference pairs. ii) **Main research question/objective:** How to improve the instruction-following capabilities of LLMs using a self-play framework that addresses limitations of existing preference learning methods. iii) **Key methodology:** SPAR employs a self-play framework where an LLM acts as both an actor and a refiner, using a tree-search algorithm to refine responses and generate valid preference pairs for training. iv) **Primary results:** After three iterations, SPAR improved a LLaMA3-8B-Instruct model to surpass GPT-4-Turbo on the IFEval benchmark, achieving an average accuracy of 81.8. v) **Principal implication for AI practitioners:** AI practitioners can use SPAR to enhance the instruction-following abilities of LLMs without relying on external models, enabling the development of more accurate and reliable AI systems.  |
| Wonderland: Navigating 3D Scenes from a Single Image (Read more on [arXiv](https://arxiv.org/abs/2412.12091) or [HuggingFace](https://huggingface.co/papers/2412.12091))| Hanwen Liang, ZanyRumata, guochengqian, vidit98, jlcao2 | Here is a concise summary of the research paper "Wonderland: Navigating 3D Scenes from a Single Image":  i) Wonderland is a novel framework for efficiently generating high-quality, wide-scope 3D scenes from a single image using a feed-forward reconstruction model operating on the latent space of a video diffusion model. ii) Main research question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? iii) Key methodology: A large-scale reconstruction model uses latents from a camera-guided video diffusion model to predict 3D Gaussian Splattings in a feed-forward manner, with a dual-branch camera conditioning module for precise pose control and a progressive training strategy. iv) Primary results: The method significantly outperforms existing methods for single-view 3D scene generation, achieving a FID score of 16.16 on the RealEstate10K dataset, compared to 20.89 for the next best method, ViewCrafter. v) Principal implication for AI practitioners: Wonderland demonstrates that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation, providing a novel and effective approach to single image 3D scene generation.  |
| GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs (Read more on [arXiv](https://arxiv.org/abs/2412.11258) or [HuggingFace](https://huggingface.co/papers/2412.11258))| junweiliang, StarYDY, zhifeichen097, spongy, Xxlbigbrother | Here is a concise summary of the research paper "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs":  i) **Summary:** This paper introduces GaussianProperty, a training-free framework that leverages Large Multimodal Models (LMMs) to assign physical properties to 3D Gaussian representations for applications in physics-based simulation and robotic grasping. ii) **Main research question/objective:** The main objective is to develop a method for accurately estimating and integrating physical properties of materials into 3D Gaussian representations from multi-view 2D images. iii) **Key methodology:** The methodology combines global-local physical property reasoning using Segment Anything (SAM) for image segmentation and GPT-4V for property recognition, followed by a multi-view projection and voting strategy to assign properties to 3D Gaussians. iv) **Primary results:** The proposed method achieved a material segmentation mean Intersection over Union (mIoU) of 55.83% on the ABO dataset, demonstrating the effective integration of physical properties into 3D Gaussian representations. v) **Principal implication for AI practitioners:** AI practitioners can leverage this method to enhance 3D models with physical properties without the need for manual annotation, enabling more realistic physics-based simulations and improved robotic grasping strategies directly from visual data.  |
| SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator (Read more on [arXiv](https://arxiv.org/abs/2412.12094) or [HuggingFace](https://huggingface.co/papers/2412.12094))| Xiaozhe Ren, Yihang Gao, Jiawei Li, Guoxuan Chen, shihan96 | Here is a concise summary of the research paper "SepLLM: Accelerating Large Language Models by Compressing One Segment into One Separator":  i) **Summary:** This paper introduces SepLLM, a novel framework that accelerates large language models (LLMs) by compressing segments of text into separator tokens within a sparse attention mechanism. ii) **Main research question/objective:** The main objective is to accelerate LLM inference and training by addressing the quadratic complexity of self-attention through a data-dependent sparse attention mechanism. iii) **Key methodology:** The key methodology involves identifying and leveraging the disproportionate attention scores of separator tokens to condense segment information, implementing a sparse attention mechanism that retains only initial, neighboring, and separator tokens, and utilizing efficient kernels for training acceleration. iv) **Primary results:** SepLLM achieves over 50% reduction in KV cache usage on the GSM8K-CoT benchmark using the Llama-3-8B backbone while maintaining comparable performance to the original model. v) **Principal implication for AI practitioners:** AI practitioners can leverage SepLLM as a plug-and-play framework to accelerate the inference and training of LLMs, particularly in streaming settings with long sequences, without significant loss of performance, by strategically managing and compressing the KV cache.  |
| Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture (Read more on [arXiv](https://arxiv.org/abs/2412.11834) or [HuggingFace](https://huggingface.co/papers/2412.11834))| wubingheng, JingzeShi | Here is a concise summary of the paper "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture":  i)  The paper introduces "Wonderful Matrices," a novel foundation model architecture that integrates sequence and state transformations to enhance efficiency and effectiveness. ii) The main research objective is to develop a foundation model architecture that combines the strengths of State Space Duality and Quadratic Causal Self-Attention algorithms while mitigating their respective limitations. iii) The key methodology involves unifying position encoding with Rotary Position Embedding, introducing Dynamic Mask Attention for selective information filtering, and designing Cross Domain Mixture of Experts for efficient parameter utilization. iv) Primary results show that Dynamic Mask Attention maintains 100% accuracy in the multi-query associative recall task, outperforming Quadratic Causal Self-Attention and State Space Duality. v) The principal implication for AI practitioners is that Wonderful Matrices provides a more efficient and effective architecture for language modeling, as demonstrated by improved performance on benchmark tasks.  |
| StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors (Read more on [arXiv](https://arxiv.org/abs/2412.11586) or [HuggingFace](https://huggingface.co/papers/2412.11586))| Jian Yang, Zeyu Cai, yingtai, JesseZhang, XiaokunSun | Here is a concise summary of the research paper "StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors":  i)  StrandHead is a novel framework that generates 3D head avatars with strand-disentangled hair from text descriptions without using 3D hair data for supervision. ii) The main research objective is to develop a method for generating realistic 3D head avatars with detailed, strand-based hair directly from text prompts. iii) The key methodology involves distilling 2D generative diffusion models, using a differentiable prismatization algorithm to convert hair strands into meshes, and applying orientation consistency and curvature regularization losses based on hair geometric priors. iv) Primary results show that StrandHead outperforms state-of-the-art methods in head and hair generation; for example, it achieved a 58.00% Text-Image Alignment Preference (TAP) score in head generation tasks. v) The principal implication for AI practitioners is that StrandHead provides a new, effective way to generate high-fidelity 3D head avatars with realistic hair from text descriptions, which can be directly integrated into existing simulation and rendering systems without requiring 3D hair data.  |
| MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes (Read more on [arXiv](https://arxiv.org/abs/2412.11457) or [HuggingFace](https://huggingface.co/papers/2412.11457))| YuLiu, BuzzBeater, JunfengNi, YixinChen, JasonAplp | Here is a concise summary of the research paper "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes":  i) **Summary:** This paper introduces MOVIS, a novel method designed to improve the structural awareness and cross-view consistency of diffusion-based novel view synthesis (NVS) models for multi-object indoor scenes.  ii) **Main research question or objective:** How can the structural awareness of current diffusion-based novel view synthesizers be enhanced to improve cross-view consistency in multi-object scenarios?  iii) **Key methodology:** MOVIS incorporates structure-aware features (depth and object mask) as inputs, employs an auxiliary novel view mask prediction task, and utilizes a structure-guided timestep sampling scheduler during training.  iv) **Primary results:** MOVIS outperforms existing methods on multi-object NVS tasks, demonstrating superior object placement, geometry, and appearance recovery; quantitatively, MOVIS achieves a PSNR of 17.432 on the C3DFS test set, compared to 14.811 for the next best method, Zero-1-to-3+.  v) **Principal implication for AI practitioners:** MOVIS provides AI practitioners with a method to generate more consistent and realistic novel views in complex multi-object scenes by enhancing the structural awareness of diffusion models, making them more viable for real-world applications like AR/VR and robotics.  |
| Whisper-GPT: A Hybrid Representation Audio Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2412.11449) or [HuggingFace](https://huggingface.co/papers/2412.11449))| prateekv | Here's a summary of the research paper "WHISPER-GPT: A Hybrid Representation Audio Large Language Model" following the specified guidelines:  i) **Summary:** This paper introduces WHISPER-GPT, a generative large language model (LLM) for speech and music that combines continuous audio representations (mel-spectrogram) with discrete acoustic tokens (ENCODEC) in a hybrid architecture.  ii) **Main research question or objective:** Can an architecture that simultaneously utilizes continuous and discrete representation in the LLM setup improve the next token prediction compared to a token-based LLM for speech and music?  iii) **Key methodology used:** The authors adapted a Whisper-like encoder-decoder architecture to a seq-to-seq model for generative modeling, replacing the Whisper encoder with a decoder and performing early fusion of learned representations with decoder-only architecture on acoustic tokens. They also employed a Transformer decoder-only architecture trained on the LibriSpeech TTS dataset and a dataset of instrumental music to predict the next coarse acoustic token.  iv) **Primary results:** The hybrid model outperformed a purely token-based GPT model in next token prediction. Specifically, for the music dataset, the hybrid model achieved a negative log-likelihood (NLL) of 2.52 compared to 2.78 for the baseline GPT-S model.  v) **Principal implication for AI practitioners:** AI/ML/Software Engineers and Data Scientists can leverage this hybrid input representation approach to achieve better performance in generative audio models, potentially enabling smaller, more efficient models with performance comparable to larger, purely token-based models.  |
| TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning (Read more on [arXiv](https://arxiv.org/abs/2412.10447) or [HuggingFace](https://huggingface.co/papers/2412.10447))| Yihuai Gao, Aaditya Prasad, Robert Holmberg, William Chong, jimmyyhwu | Here is a concise summary of the research paper "TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning":  i)  **Summary:** This paper introduces TidyBot++, an open-source holonomic mobile manipulator designed for robot learning, featuring a powered-caster mobile base and a mobile phone teleoperation interface. ii) **Main research question/objective:** The main objective is to develop an inexpensive, robust, and flexible holonomic mobile manipulator to facilitate the collection of large-scale demonstration data for mobile manipulation tasks. iii) **Key methodology:** The key methodology involves designing a holonomic base using powered casters, developing a mobile phone teleoperation interface using the WebXR API, and training diffusion policies with collected demonstration data. iv) **Primary results:** The researchers successfully trained policies for six household tasks, with the open fridge task achieving a 10/10 success rate in policy rollouts. v) **Principal implication for AI practitioners:** This open-source design and teleoperation interface can enable AI practitioners to easily collect mobile manipulation data and develop policies for real-world applications, significantly lowering the barrier to entry for mobile manipulation research.  |
| Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning (Read more on [arXiv](https://arxiv.org/abs/2412.11689) or [HuggingFace](https://huggingface.co/papers/2412.11689))| Aleksandr Beznosikov, Philip Zmushko, pichuginad, Andron00e | Here is a concise summary of the research paper "Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning":  i) This paper investigates data protection in Vertical Federated Learning (VFL) against feature reconstruction attacks, focusing on the impact of model architecture. ii) The main research objective is to determine whether Multi-Layer Perceptron (MLP)-based models are more resistant to feature reconstruction attacks than Convolutional Neural Network (CNN)-based models in VFL. iii) The key methodology involves theoretical analysis of orthogonal transformations on data and weights in VFL, and empirical evaluation of state-of-the-art Model Inversion and Feature-space Hijacking attacks on various datasets using MLP and CNN architectures. iv) The primary results show that MLP-based models, unlike CNN-based models, are resistant to UnSplit and Feature-space Hijacking attacks; for instance, the Feature-space Hijacking attack on MNIST with a CNN-based model achieved a reconstruction error of 0.25, while on an MLP-based model, the error was 0.8. v) The principal implication for AI practitioners is that using MLP architectures in VFL can enhance data protection against feature reconstruction attacks without requiring additional defense mechanisms, although they might provide less utility compared to CNNs on image datasets.  |
