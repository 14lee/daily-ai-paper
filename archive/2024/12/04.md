

## Papers for 2024-12-04

| Title | Authors | Summary |
|-------|---------|---------|
| VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation (Read more on [arXiv](https://arxiv.org/abs/2412.02259) or [HuggingFace](https://huggingface.co/papers/2412.02259))| cqf, tfl01, AI4VR, Jethro37, Cheliosoops | VideoGen-of-Thought (VGoT) is a training-free architecture for generating multi-shot, coherent videos.  The research aimed to address the challenge of creating multi-shot videos that maintain narrative logic and visual consistency across different shots.  VGoT employs a four-module pipeline: Script Generation, Keyframe Generation, Shot-Level Video Generation, and a novel cross-shot Smooth Mechanism using latent features and reset boundaries.  VGoT achieved higher Face Consistency (FC) and Style Consistency (SC) scores, particularly across shots, compared to baseline models (0.2738 cross-shot FC score for VGoT vs. a maximum of 0.0686 for baselines).  This provides AI practitioners with a novel method to enhance narrative coherence and cross-shot consistency in generated multi-shot videos, particularly improving transitions between shots for a more natural visual flow.  |
| Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability (Read more on [arXiv](https://arxiv.org/abs/2411.19943) or [HuggingFace](https://huggingface.co/papers/2411.19943))| zptu, Thu-redrobot, SihengLi, Chufan, Jiahao004 | This paper introduces cDPO, a token-level contrastive preference optimization framework for enhancing LLM reasoning capabilities.  The research investigates the impact of individual tokens, particularly "critical tokens," on the outcomes of reasoning tasks.  The core methodology involves contrastive estimation using separately trained positive and negative models on correct and incorrect reasoning trajectories, coupled with a token-level extension of Direct Preference Optimization (DPO).  On the GSM8K benchmark, cDPO achieves an average accuracy of 77.2%, significantly outperforming baseline methods (p < 0.005). This result suggests that AI practitioners can leverage token-level contrastive estimation during preference optimization to improve the accuracy of LLMs on reasoning tasks, specifically by mitigating the negative impact of critical tokens.  |
| Free Process Rewards without Process Labels (Read more on [arXiv](https://arxiv.org/abs/2412.01981) or [HuggingFace](https://huggingface.co/papers/2412.01981))| iseesaw, stingning, ganqu, wendili, lievan | This paper introduces a method for deriving process reward models (PRMs) without step-level labels. The research aimed to reduce the cost and complexity of training PRMs compared to outcome reward models (ORMs) and existing PRM training methods.  The core methodology involves parameterizing the outcome reward as the log-likelihood ratio of policy and reference language models and training an ORM on response-level data.  Experiments on MATH showed that the resulting implicit PRM, when instantiated with cross-entropy loss, outperformed a strong MCTS baseline (Math-Shepherd) by 0.6% while using less than 1/38 of the training data. This implies that AI practitioners can obtain high-performing PRMs at substantially lower cost by leveraging response-level data and this specific reward parameterization, potentially simplifying the development and deployment of reward models for complex reasoning tasks.  |
| AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information? (Read more on [arXiv](https://arxiv.org/abs/2412.02611) or [HuggingFace](https://huggingface.co/papers/2412.02611))| shijiay, MoFanCheng, BreakLee, KaituoFeng, kxgong | This paper introduces AV-Odyssey Bench, a benchmark designed to evaluate audio-visual comprehension in Multimodal Large Language Models (MLLMs).  The research investigates whether MLLMs genuinely understand audio-visual information, or if their performance relies on surface-level patterns.  The benchmark employs 4,555 multiple-choice questions across 26 tasks requiring integration of text, image/video, and audio.  On AV-Odyssey, the best-performing model, GPT-40 (audio caption method), achieved only 34.5% accuracy.  This indicates current MLLMs struggle with complex audio-visual integration, highlighting a critical area for model and dataset improvement, particularly the integration of audio information within multi-modal contexts.  |
| OmniCreator: Self-Supervised Unified Generation with Universal Editing (Read more on [arXiv](https://arxiv.org/abs/2412.02114) or [HuggingFace](https://huggingface.co/papers/2412.02114))| Harry Yang, Lan Wang, sernam, Harold328 | Here's a concise summary of the AI research paper following your specified guidelines:  i) **One-line summary:** OmniCreator, a self-supervised framework, achieves unified image and video generation and universal text-guided editing by leveraging the original video as a denoising condition.  ii) **Main research question/objective:** To develop a unified framework capable of both text-prompted image and video generation and universal text-guided editing, addressing limitations of existing methods focused on specific editing types or requiring additional controls.  iii) **Key methodology:** A self-supervised approach using original text-video pairs as conditions, with the same video serving as a denoising target, combined with an adapter and query transformer for multimodal fusion and spatiotemporal low-rank adaptations (LoRA) for efficiency.  iv) **Primary results:** OmniCreator exhibits substantial superiority over existing models, achieving an average overall user study score of 4.33 on OmniBench-99 for video editing, compared to scores ranging from 2.00 to 3.33 for other methods.  v) **Principal implication for AI practitioners:**  OmniCreatorâ€™s self-supervised approach and superior performance on a comprehensive video editing benchmark demonstrates the potential for significant advancements in controllable generative models, particularly regarding unified image/video processing and efficient, flexible editing capabilities.  The paper lacks a detailed quantitative evaluation on a standardized image editing benchmark.  |
| OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2412.02592) or [HuggingFace](https://huggingface.co/papers/2412.02592))| zichenwen, ouyanglinke, binwang, qintong21, Carkham | OHRBench, a new benchmark for evaluating the impact of OCR on Retrieval-Augmented Generation (RAG) systems, reveals that OCR noise degrades RAG performance.  The research investigates how OCR noise affects RAG by creating a dataset of PDFs, ground truth structured data, Q&As, and perturbed data with varying OCR noise levels.  The key methodology involves evaluating several OCR solutions and then systematically analyzing the impact of semantic and formatting noise on retrieval and generation components of RAG. Results show even the best OCR solution reduces end-to-end RAG F1-score by at least 2.93 points compared to ground truth, and semantic noise consistently degrades performance across different RAG components.  AI practitioners developing RAG systems should prioritize mitigating OCR noise for optimal performance, particularly focusing on semantic accuracy.  |
| Scaling Image Tokenizers with Grouped Spherical Quantization (Read more on [arXiv](https://arxiv.org/abs/2412.02632) or [HuggingFace](https://huggingface.co/papers/2412.02632))| Jiangtao Wang, kessel666, briqnn, yifAI, Doreamonzzz | This paper introduces Grouped Spherical Quantization (GSQ) for training image tokenizers.  The research aims to address limitations in current image tokenizers related to GAN-based hyperparameters, biased comparisons, and a lack of scaling analysis.  GSQ employs spherical codebook initialization, lookup regularization, and latent decomposition to improve training and reconstruction quality.  GSQ-GAN achieves a reconstruction FID (rFID) of 0.50 with 16x downsampling on ImageNet at 256x256 resolution. This research suggests that AI practitioners can achieve improved reconstruction quality and efficiency in image tokenizers using GSQ, especially for tasks involving high spatial compression.  |
| LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences (Read more on [arXiv](https://arxiv.org/abs/2412.01292) or [HuggingFace](https://huggingface.co/papers/2412.01292))| Sunxy111, Xiaomabufei, senfu, PeihaoChen, Hoyard | LSceneLLM enhances 3D scene understanding in large and complex environments. The research aimed to improve 3D Vision-Language Models' (3D-VLMs) ability to locate task-relevant visual information in large 3D scenes. The authors developed LSceneLLM, a framework incorporating a coarse scene understanding module and a scene magnifier module that uses LLM's visual preference for adaptive identification and detailed examination of relevant regions. LSceneLLM outperformed existing methods on the proposed XR-Scene cross-room understanding benchmark and other existing benchmarks; on XR-QA, LSceneLLM achieved a CIDER score of 117.21 compared to 112.80 for the next best method. AI practitioners can use the plug-and-play scene magnifier module to enhance existing 3D-VLMs for improved accuracy in tasks involving large and complex 3D scene understanding.   |
| MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation (Read more on [arXiv](https://arxiv.org/abs/2411.19067) or [HuggingFace](https://huggingface.co/papers/2411.19067))| Dongyoon Han, Song Park, Seungho Lee, Minhyun Lee, bhheo | MaskRIS improves Referring Image Segmentation (RIS) by using a novel masking-based data augmentation strategy.  The research aimed to develop a more effective data augmentation technique for RIS than conventional methods, which degrade performance due to semantic conflicts. The key methodology involves masking image and text inputs, combined with Distortion-aware Contextual Learning (DCL) to leverage both original and masked data. MaskRIS achieved state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg, increasing overall Intersection-over-Union (oIoU) scores by up to 2.25% compared to previous methods.  This implies that AI practitioners working on RIS can significantly enhance model robustness and accuracy by incorporating the MaskRIS data augmentation framework into their training pipelines.  |
| A dynamic parallel method for performance optimization on hybrid CPUs (Read more on [arXiv](https://arxiv.org/abs/2411.19542) or [HuggingFace](https://huggingface.co/papers/2411.19542))| Liu Yucheng, Luo Yu, Haihao | This paper introduces a dynamic parallel method for optimizing Large Language Model (LLM) inference on hybrid CPUs.  The research aims to address the low inference performance on hybrid CPUs caused by imbalanced hardware capabilities among cores. The proposed method dynamically balances the workload for each core before parallel work begins, integrating a new thread scheduler and CPU runtime with the Neural Speed framework.  Results show a 20%-30% improvement in prefill phase latency compared to using OpenMP in Neural Speed, and over 90% of memory bandwidth utilization is achieved for INT4 GEMV on an Ultra-125H.  This provides AI practitioners with a more efficient method for running LLM inference on hybrid CPUs, particularly relevant for client-side deployments where these processors are increasingly prevalent.  |
| VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval (Read more on [arXiv](https://arxiv.org/abs/2412.01558) or [HuggingFace](https://huggingface.co/papers/2412.01558))| Nabeel Mohammed, Md Rizwan Parvez, shafin5, dpaul06 | VideoLights is a novel framework for jointly performing video highlight detection (HD) and moment retrieval (MR).  The research aimed to improve joint HD/MR by addressing limitations in cross-task and cross-modal interactions in existing models.  The framework utilizes a Feature Refinement and Alignment (FRA) module, Bi-Directional Cross-Modal Fusion (Bi-CMF) network,  Unidirectional Joint-Task Feedback Mechanism (Uni-JFM), and leverages LVLMs like BLIP-2. On the QVHighlights dataset, VideoLights-B-pt achieved a state-of-the-art R@0.5 of 70.36% for moment retrieval. This research provides AI practitioners with a new state-of-the-art model and framework for developing more robust and effective video understanding systems for tasks like content management and recommendation.  |
