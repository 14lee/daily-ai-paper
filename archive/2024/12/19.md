

## Papers for 2024-12-19

| Title | Authors | Summary |
|-------|---------|---------|
| TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks (Read more on [arXiv](https://arxiv.org/abs/2412.14161) or [HuggingFace](https://huggingface.co/papers/2412.14161))| Kritanjali Jain, Yuxuan Tang, Boxuan Li, Yufan Song, Frank F. Xu | Here is a concise summary of the paper "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks" based on your specified guidelines:  i) **Summary:** This paper introduces TheAgentCompany, a benchmark for evaluating large language model (LLM) agents on realistic, consequential tasks within a simulated software company environment. ii) **Main research question or objective:** To assess the capability of LLM agents to autonomously perform complex, multi-step, work-related tasks in a realistic setting. iii) **Key methodology used:** A self-contained, simulated software company environment was created using internal websites and data, with tasks requiring agents to browse the web, code, run programs, and communicate with simulated coworkers. iv) **Primary results:** The best-performing agent, powered by Claude 3.5 Sonnet, achieved a 24.0% task completion rate and a 34.4% partial completion score. v) **Principal implication for AI practitioners:** The benchmark demonstrates that while current LLM agents can complete some work-related tasks, significant improvements are needed, particularly in handling complex user interfaces, social interactions, and tasks that lack public training data before they can be reliably deployed for a wide range of real-world applications.  |
| AniDoc: Animation Creation Made Easier (Read more on [arXiv](https://arxiv.org/abs/2412.14173) or [HuggingFace](https://huggingface.co/papers/2412.14173))| Wen Wang, Qiuyu Wang, Hanlin Wang, Hao Ouyang, Yihao Meng | Here is a concise summary of the research paper "AniDoc: Animation Creation Made Easier":  i)  AniDoc is a novel AI model designed to automate 2D animation coloring by converting sketch sequences into colored animations based on a reference character image. ii) Main research question/objective: How to automate the colorization of 2D animation line art while maintaining fidelity to a reference character design and ensuring temporal consistency across frames? iii) Key methodology: A video diffusion model with correspondence-guided colorization, binarization, background augmentation, and a two-stage sparse sketch training strategy. iv) Primary results: AniDoc achieved a PSNR of 19.23, demonstrating superior performance in colorization accuracy compared to existing methods. v) Principal implication for AI practitioners: AI practitioners can utilize AniDoc to significantly reduce the labor costs and time required for 2D animation production by automating the colorization process.  |
| FashionComposer: Compositional Fashion Image Generation (Read more on [arXiv](https://arxiv.org/abs/2412.14168) or [HuggingFace](https://huggingface.co/papers/2412.14168))| Hao Luo, Xiaogang Xu, Xi Chen, Yiyang Wang, Sihui Ji | Here is a concise summary of the research paper "FashionComposer: Compositional Fashion Image Generation":  i)  FashionComposer is a novel framework for generating fashion images that allows for detailed control over garment styles, human poses, and appearances using multi-modal inputs. ii) The main research objective is to develop a highly flexible system capable of handling diverse input modalities and composing multiple visual assets (garments, faces) in a single fashion image generation process. iii) The key methodology involves a diffusion-based model with a universal framework for multi-modal inputs, a reference UNet for extracting appearance features from an "asset library", and a subject-binding attention mechanism to bind appearance features to corresponding text features. iv) The primary result is that FashionComposer outperforms existing methods in multi-object reference generation, achieving a CLIP-I score of 77.60 compared to 69.70 for Emu2. v) For AI practitioners, FashionComposer offers a powerful and flexible framework for compositional fashion image generation, which has direct applications in virtual try-on, controllable model image generation, and human album generation.  |
| Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning (Read more on [arXiv](https://arxiv.org/abs/2412.12953) or [HuggingFace](https://huggingface.co/papers/2412.12953))| Rudolf Lioutikov, Pulkit Agrawal, Jyothish Pari, Moritz Reuss | Here's a concise summary of the research paper, strictly adhering to the specified guidelines:  i) **Summary:** The paper introduces Mixture-of-Denoising Experts (MoDE), a novel policy for Imitation Learning that uses a Mixture-of-Experts Transformer architecture with noise-conditioned routing and self-attention for efficient multitask learning. ii) **Main research question or objective:** The main objective is to develop a more computationally efficient Diffusion Policy for Imitation Learning that maintains or surpasses the performance of state-of-the-art Transformer-based Diffusion Policies. iii) **Key methodology used:** The key methodology is a Mixture-of-Experts (MoE) Transformer architecture with a novel noise-conditioned router that assigns tokens to experts based on noise levels during the denoising process, combined with a noise-conditioned self-attention mechanism. iv) **Primary results:** MoDE outperforms existing Diffusion Policies on 134 tasks across four benchmarks, achieving 4.01 on the CALVIN ABC benchmark and surpassing baselines by an average of 57% while using 90% fewer FLOPs. v) **Principal implication for AI practitioners:** AI practitioners can leverage MoDE's architecture for more efficient and scalable Imitation Learning, reducing computational costs during training and inference of Diffusion Policies without sacrificing performance, particularly in multitask settings.  |
| Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation (Read more on [arXiv](https://arxiv.org/abs/2412.14015) or [HuggingFace](https://huggingface.co/papers/2412.14015))| Jiaming Sun, Songyou Peng, Jingxiao Chen, Sida Peng, Haotong Lin | Here is a concise summary of the research paper "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation" following the specified guidelines:  i) **Summary:** This paper introduces "Prompt Depth Anything," a novel paradigm for metric depth estimation that utilizes low-cost LiDAR data as a prompt to guide a depth foundation model, achieving accurate depth output at up to 4K resolution.  ii) **Main research question or objective:** How to effectively prompt depth foundation models to achieve accurate metric depth estimation at high resolution.  iii) **Key methodology:** A concise prompt fusion architecture is used to integrate LiDAR depth at multiple scales within the depth decoder, combined with a scalable data pipeline that includes synthetic LiDAR simulation and real data pseudo-GT depth generation, along with an edge-aware depth loss.  iv) **Primary results:** The method achieves state-of-the-art results on ARKitScenes and ScanNet++ datasets, with a quantitative finding of 0.0132 L1 error on the ARKitScenes dataset at 384 x 512 resolution.  v) **Principal implication for AI practitioners:** AI practitioners can leverage Prompt Depth Anything to enhance the accuracy and resolution of metric depth estimation in applications such as 3D reconstruction and robotic grasping by effectively integrating low-cost LiDAR prompts with depth foundation models.  |
| GUI Agents: A Survey (Read more on [arXiv](https://arxiv.org/abs/2412.13501) or [HuggingFace](https://huggingface.co/papers/2412.13501))| Namyong Park, Gang Wu, Yu Wang, Jian Chen, dangmn | Here is a concise summary of the research paper "GUI Agents: A Survey":  i) This survey provides a comprehensive overview of GUI agents powered by Large Foundation Models (LFMs) that automate human-computer interactions. ii) The main objective is to categorize and analyze existing GUI agent benchmarks, evaluation metrics, architectures, and training methods. iii) The key methodology used is a literature review, synthesizing various types of contributions within the field and proposing a unified framework based on GUI agents' perception, reasoning, planning, and acting capabilities. iv) The primary results include a structured analysis of datasets (e.g., Mind2Web contains 2000 diverse tasks) and environments for evaluating GUI agents across various platforms, along with architectural designs and training strategies. v) The principal implication for AI practitioners is the need for standardized benchmarks and evaluation metrics to systematically assess and advance the development of GUI agents.  |
| AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities (Read more on [arXiv](https://arxiv.org/abs/2412.14123) or [HuggingFace](https://huggingface.co/papers/2412.14123))| Loic Landrieu, Clement Mallet, Nicolas Gonthier, Guillaume Astruc | Here is a concise summary of the research paper "AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities":  i)  AnySat is a novel self-supervised multimodal Earth observation (EO) model designed to handle heterogeneous data with varying resolutions, scales, and modalities. ii)  The main research objective is to develop a single EO model capable of integrating diverse datasets for training and prediction without modality-specific adaptations. iii)  The key methodology is a joint embedding predictive architecture (JEPA) with scale-adaptive spatial encoders, trained on a new multimodal dataset collection called GeoPlex. iv)  The primary results show that AnySat achieves state-of-the-art or near state-of-the-art performance on multiple EO tasks; for instance, it achieved a 72.8 weighted F1 score on the TreeSatAI-TS classification task. v)  For AI practitioners, AnySat offers a versatile pretrained model that can be fine-tuned or linearly probed for various downstream EO tasks, even with new combinations of modalities not seen during pretraining, simplifying the development of applications with diverse EO data.  |
| RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2412.13746) or [HuggingFace](https://huggingface.co/papers/2412.13746))| Yubo Chen, Pengfei Cao, Tianyi Men, Hongbang Yuan, Zhuoran Jin | Here is a concise 4-5 sentence summary of the paper:  i)  **Summary:** The paper introduces RAG-RewardBench, a benchmark for evaluating reward models (RMs) in retrieval-augmented generation (RAG) systems tailored to align with human preferences. ii) **Research Question/Objective:** How to evaluate and select a reliable reward model for preference alignment in RAG language models. iii) **Methodology:** The authors designed four RAG-specific scenarios (multi-hop reasoning, fine-grained citation, appropriate abstain, conflict robustness), incorporated 18 RAG subsets, six retrievers, and 24 RAG language models, and used an LLM-as-a-judge approach for preference annotation. iv) **Results:** Existing RMs are challenged by RAG-RewardBench, with the top-ranked RM, Skywork-Critic-Llama-3.1-70B, achieving only 78.3% accuracy. v) **Implication:** AI practitioners should prioritize developing specialized reward models tailored for RAG systems to improve the alignment of these models with human preferences, as existing reward models show limitations in RAG-specific scenarios.  |
| Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN (Read more on [arXiv](https://arxiv.org/abs/2412.13795) or [HuggingFace](https://huggingface.co/papers/2412.13795))| Shiwei Liu, Lu Yin, Pengxiang Li | Here's a concise summary of the research paper "Mix-LN: Unleashing the Power of Deep Layers by Combining Pre-LN and Post-LN":  i) **Summary**: This paper introduces Mix-LN, a novel normalization technique that combines Pre-Layer Normalization (Pre-LN) and Post-Layer Normalization (Post-LN) to improve the training and performance of deep layers in Large Language Models (LLMs). ii) **Main research question/objective**: The main research objective is to investigate whether the choice of layer normalization (Pre-LN vs. Post-LN) impacts the effectiveness of deeper layers in LLMs and to develop a method that addresses the limitations of both approaches. iii) **Key methodology**: The authors empirically evaluated layer effectiveness using angular distance and performance drop metrics across various model sizes (70M to 7B parameters) and compared Pre-LN, Post-LN, and the proposed Mix-LN, which applies Post-LN to earlier layers and Pre-LN to deeper layers. iv) **Primary results**: Mix-LN consistently outperformed both Pre-LN and Post-LN in pre-training; specifically, Mix-LN achieved a perplexity of 18.18 on the LLaMA-1B model, compared to 18.65 for Pre-LN. v) **Principal implication for AI practitioners**: AI practitioners can leverage Mix-LN to enhance the training of LLMs by ensuring more uniform gradient norms across all layers, leading to improved model capacity without increasing model size.  |
| Learning from Massive Human Videos for Universal Humanoid Pose Control (Read more on [arXiv](https://arxiv.org/abs/2412.14172) or [HuggingFace](https://huggingface.co/papers/2412.14172))| Junjie Ye, Tianheng Shi, Siqi Song, Siheng Zhao, Jiageng Mao | Here's a concise summary of the AI research paper "Learning from Massive Human Videos for Universal Humanoid Pose Control":  **Summary:**  i)  This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, and UH-1, a Transformer-based model for universal language-conditioned pose control of humanoid robots. ii)  The main research objective is to investigate whether a universal humanoid pose control model can be trained using large-scale text-action pairs derived from massive human videos. iii)  The key methodology involves curating Humanoid-X through data mining, video captioning, motion retargeting from humans to humanoids, and reinforcement learning, followed by training UH-1 to map text instructions to humanoid actions using a Transformer architecture. iv)  The primary results show that UH-1 achieves state-of-the-art performance on the HumanoidML3D benchmark, with a Frechet Inception Distance (FID) score of 0.379. v)  The principal implication for AI practitioners is that leveraging massive human video data and the proposed training pipeline can enable the development of highly generalizable and scalable humanoid control models, significantly advancing the deployment of adaptable humanoid robots in real-world applications.  |
| ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2412.12571) or [HuggingFace](https://huggingface.co/papers/2412.12571))| Yupeng Shi, Zhi-Fan Wu, Wei Wang, Lianghua Huang, bibona | Here is a concise summary of the research paper "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers":  **i) Summary:** ChatDiT is a zero-shot, general-purpose, interactive visual generation framework that uses pretrained diffusion transformers to perform various visual tasks based on free-form natural language instructions, without any additional training.  **ii) Main research question or objective:** The main objective was to develop a training-free framework leveraging the inherent in-context generation capabilities of pretrained diffusion transformers for interactive and general-purpose image generation.  **iii) Key methodology used:** The methodology involved a multi-agent system with Instruction-Parsing, Strategy-Planning, and Execution Agents, using an in-context toolkit to perform actions with diffusion transformers.  **iv) Primary results:** ChatDiT achieved a Top-1 performance score of 23.19 out of 100 on the IDEA-Bench, outperforming other models.  **v) Principal implication for AI practitioners:** AI practitioners can leverage ChatDiT as a baseline for zero-shot task generalization in image generation, but should be aware of its limitations in handling long contexts and preserving fine-grained details, and work towards addressing these.  |
| VidTok: A Versatile and Open-Source Video Tokenizer (Read more on [arXiv](https://arxiv.org/abs/2412.13061) or [HuggingFace](https://huggingface.co/papers/2412.13061))| Li Song, Xinle Cheng, Junliang Guo, Tianyu He, Anni Tang | Here is a concise summary of the paper "VidTok: A Versatile and Open-Source Video Tokenizer" adhering to the specified guidelines:  **Summary:** i)  The paper introduces VidTok, an open-source video tokenizer that achieves state-of-the-art performance in both continuous and discrete video tokenization. ii) The main research objective is to develop a versatile video tokenizer that outperforms existing methods in video reconstruction quality across various metrics. iii) The key methodology includes a novel model architecture with separate spatial and temporal sampling, the integration of Finite Scalar Quantization (FSQ) for discrete tokenization, and a two-stage training strategy. iv) In discrete tokenization, VidTok with FSQ (codebook size 262,144) achieves a PSNR of 29.82 on the MCL-JCV dataset, outperforming previous methods. v) For AI practitioners, VidTok offers an advanced tool for video generation and understanding tasks, providing improved video tokenization performance.  |
| CAD-Recode: Reverse Engineering CAD Code from Point Clouds (Read more on [arXiv](https://arxiv.org/abs/2412.14042) or [HuggingFace](https://huggingface.co/papers/2412.14042))| Anis Kacem, Kseniya Cherenkova, Dimitrios Mallis, Elona Dupont, Danila Rukhovich | Here is a concise summary of the research paper "CAD-Recode: Reverse Engineering CAD Code from Point Clouds" based on your specific guidelines:  i)  CAD-Recode translates 3D point clouds into executable Python code to reconstruct CAD models. ii) The main research objective is to develop a method for reverse engineering CAD models from point clouds by leveraging the code generation capabilities of large language models (LLMs). iii) The key methodology involves fine-tuning a pre-trained LLM (Qwen2-1.5B) augmented with a point cloud projector to map input point clouds into Python code representations of CAD sketch-extrude sequences, utilizing a novel synthetic dataset of one million CAD models. iv) The primary results show that CAD-Recode achieves a 10 times lower mean Chamfer distance compared to state-of-the-art methods on the DeepCAD dataset. v) The principal implication for AI practitioners is that CAD-Recode offers a new approach to CAD model reconstruction, providing an effective way to generate editable and interpretable CAD models directly from point cloud data using LLMs, without the need for large, hand-crafted datasets.  |
| AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge (Read more on [arXiv](https://arxiv.org/abs/2412.13670) or [HuggingFace](https://huggingface.co/papers/2412.13670))| Shuai Zhao, Ruiwen Zhou, Yuxi Xie, Liangming Pan, Xiaobao Wu | Here is a concise summary of the research paper "AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge":  i) **Summary:** This paper introduces AntiLeak-Bench, a framework for automatically constructing contamination-free benchmarks for evaluating large language models (LLMs) using updated real-world knowledge. ii) **Main research question/objective:** To develop a method for creating LLM evaluation benchmarks that are free from data contamination and can be easily updated without human labor. iii) **Key methodology:** The authors use Wikidata to identify knowledge updated after an LLM's cutoff time, construct question-answering samples based on this knowledge with supporting documents from Wikipedia, and automate the entire benchmark creation and update process. iv) **Primary results:** Evaluations on AntiLeak-Bench show most models score below 50 in Exact Match (EM), with only GPT-40-mini and GPT-40 achieving EM scores around 70. v) **Principal implication for AI practitioners:** AI practitioners should use AntiLeak-Bench to obtain a more reliable assessment of LLMs' true capabilities, ensuring evaluations are not inflated by data contamination, especially when evaluating on knowledge-dependent tasks.  |
| LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer (Read more on [arXiv](https://arxiv.org/abs/2412.13871) or [HuggingFace](https://huggingface.co/papers/2412.13871))| Xuesong Yang, Yidan Zhang, Yifan Liu, Yipeng Zhang, guozonghao96 | Here is a concise summary of the research paper "LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer":  i) **Summary:** The paper introduces LLaVA-UHD v2, a multimodal large language model (MLLM) that integrates a high-resolution feature pyramid via a hierarchical window transformer to enhance visual understanding. ii) **Main research question/objective:** The main objective is to address the limitation of vision transformers (ViTs) in capturing diverse visual granularity in MLLMs by constructing and integrating a high-resolution feature pyramid. iii) **Key methodology:** The key methodology involves a Hiwin transformer comprising an inverse feature pyramid constructed by a ViT-derived feature up-sampling process and a hierarchical window attention mechanism that condenses multi-level feature maps. iv) **Primary results:** LLaVA-UHD v2 achieved superior performance over existing MLLMs, demonstrating an average boost of 3.7% across 14 benchmarks compared with the baseline method. v) **Principal implication for AI practitioners:** AI practitioners can leverage the Hiwin transformer to develop MLLMs capable of handling tasks requiring diverse visual granularity, such as high-resolution image perception and visual grounding, with improved accuracy.  |
