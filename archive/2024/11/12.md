

## Papers for 2024-11-12

| Title | Authors | Summary |
|-------|---------|---------|
| Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2411.07232) or [HuggingFace](https://huggingface.co/papers/2411.07232))| Gal Chechik, Lior Wolf, Dvir Samuel Yuval Atzmon, Rinon Gal, Yoad Tewel | Add-it is a training-free method for inserting objects into images based on text prompts.  The objective is to develop a method for adding objects to images based on textual instructions that preserves image context and structure while placing objects naturally within the scene.  The method leverages pretrained text-to-image diffusion models, incorporating a weighted extended self-attention mechanism that balances information from a source image, a target image, and a text prompt, alongside a novel Subject-Guided Latent Blending mechanism and a structure transfer step.  On the Additing Affordance Benchmark, which evaluates the plausibility of object placement, Add-it achieves an affordance score of 0.828, significantly outperforming other methods.  Human evaluations on the Emu Edit Benchmark favored Add-it outputs in 80% of cases.  AI practitioners can leverage Add-it to enhance existing text-to-image models for object insertion tasks without requiring additional training or fine-tuning of these large models, thereby enabling more realistic image editing applications.  |
| OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision (Read more on [arXiv](https://arxiv.org/abs/2411.07199) or [HuggingFace](https://huggingface.co/papers/2411.07199))| Xinrun Du, Weiming Ren, Zheyang Xiong, Cong Wei, wenhu | OmniEdit is an instruction-based image editing model trained using specialist supervision.  The research aims to address limitations in existing instruction-guided image editing models, such as biased editing capabilities and poor data quality.  The key methodology involves training a generalist editing model supervised by seven specialist models, utilizing importance sampling based on large multimodal model (LMM) scoring, and introducing a novel diffusion-transformer architecture called EditNet.  OMNI-EDIT achieved a 0.20 higher accuracy compared to the strongest baseline CosXL-Edit on the proposed OMNI-EDIT-BENCH dataset. This implies that AI practitioners can leverage specialist models and LMM-based scoring during training to develop more generalized and robust image editing models capable of performing diverse editing tasks on images with varying resolutions and aspect ratios.  |
| Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.07140) or [HuggingFace](https://huggingface.co/papers/2411.07140))| Hui Huang, Yingshui Tan, Jiaheng Liu, Shilong Li, Yancheng He | Chinese SimpleQA is a benchmark to evaluate the factuality of large language models (LLMs) in answering short, fact-seeking questions in Chinese.  The research aimed to create a comprehensive Chinese benchmark for evaluating LLM factuality. The methodology involved automated question-answer pair generation from knowledge sources, followed by human verification and filtering for difficulty and adherence to static answer criteria.  Only two closed-source LLMs (o1-preview and Doubao-pro-32k) surpassed the 60% accuracy threshold.  The benchmark highlights the need for continued improvement in Chinese LLM factuality and provides a resource for evaluating and enhancing performance in Chinese knowledge domains.  |
| Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2411.07126) or [HuggingFace](https://huggingface.co/papers/2411.07126))| Tiffany Cai, Yogesh Balaji, Maciej Bala, Yuval Atzmon, NVIDIA | Edify Image is a family of diffusion models for generating high-quality, photorealistic images.  The research aimed to develop diffusion models capable of generating high-resolution images with precise controllability. The key innovation is the Laplacian Diffusion Model, a multi-scale approach where image frequency bands are attenuated at varying rates during a cascaded diffusion process.  The two-stage text-to-image model can generate images at 1K resolution, and an upsampler further refines these to 4K.  AI practitioners can leverage these models for various applications like text-to-image synthesis, upsampling, and image editing with ControlNets, leveraging the novel Laplacian diffusion approach for enhanced control over image generation at multiple scales.  |
| IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2411.06208) or [HuggingFace](https://huggingface.co/papers/2411.06208))| Yongbin Li, Fei Huang, Cheng Fu, Haiyang Yu, Xinghua Zhang | IOPO enhances large language models' (LLMs) ability to follow complex instructions.  The research aims to improve LLMs' handling of intricate, multi-constraint instructions.  The authors introduce a new benchmark, TRACE, and an alignment method called Input-Output Preference Optimization (IOPO), which considers both input and output preferences.  IOPO demonstrated an 8.15% improvement on in-domain data and a 6.29% improvement on out-of-domain data compared to Supervised Fine-Tuning (SFT) regarding complex instruction following.  This finding provides AI practitioners with a novel alignment technique to optimize LLMs for applications requiring nuanced instruction understanding and adherence.   |
| M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework (Read more on [arXiv](https://arxiv.org/abs/2411.06176) or [HuggingFace](https://huggingface.co/papers/2411.06176))| Maojia Song, Chaoqun Liu, Hou Pong Chan, Liying Cheng, Yew Ken Chia | M-LongDoc introduces a benchmark and retrieval-aware tuning framework for multimodal long document understanding.  The research aims to improve large multimodal models' ability to understand and answer questions on lengthy, complex multimodal documents.  A retrieval-aware tuning approach is proposed, incorporating distracting content from different modalities and pages during training.  Experiments show a 4.6% relative improvement in answer correctness using this tuning method compared to baseline open-source models. This improved performance enables more efficient and accurate processing of lengthy multimodal documents, benefiting AI practitioners developing document understanding applications.  |
| Watermark Anything with Localized Messages (Read more on [arXiv](https://arxiv.org/abs/2411.07231) or [HuggingFace](https://huggingface.co/papers/2411.07231))| Matthijs Douze, Teddy Furon, Alain Durmus, Pierre Fernandez, Tom Sander | The Watermark Anything Model (WAM) performs localized image watermarking, enabling segmentation of watermarked areas and extraction of multiple messages.  The research aimed to develop a watermarking method robust to image manipulations like splicing and inpainting, even with small watermarked areas.  A two-stage training process was employed: initial training for robustness at low resolution followed by fine-tuning for imperceptibility and multiple watermark handling using a JND map.  WAM achieved over 85% mIoU for detection of watermarked areas when hiding five 32-bit messages in 10% areas of an image, even after horizontal flips and contrast adjustments.  AI practitioners can utilize WAM for robust localization of watermarked areas and extraction of distinct messages from within a single image, enabling novel applications like verification of content origin and detection of AI-generated objects within images.  |
| Counterfactual Generation from Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.07180) or [HuggingFace](https://huggingface.co/papers/2411.07180))| Ryan Cotterell, Anej Svete, vesteinn, Shauli | This paper introduces a framework for generating true counterfactual strings from language models.  The research aimed to understand and mitigate the unintended side effects of common language model intervention techniques. The key methodology involved formulating language models as Generalized Structural-equation Models (GSEMs) using the Gumbel-max trick, enabling counterfactual reasoning.  Results showed that even "minimal" interventions like MEMIT and linear steering induce significant semantic shifts in generated text, with instruction tuning interventions showing the most unintended side-effects (sharing only 24% of tokens with original strings on average).  This implies that AI practitioners should carefully evaluate the potential for unintended consequences, even with seemingly targeted interventions, and consider the proposed GSEM framework for analyzing and mitigating these effects.  |
| Game-theoretic LLM: Agent Workflow for Negotiation Games (Read more on [arXiv](https://arxiv.org/abs/2411.05990) or [HuggingFace](https://huggingface.co/papers/2411.05990))| Julie Chen, Alfonso Amayuelas, Lingyao Li, Ollie Liu, Wenyue Hua | This paper investigates the rationality of Large Language Models (LLMs) in strategic decision-making within game-theoretic scenarios.  The research objective is to evaluate LLM rationality in both complete and incomplete information games and explore methods to enhance it.  The authors design and implement game-theory-inspired workflows, including dominant strategy search and backward induction, to guide LLM reasoning.  In "Deal or No Deal",  Claude-3.5 Sonnet with workflow achieved a 95.45% agreement rate.  A key implication for AI practitioners is that incorporating structured, game-theoretic workflows into LLM agents can significantly improve their negotiation performance and strategic decision-making in complex, multi-agent environments, but the choice of *whether* to use a workflow is itself a strategic decision.  |
| Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.06272) or [HuggingFace](https://huggingface.co/papers/2411.06272))| Yiyan Qi, Zhouchi Lin, Huanyi Su, Junxi Liu, Xiaojun Wu | Golden Touchstone is a bilingual benchmark for evaluating financial large language models (LLMs).  The research aimed to create a comprehensive, bilingual benchmark to evaluate FinLLMs on a wider range of tasks and in both English and Chinese. The benchmark includes 22 datasets across eight core financial NLP tasks,  and performance was assessed for several LLMs including GPT-40, Llama-3, and a newly developed model, Touchstone-GPT, trained using continuous pre-training and financial instruction tuning.  Llama-3 achieved the highest Weighted-F1 score (0.5116) on the English stock movement prediction task, though all models underperformed on this challenging task.  This suggests that current LLMs struggle with complex financial prediction tasks and that benchmarks like Golden Touchstone are crucial for directing further research and model development in financial AI.  |
| Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction (Read more on [arXiv](https://arxiv.org/abs/2411.06424) or [HuggingFace](https://huggingface.co/papers/2411.06424))| Adam Mahdi, Harry Mayne, Filip Sondej, Yushi Yang | This paper investigates the mechanisms by which Direct Preference Optimization (DPO) reduces toxicity in language models. The research aims to determine how DPO’s internal mechanisms lead to toxicity reduction in language models, challenging the existing explanation that it primarily dampens the most toxic MLP neurons.  The study uses ablation of toxic neurons, activation patching, and projection of neuron activation changes onto a toxicity probe in GPT-2 medium.  Results show that dampening toxic neurons accounts for only 31.8% of the total toxicity reduction, with a significant portion coming from promoting anti-toxicity via other neuron groups and noisy adjustments across many neurons. This suggests for AI practitioners that mitigating toxicity in LLMs requires a more nuanced approach than simply targeting the most toxic neurons, and that a more holistic understanding of neuron dynamics is essential for effective toxicity reduction.  |
| KMM: Key Frame Mask Mamba for Extended Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2411.06481) or [HuggingFace](https://huggingface.co/papers/2411.06481))| Feng Chen, Qi Chen, Akide Liu, Zeyu Zhang, Ha0Tang | This paper introduces Key Frame Mask Mamba (KMM) for generating extended human motion sequences from text.  The research aims to address limitations of existing methods, specifically memory decay and weak text-motion alignment, in generating long and complex motions from text prompts. The core methodology involves a novel key frame masking strategy based on local density and a contrastive learning approach for text-motion alignment within the Mamba architecture.  On the BABEL dataset, KMM achieved a 57% improvement in Frechet Inception Distance (FID) compared to previous state-of-the-art methods. This implies that AI practitioners can leverage KMM to generate higher-quality, more text-aligned extended motion sequences, potentially benefiting applications in animation, gaming, and virtual reality.  |
