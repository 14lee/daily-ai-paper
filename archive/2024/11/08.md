

## Papers for 2024-11-08

| Title | Authors | Summary |
|-------|---------|---------|
| OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.04905) or [HuggingFace](https://huggingface.co/papers/2411.04905))| Jiaran Hao, Jason Klein Liu, Tianhao Cheng, Siming Huang, Zenithwang | OpenCoder is a top-tier, open-source code large language model (LLM) with reproducible datasets and training pipelines.  The research aimed to create a high-performing, fully transparent code LLM and investigate data curation strategies for such models. Key methodologies included code-optimized data cleaning and deduplication, recall of code-related text corpora, and use of high-quality synthetic data in annealing and supervised fine-tuning stages. OpenCoder-8B achieved a zero-shot pass@1 rate of 68.9% on HumanEval.  The transparent, reproducible nature of OpenCoder provides a powerful model and robust foundation for researchers and practitioners to accelerate and reproduce advancements in code AI.   |
| ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2411.05003) or [HuggingFace](https://huggingface.co/papers/2411.05003))| David E. Jacobs, Nikhil Karnad, Shiran Zada, Roni Paiss, David Junhao Zhang | ReCapture enables generating novel camera trajectories for existing user-provided videos while preserving scene content and dynamics.  The research aims to develop a method for generating videos with new camera trajectories from single user-provided videos without needing paired training data.  The method uses masked video fine-tuning with spatial and temporal Low-Rank Adaptations (LoRAs) applied to a pre-trained video diffusion model, conditioned on an intermediate "anchor video" generated via either point cloud rendering or multi-view diffusion. On the Kubric-4D dataset, ReCapture achieves a PSNR of 20.92, outperforming existing 4D reconstruction and generative methods. This provides AI practitioners with a technique to manipulate camera motion in existing videos without requiring extensive 4D datasets or explicit 3D scene representations, facilitating applications in video editing and content creation.  |
| BitNet a4.8: 4-bit Activations for 1-bit LLMs (Read more on [arXiv](https://arxiv.org/abs/2411.04965) or [HuggingFace](https://huggingface.co/papers/2411.04965))| Furu Wei, Shuming Ma, Hongyu Wang | BitNet a4.8 introduces a hybrid quantization and sparsification strategy enabling 4-bit activations for 1-bit Large Language Models (LLMs).  The research aimed to reduce the inference cost of 1-bit LLMs while maintaining performance comparable to higher-precision models like BitNet b1.58.  The method involves using 4-bit activations for inputs to attention and feed-forward network layers, sparsifying intermediate states with 8-bit quantization, and a two-stage training recipe from 8-bit to 4-bit activations.  For a 7B parameter model, BitNet a4.8 achieved similar performance to BitNet b1.58 on downstream tasks, while having only 55% activated parameters (3.4B).  This allows AI practitioners to deploy and infer large language models more efficiently with reduced computational and memory requirements by leveraging 4-bit activations and sparsity.  |
| DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion (Read more on [arXiv](https://arxiv.org/abs/2411.04928) or [HuggingFace](https://huggingface.co/papers/2411.04928))| Zilong Chen, Fangfu Liu, Shuo Chen, Wenqiang Sun, yikaiw | DimensionX generates 3D and 4D scenes from a single image using controllable video diffusion.  The research aims to create photorealistic 3D and 4D scenes from single images using controllable video diffusion, addressing the limited spatial and temporal control in existing video diffusion models. The key methodology is ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from specifically curated datasets, enabling control over individual dimensions and their combination.  On the Tank and Temples dataset for sparse-view 3D generation, DimensionX achieves 20.42 PSNR, 0.668 SSIM, and 0.185 LPIPS, outperforming baseline methods. This provides AI practitioners with a more controllable and effective approach for generating 3D and 4D content from limited input data, enabling applications in various fields like virtual reality and content creation.  |
| Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2411.04996) or [HuggingFace](https://huggingface.co/papers/2411.04996))| Ning Dong, Srinivasan Iyer, Liang Luo, Lili Yu, WxWx | Mixture-of-Transformers (MoT) accelerates multi-modal foundation model pretraining by decoupling non-embedding parameters by modality.  The paper investigates whether modality-specific parameterization in transformers can improve multi-modal pretraining efficiency without compromising performance.  MoT isolates parameters like feed-forward networks, attention matrices, and layer normalization by modality while maintaining global self-attention across all input tokens.  This creates separate transformer towers for each modality.  In the Chameleon 7B text and image generation setting, MoT matched dense model performance using only 55.8% of the FLOPs. Across various multi-modal datasets and training setups (Chameleon, Chameleon+Speech, Transfusion), MoT consistently reduced training FLOPs and wall-clock time, particularly for image generation. Further analysis comparing MoT against Mixture-of-Experts and analyzing modality separation effects via Leave-One-Out analysis is provided, but the methodology used in these analyses is not fully clear.  AI practitioners can use MoT to significantly reduce computational costs and training time for large multi-modal foundation models without significant performance degradation, especially in image-related tasks.  |
| Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2411.04496) or [HuggingFace](https://huggingface.co/papers/2411.04496))| Ho-Jin Choi, Kyeongjin Oh, Junyoung Youn, Dokyong Lee, Young-Jun Lee | THANOS enhances LLM-based conversational agents by infusing them with a "skill-of-mind" process.  The research aims to improve the quality and social appropriateness of LLM responses in interactive dialogue settings by incorporating conversational skills.  A new skill-of-mind-annotated dataset, MULTIFACETED SKILL-OF-MIND, containing roughly 100K conversations, was created and used to fine-tune LLaMA models of varying sizes (1B, 3B, and 8B parameters).  THANOS 8B achieved an average of 29.7% accuracy on skill classification across multiple datasets, a substantial improvement over baseline LLM-based agents. AI practitioners can use THANOS and the MULTIFACETED SKILL-OF-MIND dataset to develop more socially adept and engaging conversational agents by grounding response generation in relevant conversational skills.  |
| TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.04709) or [HuggingFace](https://huggingface.co/papers/2411.04709))| Yi Yang, Wenhao Wang | TIP-I2V is a novel million-scale dataset of user-provided text and image prompts for image-to-video generation.  The research aimed to create a dedicated dataset for studying user prompts in image-to-video generation, which was lacking previously. The dataset was curated by collecting text and image prompts from Pika Discord channels, along with generated videos from five state-of-the-art image-to-video models. The authors found significant semantic differences between TIP-I2V prompts and those in existing text-to-video (VidProM) and text-to-image (DiffusionDB) datasets, with TIP-I2V focusing on animating existing image content.  In benchmark evaluations using TIP-I2V, the early commercial model Pika outperformed the latest open-source model, CogVideoX-5B, in 8 out of 10 evaluation dimensions. This finding indicates that AI practitioners should consider real-world user prompt data when developing and evaluating image-to-video models.  |
| DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation (Read more on [arXiv](https://arxiv.org/abs/2411.04999) or [HuggingFace](https://huggingface.co/papers/2411.04999))| Chris Paxton, Soumith Chintala, Mohit Warke, Zhanqiu Guo, Peiqi Liu | DynaMem is a novel spatio-semantic memory architecture for open-vocabulary mobile manipulation in dynamic environments.  The research aimed to address the limitation of current open-vocabulary mobile manipulation systems that assume static environments, hindering real-world applicability.  The core methodology involves a dynamic 3D voxel map that adds and removes points based on observed changes, combined with either vision-language model features or multimodal LLM queries for object localization. In real-world robot experiments, DynaMem achieved a 70% pick-and-drop success rate on non-stationary objects, a 2x improvement over static baselines. This improvement demonstrates the value of dynamic memory for real-world robotic manipulation systems and offers AI practitioners a more robust approach for object interaction in changeable environments.  |
| Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks? (Read more on [arXiv](https://arxiv.org/abs/2411.05000) or [HuggingFace](https://huggingface.co/papers/2411.05000))| Samuel Albanie, Kai Han, Jonathan Roberts | This paper evaluates the long-context retrieval capabilities of 17 Large Language Models (LLMs).  The research investigates how effectively LLMs utilize their context windows, particularly in following "threads" of linked information.  The study uses synthetically generated datasets of key-value pairs (UUIDs) with varying context lengths up to 900k tokens and tests performance on single/multiple needle retrieval, conditional retrieval, and threading/multi-threading tasks.  Results show performance degradation with increasing context lengths and thread lengths in most models; for example, Gemini 1.5 Flash achieves 24% accuracy on multiple needle retrieval with 10 needles at a context length of 128k characters, but only 10% accuracy at 630k characters. This suggests the existence of a task-specific effective context limit shorter than the advertised model limit, which has implications for practical deployment scenarios.  |
| GazeGen: Gaze-Driven User Interaction for Visual Content Generation (Read more on [arXiv](https://arxiv.org/abs/2411.04335) or [HuggingFace](https://huggingface.co/papers/2411.04335))| Kao-Den Chang, Wei-Te Mark Ting, Sai Qian Zhang, Ziyun Li, He-Yen Hsieh | GazeGen is a novel system for generating and editing visual content using real-time gaze tracking.  The research aimed to create a hands-free, intuitive system for visual content manipulation using eye gaze.  The system combines a novel lightweight gaze estimation model (DFT Gaze) with object detection and generative AI techniques like Stable Diffusion.  DFT Gaze, with only 281K parameters, achieved a mean angular gaze error of 2.14° on the AEA dataset and operates 2x faster on edge devices than a larger model. This efficient and accurate real-time gaze estimation allows AI practitioners to develop novel human-computer interaction methods for visual content creation and editing accessible on resource-constrained devices.  |
| RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval (Read more on [arXiv](https://arxiv.org/abs/2411.04752) or [HuggingFace](https://huggingface.co/papers/2411.04752))| Subhankar Maity, Aniket Deroy | This paper presents a novel approach for retrieving information from code-mixed text.  The research aimed to improve information retrieval from Roman transliterated Bengali mixed with English, particularly in online conversations.  The methodology involved using GPT-3.5 Turbo with carefully crafted prompts and integrating the output into a mathematical model considering sequential document dependencies.  Results showed a marginal improvement in Mean Average Precision (MAP) from 0.701773 to 0.703734 in the best-performing submission. This suggests that prompting LLMs combined with mathematical modeling can offer minor improvements for information retrieval in code-mixed text, but further research is needed for substantial gains.  |
| SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.04989) or [HuggingFace](https://huggingface.co/papers/2411.04989))| Igor Gilitschenski, Yash Kant, Ziyi Wu, Sherwin Bahmani, Koichi Namekata | SG-I2V offers zero-shot control over object and camera trajectories in image-to-video generation.  The research aimed to develop a method for controllable image-to-video generation without the computational expense of fine-tuning or reliance on external datasets.  The key methodology involved modifying the spatial self-attention mechanism within a pre-trained video diffusion model (SVD) to align feature maps across frames and then optimizing the latent representations to enforce feature similarity along specified trajectories.  On the VIPSeg dataset, SG-I2V achieved a mean object motion control (ObjMC) score of 14.43, demonstrating competitive motion fidelity compared to supervised methods.  This offers AI practitioners a computationally efficient method for controlling video generation dynamics without requiring training data with motion annotations, streamlining the creation of videos with user-specified motion patterns.  |
| VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos (Read more on [arXiv](https://arxiv.org/abs/2411.04923) or [HuggingFace](https://huggingface.co/papers/2411.04923))| Eric Xing, Jiale Cao, Wenqi Zhu, Hanan Gani, Shehan Munasinghe | VideoGLaMM is a large multimodal model designed for pixel-level visual grounding in videos, connecting language instructions with spatio-temporal visual content.  The research aimed to develop a model capable of generating text responses intertwined with spatio-temporal object masks, demonstrating a fine-grained understanding of video content.  The key methodology involved a dual vision encoder (spatial and temporal), a large language model (LLM), a spatio-temporal pixel decoder, and tunable Vision-Language (V→L and L→V) adapters, trained on a newly curated dataset of grounded video-QA triplets.  VideoGLaMM achieved a mean Intersection over Union (mIOU) of 62.34% and a Recall of 0.103 on a grounded conversation generation task. This impactful mIOU result indicates that AI practitioners can leverage VideoGLaMM's architecture and training methods to develop models for tasks requiring precise alignment of textual descriptions and visual elements in videos, like video captioning and content retrieval.  |
| SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2411.05007) or [HuggingFace](https://huggingface.co/papers/2411.05007))| Xiuyu Li, Tianle Cai, Zhekai Zhang, Yujun Lin, Muyang Li | SVDQuant is a post-training quantization technique for 4-bit weights and activations in diffusion models.  The research aims to accelerate diffusion models while preserving image quality by quantizing both weights and activations to 4 bits.  The key methodology involves migrating outliers from activations to weights via smoothing, then absorbing these magnified weight outliers using a 16-bit low-rank branch derived from Singular Value Decomposition (SVD), and finally fusing computations with a specialized inference engine called Nunchaku. On the 12B FLUX.1 model, SVDQuant achieved a 3.5x reduction in DiT inference memory and a 3.0x speedup compared to the 4-bit weight-only quantized (NF4 W4A16) baseline on an NVIDIA RTX 4090 GPU. This allows practitioners to deploy large diffusion models on resource-constrained hardware like laptops and accelerate interactive applications.  |
