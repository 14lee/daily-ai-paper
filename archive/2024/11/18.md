

## Papers for 2024-11-18

| Title | Authors | Summary |
|-------|---------|---------|
| LLaVA-o1: Let Vision Language Models Reason Step-by-Step (Read more on [arXiv](https://arxiv.org/abs/2411.10440) or [HuggingFace](https://huggingface.co/papers/2411.10440))| LiYuan, sunlichao137, Yibing, Pengjin, Xkev | LLaVA-01 is a vision-language model designed for improved multi-stage, structured reasoning.  The research aimed to enhance visual reasoning capabilities in VLMs, particularly for complex tasks requiring systematic analysis.  The authors fine-tuned Llama-3.2-11B-Vision-Instruct on a new 100k sample dataset with structured reasoning annotations (LLaVA-01-100k) and introduced stage-level beam search for inference.  LLaVA-01 outperformed the base Llama model by 6.9% on average across six multimodal reasoning benchmarks and surpassed some larger, closed-source models.  This indicates that training with structured reasoning data and employing stage-level beam search can significantly improve the performance and scalability of VLMs for reasoning-intensive tasks.  |
| GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2411.08033) or [HuggingFace](https://huggingface.co/papers/2411.08033))| doubling, hongfz16, ZhaoyangLyu, sczhou, yslan | GaussianAnything introduces a novel framework for 3D generation using a point cloud-structured latent space and cascaded diffusion. The objective is to develop a scalable and interactive 3D generation method addressing challenges in input formats, latent space design, and output representations of existing 3D diffusion models. The method employs a 3D VAE encoding multi-view posed RGB-D-N renderings into a point cloud-structured latent space, followed by cascaded latent diffusion modeling using DiT and flow matching. On the Objaverse dataset, GaussianAnything achieved a Minimum Matching Distance (MMD) of 15.48%, outperforming other image-conditioned methods. The proposed point cloud-structured latent space enables geometry-texture disentanglement and interactive 3D editing, offering AI practitioners a new approach for controllable 3D content creation.  |
| The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use (Read more on [arXiv](https://arxiv.org/abs/2411.10323) or [HuggingFace](https://huggingface.co/papers/2411.10323))| Mingyu Ouyang, AnalMom, QuStar, SiyuanH | This paper presents a preliminary case study of Claude 3.5 Computer Use, a new API-based GUI agent.  The research explores Claude 3.5's capability in real-world desktop environments across web search, workflow, productivity software, and video game domains. The methodology involves curating and testing Claude 3.5 on 20 designed tasks across 12 software or websites, analyzing its planning, action execution, and critic feedback.  Claude 3.5 successfully completed 14 out of 20 tasks (70% success rate).  The results highlight Claude 3.5's potential for automating desktop tasks but also reveal limitations related to scrolling-based navigation, text selection accuracy, and contextually aware navigation that AI practitioners should consider when deploying such models in real-world applications.  |
| Number it: Temporal Grounding Videos like Flipping Manga (Read more on [arXiv](https://arxiv.org/abs/2411.10332) or [HuggingFace](https://huggingface.co/papers/2411.10332))| Vito328, zhouzhouyi, tms28k, kaleidudu, Liang0223 | NumPro enhances Video Temporal Grounding (VTG) in Video Large Language Models (Vid-LLMs) using frame number overlays.  The research aims to improve Vid-LLM performance on VTG tasks, specifically addressing their difficulty in pinpointing event timestamps despite strong visual comprehension.  The core methodology involves augmenting video frames with numerical identifiers, enabling Vid-LLMs to associate visual content with temporal information through a "manga-like" numbered panel approach.  NumPro-FT, fine-tuned on a NumPro-enhanced dataset, achieves a new state-of-the-art on Charades-STA, surpassing previous SOTA by 11.8% on R@0.3. This provides AI practitioners with a simple, yet effective method to significantly boost VTG performance in Vid-LLMs without requiring complex architectural modifications or extensive retraining.  |
