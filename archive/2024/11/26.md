

## Papers for 2024-11-26

| Title | Authors | Summary |
|-------|---------|---------|
| Material Anything: Generating Materials for Any 3D Object via Diffusion (Read more on [arXiv](https://arxiv.org/abs/2411.15138) or [HuggingFace](https://huggingface.co/papers/2411.15138))| Qing Wang, Ziwei Liu, Tengfei Wang, xanderhuang | Material Anything generates physically-based rendering (PBR) materials for 3D objects under diverse lighting and texture conditions. The objective is to create a robust, automated method for generating realistic PBR materials for any 3D object, regardless of its initial texture or lighting. The method uses a two-stage pipeline: an image-space material diffusion model with a confidence mask to handle various lighting scenarios, followed by UV-space material refinement for consistency. On a dataset of textured objects, Material Anything achieves a CLIP score of 89.70, demonstrating improved alignment with text prompts compared to existing methods. This provides AI practitioners with a unified framework for efficient, high-quality PBR material generation, potentially streamlining workflows in applications like game development, virtual reality, and product visualization.  |
| Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator (Read more on [arXiv](https://arxiv.org/abs/2411.15466) or [HuggingFace](https://huggingface.co/papers/2411.15466))| Sungroh Yoon, Heeseung Kim, Jooyoung Choi, Chaehun Shin | Diptych Prompting performs zero-shot subject-driven text-to-image generation through diptych inpainting with a large-scale text-to-image model.  The research aimed to develop a zero-shot method for subject-driven text-to-image generation that improves subject alignment compared to existing encoder-based image prompting methods. The key methodology involved arranging a reference image in the left panel of a diptych, masking the right panel, and using a text prompt describing the desired context for inpainting the right panel with FLUX, while enhancing cross-attention between panels and removing the reference image background. In a human preference study focusing on subject alignment, Diptych Prompting achieved a 77.9% win rate compared to existing methods.  This provides AI practitioners with a novel, effective technique for zero-shot, subject-driven image generation using the inpainting capabilities of large-scale text-to-image models.  |
| From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge (Read more on [arXiv](https://arxiv.org/abs/2411.16594) or [HuggingFace](https://huggingface.co/papers/2411.16594))| Chengshuai Zhao, Alimohammad Beigi, Liangjie Huang, Bohan Jiang, Dawei Li | This paper surveys the emerging field of using large language models (LLMs) as judges for various AI tasks.  The paper aims to provide a comprehensive overview of LLM-based judgment to advance the field.  The authors categorize and analyze existing LLM-as-a-judge methods based on input (point-wise, pair/list-wise) and output (score, ranking, selection) formats, and propose a taxonomy spanning judging attributes, methodologies (tuning, prompting), and applications (evaluation, alignment, retrieval, reasoning). In a benchmark by Zheng et al. (2023), GPT-4 achieved near-human performance when judging open-ended text generation.  AI practitioners can leverage LLMs as automated judges for enhanced evaluations, alignment procedures, retrieval tasks, and complex reasoning pipelines, potentially achieving human-level performance in judging open-ended text generation.  |
| Knowledge Transfer Across Modalities with Natural Language Supervision (Read more on [arXiv](https://arxiv.org/abs/2411.15611) or [HuggingFace](https://huggingface.co/papers/2411.15611))| Marco Grangetto, Emanuele Aiello, luca-molinaro, carloalbertobarbano | This paper introduces Knowledge Transfer, a method for teaching pre-trained visual models novel concepts using only textual descriptions.  The research aims to determine if leveraging pre-existing visual knowledge within a model, combined with textual descriptions, can enable the model to learn new visual concepts without visual examples.  The core methodology involves synthesizing images via model inversion based on textual descriptions of novel concepts, and then fine-tuning the visual encoder with a contrastive loss (InfoNCE) to align visual and textual features.  In experiments on rare image concepts,  CLIP ViT-B/32 achieved 100% accuracy on "Gyroscope" after Knowledge Transfer, compared to 0% baseline accuracy.  This demonstrates the potential for AI practitioners to efficiently introduce new concepts into pre-trained visual models without the need for extensive labeled image datasets, facilitating rapid model adaptation and reducing data acquisition costs.  |
| MH-MoE:Multi-Head Mixture-of-Experts (Read more on [arXiv](https://arxiv.org/abs/2411.16205) or [HuggingFace](https://huggingface.co/papers/2411.16205))| Furu Wei, Shuming Ma, Xun Wu, Shaohan Huang | This paper presents a novel implementation of Multi-Head Mixture-of-Experts (MH-MoE) for improved efficiency and performance.  The objective is to maintain FLOPS and parameter parity with standard Sparse Mixture-of-Experts (SMoE) models while leveraging the multi-head mechanism of MH-MoE. The key methodology involves adding a "heads" dimension and two linear projection layers, adjusting the intermediate dimension and number of experts to maintain FLOPS parity.  Experiments on language models show that MH-MoE achieves a perplexity of 10.51 on the RedPajama dataset with 3 heads and 100,000 training steps, outperforming standard SMoE (10.90) and fine-grained SMoE (10.74).  This implies that AI practitioners can leverage this MH-MoE implementation to improve the performance and efficiency of large language models by using a multi-head attention structure within the MoE framework.  |
| DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation (Read more on [arXiv](https://arxiv.org/abs/2411.16657) or [HuggingFace](https://huggingface.co/papers/2411.16657))| Mohit Bansal, Jaehong Yoon, Han Lin, Jialu Li, Zun Wang | DREAMRUNNER generates long-form, multi-scene storytelling videos with fine-grained control over object motions and appearances.  The research addresses the challenge of creating coherent and dynamic storytelling videos with complex object interactions and transitions.  The methodology involves hierarchical story planning with an LLM, retrieval-augmented test-time adaptation for learning motion and subject priors, and a novel spatial-temporal region-based 3D attention and prior injection module (SR3AI) for video generation.  On the DreamStorySet benchmark, DREAMRUNNER achieved a 13.1% relative improvement in character consistency (CLIP score) compared to VLogger.  This improvement in character consistency offers AI practitioners a more effective method for generating realistic and coherent characters in long-form video content, contributing to more engaging and believable storytelling.  |
| Factorized Visual Tokenization and Generation (Read more on [arXiv](https://arxiv.org/abs/2411.16681) or [HuggingFace](https://huggingface.co/papers/2411.16681))| Zheng Zhang, Pichao Wang, Ziteng Gao, Jianxiong Gao, Zechen Bai | FQGAN improves visual tokenization for image generation by factorizing large codebooks.  The research aims to address the instability and performance saturation of traditional VQ-based tokenizers when scaling codebook size.  The core methodology involves decomposing a large codebook into smaller sub-codebooks, applying disentanglement regularization, and integrating representation learning with pre-trained vision models like CLIP and DINOv2.  FQGAN achieves state-of-the-art reconstruction FID (rFID) of 0.24 on ImageNet 256x256 validation set with an 8x downsampling ratio and a factorized 3x16,384 codebook.  This indicates that AI practitioners can use FQGAN to achieve significantly improved image reconstruction quality and potentially better downstream generation performance when using VQ-based tokenizers.  |
| O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson? (Read more on [arXiv](https://arxiv.org/abs/2411.16489) or [HuggingFace](https://huggingface.co/papers/2411.16489))| Yuxiang Zheng, Yixiu Liu, Xuefeng Li, Haoyang Zou, Zhen Huang | This paper examines replicating OpenAI's O1 model capabilities, particularly focusing on knowledge distillation.  The research aims to evaluate if simple distillation from O1's API, combined with supervised fine-tuning, can surpass O1-preview performance. The key methodology involved distilling O1's API responses for long-thought chains and fine-tuning a base language model (Qwen2.5-Math-72B) on this distilled data. Their distilled and fine-tuned 72B parameter model outperformed O1-preview on the AIME2024 (American Invitational Mathematics Examination) dataset, scoring 13/30 compared to O1-preview's 12/30.  The primary implication for AI practitioners is that while distillation offers rapid performance gains, over-reliance on it may hinder the development of novel AI techniques and potentially create a technological dependency, limiting future breakthroughs.  |
| GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI (Read more on [arXiv](https://arxiv.org/abs/2411.14522) or [HuggingFace](https://huggingface.co/papers/2411.14522))| Zhe Chen, Bin Fu, Wei Li, Yanzhou Su, foreverbeliever | GMAI-VL, a large vision-language model, achieves state-of-the-art results on multimodal medical tasks using the new GMAI-VL-5.5M dataset. The research aimed to improve general medical AI (GMAI) by addressing the lack of specialized medical knowledge in existing large vision-language models.  Researchers created the GMAI-VL-5.5M dataset by converting 219 specialized medical imaging datasets into 5.5 million image-text pairs using an annotation-guided data generation methodology and a three-stage training process (shallow alignment, deep alignment, instruction tuning) for the GMAI-VL model.  GMAI-VL achieved an average accuracy of 88.48% on the OmniMedVQA benchmark.  This provides AI practitioners with a high-performing, specialized model and a comprehensive multimodal dataset for developing and evaluating general medical AI applications.  |
| One Diffusion to Generate Them All (Read more on [arXiv](https://arxiv.org/abs/2411.16318) or [HuggingFace](https://huggingface.co/papers/2411.16318))| Aniruddha Kembhavi, Christopher Clark, Sangho Lee, Tuan Pham, Duong H. Le | OneDiffusion is a unified diffusion model for bidirectional image synthesis and understanding across diverse tasks.  The research aimed to develop a single diffusion model capable of performing multiple image-related tasks without task-specific modules or training.  The core methodology involves modeling all inputs and outputs as a sequence of “views” with varying noise levels during training, enabling flexible conditioning and generation at inference.  On the GenEval benchmark for text-to-image generation at 1024x1024 resolution, OneDiffusion achieved a score of 0.65. This unified approach offers AI practitioners a more versatile and scalable solution for image-related tasks, potentially simplifying model development and deployment by eliminating the need for multiple specialized models.  |
| VisualLens: Personalization through Visual History (Read more on [arXiv](https://arxiv.org/abs/2411.16034) or [HuggingFace](https://huggingface.co/papers/2411.16034))| Zhaojiang Lin, Yi Lu, Kai Sun, Deqing Fu, Wang Bill Zhu | VisualLens is a novel approach for personalized recommendations leveraging a user's task-agnostic visual history.  The research investigates whether visual history can improve personalized recommendations.  The methodology involves retrieving relevant images from the user's history, generating a preference profile using image embeddings, captions, and extracted aspect words, and matching this profile to candidate items using a multimodal LLM.  VisualLens achieved 82-91% Hit@10 on created benchmarks, outperforming state-of-the-art methods like UniMP by ~10% and GPT-40 by up to 4.6% on Hit@3. This suggests AI practitioners can leverage users' visual data, such as photos from reviews or social media, to significantly enhance personalization in recommendation systems, even outperforming large language models.  |
| Cautious Optimizers: Improving Training with One Line of Code (Read more on [arXiv](https://arxiv.org/abs/2411.16085) or [HuggingFace](https://huggingface.co/papers/2411.16085))| Qiang Liu, Bo Liu, Lizhang Chen, Kaizhao Liang | Cautious Optimizers improve the training speed of momentum-based optimizers with a simple, single-line code modification.  The research aims to develop a faster and more stable optimizer for large model training that requires minimal implementation effort. The core methodology involves introducing a mask that selectively applies updates based on alignment between the proposed update direction and the current gradient.  On the LLaMA 1B language model, the Cautious AdamW variant achieved a 1.47x speedup compared to standard AdamW. This allows AI practitioners to train large models more efficiently with virtually no code changes or computational overhead, potentially enabling faster experimentation and model development cycles.  |
| The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz (Read more on [arXiv](https://arxiv.org/abs/2411.14486) or [HuggingFace](https://huggingface.co/papers/2411.14486))| Forrest McKee, David Noever | This research evaluates large language models' (LLMs) ability to acknowledge uncertainty on unsolvable problems.  The research sought to determine how well LLMs admit ignorance rather than generate incorrect responses to fundamentally unsolvable questions.  Twelve state-of-the-art LLMs, both open and closed-source, were tested on a curated dataset of 675 unsolvable graduate-level problems using multiple-choice questions that included "I don't know" as a correct answer.  The best-performing models achieved 62-68% accuracy in admitting "I don't know," with GPT-4 demonstrating higher uncertainty acknowledgement on more challenging problems (35.8%) compared to simpler problems (20.0%). This finding highlights the importance of incorporating uncertainty recognition into LLM training and evaluation frameworks, prompting AI practitioners to develop methods for LLMs to distinguish between solvable and unsolvable problems as a potential marker for advanced reasoning capabilities and a critical aspect of responsible AI development.  |
| SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis (Read more on [arXiv](https://arxiv.org/abs/2411.16443) or [HuggingFace](https://huggingface.co/papers/2411.16443))| Soonwoo Kwon, Jin-Young Kim, Jiho Jang, Byeongjun Park, Hyojun Go | SplatFlow is a novel framework for text-driven 3D Gaussian Splatting (3DGS) scene generation and editing.  The research aims to create a unified framework for generating and editing 3DGS scenes from text prompts, addressing the limitations of existing specialized methods.  The core methodology involves a multi-view rectified flow (RF) model trained to generate multi-view consistent images, depths, and camera poses, along with a Gaussian Splatting Decoder (GSDecoder) to convert these into 3DGS representations. On the MVImgNet dataset, SplatFlow achieves a FID score of 34.85, outperforming the Director3D baseline (FID 39.55). This provides AI practitioners with a more versatile and efficient tool for generating and editing complex 3D scenes directly from text prompts, simplifying content creation pipelines.  |
| Predicting Emergent Capabilities by Finetuning (Read more on [arXiv](https://arxiv.org/abs/2411.16035) or [HuggingFace](https://huggingface.co/papers/2411.16035))| Sergey Levine, Dan Klein, Eric Wallace, sea-snell | This paper investigates predicting the emergence of capabilities in large language models (LLMs).  The research asks: can few-shot emergent capabilities in future, larger LLMs be predicted by finetuning current, smaller LLMs?  The core methodology involves finetuning smaller LLMs with varying amounts of data, fitting a parametric "emergence law" to model how the point of emergence shifts with data, and extrapolating this law to the few-shot setting.  On MMLU, the method predicts emergence using models trained with ~10²² FLOPS, while the smallest post-emergence model required ~5 * 10²² FLOPS, enabling prediction 4-5x in advance in terms of FLOPS. This allows AI practitioners to potentially assess the future capabilities and emergent behavior of larger LLMs before they are trained, informing architectural choices and resource allocation.  |
| SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation (Read more on [arXiv](https://arxiv.org/abs/2411.14525) or [HuggingFace](https://huggingface.co/papers/2411.14525))| Zhongying Deng, Haoyu Wang, Yanjun Li, Ying Chen, Jin Ye | This paper benchmarks the transfer learning capabilities of full-body CT pre-trained models for volumetric medical image segmentation.  The research investigates under what conditions pre-trained models can effectively transfer to diverse downstream medical image segmentation tasks across varying modalities, targets, and dataset sizes.  The study employs STU-Net, a scalable U-Net architecture, pre-trained on the TotalSegmentor dataset and fine-tuned on 87 public datasets. Fine-tuning improved average Dice Similarity Coefficient (DSC) by 2.80% for the STU-Net-huge model across all datasets. This research demonstrates the efficacy of full-body CT pre-training for cross-modality and cross-target transfer in medical image segmentation, offering AI practitioners pre-trained models and a benchmark for developing and evaluating transfer learning techniques for volumetric medical image analysis.  |
| From CISC to RISC: language-model guided assembly transpilation (Read more on [arXiv](https://arxiv.org/abs/2411.16341) or [HuggingFace](https://huggingface.co/papers/2411.16341))| Abdulrahman Mahmoud, Rania Hossam, Chaimaa Abi, Ahmed Heakl | CRT, a lightweight LLM-based transpiler, automatically converts x86 assembly code to ARM and RISC-V assembly.  The research aimed to develop a direct translation method between x86 (CISC) and ARM/RISC-V (RISC) architectures that preserves correctness without virtualization overhead.  The methodology involved training various small-scale LLMs on a dataset of 500k C programs compiled to x86 and ARM/RISC-V, employing an extended tokenizer and hardware-informed training optimizations.  The transpiler achieved 79.25% translation accuracy from x86 to ARMv5 and 88.68% accuracy from x86 to RISC-V64.  This demonstrates the potential of using LLMs for efficient cross-architecture assembly transpilation, offering AI practitioners a new approach to code portability across diverse hardware ISAs without reliance on dynamic binary translation or emulation.  |
| Best of Both Worlds: Advantages of Hybrid Graph Sequence Models (Read more on [arXiv](https://arxiv.org/abs/2411.15671) or [HuggingFace](https://huggingface.co/papers/2411.15671))| Bryan Perozzi, Clayton Sanford, Mahdi Karami, Ali Parviz, Ali Behrouz | This paper investigates the strengths and weaknesses of different sequence models for graph-structured data.  The research aims to determine which sequence models and tokenization strategies are most effective for various graph tasks.  The authors introduce a unifying framework, Graph Sequence Model (GSM), and analyze sequence model performance on tasks including counting, connectivity, and shortest path. Results show no single sequence model or tokenizer consistently outperforms others across all tasks; for instance, a hybrid model combining Mamba and Transformer layers improved performance in most cases.  This suggests AI practitioners should carefully select tokenization and sequence models based on the specific graph task, considering factors like local vs. global information needs and node ordering.  |
