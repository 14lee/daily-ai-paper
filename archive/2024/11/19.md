

## Papers for 2024-11-19

| Title | Authors | Summary |
|-------|---------|---------|
| BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices (Read more on [arXiv](https://arxiv.org/abs/2411.10640) or [HuggingFace](https://huggingface.co/papers/2411.10640))| wolf1110, AJZhou, liuyangbian, yina0, lucky-lance | BlueLM-V-3B is a 3B parameter multimodal large language model designed for efficient deployment on mobile devices.  The research aimed to develop an MLLM that performs well on mobile hardware despite memory and computational limitations.  The authors co-designed the model architecture and system, featuring a relaxed aspect ratio matching method for dynamic image resolution, batched image encoding, and token downsampling.  On the MediaTek Dimensity 9300 processor, BlueLM-V-3B achieves a generation speed of 24.4 tokens/s with 4-bit LLM weight quantization and a memory usage of 2.2GB. This work enables AI practitioners to deploy performant MLLMs on resource-constrained mobile devices, facilitating broader access to complex multimodal AI capabilities on personal devices.  |
| Generative World Explorer (Read more on [arXiv](https://arxiv.org/abs/2411.11844) or [HuggingFace](https://huggingface.co/papers/2411.11844))| Daniel Khashabi, Alan Yuille, Tianmin Shu, jienengchen, TaiMingLu | Genex enables embodied agents to mentally explore 3D environments and update beliefs without physical movement.  The research aimed to develop a framework for imaginative exploration in physical worlds to improve decision-making in partially observable environments.  A video diffusion model conditioned on egocentric panoramic view and movement direction generates future observations, enabling belief revision.  On the Genex-DB dataset, Genex achieved a 69.5 FVD score for video generation quality and below 0.1 latent MSE for long-range imaginative exploration consistency. This work introduces a novel approach for AI practitioners to integrate generative video into partially observable decision processes, offering potential for enhanced planning and multi-agent interaction in embodied AI systems by enabling belief updates based on imagined, rather than physically experienced, observations.  |
| AnimateAnything: Consistent and Controllable Animation for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.10836) or [HuggingFace](https://huggingface.co/papers/2411.10836))| Rong Zhang, Hong Li, Chi Wang, Guojun Lei, yikaiw | AnimateAnything introduces a two-stage pipeline for generating controllable and consistent videos from images and various control signals.  The research aims to address the challenge of integrating diverse control signals like camera trajectories, text prompts, and user motion annotations for precise video manipulation. The key methodology involves converting all visual control signals into a unified optical flow representation, which then guides a video diffusion model.  On the OpenVid dataset, AnimateAnything achieved an Aesthetic Quality score of 0.600, outperforming comparison methods.  This unified optical flow approach offers AI practitioners a more robust and flexible method for controlling video generation, potentially improving applications like film production and virtual reality.  |
| Drowning in Documents: Consequences of Scaling Reranker Inference (Read more on [arXiv](https://arxiv.org/abs/2411.11767) or [HuggingFace](https://huggingface.co/papers/2411.11767))| Michael Carbin, Matei Zaharia, Erik Lindgren, Mathew Jacob, mrdrozdov | This paper investigates the impact of scaling the number of reranked documents on retrieval quality.  The research questions how the performance of state-of-the-art rerankers changes when scoring progressively more documents, including the entire dataset.  The authors evaluate open and closed-source rerankers on eight academic and enterprise information retrieval benchmarks, measuring Recall@10 and Recall@100 at various reranking depths (K).  Results show Recall@10 drops dramatically for many rerankers as K increases beyond 100, often falling below the performance of standalone retrievers; for example, average Recall@10 across enterprise datasets using voyage-rerank-lite-1 decreased from 0.7 to roughly 0.2 as K increased from 100 to 5000.  AI practitioners should carefully consider the number of documents (K) provided to rerankers as excessively large K can significantly degrade performance, and listwise reranking with LLMs may offer increased robustness.  |
| Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering (Read more on [arXiv](https://arxiv.org/abs/2411.09213) or [HuggingFace](https://huggingface.co/papers/2411.09213))| Thien Huu Nguyen, Chien Van Nguyen, Nghia Trung Ngo, Franck-Dernoncourt | This paper introduces MedRGB, a benchmark for evaluating retrieval-augmented generation (RAG) systems in medical question answering. The research aimed to assess the performance of RAG systems in practical medical scenarios, including handling noise, integrating multiple information sources, and resisting factual errors.  The methodology involved creating multiple test scenarios (standard RAG, sufficiency, integration, and robustness) and evaluating state-of-the-art and open-source LLMs across these scenarios using four medical QA datasets supplemented with noise and adversarial information. Results revealed that Llama-3-70b achieved the highest noise detection accuracy in the sufficiency test, but all models struggled with factual error detection in the robustness test, with GPT-3.5 having the highest detection rate despite the lowest performance. The key implication for AI practitioners is the need for specialized modules and improved model robustness beyond target accuracy when developing reliable medical RAG systems, as current models have limited ability to handle noise and misinformation within retrieved content.   |
| SlimLM: An Efficient Small Language Model for On-Device Document Assistance (Read more on [arXiv](https://arxiv.org/abs/2411.09944) or [HuggingFace](https://huggingface.co/papers/2411.09944))| Viet Dac Lai, Seunghyun Yoon, Phat T. Nguyen, Thang M. Pham, Franck-Dernoncourt | SlimLM models are optimized for on-device document assistance tasks. The research aimed to develop efficient small language models (SLMs) for document processing on mobile devices, addressing the trade-off between model size, performance, and resource constraints.  The key methodology involved pre-training SlimLM models (ranging from 125M to 1B parameters) on the SlimPajama-627B dataset and fine-tuning them on DocAssist, a specialized dataset for summarization, question suggestion, and question answering.  SlimLM-1B achieved a ROUGE-L score of 0.48, approaching the performance of the larger Qwen2-1.5B-Instruct model. The primary implication for AI practitioners is the ability to deploy performant document processing capabilities directly on mobile devices, potentially reducing server costs and enhancing user privacy.  |
| SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2411.10510) or [HuggingFace](https://huggingface.co/papers/2411.10510))| Haomiao Jiang, Joshua Geddes, mnandwana, helloterran, josephliu-roblox | SmoothCache is a model-agnostic inference acceleration technique for Diffusion Transformers (DiT).  The research aimed to develop a universal caching scheme to speed up DiT inference across various modalities without compromising generation quality. The methodology involved leveraging layer-wise representation errors from a small calibration set to adaptively cache and reuse key features during inference.  Experiments showed up to a 71% speedup while maintaining or improving generation quality on models like DiT-XL, Open-Sora, and Stable Audio Open. This technique offers AI practitioners a simple, training-free method to significantly reduce DiT inference latency, potentially enabling real-time applications.  |
| Top-$nσ$: Not All Logits Are You Need (Read more on [arXiv](https://arxiv.org/abs/2411.07641) or [HuggingFace](https://huggingface.co/papers/2411.07641))| Liusheng Huang, Hongli Xu, Jianchun Liu, tomorrowdawn | Top-ησ, a novel sampling method for large language models (LLMs), operates directly on pre-softmax logits by leveraging a statistical threshold.  The research aims to improve LLM reasoning task performance by developing a sampling method that filters irrelevant tokens more effectively than existing approaches.  The key methodology involves separating logits into noisy and informative regions based on their statistical properties, specifically by capturing a region extending *n* standard deviations (σ) below the maximum logit value.  On the GSM8K dataset, top-ησ achieves 74.61% accuracy at a temperature of 3.0, while other comparable sampling methods fail completely. AI practitioners can utilize top-ησ to potentially improve the performance and stability of LLMs in reasoning tasks, especially at higher temperatures, where traditional sampling methods often degrade.  The paper mentions an incomplete preprint version, stating some experimental results and appendices will be added later.  |
| StableV2V: Stablizing Shape Consistency in Video-to-Video Editing (Read more on [arXiv](https://arxiv.org/abs/2411.11045) or [HuggingFace](https://huggingface.co/papers/2411.11045))| Dong Liu, Yunwei Lan, Kaidong Zhang, Rui Li, Chang Liu | StableV2V is a novel video editing method that aims to maintain shape consistency between user prompts and edited video content.  The paper addresses the problem of existing video editing methods often producing results inconsistent with user-desired shapes, especially when prompts introduce significant shape changes.  The key methodology involves a three-stage pipeline: a prompted first-frame editor, an iterative shape aligner (ISA) that simulates and refines the depth map of edited frames based on source video motion, and a conditional image-to-video generator that propagates edited content.  On the DAVIS-EDIT benchmark, StableV2V achieves a DOVER score of 67.78/70.80 for text-based editing, outperforming comparable methods.  This implies that AI practitioners can leverage StableV2V's shape-consistent editing approach to develop more robust and user-intuitive video editing tools, particularly for tasks involving significant shape transformations.  |
| LLäMmlein: Compact and Competitive German-Only Language Models from Scratch (Read more on [arXiv](https://arxiv.org/abs/2411.11171) or [HuggingFace](https://huggingface.co/papers/2411.11171))| Andreas Hotho, Julia Wunderle, Jan Pfister | This paper introduces LLäMmlein, two German-only decoder-only LLMs (120M and 1B parameters) trained from scratch.  The objective was to create high-performing, transparent German language models and address the performance gap of existing German LLMs compared to English models.  The methodology involved preprocessing a filtered RedPajama V2 dataset, training a custom German tokenizer, and pretraining the models using a TinyLlama framework.  LLäMmlein 1B achieved state-of-the-art performance on the EuroParl token classification task within the SuperGLEBer benchmark with a score of 0.732.  The open-sourcing of the models, code, and data provides AI practitioners with resources for further German NLP research, including domain adaptation and the creation of a dedicated German instruction dataset.  |
| Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts (Read more on [arXiv](https://arxiv.org/abs/2411.10669) or [HuggingFace](https://huggingface.co/papers/2411.10669))| Nanyi Fei, Hongpeng Lin, Guoxing Yang, Yanqi Dai, Jinqiang Long | Awaker2.5-VL is a Mixture of Experts (MoE) architecture designed to address the "multi-task conflict" issue in Multimodal Large Language Models (MLLMs).  The research aimed to improve MLLM performance on diverse tasks by mitigating interference between different data distributions and representations.  The key methodology involves a sparsely activated MoE structure with Low-Rank Adaptation (LoRA) experts and a simplified routing strategy based on instruction embeddings. On the MME-Realworld-CN benchmark, Awaker2.5-VL achieved an overall score of 62.7, surpassing all other compared models.  This indicates that incorporating MoE with LoRA and a stable routing strategy can be an effective approach for scaling MLLMs and improving performance across diverse multimodal tasks, offering a potential solution to the multi-task conflict issue.  |
| FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on (Read more on [arXiv](https://arxiv.org/abs/2411.10499) or [HuggingFace](https://huggingface.co/papers/2411.10499))| Chengming Xu, Qingdong He, Donghao Luo, Xiaobin Hu, Boyuan Jiang | FitDiT is a novel Diffusion Transformer (DiT)-based model for high-fidelity image-based virtual try-on.  The research aims to address the challenges of preserving rich texture details and achieving accurate size-aware fitting in virtual try-on applications.  The key methodology involves customizing a DiT architecture with structure slimming, garment condition modulation, garment feature injection, a dilated-relaxed mask strategy, and frequency-domain learning.  FitDiT achieved a 71.6% reduction in KID error compared to the second-best method on the unpaired VITON-HD dataset, indicating improved garment texture preservation. This improvement in texture fidelity using the DiT architecture provides AI practitioners developing virtual try-on applications with a more effective model for generating realistic and detailed synthesized images of people wearing clothes.  |
| Adaptive Decoding via Latent Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2411.09661) or [HuggingFace](https://huggingface.co/papers/2411.09661))| Jason Weston, Asli Celikyilmaz, Ping Yu, Ilia Kulikov, Shehzaad Dhuliawala | This paper introduces Adaptive Decoding, a method for dynamically adjusting the sampling temperature of large language models (LLMs) during text generation.  The research aims to address the suboptimality of fixed temperature decoding for tasks requiring varying levels of creativity and factual accuracy.  The core methodology involves adding an ADAPTIVEDECODER module to the LLM, trained using Latent Preference Optimization (LPO) to learn optimal temperature values for different prompts or tokens.  Results on the UltraMathStories dataset, a combination of math, creative writing, and general instruction-following tasks, show that Adaptive Decoding outperforms all fixed temperature decoding strategies.  This implies that AI practitioners can leverage Adaptive Decoding to improve LLM performance on diverse tasks without manual temperature tuning, automating the balance between creative and factual generation.  |
