

## Papers for 2024-11-15

| Title | Authors | Summary |
|-------|---------|---------|
| MagicQuill: An Intelligent Interactive Image Editing System (Read more on [arXiv](https://arxiv.org/abs/2411.09703) or [HuggingFace](https://huggingface.co/papers/2411.09703))| Qiuyu Wang, Hao Ouyang, wwen1997, bruceyyu, LiuZichen | MagicQuill is an interactive image editing system built upon diffusion models that allows users to make edits using brushstrokes, which are interpreted by a multimodal large language model (MLLM).  The research aimed to develop a robust, open-source, interactive, and precise image editing system that simplifies the process of making detailed image edits.  The system combines a dual-branch Editing Processor (inpainting and control branches) with a Painting Assistor (MLLM for prompt prediction) and an Idea Collector (user interface for brushstroke input).  Compared to baselines, MagicQuill achieved improved edge alignment and color fidelity with a lower LPIPS score of 0.0667 and a higher PSNR of 27.282 on a constructed test dataset.  The paper does not report standard deviations for these or other metrics, making statistical significance unclear. It is unclear how ground truth images were obtained for this evaluation.  AI practitioners can leverage this architecture to develop more user-friendly and precise image editing tools, integrating MLLMs to understand user intent from freehand input and enhance generative control in diffusion-based editing.  However, the paper does not adequately discuss the generalizability of the Draw&Guess dataset and the robustness of the trained MLLM across diverse user sketch styles and potential ambiguities.  |
| LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.09595) or [HuggingFace](https://huggingface.co/papers/2411.09595))| Jun Zhu, Hang Su, Yikai Wang, Jonathan Lorraine, Zhengyi Wang | LLaMA-Mesh enables large language models (LLMs) to generate 3D meshes directly from text prompts.  The research aimed to unify 3D mesh generation and text generation within a single LLM framework.  The key methodology involved representing 3D mesh vertex coordinates and face definitions as plain text within the OBJ file format, enabling direct integration with the LLM without vocabulary expansion.  LLaMA-Mesh achieved mesh generation quality comparable to specialized models while retaining language capabilities, scoring 61.74 on MMLU (5-shot) compared to the baseline LLaMA3.1 (8B) score of 66.07. This allows AI practitioners to leverage the text-based knowledge embedded in LLMs for 3D content creation, opening up new possibilities for language-driven 3D modeling.  |
| Cut Your Losses in Large-Vocabulary Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.09009) or [HuggingFace](https://huggingface.co/papers/2411.09009))| Philipp Krähenbühl, Vladlen Koltun, Alexander Hertzberg, Brody Huval, erikwijmans | Cut Cross-Entropy (CCE) reduces memory footprint of cross-entropy loss in large language models.  The authors aimed to address the disproportionately large memory consumption of cross-entropy loss computation in large language models, especially those with extensive vocabularies. CCE computes cross-entropy without materializing the full logit matrix, instead calculating logits on-the-fly and leveraging sparsity in the softmax gradient. Using CCE with the Gemma 2 (2B) model, memory for loss computation decreased from 24GB to 1MB, and overall classifier head memory from 28GB to 1GB. This allows practitioners training LLMs to significantly increase batch size during training or train larger models on existing hardware due to reduced memory requirements.  |
| ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction? (Read more on [arXiv](https://arxiv.org/abs/2411.06469) or [HuggingFace](https://huggingface.co/papers/2411.06469))| Zhongwei Wan, Che Liu, Shan Chen, Jian Yu, canyuchen | ClinicalBench benchmarks LLMs and traditional ML models on clinical prediction tasks.  The research investigates whether LLMs can outperform traditional ML models in clinical prediction. The benchmark uses two clinical databases (MIMIC-III and MIMIC-IV) and evaluates performance on three common clinical prediction tasks (length-of-stay, mortality, and readmission) with various LLMs (general-purpose and medical) and traditional ML models, using prompting and fine-tuning strategies. Across all tasks and datasets, traditional ML models generally outperformed LLMs, with XGBoost achieving a Macro F1-score of 67.94% on length-of-stay prediction in MIMIC-III, substantially higher than LLMs.  AI practitioners should exercise caution when applying LLMs to clinical prediction tasks, as they currently do not demonstrate superiority over established ML methods, despite strong performance on medical question answering benchmarks.  |
| Hermes: A Large Language Model Framework on the Journey to Autonomous Networks (Read more on [arXiv](https://arxiv.org/abs/2411.06490) or [HuggingFace](https://huggingface.co/papers/2411.06490))| Merouane Debbah, Antonio De Domenico, Ali Maatouk, Fadhel Ayed, nicopi | Hermes is a chain-of-agent LLM framework for modeling and automating cellular network operations using "blueprints" for constructing Network Digital Twins (NDTs).  The research investigates whether LLMs can effectively model network behavior and advance network autonomy.  The key methodology involves a three-phase process where a "Designer" LLM agent creates a blueprint for a NDT, a "Coder" agent translates it into Python code, and a feedback loop refines the blueprint based on numerical evaluation. When using GPT-40 as the LLM, Hermes achieved a success rate of 82.5% in modeling power control and energy saving tasks, compared to 25% for chain-of-thought and 55% for Hermes-coder (without the Designer). The success rate varies based on the complexity of the modeling task and with the specific LLMs being employed and increases substantially with the inclusion of domain specific models in the model repository. This indicates that integrating structured blueprints with domain expertise enhances LLM reliability in network modeling tasks and paves the way for more robust autonomous network operations using LLMs.  |
| Sharingan: Extract User Action Sequence from Desktop Recordings (Read more on [arXiv](https://arxiv.org/abs/2411.08768) or [HuggingFace](https://huggingface.co/papers/2411.08768))| Kehong Yuan, Jue Zhang, Xiaoting Qin, Yi Ren, Yanting Chen | Sharingan introduces two VLM-based methods to extract user action sequences from desktop recordings: Direct Frame-Based (DF) and Differential Frame-Based (DiffF).  The research aims to determine the efficacy of VLMs in extracting user actions from desktop video recordings.  Both methods use VLMs (GPT and Gemini series) to process video frames, with DiffF incorporating explicit frame difference detection.  On the ACTONE dataset, the DF approach with GPT-40 achieved 70-80% accuracy in identifying operation types, with extracted sequences being replayable via RPA.  This work enables AI practitioners to explore desktop video as a data source for RPA, automated tutorial generation, and user behavior analysis.  |
