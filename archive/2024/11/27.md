

## Papers for 2024-11-27

| Title | Authors | Summary |
|-------|---------|---------|
| ShowUI: One Vision-Language-Action Model for GUI Visual Agent (Read more on [arXiv](https://arxiv.org/abs/2411.17465) or [HuggingFace](https://huggingface.co/papers/2411.17465))| Shiwei Wu, Zhengyuan Yang, Difei Gao, Linjie Li, Kevin Qinghong Lin | ShowUI is a vision-language-action model designed for building GUI visual agents.  The research aimed to develop a lightweight, efficient model for GUI automation tasks like navigation and grounding by addressing challenges in visual modeling, action integration, and training data curation.  The key methodologies included UI-Guided Visual Token Selection for efficient visual processing, Interleaved Vision-Language-Action Streaming to unify different modalities, and a curated dataset with a rebalancing strategy.  ShowUI achieved 75.1% accuracy on zero-shot screenshot grounding using a 2B parameter model trained on 256K data. This implies that AI practitioners can leverage ShowUI's efficient architecture and training methods to build performant GUI agents with limited computational resources and training data.  |
| Star Attention: Efficient LLM Inference over Long Sequences (Read more on [arXiv](https://arxiv.org/abs/2411.17116) or [HuggingFace](https://huggingface.co/papers/2411.17116))| Boris Ginsburg, Fei Jia, Shantanu Acharya | Star Attention is a block-sparse attention mechanism for efficient inference of transformer-based LLMs on long sequences.  The research aimed to reduce the computational cost and improve the speed of LLM inference on long sequences. The two-phase method processes context with blockwise-local attention using anchor blocks, followed by global attention for query and response tokens to all cached key-value vectors.  Star Attention achieved up to 11x speedup versus Ring Attention while maintaining 95-100% accuracy on the RULER benchmark with sequence lengths up to 128K. This allows AI practitioners to utilize LLMs with significantly longer context lengths while maintaining high accuracy and drastically reduced inference time and computational cost.  |
| Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration (Read more on [arXiv](https://arxiv.org/abs/2411.17686) or [HuggingFace](https://huggingface.co/papers/2411.17686))| Honggang Chen, Donglin Wang, Pengxiang Ding, Xuyang Liu, Yuhang Han | This paper introduces a unified "filter-correlate-compress" paradigm for training-free token reduction in Multimodal Large Language Models (MLLMs).  The research aims to accelerate MLLM inference by reducing visual token quantity while preserving essential information, without requiring retraining. The proposed FiCoCo method suite, implementing this paradigm, decomposes token reduction into three distinct pipeline stages: filtering redundant tokens, correlating discarded information to retained tokens, and compressing the token set.  Experimental results on LLaVA-1.5-7B show up to an 82.4% FLOPs reduction with minimal performance impact, outperforming other training-free methods.  This offers AI practitioners a plug-and-play method for significantly improving the inference efficiency of MLLMs, facilitating practical deployment of these computationally demanding models.  |
| MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2411.15296) or [HuggingFace](https://huggingface.co/papers/2411.15296))| Xinyu Fang, Bo Li, Shukang Yin, Chaoyou Fu, yifanzhang114 | This paper surveys evaluation methods for Multimodal Large Language Models (MLLMs).  The objective is to provide a comprehensive overview of MLLM evaluation to aid researchers in selecting appropriate benchmarks and developing better evaluation methods.  The paper categorizes benchmarks by evaluated capabilities (foundational, behavioral, application-focused), summarizes benchmark construction processes, and discusses evaluation methods (human, LLM/MLLM, script-based) and metrics.  MME-RealWorld, the largest manually annotated benchmark, contains 29K question-answer pairs and achieves a maximum accuracy of only 60% with state-of-the-art MLLMs on several real-world tasks.  AI practitioners should consider the limitations of current MLLMs on complex real-world tasks when designing applications and prioritize benchmark selection and development based on specific application requirements.  |
| TEXGen: a Generative Diffusion Model for Mesh Textures (Read more on [arXiv](https://arxiv.org/abs/2411.14740) or [HuggingFace](https://huggingface.co/papers/2411.14740))| Ying-Tian Liu, Yuan-Chen Guo, Xin Yu, Lp256, yuanze1024 | TEXGen is a generative diffusion model for synthesizing high-resolution textures for 3D meshes.  The research aimed to develop a feed-forward model for generalizable mesh texturing, avoiding test-time optimization common in previous methods.  A novel hybrid 2D-3D network architecture, combining UV space convolutions with 3D point cloud attention, was employed.  The model achieved a FID score of 34.53 and KID score of 11.94 × 10⁻⁴ on multi-view renderings of textured meshes, outperforming existing methods.  This provides AI practitioners with a fast and effective method for generating high-quality textures for diverse 3D models, eliminating the need for computationally expensive per-object optimization.  |
| Pathways on the Image Manifold: Image Editing via Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.16819) or [HuggingFace](https://huggingface.co/papers/2411.16819))| David Bensaïd, Roy Velich, Daniel Silver, Gal Yona, Noam Rotstein | Frame2Frame (F2F) reformulates image editing as a video generation task to improve edit accuracy and image preservation.  The research aims to overcome limitations of existing text-guided diffusion models for image editing, such as difficulty adhering to complex edit instructions and loss of source image fidelity.  F2F uses a three-step process: generating temporal editing captions from source image and edit prompt using a VLM (ChatGPT-40), generating a video sequence with a pretrained video diffusion model (CogVideoX) conditioned on the temporal caption, and selecting the optimal edited frame using a VLM.  On the TEdBench benchmark, F2F achieved a CLIP score of 0.63 for target edit accuracy, outperforming competing methods. This approach offers AI practitioners a novel method for high-fidelity image manipulation by leveraging the temporal coherence of video generation models, though the computational cost and potential for unintended camera motion effects are noted as limitations.  |
| SketchAgent: Language-Driven Sequential Sketch Generation (Read more on [arXiv](https://arxiv.org/abs/2411.17673) or [HuggingFace](https://huggingface.co/papers/2411.17673))| Judith E Fan, Alex Zhao, Kristine Zheng, Tamar Rott Shaham, Yael Vinker | SketchAgent generates sketches from text prompts using a sequential, stroke-based approach guided by multimodal large language models (LLMs).  The objective is to create a language-driven sketching system capable of generating diverse, dynamic sketches and supporting human-computer collaborative sketching. The methodology involves prompting a frozen multimodal LLM to generate string-based drawing actions on a numbered grid canvas, which are then converted into Bézier curves and rendered.  Using Claude3.5-Sonnet as the backbone LLM, SketchAgent achieved a Top-1 CLIP zero-shot classification accuracy of 23% on a 50-category QuickDraw sketch generation task.  This sequential approach, leveraging off-the-shelf LLMs, offers AI practitioners a new method for developing interactive and dynamic sketch generation systems, eliminating the need for training or fine-tuning specialized models.  |
| Learning 3D Representations from Procedural 3D Programs (Read more on [arXiv](https://arxiv.org/abs/2411.17467) or [HuggingFace](https://huggingface.co/papers/2411.17467))| Zezhou Cheng, Xuweiyi Chen | This paper investigates learning 3D representations from procedurally generated data rather than semantically rich datasets.  The research explores whether self-supervised learning methods can effectively learn 3D representations from synthetic shapes created via procedural programs and how these compare to representations learned from real-world 3D models. The study uses Point-MAE, a masked autoencoding framework, to train on a synthetic dataset of 150K procedurally generated 3D point clouds and compares performance with Point-MAE trained on ShapeNet.  On ScanObjectNN's PB-T50-RS benchmark, Point-MAE trained on synthetic shapes achieves 85.46% accuracy, compared to 85.18% for Point-MAE trained on ShapeNet. This suggests that procedurally generated data can be a viable alternative to real-world datasets for self-supervised 3D representation learning, potentially mitigating challenges related to data acquisition and copyright for AI practitioners working with 3D data.  |
| SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE (Read more on [arXiv](https://arxiv.org/abs/2411.16856) or [HuggingFace](https://huggingface.co/papers/2411.16856))| XIngang Pan, Tengfei Wang, Shangchen Zhou, Yushi Lan, Yongwei Chen | SAR3D is a novel framework for fast 3D object generation and detailed understanding. The research sought to determine if autoregressive models could be effectively applied to both fast 3D object generation and detailed understanding. The key methodology involves a multi-scale 3D Vector-Quantized Variational Autoencoder (VQVAE) to tokenize 3D objects and a next-scale prediction training approach for autoregressive modeling. SAR3D achieves 3D object generation in 0.82 seconds on an A6000 GPU. This fast generation speed, coupled with the model's ability to facilitate detailed 3D understanding through LLM finetuning, offers AI practitioners a more efficient method for both creating and interpreting 3D content.  |
| DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting (Read more on [arXiv](https://arxiv.org/abs/2411.17223) or [HuggingFace](https://huggingface.co/papers/2411.17223))| Ping Hu, Liqian Ma, Lu Zhang, Pengxiang Li, Yicheng Yang | DreamMix is a diffusion-based generative model for subject-driven image inpainting that allows editing object attributes while preserving identity.  The research aimed to improve the editability of inserted objects in subject-driven image inpainting while maintaining identity preservation. The key methodology involves a disentangled inpainting framework with local content generation and global context harmonization, an attribute decoupling mechanism, and a textual attribute substitution module. In user studies, DreamMix received a 55% preference for identity preservation and a 74% preference for attribute editing.  This provides AI practitioners with a more controllable and effective tool for customized image inpainting applications, enhancing both object insertion accuracy and text-driven attribute editing.  |
| VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models (Read more on [arXiv](https://arxiv.org/abs/2411.17451) or [HuggingFace](https://huggingface.co/papers/2411.17451))| Yifan Song, Xuqing Yang, Zhihui Xie, Yuancheng Wei, Lei Li | VL-RewardBench is introduced as a challenging benchmark for evaluating vision-language generative reward models (VL-GenRMs).  The research aimed to create a robust benchmark to assess the reliability and effectiveness of VL-GenRMs in aligning and evaluating multimodal AI systems.  The benchmark was constructed using an AI-assisted annotation pipeline incorporating ensemble filtering with small LVLMs for general and hallucination tasks, and AI-aided preference labeling for complex reasoning tasks, across datasets like WildVision, VLFeedback, and MMMU-Pro.  Evaluation across 16 LVLMs revealed that even GPT-4o achieved only 62.4% macro-average accuracy on the benchmark, with many smaller models performing near chance levels. The strong correlation (Pearson’s r > 0.9) between VL-RewardBench performance and downstream Best-of-N sampling accuracy on MMMU-Pro provides AI practitioners with a reliable metric for selecting and developing effective VL-GenRMs for practical alignment tasks.  |
| SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis (Read more on [arXiv](https://arxiv.org/abs/2411.16173) or [HuggingFace](https://huggingface.co/papers/2411.16173))| Yong Man Ro, Hosu Lee, Hyunjun Kim, Junho Kim | SALOVA enhances long-form video understanding in Large Multi-modal Models (LMMs) by retrieving relevant video segments.  The research aimed to improve LMM comprehension of lengthy videos, addressing limitations in context length and memory overhead.  The key methodology involved a novel video-LLM framework with a dynamic routing mechanism and spatio-temporal projector to retrieve relevant segments based on user queries, trained on a newly created "SceneWalk" dataset of densely captioned long videos.  SALOVA-Qwen (7B) achieved 55.6% accuracy on the Video-MME long video benchmark, surpassing other open-sourced models with similar parameter sizes.  This targeted retrieval approach offers AI practitioners a more efficient and contextually aware method for processing long videos, minimizing information loss and improving response relevance in LMMs.  |
| Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens (Read more on [arXiv](https://arxiv.org/abs/2411.17691) or [HuggingFace](https://huggingface.co/papers/2411.17691))| Haitao Mi, Zhisong Zhang, Thomas Hartvigsen, Tao Ge, Xu Ouyang | This paper investigates the impact of low-bit quantization on large language models (LLMs) at different training levels.  The research aims to understand how quantization-induced degradation (QiD) relates to training tokens, model size, and bit width.  The researchers analyzed over 1500 quantized LLM checkpoints from the Pythia suite, using GPTQ for 2-, 3-, and 4-bit quantization and measuring QiD on the RefinedWeb dataset.  They derived scaling laws, finding that a 70B parameter LLM requires over 17 trillion training tokens to achieve a QiD greater than 0.2 with 4-bit quantization.  AI practitioners should consider an LLM’s training level when evaluating or applying low-bit quantization, as fully trained models exhibit significantly higher QiD, posing challenges for deployment.  |
| MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts (Read more on [arXiv](https://arxiv.org/abs/2411.14721) or [HuggingFace](https://huggingface.co/papers/2411.14721))| Jingdi Le, Wei Liu, Yunqing Liu, Jiatong Li, qq8933 | MolReFlect improves molecule-caption translation in LLMs by focusing on fine-grained alignments between molecular sub-structures and textual phrases.  The research aimed to address the challenge of aligning molecules and their corresponding captions with greater granularity and explainability than existing methods.  A teacher-student framework was used, where a larger teacher LLM extracts fine-grained alignments, which are then refined and used to fine-tune a smaller student LLM via Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT).  On the ChEBI-20 dataset, MolReFlect with Mistral-7B achieved a BLEU-4 score of 0.608 for molecule-to-caption generation, outperforming the previous best score by 4.6%. This work highlights the importance of fine-grained alignments for improving the accuracy and explainability of LLMs in molecule-caption translation, enabling more effective application in molecule discovery and related tasks.  |
| Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI) (Read more on [arXiv](https://arxiv.org/abs/2411.16754) or [HuggingFace](https://huggingface.co/papers/2411.16754))| Abhilekh Borah, Sainath Reddy Sankepally, Subhankar Ghosh, Shashwat Bajpai, Nasrin Imanpour | This paper introduces a benchmark and a metric for evaluating AI-generated image detection and quality.  The research aims to assess the effectiveness of current AI-generated image detection (AGID) methods and propose a new evaluation framework.  The researchers created the Visual Counter Turing Test (VCT²) benchmark dataset (~130K images) using prompts from Twitter and MS COCO and tested 15 state-of-the-art AGID methods.  Results show significant limitations in existing AGID methods, with Midjourney 6 generated images achieving a 93.65 on the newly proposed Visual AI Index (VAI), exceeding the average real image VAI score of 85.61. This indicates a need for AI practitioners to develop more robust AGID techniques capable of detecting high-quality synthetic images generated by advanced models like Midjourney 6, as current methods are proving insufficient.  |
| AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.17383) or [HuggingFace](https://huggingface.co/papers/2411.17383))| Xiaodong Cun, Yong Zhang, Juan Cao, Ziyao Huang, Ziyi Xu | AnchorCrafter generates realistic anchor-style product promotion videos by animating human images with objects and motion controls.  The research aimed to address the limitations of existing pose-guided human video generation methods in depicting realistic human-object interactions (HOI).  The system uses a diffusion-based video generation model with novel HOI-appearance perception, HOI-motion injection, and HOI-region reweighting loss components.  AnchorCrafter achieved a 0.848 Object-IoU, significantly higher than comparison methods, demonstrating improved object motion accuracy. This work provides AI practitioners with a tool for creating realistic and controllable product promotion videos with animated human presenters interacting naturally with products, advancing the field of video generation for e-commerce and related applications.  |
