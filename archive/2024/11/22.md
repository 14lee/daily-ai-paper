

## Papers for 2024-11-22

| Title | Authors | Summary |
|-------|---------|---------|
| Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2411.10442) or [HuggingFace](https://huggingface.co/papers/2411.10442))| Yangzhou Liu, Yue Cao, Wenhai Wang, Zhe Chen, Weiyun Wang | This paper introduces Mixed Preference Optimization (MPO) to improve multimodal reasoning in Large Language Models (LLMs).  The research aims to address the limited multimodal reasoning capabilities and distribution shift issues observed in open-source Multimodal LLMs (MLLMs), particularly with Chain-of-Thought (CoT) prompting.  The authors develop MPO, combining supervised fine-tuning loss with preference, quality, and generation losses, and create MMPR, a large-scale multimodal reasoning preference dataset, using automated pipelines.  InternVL2-8B-MPO, trained with MPO, achieves 67.0% accuracy on MathVista, an 8.7 point improvement over the baseline InternVL2-8B and comparable to the much larger InternVL2-76B. This suggests that MPO and MMPR can significantly improve the reasoning performance of smaller MLLMs, offering a potential pathway for developing more efficient and capable models for AI practitioners.  |
| Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions (Read more on [arXiv](https://arxiv.org/abs/2411.14405) or [HuggingFace](https://huggingface.co/papers/2411.14405))| Tianqi Shi, Hao Wang, Bo Zeng, Huifeng Yin, Yu Zhao | Marco-01 is a large language model developed to enhance reasoning abilities for complex problem-solving.  The research aims to determine if an OpenAI-style model can generalize to domains lacking clear standards and quantifiable rewards.  The model uses Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), and a reflection mechanism.  Marco-01 achieved a 90.40% accuracy on the English MGSM dataset, a +6.17% improvement over the baseline Qwen2-7B-Instruct. This indicates that combining CoT, MCTS, and reflection mechanisms can significantly improve the reasoning abilities of LLMs, offering AI practitioners new techniques for developing models capable of tackling complex, open-ended problems.  |
| OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs (Read more on [arXiv](https://arxiv.org/abs/2411.14199) or [HuggingFace](https://huggingface.co/papers/2411.14199))| Amanpreet Singh, Weijia Shi, Rulin Shao, jacquelinehe, akariasai | OpenScholar is a retrieval-augmented language model for synthesizing scientific literature.  The research investigated whether large language models can effectively assist scientists in synthesizing the growing body of scientific literature.  The study developed OpenScholar, a specialized retrieval-augmented LM that synthesizes citation-backed responses by retrieving from a datastore of 45 million open-access papers and iteratively refining outputs using self-feedback.  OpenScholar-8B outperformed GPT-40 by 5% and PaperQA2 by 7% in correctness on the ScholarQABench benchmark.  AI practitioners can leverage OpenScholar and similar retrieval-augmented LMs to access, synthesize, and cite scientific literature more effectively and accurately.  |
| Multimodal Autoregressive Pre-training of Large Vision Encoders (Read more on [arXiv](https://arxiv.org/abs/2411.14402) or [HuggingFace](https://huggingface.co/papers/2411.14402))| Michal Klein, Philipp Dufter, Xiujun Li, Mustafa Shukor, efini | AIMv2, a family of vision encoders, is pre-trained using a multimodal autoregressive objective.  The research aims to develop a scalable and effective pre-training method for vision encoders that generalizes well to diverse downstream tasks.  The method involves training a vision transformer encoder with a causal multimodal decoder that autoregressively generates image patches and text tokens from a unified multimodal sequence of image and text embeddings.  The AIMv2-3B model achieved 89.5% top-1 accuracy on ImageNet-1k with a frozen trunk after high-resolution fine-tuning.  This offers AI practitioners a straightforward, scalable, and high-performing vision encoder for various vision and multimodal applications, including zero-shot image recognition and multimodal instruction tuning.  |
| Ultra-Sparse Memory Network (Read more on [arXiv](https://arxiv.org/abs/2411.12364) or [HuggingFace](https://huggingface.co/papers/2411.12364))| Defa Zhu, Qiyang Min, Taoer, xyzed, FetchFortune | UltraMem, a novel architecture employing large-scale, ultra-sparse memory layers, aims to improve inference efficiency in large language models.  The research sought to reduce inference latency while maintaining or exceeding the performance of Mixture of Experts (MoE) models, addressing MoE's high memory access costs. The key methodology involves using Tucker decomposition for query-key retrieval within a memory layer and implicit value expansion to reduce memory access during training.  Experiments show UltraMem achieves up to 6x faster inference than MoE with the same parameter count and computational cost at a batch size of 64. This allows AI practitioners to deploy larger language models with improved inference speed in resource-constrained environments and potentially improve scaling properties for even larger models.  |
| Hymba: A Hybrid-head Architecture for Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.13676) or [HuggingFace](https://huggingface.co/papers/2411.13676))| Zijia Chen, Wonmin Byeon, Shizhe Diao, Yonggan Fu, Xin Dong | Hymba, a family of small language models (SLMs), integrates transformer attention and state space models (SSMs) within a hybrid-head parallel architecture for enhanced efficiency and performance.  The research aimed to develop more efficient and performant SLMs by combining the strengths of attention mechanisms and SSMs while mitigating their individual weaknesses.  The key methodology involved fusing attention and SSM heads in parallel within the same layer, incorporating learnable meta tokens, optimizing KV cache usage, and scaling model size and training data.  Hymba-1.5B outperforms Llama-3.2-3B (a 3B parameter model) by 1.32% on average accuracy across commonsense reasoning tasks, while requiring an 11.67× smaller cache size and achieving 3.49× higher throughput. This result signifies that AI practitioners can achieve comparable or better performance with significantly smaller and more efficient SLMs using hybrid architectures like Hymba, potentially enabling broader deployment on resource-constrained devices.  |
| Natural Language Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2411.14251) or [HuggingFace](https://huggingface.co/papers/2411.14251))| Mengyue Yang, Haotian Fu, Ziyu Wan, Xidong Feng, Benjamin-eecs | This paper introduces Natural Language Reinforcement Learning (NLRL), a novel RL paradigm that uses natural language to represent core RL components.  The objective is to improve reinforcement learning efficiency, stability, and interpretability by leveraging natural language and large language models (LLMs).  The core methodology involves redefining RL principles (objectives, policy, value function, Bellman equation) as language-based constructs and implementing them with LLMs via prompting and gradient-based training.  In Tic-Tac-Toe experiments, NLRL achieved higher win rates against baseline models, including a traditional PPO agent, reaching a win rate of 0.9.  NLRL offers AI practitioners a new framework for building more interpretable and potentially more efficient RL agents by integrating the strengths of large language models into the reinforcement learning process, although the paper's empirical evaluation focuses on relatively simple environments.  |
| Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.14432) or [HuggingFace](https://huggingface.co/papers/2411.14432))| Winston Hu, Jingkang Yang, Hai-Long Sun, Zuyan, THUdyh | Insight-V is a system for enhancing visual reasoning in Multimodal Large Language Models (MLLMs).  The research aimed to improve long-chain visual reasoning in MLLMs, addressing the lack of robust datasets and training strategies.  A two-step pipeline generated structured reasoning data: a progressive strategy created diverse reasoning paths, and multi-granularity assessment ensured data quality; a multi-agent system, consisting of reasoning and summarization agents, was trained using supervised fine-tuning and iterative Direct Preference Optimization.  Insight-V improved the performance of LLaVA-NeXT by an average of 7.0% across seven visual reasoning benchmarks.  This suggests AI practitioners can significantly enhance MLLM visual reasoning capabilities by using specialized data generation pipelines and multi-agent system architectures with iterative DPO training.  |
| Stable Flow: Vital Layers for Training-Free Image Editing (Read more on [arXiv](https://arxiv.org/abs/2411.14430) or [HuggingFace](https://huggingface.co/papers/2411.14430))| Kfir Aberman, Egor Nemchinov, Ohad Fried, Or Patashnik, omriav | Stable Flow leverages the reduced diversity of flow-based diffusion models for consistent, training-free image editing.  The research aimed to identify crucial layers in Diffusion Transformer (DiT) models for effective image editing without retraining. The methodology involved systematically bypassing individual DiT layers during image generation and measuring the perceptual impact using DINOv2, identifying "vital layers" essential for image formation.  Injecting features from a source image into the vital layers of the edited image's generation trajectory resulted in a CLIP image-text direction similarity score of 0.14, higher than other compared methods.  This allows AI practitioners to perform various image edits, including non-rigid transformations and object manipulation, using a single, training-free mechanism by targeting these vital layers in flow-based DiT models.  |
| UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages (Read more on [arXiv](https://arxiv.org/abs/2411.14343) or [HuggingFace](https://huggingface.co/papers/2411.14343))| Tae-Sun Chung, Akhil Kedia, Bethel Melesse Tessema | UnifiedCrawl improves Large Language Model (LLM) performance on low-resource languages using consumer-grade hardware. The research aimed to improve LLM performance in low-resource languages given data scarcity and limited compute resources. The authors developed UnifiedCrawl, a method to efficiently extract monolingual data from the Common Crawl corpus, and fine-tuned multilingual LLMs using quantization and low-rank adapters (QLoRA). Fine-tuning a 4.5B parameter XGLM model with UnifiedCrawl-Amharic data using QLoRA resulted in a 45% perplexity reduction from 35.6 to 19.6 compared to the original XGLM model. This demonstrates that using UnifiedCrawl and QLoRA allows practitioners to adapt large, pre-trained multilingual LLMs for low-resource languages using readily available hardware, promoting wider accessibility and affordability.  |
| MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control (Read more on [arXiv](https://arxiv.org/abs/2411.13807) or [HuggingFace](https://huggingface.co/papers/2411.13807))| Zhenguo Li, Lanqing Hong, Bo Xiao, Kai Chen, Ruiyuan Gao | MagicDriveDiT generates high-resolution, long street-view videos for autonomous driving applications with precise control. The objective is to synthesize realistic and controllable high-resolution, long street-view videos suitable for autonomous driving applications. The paper uses a DiT-based diffusion model with flow matching, spatial-temporal conditional encoding, and a progressive bootstrapping training strategy incorporating variable video lengths and resolutions.   MagicDriveDiT achieves a Frechet Video Distance (FVD) score of 94.84, significantly lower than baseline models, on the nuScenes dataset. AI practitioners working with autonomous driving systems can leverage MagicDriveDiT to create high-quality, controllable synthetic video datasets for training and testing perception models, potentially reducing reliance on real-world data collection.  |
| Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.14257) or [HuggingFace](https://huggingface.co/papers/2411.14257))| Neel Nanda, Senthooran Rajamanoharan, Oscar Obeso, Javier Ferrando | This paper investigates the mechanisms behind hallucinations in large language models, specifically focusing on entity recognition. The research aims to understand how language models determine whether they possess knowledge about a given entity and how this relates to hallucination. The researchers use sparse autoencoders (SAEs) to identify directions in the representation space of the model that correlate with known and unknown entities. They find that manipulating these "entity recognition" directions can causally influence the model's refusal to answer or its tendency to hallucinate, achieving nearly 100% refusal for unknown entities when steering with the discovered latent direction. Steering with unknown entity latents disrupts the factual recall mechanism by reducing attention paid to entity tokens by downstream attention heads. This finding suggests that AI practitioners can potentially leverage and manipulate these latent directions to control hallucination and refusal behaviors in language models, directly impacting the reliability and factuality of generated text.  |
| Patience Is The Key to Large Language Model Reasoning (Read more on [arXiv](https://arxiv.org/abs/2411.13082) or [HuggingFace](https://huggingface.co/papers/2411.13082))| Yijiong Yu | This paper proposes a method to improve large language model reasoning by encouraging more detailed reasoning processes.  The research aims to enhance complex problem-solving in LLMs without requiring extensive, costly training data.  The key methodology involves using preference optimization (DPO) to train a model to favor detailed reasoning processes (positive examples) over concise answers (negative examples).  Results demonstrate a 6.7% improvement on the GSM8k benchmark.  This suggests AI practitioners can significantly improve LLM performance on complex tasks by training for more patient and thorough reasoning, even with limited data, though at the cost of increased inference time.  |
