

## Papers for 2024-11-25

| Title | Authors | Summary |
|-------|---------|---------|
| Style-Friendly SNR Sampler for Style-Driven Generation (Read more on [arXiv](https://arxiv.org/abs/2411.14793) or [HuggingFace](https://huggingface.co/papers/2411.14793))| Sungroh Yoon, Heeseung Kim, Yeongtak, chaehun, jychoi | This paper introduces a Style-friendly SNR sampler to improve style learning in text-to-image diffusion models during fine-tuning.  The research aims to address the limitations of existing fine-tuning methods, which often fail to capture new artistic styles due to the use of object-centric objectives and noise distributions.  The key methodology involves adjusting the noise level sampling during fine-tuning by biasing the signal-to-noise ratio (SNR) distribution towards higher noise levels (lower log-SNR values) where style features are observed to emerge.    Experiments using FLUX-dev on the StyleDrop dataset showed a DINO image similarity score of 0.461 for the proposed method compared to 0.373 for the standard SD3 sampler, demonstrating improved style alignment.  The Style-friendly SNR sampler enables more effective style template learning for personalized content creation, allowing AI practitioners to fine-tune text-to-image diffusion models for higher-fidelity style-driven generation.  |
| TÜLU 3: Pushing Frontiers in Open Language Model Post-Training (Read more on [arXiv](https://arxiv.org/abs/2411.15124) or [HuggingFace](https://huggingface.co/papers/2411.15124))| Hamish Ivison, Shengyi Huang, Valentina Pyatkin, Jacob Morrison, Nathan Lambert | TÜLU 3 is a family of open-source, state-of-the-art language models fine-tuned for enhanced post-training capabilities. The research aimed to develop a robust, open post-training recipe for language models that rivals closed, proprietary methods.  Key methodologies included supervised fine-tuning, preference tuning with Direct Preference Optimization (DPO), and a novel Reinforcement Learning with Verifiable Rewards (RLVR) approach.  TÜLU 3 70B outperformed Llama 3.1 Instruct 70B by 3.2 points on an aggregate evaluation suite. The primary implication for AI practitioners is the availability of a comprehensive, open-source recipe and accompanying resources (data, code, evaluation framework) to reproduce and adapt state-of-the-art post-training techniques for their own language models.  |
| A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection (Read more on [arXiv](https://arxiv.org/abs/2411.12946) or [HuggingFace](https://huggingface.co/papers/2411.12946))| Shaun Khoo, shingurding, gabrielchua | This paper introduces a data-free methodology for developing LLM guardrails, focusing on off-topic prompt detection.  The research aimed to create a method for developing effective LLM guardrails in pre-production environments where real-world user data is unavailable.  The key methodology involved using LLMs to generate synthetic datasets of on-topic and off-topic prompts and then training classifier models on this data. Fine-tuned cross-encoder and bi-encoder models achieved an F1 score of 0.99 on a synthetic dataset generated by GPT-40. This methodology enables AI practitioners to deploy LLM applications with pre-built safety measures for off-topic prompt detection even before real-world data becomes available, minimizing potential misuse from the outset.  |
| OminiControl: Minimal and Universal Control for Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2411.15098) or [HuggingFace](https://huggingface.co/papers/2411.15098))| Xinchao Wang, Qiaochu Xue, Xingyi Yang, Songhua Liu, Zhenxiong Tan | OminiControl integrates image conditions into Diffusion Transformers (DiTs) for diverse control tasks.  The research aimed to develop a parameter-efficient method for both spatially and non-spatially aligned image control in DiTs.  The key methodology involves reusing the model's VAE encoder for processing condition images and integrating them as tokens within the DiT's multi-modal attention mechanism.  On the Canny-to-image task, OminiControl achieved a 0.38 F1-Score, significantly outperforming Stable Diffusion 1.5 based ControlNet (0.34) and T2I-Adapter (0.22), as well as Flux.1-based ControlNetPro (0.21). This allows AI practitioners to utilize a unified and efficient approach for implementing diverse image-based control within DiT architectures, simplifying implementation and reducing parameter overhead compared to previous specialized methods.  |
| Large Multi-modal Models Can Interpret Features in Large Multi-modal Models (Read more on [arXiv](https://arxiv.org/abs/2411.14982) or [HuggingFace](https://huggingface.co/papers/2411.14982))| Ziwei Liu, Bo Li, Yifei Shen, Kaichen Zhang | This paper presents a framework for interpreting and steering the internal representations of large multimodal models (LMMs).  The research aims to understand the internal neural representations of LMMs, particularly how they encode semantic information. The key methodology involves training a Sparse Autoencoder (SAE) on LLaVA-NeXT data integrated into a specific LMM layer and interpreting learned features using a larger LMM (LLaVA-OV-72B) in a zero-shot manner.  Results show the SAE features can steer LMM behavior, with some features exhibiting IOU scores above 0.5 with ground truth segmentation masks based on automatically generated explanations.  This framework allows AI practitioners to better understand and potentially control the behavior of LMMs, including mitigating hallucinations and prompting desired outputs by manipulating specific internal features.  |
| VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection (Read more on [arXiv](https://arxiv.org/abs/2411.14794) or [HuggingFace](https://huggingface.co/papers/2411.14794))| Xiu Su, Le Zhuo, Hairong Shi, Wei Huang, Songhao Han | VideoEspresso is a new dataset and framework for improving video reasoning capabilities of Large Vision Language Models (LVLMs). The research aimed to address the scarcity of high-quality, large-scale datasets for video reasoning tasks.  The key methodology involved a semantic-aware pipeline to construct a VideoQA dataset with multimodal Chain-of-Thought (CoT) annotations, coupled with a Hybrid LVLMs Collaboration framework for reasoning.  The proposed method outperformed existing baselines on 12 out of 14 video reasoning tasks, achieving 34.1% average accuracy, surpassing the top open-source model (InternVL2) by 5.4% and the closed-source model (GPT-40) by 7.7%. This dataset and framework provide AI practitioners with new resources and methods for developing and evaluating LVLMs with enhanced video reasoning capabilities, leading to more cost-effective and accurate performance.  |
| Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2411.14762) or [HuggingFace](https://huggingface.co/papers/2411.14762))| Pieter Abbeel, Jinwoo Shin, Sihyun Yu, Huiwon Jang, younggyoseo | CoordTok, a novel video tokenizer, efficiently encodes long videos into a compact set of tokens by reconstructing patches based on sampled coordinates.  The research aimed to develop a more efficient video tokenizer that leverages temporal coherence and scales to long video clips.  The key methodology involved encoding videos into factorized triplane representations and training a decoder to reconstruct patches corresponding to randomly sampled (x,y,t) coordinates.  CoordTok encodes a 128-frame, 128x128 resolution video into 1280 tokens, achieving similar reconstruction quality as baselines requiring 6144 or 8192 tokens.  This efficient tokenization enables AI practitioners to train memory-intensive video generation models, like diffusion transformers, on significantly longer video sequences than previously feasible.  |
| Novel View Extrapolation with Video Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2411.14208) or [HuggingFace](https://huggingface.co/papers/2411.14208))| Shijian Lu, Ling Shao, KunhaoLiu | ViewExtrapolator leverages stable video diffusion (SVD) to refine artifact-prone novel views rendered by radiance fields or point clouds, enabling novel view extrapolation beyond training views. The research aims to improve novel view extrapolation, where synthesized views are far outside the range of training views, which is a weakness of current radiance field methods. The key methodology involves rendering a video transitioning from a training view to the extrapolated view, then refining it with SVD by modifying its denoising process and using guidance and resampling annealing. On the LLFF-Extra dataset, ViewExtrapolator achieves a 0.378 LPIPS score compared to 0.429 for the baseline DRGS method.  The paper does not specify if tuning SVD was required and if results improved further by fine-tuning SVD model. AI practitioners can utilize ViewExtrapolator as a post-processing method to significantly improve the visual quality of novel view extrapolations generated from existing 3D rendering techniques like radiance fields or point clouds. It should be noted that performance degrades with dynamic videos and extreme novel view angles.  |
| MyTimeMachine: Personalized Facial Age Transformation (Read more on [arXiv](https://arxiv.org/abs/2411.14521) or [HuggingFace](https://huggingface.co/papers/2411.14521))| David W. Jacobs, Annie N. Wang, Bang Gong, Jiaye Wu, Luchao Qi | MyTimeMachine (MyTM) personalizes facial age transformation using a few subject-specific images and a global aging prior.  The research aimed to develop a personalized age transformation method that accurately reflects an individual's appearance at a target age.  MyTM leverages a novel Adapter Network trained on a personal photo collection (~50 images) to modify the latent features of a global age transformation network (SAM). In age regression evaluations, MyTM achieved an 11.7% improvement in identity preservation (IDsim = 0.67) compared to the best-performing baseline (FADING).  AI practitioners can use MyTM to generate more accurate and personalized age-transformed faces, crucial for applications like visual effects in film or age progression for forensic investigations.  |
| BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games (Read more on [arXiv](https://arxiv.org/abs/2411.13543) or [HuggingFace](https://huggingface.co/papers/2411.13543))| Maciej Wolczyk, Ulyana Piterbarg, Samuel Coward, Bartłomiej Cupiał, pagli98 | BALROG benchmarks the agentic capabilities of large language models (LLMs) and vision-language models (VLMs) in complex game environments.  The research aims to evaluate LLMs' and VLMs' long-horizon reasoning and decision-making capabilities in dynamic settings.  The benchmark uses six reinforcement learning environments: BabyAI, Crafter, TextWorld, Baba Is AI, MiniHack, and NetHack, with varying complexities and textual and visual observation modalities.  GPT-4 achieved the highest average progression across all environments in the language-only setting at 32.34%. The significant performance gap between simpler and more complex games, as well as the drop in performance when using visual observations, highlights the need for AI practitioners to focus on improving VLMs' vision-based decision-making and LLMs' long-horizon planning abilities for more effective agent development.  |
| One to rule them all: natural language to bind communication, perception and action (Read more on [arXiv](https://arxiv.org/abs/2411.15033) or [HuggingFace](https://huggingface.co/papers/2411.15033))| Giuseppe Boccignone, Dimitri Ognibene, colo286 | This paper presents a novel architecture for robot task planning using Large Language Models (LLMs).  The research aims to enable robots to understand natural language commands and autonomously generate actionable plans in dynamic environments. The core methodology involves a modified ReAct framework integrating LLMs with a semantic mapping system using scene graphs and feedback loops for real-time adaptation. In preliminary tests on simple robotic requests, the system achieved a 90% success rate.  AI practitioners can leverage this approach to develop more robust and adaptable robots capable of understanding and executing complex tasks in real-world settings using natural language instructions.  |
| WildLMa: Long Horizon Loco-Manipulation in the Wild (Read more on [arXiv](https://arxiv.org/abs/2411.15131) or [HuggingFace](https://huggingface.co/papers/2411.15131))| Ge Yang, Sai Aneesh Suryadevara, Xuanbin Peng, Yuchen Song, Ri-Zhao Qiu | WildLMa is a framework for enabling quadruped robots to perform long-horizon loco-manipulation tasks in real-world environments.  The research aims to develop a system that allows quadruped robots to perform complex, long-horizon manipulation tasks in unstructured environments.  The methodology involves adapting a learned low-level whole-body controller for VR teleoperation, creating a library of generalizable visuomotor skills via imitation learning and heuristics (WildLMa-Skill), and using an LLM-based planner to coordinate skills for long-horizon tasks (WildLMa-Planner).  WildLMa achieved a 71.2% average success rate across tabletop grasping, button pressing, and ground grasping tasks, exceeding baseline imitation learning methods by at least 20%. This work provides AI practitioners with a practical framework and techniques for developing robust and generalizable loco-manipulation skills for quadruped robots, potentially enabling real-world deployment for tasks such as cleaning or fetching objects.  |
