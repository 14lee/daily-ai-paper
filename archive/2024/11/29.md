

## Papers for 2024-11-29

| Title | Authors | Summary |
|-------|---------|---------|
| Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2411.18203) or [HuggingFace](https://huggingface.co/papers/2411.18203))| Jingdi Lei, jwu323, ZonglinY, Duke-de-Artois, qq8933 | Critic-V is a framework for enhancing the reasoning capabilities of Vision-Language Models (VLMs).  The research aims to address the issue of VLMs generating inaccurate or irrelevant responses in multimodal reasoning tasks.  The key methodology involves a Reasoner-Critic architecture, where a Reasoner VLM generates reasoning paths and a Critic VLM provides feedback for refinement using Direct Preference Optimization (DPO) trained on a critique-VQA dataset.  Qwen2-VL-7B with Critic-V achieved the highest scores on five out of eight benchmarks, with an 11.8% improvement on MathVista compared to the baseline. This provides AI practitioners with a method to improve the reliability and accuracy of VLMs in reasoning-heavy multimodal applications by integrating an external critic model for real-time feedback during inference.  |
| ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting (Read more on [arXiv](https://arxiv.org/abs/2411.17176) or [HuggingFace](https://huggingface.co/papers/2411.17176))| Hangwei Qian, Weijia Wu, Zhuohang Dang, Changliang Xia, ChengyouJia | ChatGen automates the text-to-image generation process from free-form user input.  The research aimed to develop a model that automatically generates prompts, selects appropriate models, and configures arguments for text-to-image generation from freestyle user text, image, or chat history.  The authors introduce a multi-stage evolution strategy (ChatGen-Evo) incorporating supervised fine-tuning for prompt generation, ModelTokens for model selection, and in-context learning for argument configuration.  ChatGen-Evo achieved a Unified Metric score of 65.9 in supervised settings, surpassing other baselines and demonstrating comparable performance to a much larger 8B parameter model while using only 2B parameters.  This work suggests that focusing on stage-wise training for complex automated text-to-image generation tasks can yield significant performance improvements with smaller models, offering a potential path towards more efficient and accessible automated image generation for AI practitioners.  |
| TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2411.18350) or [HuggingFace](https://huggingface.co/papers/2411.18350))| Barbara Hammer, Robin Chan, Petra Bevandic, rizavelioglu | TryOffDiff reconstructs standardized garment images from photos of clothed individuals. The research objective is to generate canonical garment images from real-world photos, a task termed Virtual Try-Off (VTOFF). The key methodology involves adapting Stable Diffusion with SigLIP-based visual conditioning, replacing text prompts with image features. On the modified VITON-HD dataset, TryOffDiff achieves a DISTS score of 22.5, outperforming adapted VTON and pose transfer baselines.  The paper mentions no background removal post-processing was applied to TryOffDiff while some form of removal *was* applied to baseline models; how this affects the comparison remains unclear. This work provides AI practitioners with a novel approach for high-fidelity garment reconstruction, potentially improving e-commerce product imagery and generative model evaluation.  |
| Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.17041) or [HuggingFace](https://huggingface.co/papers/2411.17041))| Jong Chul Ye, Bryan S Kim, kjm981995 | Free$^2$Guide enhances text-video alignment in diffusion-based generative models without needing reward function gradients.  The research aims to improve text alignment in text-to-video generation using non-differentiable reward functions like Large Vision-Language Models (LVLMs). The method approximates guidance by combining path integral control with zeroth-order gradient estimations and enables ensembling multiple reward models.  Using GPT-40 with LaVie for text-video alignment showed a 28.6% improvement on the Spatial Relationship metric compared to the baseline LaVie model. This offers AI practitioners a way to leverage powerful black-box LVLMs for improved text-video alignment without needing model fine-tuning or differentiable reward functions, thereby potentially reducing computational overhead.  |
| Morph: A Motion-free Physics Optimization Framework for Human Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2411.14951) or [HuggingFace](https://huggingface.co/papers/2411.14951))| Hao Liu, Xin Zhao, Ruibing Hou, Mingshuang Luo, Zhuo Li | Morph enhances the physical plausibility of generated human motion without using real motion data.  The research aimed to develop a model-agnostic physics optimization method that doesn't require costly real motion capture data.  A two-stage process trains a Motion Physics Refinement (MPR) module on synthetic noisy motion data from a generator, then uses the refined output to fine-tune the original generator. On the HumanML3D dataset, Morph-MoMask reduced ground penetration errors from 23.152 to 0.0.  AI practitioners can use Morph to improve the physical realism of generated motions across diverse motion generation models and tasks (text-to-motion, music-to-dance) without needing expensive real-world motion datasets.  |
| LongKey: Keyphrase Extraction for Long Documents (Read more on [arXiv](https://arxiv.org/abs/2411.17863) or [HuggingFace](https://huggingface.co/papers/2411.17863))| Jean Paul Barddal, Cinthia Obladen de Almendra Freitas, Jeovane Honorio Alves, RaduState | LongKey is a novel framework for extracting keyphrases from long documents.  The research aimed to address the limitations of existing keyphrase extraction methods in processing long-context documents (greater than 512 tokens).  The methodology involves using Longformer for word embeddings, a max-pooling-based keyphrase embedding pooler, and a ranking loss combined with a chunking loss for candidate scoring.  On the LDKP10K dataset, LongKey achieved an F1@5 score of 41.81%. The keyphrase embedding pooler significantly contributes to LongKeyâ€™s improved performance, offering AI practitioners a more effective technique for extracting keyphrases from lengthy texts, enhancing information retrieval and summarization tasks.  |
