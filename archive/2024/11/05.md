

## Papers for 2024-11-05

| Title | Authors | Summary |
|-------|---------|---------|
| AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents (Read more on [arXiv](https://arxiv.org/abs/2410.24024) or [HuggingFace](https://huggingface.co/papers/2410.24024))| Hao Yu, Siyi Cheng, Xueqiao Sun, Xiao Liu, Yifan Xu | ANDROIDLAB is a framework for training and evaluating autonomous agents interacting with Android devices.  The research aimed to create a standardized environment and benchmark for Android agents using both large language models (LLMs) and large multimodal models (LMMs). They developed a benchmark with 138 tasks across 9 apps, and created the Android Instruct Dataset for fine-tuning models. Fine-tuning with their dataset improved the success rate of open-source LLMs from 4.59% to 21.50%, and LMMs from 1.93% to 13.28%. This resource allows AI practitioners to train and systematically evaluate open-source Android agent models using a standardized benchmark and dataset, facilitating development and comparison of new agent models.  |
| WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2411.02337) or [HuggingFace](https://huggingface.co/papers/2411.02337))| Hanyu Lai, Iat Long Iong, Xiao Liu, Zehan Qi, tianjiezhang | WEBRL is a novel reinforcement learning framework for training large language model (LLM) web agents in online environments.  The research aimed to improve the performance of open-source LLMs on web-based tasks, addressing challenges like task scarcity, sparse feedback, and policy distribution drift. The study uses a self-evolving online curriculum, an outcome-supervised reward model, and adaptive reinforcement learning strategies in online web environments.  Llama-3.1-8B, trained with WEBRL, achieved a 42.4% success rate on WebArena-Lite, surpassing previous state-of-the-art open LLM-based web agents and even proprietary LLMs like GPT-4-Turbo (17.6%). This implies that WEBRL can significantly enhance the performance of open-source LLMs in web-based tasks, making autonomous web agents more accessible and powerful for AI practitioners.  |
| Training-free Regional Prompting for Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2411.02395) or [HuggingFace](https://huggingface.co/papers/2411.02395))| Wenzhao Zheng, Jianjin Xu, wanghaofan, wangyida, antonio-c | This paper introduces a training-free regional prompting method for diffusion transformers.  The objective is to enhance compositional text-to-image generation in diffusion transformer models, specifically FLUX.1, by enabling them to handle complex, multi-regional prompts with precise layout control.  The key methodology involves manipulating the attention maps within the diffusion transformer architecture based on user-provided or LLM-generated regional prompt-mask pairs.  Results show the method generates images that adhere to multiple regional prompts simultaneously and achieves up to 9x faster inference speed compared to an RPG-based regional control method for 16 masks.  This provides AI practitioners with a more efficient and flexible approach to achieving fine-grained control over image generation using diffusion transformers without requiring model retraining or additional training data.  |
| DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.00836) or [HuggingFace](https://huggingface.co/papers/2411.00836))| Bin Hu, Junyu Zhang, Xingang Guo, Chengke Zou, Ray2333 | DYNAMATH, a dynamic visual benchmark, evaluates the robustness of Vision Language Models (VLMs) in mathematical reasoning.  The research investigated whether VLMs' reasoning procedures are robust to problem variations that pose no challenge to humans.  The key methodology involved creating 501 seed questions as Python programs, enabling generation of 5,010 concrete questions with variations in visual and textual content.  Evaluation showed the worst-case accuracy (percentage of correctly answered seed questions across all variants) of the best performing VLM, Claude-3.5, was 35.3%, significantly lower than its average-case accuracy.  This substantial difference between average-case and worst-case accuracy highlights the unreliability of current VLMs when handling variations in mathematical reasoning tasks, signaling a critical area for improvement in model robustness.  |
| Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent (Read more on [arXiv](https://arxiv.org/abs/2411.02265) or [HuggingFace](https://huggingface.co/papers/2411.02265))| Jiaqi Zhu, Xingwu Sun, Ruobing-Xie, Mimosa77, YanfengChen | Tencent introduces Hunyuan-Large, a 389 billion parameter Mixture-of-Experts (MoE) model with 52 billion activated parameters.  The objective was to develop a large, open-source MoE model with superior performance across diverse NLP tasks compared to similar-sized models. They leveraged large-scale synthetic data (7 trillion tokens), a novel recycle routing strategy within the MoE architecture, and explored scaling laws for MoE models.  Hunyuan-Large achieved 88.4% on MMLU, outperforming the LLama3.1-70B model and exhibiting comparable performance to the significantly larger LLama3.1-405B. The release of Hunyuan-Large offers AI practitioners a powerful, open-source MoE model for a wide range of applications, as well as insights into effective MoE model training for future development.  |
| How Far is Video Generation from World Model: A Physical Law Perspective (Read more on [arXiv](https://arxiv.org/abs/2411.02385) or [HuggingFace](https://huggingface.co/papers/2411.02385))| Yang Zhao, Zhijie Lin, Rui Lu, Bingyi Kang, Yang130 | Here's a summary of the AI research paper following the specified guidelines:  i) **1-line summary:**  A study evaluates the ability of scaled video generation models to learn and generalize fundamental physical laws from visual data alone.  ii) **Main research question/objective:** Can video generation models, scaled in data and parameters, discover and generalize fundamental physical laws solely from visual observations without human priors?  iii) **Key methodology:** A 2D physics simulation testbed generated videos of objects governed by deterministic physical laws (uniform linear motion, elastic collisions, parabolic motion).  Diffusion-based video generation models were trained and evaluated on in-distribution, out-of-distribution, and combinatorial generalization tasks.  Quantitative metrics assessed adherence to physical laws.  iv) **Primary results:**  While scaling improved in-distribution generalization, out-of-distribution generalization remained poor, with velocity errors an order of magnitude higher than in-distribution errors even with maximum model size and data.  Combinatorial generalization showed improvement with scaling but was still imperfect (67% to 10% reduction in “abnormal” cases). Analysis revealed a “case-based” generalization mechanism, prioritizing color over shape, size, and velocity.  v) **Principal implication for AI practitioners:** Scaling alone is insufficient for video generation models to uncover fundamental physical laws; models prioritize superficial visual features over underlying physical principles, necessitating further research on generalization mechanisms beyond simple scaling.  The significant gap between in-distribution and out-of-distribution generalization suggests that current approaches have significant limitations in truly understanding and modeling the physical world.  |
| Survey of Cultural Awareness in Language Models: Text and Beyond (Read more on [arXiv](https://arxiv.org/abs/2411.00860) or [HuggingFace](https://huggingface.co/papers/2411.00860))| Junho Myung, Arnav Arora, Junyeong Park, jinjh0123, sidicity | This paper surveys research on incorporating cultural awareness into text-based and multimodal language models (LLMs).  The survey aims to consolidate research on making LLMs culturally inclusive, encompassing benchmarks, training data creation, and alignment methodologies. The authors review over 300 papers, categorizing cultural awareness efforts across various modalities, including image, video, and audio, in addition to text.  Multilingual descriptions in image captioning benchmarks yield 29.9% more objects, 24.5% more relations, and 46.0% more attributes compared to monolingual captions. AI practitioners should consider incorporating culture-specific data and benchmarks in the development and evaluation of LLMs to mitigate biases and improve cross-cultural understanding, but should carefully evaluate sources for bias, inconsistencies in culture definitions, and the ethical implications of cultural alignment.  |
| LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2411.00918) or [HuggingFace](https://huggingface.co/papers/2411.00918))| Quang Pham, Van Nguyen, Luong Tran, doantienthongbku, DavidNguyen | LibMoE is a modular toolkit for streamlining the research, training, and evaluation of Mixture of Experts (MoE) algorithms in Large Language Models (LLMs).  The research aimed to develop a comprehensive framework making MoE algorithm research more accessible and standardized.  The key methodology involved implementing various state-of-the-art MoE algorithms within a modular framework incorporating distributed training and zero-shot evaluation across 11 benchmarks, utilizing sparse upcycling from pre-trained LLM checkpoints. Results showed no single MoE algorithm consistently outperformed others across all benchmarks, with performance averaging 55-56% accuracy across the tasks.  A key implication for AI practitioners is that the standard Sparse Mixture of Experts (SMoE) strategy remains a highly competitive choice due to its simplicity and scalability, despite the existence of more complex MoE algorithms.  |
| Sparsing Law: Towards Large Language Models with Greater Activation Sparsity (Read more on [arXiv](https://arxiv.org/abs/2411.02335) or [HuggingFace](https://huggingface.co/papers/2411.02335))| Chaojun Xiao, Yingfa Chen, Chenyang Song, Yuqi Luo, SillyXu | This paper investigates scaling properties and influential factors of intrinsic activation sparsity in decoder-only Transformer LLMs.  The research aims to understand how to achieve greater activation sparsity in LLMs without compromising performance. Researchers used a proposed metric, PPL-p% sparsity, to measure activation sparsity while controlling for performance degradation (perplexity).  They found ReLU-activated LLMs achieve greater sparsity than SiLU-activated LLMs at the same parameter scale, while maintaining comparable performance. Specifically, ReLU activation ratio on a 0.1B parameter model converges to approximately 6.14%  with sufficient training data, whereas SiLU converges to approximately 40.9%. These findings suggest AI practitioners should consider ReLU as the activation function when aiming to maximize activation sparsity for efficiency and interpretability gains in LLMs.  |
| GenXD: Generating Any 3D and 4D Scenes (Read more on [arXiv](https://arxiv.org/abs/2411.02319) or [HuggingFace](https://huggingface.co/papers/2411.02319))| Linjie Li, Zhiwen Yan, Kevin Lin, Chung-Ching Lin, Yuyang Zhao | GenXD is a unified model for generating 3D and 4D scenes from single or multiple conditioned images. The research aimed to develop a unified framework for generating consistent and high-quality 3D (static viewpoint changes) and 4D (spatial and temporal changes) content. The authors curated a large-scale 4D dataset (CamVid-30K) from videos, estimating camera poses and object motion, and designed GenXD with multiview-temporal modules within a masked latent conditioned diffusion model.  On the Cam-DAVIS benchmark, GenXD achieved an FID score of 101.78 for single view 4D generation, surpassing existing camera-conditioned video generation methods. This allows AI practitioners to generate videos aligned with camera trajectories and containing realistic object motion, advancing the capabilities of 3D and 4D content creation.  |
| DynaSaur: Large Language Agents Beyond Predefined Actions (Read more on [arXiv](https://arxiv.org/abs/2411.01747) or [HuggingFace](https://huggingface.co/papers/2411.01747))| Ryan A. Rossi, Seunghyun Yoon, Viet Dac Lai, Dang Nguyen, Franck-Dernoncourt | DynaSaur is an LLM agent framework that dynamically creates and composes actions as Python functions, accumulating them for reuse in subsequent tasks.  The research aims to address limitations of existing LLM agents restricted to predefined action sets by enabling dynamic action creation and composition.  The key methodology involves representing actions as Python functions, executing them through an interpreter, and accumulating generated actions. DynaSaur outperformed baseline models on the GAIA benchmark, achieving an average exact match percentage of 51.61% with GPT-40 on Level 1 tasks. This framework allows AI agents greater flexibility in problem-solving and adaptability to diverse tasks by generating and executing arbitrary actions, which is highly relevant for building more general and versatile agents.  |
| Adaptive Caching for Faster Video Generation with Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2411.02397) or [HuggingFace](https://huggingface.co/papers/2411.02397))| Menglin Jia, Ding Liu, Sen He, Haozhe Liu, kumarak | AdaCache accelerates video diffusion transformer inference by adaptively caching and reusing computations.  The research aims to reduce the computational cost of generating high-fidelity videos with Diffusion Transformers (DiTs), especially over longer durations.  The core method involves a content-dependent caching schedule within transformer blocks, guided by a distance metric measuring the change in residual connections between diffusion steps, and further regularized by a motion estimation component (MoReg).  AdaCache achieves up to a 4.7× speedup on Open-Sora 720p - 2s video generation compared to the baseline, with comparable or slightly reduced quality based on quantitative metrics.  This training-free, plug-and-play method allows AI practitioners to significantly improve the inference latency of video DiTs without requiring model retraining or sacrificing substantial generation quality.  |
| Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2411.00743) or [HuggingFace](https://huggingface.co/papers/2411.00743))| Virginia Smith, Mona Diab, Aashiq Muhamed | Specialized Sparse Autoencoders (SSAEs) are introduced to capture rare concepts in foundation models.  The research aims to address the challenge of current Sparse Autoencoders (SAEs) failing to capture rare, yet crucial, concepts within subdomains of data.  The key methodology involves finetuning general-purpose SAEs on subdomain data selected via dense retrieval and trained with Tilted Empirical Risk Minimization (TERM).  SSAEs achieved a 12.5% increase in worst-group classification accuracy compared to general-purpose SAEs on the Bias in Bios dataset when used to remove spurious gender information. This result indicates that SSAEs offer a more powerful lens for inspecting subdomain-specific features in foundation models, potentially leading to improvements in fairness and bias mitigation by enhancing the representation of underrepresented groups or tail concepts.  |
| Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2411.01192) or [HuggingFace](https://huggingface.co/papers/2411.01192))| Muhammad Abdul-Mageed, Fakhraddin Alwajih, Abdellah El Mekki, El Moatez Billah Nagoudi, Gagan Bhatia | This paper introduces Swan, a family of Arabic-centric embedding models, and ArabicMTEB, a benchmark for evaluating them.  The research aimed to develop improved Arabic text embedding models addressing dialectal and cultural nuances not captured by existing multilingual models.  The researchers trained Swan-Small and Swan-Large models using a diverse corpus of Arabic text, including MSA, dialectal variations, and cross-lingual data, and evaluated them on ArabicMTEB, covering retrieval, classification, and bitext mining tasks. Swan-Large achieved a state-of-the-art average score of 62.45 on ArabicMTEB, outperforming Multilingual-E5-large (61.65). This provides AI practitioners with new state-of-the-art, cost-effective Arabic embedding models and a benchmark for developing and evaluating future Arabic-centric NLP systems.  |
