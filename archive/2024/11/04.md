

## Papers for 2024-11-04

| Title | Authors | Summary |
|-------|---------|---------|
| OS-ATLAS: A Foundation Action Model for Generalist GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2410.23218) or [HuggingFace](https://huggingface.co/papers/2410.23218))| Fangzhi Xu, Zhenyu Wu, Zhiyong Wu, heroding77, QiushiSun | OS-Atlas is a large action model designed to improve GUI agent performance in grounding and out-of-distribution (OOD) scenarios.  The research aimed to develop a foundation model for GUI agents that excels in grounding and generalizes to unseen interfaces, addressing the limitations of existing open-source models.  The authors created a multi-platform GUI grounding data synthesis toolkit and curated the largest open-source, multi-platform GUI grounding dataset to date, containing over 13 million GUI elements across web, desktop, and mobile platforms.  OS-Atlas-Base achieved state-of-the-art grounding accuracy of 82.47% on ScreenSpot benchmark. This work provides AI practitioners with a high-performing, open-source foundation model and dataset, facilitating the development of more robust and generalizable GUI agents.  |
| Constant Acceleration Flow (Read more on [arXiv](https://arxiv.org/abs/2411.00322) or [HuggingFace](https://huggingface.co/papers/2411.00322))| Youngjoon Hong, Taehoon Lee, Sihyeon Kim, Sojin Lee, Dogyun Park | Constant Acceleration Flow (CAF) is a novel ODE-based generative model for faster, high-quality image generation.  The research aimed to improve the speed and accuracy of diffusion-based image generation by addressing limitations of constant velocity models like Rectified Flow.  CAF introduces a constant acceleration term into the ODE trajectory and employs initial velocity conditioning and a reflow process to improve trajectory estimation.  On CIFAR-10 with conditional settings, CAF achieved a Fréchet Inception Distance (FID) of 1.39 in one-step generation, surpassing state-of-the-art baselines.  AI practitioners can leverage CAF for faster, higher-quality image generation in applications requiring few-step inference.  |
| Randomized Autoregressive Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2411.00776) or [HuggingFace](https://huggingface.co/papers/2411.00776))| Liang-Chieh Chen, Xiaohui Shen, Xueqing Deng, turkeyju, yucornetto | This paper introduces Randomized AutoRegressive modeling (RAR) for enhanced visual generation using autoregressive transformers.  The objective is to improve autoregressive image generation quality while maintaining compatibility with language modeling frameworks.  RAR uses a randomness annealing training strategy where input image tokens are randomly permuted during training with a probability that linearly decays from 1 to 0, encouraging bidirectional context learning. On ImageNet-256, RAR achieves a FID score of 1.48, surpassing previous autoregressive and even some leading diffusion and masked transformer models.  This implies that AI practitioners can leverage RAR to develop higher-quality autoregressive image generation models that are also compatible with existing language modeling architectures and optimization techniques.  |
| Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation (Read more on [arXiv](https://arxiv.org/abs/2411.00412) or [HuggingFace](https://huggingface.co/papers/2411.00412))| Leon Bergen, Duncan Watson-Parris, Yadi Cao, yuqirose, Bohan22 | The paper introduces a two-stage training method to improve LLM performance on scientific problems, balancing inherent reasoning and external tool use.  The research aims to address the issue of LLMs over-relying on tools or hallucinating answers for complex scientific problems.  The methodology involves World Knowledge Distillation (WKD) to internalize domain knowledge and Tool Usage Adaptation (TUA) to train adaptive tool usage based on problem complexity. Results show an average 28.18% improvement in answer accuracy and a 13.89% improvement in tool usage precision across six scientific datasets. This implies that AI practitioners can enhance LLM accuracy and efficiency on scientific tasks by training models to adaptively leverage external tools based on problem difficulty.  |
| Personalization of Large Language Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2411.00027) or [HuggingFace](https://huggingface.co/papers/2411.00027))| Yijia Shao, Branislav Kveton, Ryan A. Rossi, Zhehao Zhang, Franck-Dernoncourt | This paper surveys techniques for personalizing Large Language Models (LLMs).  The authors aim to unify the disparate research on personalized text generation and downstream task personalization using LLMs.  They propose taxonomies for personalization granularity (user-level, persona-level, global preference), techniques (RAG, prompting, representation learning, RLHF), evaluation metrics (intrinsic, extrinsic), and datasets.  One study found that larger LLMs (100B+ parameters) performed comparably or better than traditional recommender systems in user rating prediction after fine-tuning with minimal user interaction data.  AI practitioners can leverage these taxonomies and techniques, along with insights into evaluation and datasets, to build more user-centric and effective personalized LLM applications.  |
| SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models (Read more on [arXiv](https://arxiv.org/abs/2411.00233) or [HuggingFace](https://huggingface.co/papers/2411.00233))| Sergio Martin, Clara Pérez-Molina, sascha-kirch, jolalde5 | SambaMixer is a novel structured state space model (SSM) for predicting the state of health (SOH) of Li-ion batteries.  The objective is to develop a deep learning model capable of accurately predicting Li-ion battery SOH using multivariate time series data from discharge cycles. The proposed SambaMixer model uses a MambaMixer architecture incorporating anchor-based resampling of time series data, positional encodings based on sample time and time between discharge cycles, and a regression head.  On the NASA battery dataset, SambaMixer achieved a Mean Absolute Error (MAE) of 1.072% for SOH prediction.  This result suggests that SambaMixer, using Mamba SSMs, offers a performant and efficient alternative to transformer-based models for multivariate time series prediction tasks relevant to battery health management.  |
| In-Context LoRA for Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2410.23775) or [HuggingFace](https://huggingface.co/papers/2410.23775))| Huanzhang Dou, Yupeng Shi, Zhi-Fan Wu, Wei Wang, lhhuang | This paper introduces In-Context LoRA (IC-LORA), a method for adapting text-to-image diffusion transformers to diverse generative tasks.  The research investigates whether existing text-to-image DiTs possess inherent in-context generation capabilities and, if so, how to effectively leverage them.  The key methodology involves concatenating images and their corresponding captions, then fine-tuning a LoRA with small task-specific datasets (20-100 samples).  Qualitative results demonstrate high-fidelity image set generation across various tasks, including portrait photography, font design, and home decoration. The paper does not present quantitative benchmarks, so specific performance metrics like FID or CLIP scores are unavailable.  This pipeline offers AI practitioners a simplified and computationally efficient approach to adapt pre-trained text-to-image models for various downstream tasks without extensive training or architectural modifications, emphasizing the potential of inherent in-context learning capabilities within these models.  |
| M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation (Read more on [arXiv](https://arxiv.org/abs/2410.21157) or [HuggingFace](https://huggingface.co/papers/2410.21157))| Shukai Liu, Jian Yang, Congnan Liu, Ken Deng, Jiaheng Liu | This paper introduces M²RC-EVAL, a benchmark for evaluating repository-level code completion in multiple programming languages.  The objective is to address the limitations of existing benchmarks that focus on few languages and lack fine-grained analysis, hindering comprehensive evaluation of multilingual code LLMs.  The researchers created M²RC-EVAL by collecting data from The Stack v2, selecting completion positions based on abstract syntax tree (AST) nodes, and adding bucket-level and semantic-level annotations.  After fine-tuning StarCoder-7B on the accompanying M²RC-INSTRUCT dataset, the model achieved 44.4% exact match and 71.4% edit similarity on M²RC-EVAL, significantly outperforming the non-finetuned model. The demonstrated effectiveness of cross-file context and fine-tuning on M²RC-INSTRUCT  indicates that AI practitioners should incorporate these elements when developing or improving code LLMs for real-world repository-level completion tasks, particularly in multilingual settings.  |
| HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2410.22901) or [HuggingFace](https://huggingface.co/papers/2410.22901))| Chenhui Xue, Chaojie Yang, Tian Li, Nianhong Jiao, Shengkai Zhang | HelloMeme introduces Spatial Knitting Attentions (SK Attentions) to enhance text-to-image diffusion models for complex downstream tasks like meme video generation.  The research aimed to develop a method for adapting pre-trained text-to-image models to specialized tasks without sacrificing generalization performance.  The core methodology involves integrating adapters employing SK Attentions into the diffusion model's UNet architecture, facilitating the fusion of high-level (head pose, facial expression) and fidelity-rich (reference image) features. In self-reenactment experiments, the method achieved an average PSNR of 31.08 dB, outperforming other open-source state-of-the-art methods. This method provides AI practitioners with a plugin-based approach for post-training text-to-image models, enabling adaptation to tasks requiring high fidelity and complex control while preserving the base model's capabilities.  |
| Zipfian Whitening (Read more on [arXiv](https://arxiv.org/abs/2411.00680) or [HuggingFace](https://huggingface.co/papers/2411.00680))| Hidetoshi Shimodaira, Hiroto Kurita, Han Bao, Sho Yokoi | This paper proposes Zipfian whitening, a post-processing method for word embeddings that incorporates word frequency.  The research investigates whether accounting for the non-uniform distribution of word frequencies (Zipf's law) when symmetrizing word embedding spaces improves downstream task performance.  The key methodology involves performing PCA whitening weighted by empirical word frequencies, emphasizing low-frequency words.  Zipfian whitening consistently outperformed standard centering/whitening and other baselines, achieving a 66.92% score on the STS-B benchmark using GloVe embeddings.  AI practitioners should consider using Zipfian whitening as a post-processing step for word embeddings, as it demonstrably improves performance on downstream tasks by better capturing the information content of rare words.  |
| WikiNER-fr-gold: A Gold-Standard NER Corpus (Read more on [arXiv](https://arxiv.org/abs/2411.00030) or [HuggingFace](https://huggingface.co/papers/2411.00030))| Pierre-François Marteau, Nicolas Béchet, Danrun Cao | This paper presents WikiNER-fr-gold, a manually corrected version of a subset of the French portion of the WikiNER corpus for Named Entity Recognition (NER).  The objective was to create a gold-standard NER dataset by correcting inconsistencies and errors in the silver-standard WikiNER-fr.  The authors manually reviewed and corrected 20% (26,818 sentences, ~700,000 tokens) of the French portion of the WikiNER corpus, using a labeling tool and referring to Wikipedia pages for disambiguation and consistency checks.  The corrected sub-corpus, WikiNER-fr-gold, exhibits improved annotation consistency compared to the original WikiNER-fr. This provides AI practitioners with a higher-quality gold-standard French NER dataset for training and evaluating NER models, potentially improving their performance.  |
| Survey of User Interface Design and Interaction Techniques in Generative AI Applications (Read more on [arXiv](https://arxiv.org/abs/2410.22370) or [HuggingFace](https://huggingface.co/papers/2410.22370))| Reuben Luera, puneetm, zhangry868, subright, Franck-Dernoncourt | This paper surveys user interface (UI) design and interaction techniques in user-guided generative AI applications.  The objective is to create a design compendium of current UI/UX trends and techniques for generative AI, focusing on user-guided interactions. The methodology involved surveying over 100 research articles on generative AI, categorizing UI interaction techniques, layouts, and human-AI engagement levels. The survey identified common interaction patterns like prompting, selection, system manipulation, and object manipulation, as well as prevalent UI layouts like conversational and canvas-based interfaces. One key finding is that users utilizing hybrid interactions in DirectGPT completed tasks 50% faster compared to single-dimensional interactions like those in ChatGPT. This implies that AI practitioners should consider incorporating multimodal and hybrid interaction designs to optimize user workflow and efficiency in generative AI applications.  |
| GRS-QA -- Graph Reasoning-Structured Question Answering Dataset (Read more on [arXiv](https://arxiv.org/abs/2411.00369) or [HuggingFace](https://huggingface.co/papers/2411.00369))| Jincen Shuai, Devasha Trivedi, Anish Pahilajani, Franck-Dernoncourt, namyongp | GRS-QA, a new dataset, is introduced for evaluating multi-hop question answering models with explicit reasoning structures.  The research aimed to investigate the impact of reasoning structures on Large Language Model (LLM) performance in multi-hop question answering.  The authors constructed reasoning graphs from existing multi-hop QA datasets, categorizing them by structure and generating negative samples by perturbing graph structures.  When using retrieved evidence, GPT-3.5 achieved an F1 score of 0.70 on bridge_2_1 questions and 0.78 on comparison_2_1 questions.  AI practitioners should consider reasoning structures alongside semantic content when developing and evaluating multi-hop QA models, as model performance varies significantly with differing reasoning graph complexities.  |
