

## Papers for 2024-11-13

| Title | Authors | Summary |
|-------|---------|---------|
| SAMPart3D: Segment Any Part in 3D Objects (Read more on [arXiv](https://arxiv.org/abs/2411.07184) or [HuggingFace](https://huggingface.co/papers/2411.07184))| Xiaoyang Wu, Liangjun Lu, Yuan-Chen Guo, Yukun Huang, Yunhan Yang | SAMPart3D is a zero-shot 3D part segmentation framework. The objective is to segment 3D objects into semantic parts at multiple granularities without predefined part labels or text prompts. The methodology involves a two-stage 2D-to-3D distillation process from DINOv2 and SAM, followed by semantic querying with Multimodal Large Language Models (MLLMs). On the PartObjaverse-Tiny dataset, SAMPart3D achieved 53.7% mean Intersection over Union (mIoU) for class-agnostic part segmentation. This provides AI practitioners with a scalable and flexible method for zero-shot 3D part segmentation, facilitating applications like part-level editing and interactive segmentation.  |
| JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2411.07975) or [HuggingFace](https://huggingface.co/papers/2411.07975))| Chengyue Wu, Wen Liu, Xiaokang Chen, Xingchao Liu, Yiyang Ma | JanusFlow is a unified multimodal model for image understanding and generation.  The research aimed to create a single model capable of both image understanding and generation using rectified flow within an autoregressive LLM framework.  The key methodology involved integrating rectified flow with an LLM, decoupling vision encoders for understanding and generation, and aligning their representations during training.  On the MJHQ FID-30k benchmark, JanusFlow achieved a score of 9.51, outperforming other 1.3B parameter models.  This provides AI practitioners with a more efficient and versatile vision-language model architecture that requires fewer parameters than alternative approaches while achieving state-of-the-art or comparable performance.  |
| Stronger Models are NOT Stronger Teachers for Instruction Tuning (Read more on [arXiv](https://arxiv.org/abs/2411.07133) or [HuggingFace](https://huggingface.co/papers/2411.07133))| Radha Poovendran, Luyao Niu, Fengqing Jiang, Zhangchen Xu, yuchenlin | This paper investigates the impact of response generator model selection on instruction-tuned LLM performance.  The research questions which models are the most effective response generators for instruction tuning and how to determine effective response generators without instruction tuning.  The authors fine-tuned five base LLMs on instruction datasets generated by 20 different response generators and evaluated them on AlpacaEval 2 and Arena-Hard benchmarks.  Gemma-2-9b-it and Qwen2.5-72B-Instruct emerged as the two best response generators, outperforming larger models and even GPT-4 in some cases (e.g., average performance of 13.92% and 16.15% on Llama-3.1-Minitron-4B, respectively, compared to 5.72% for GPT-4). The proposed Compatibility-Adjusted Reward (CAR) metric, accounting for both response quality and compatibility with the base model, outperformed baseline metrics in predicting response generator effectiveness. AI practitioners should prioritize response generators with high compatibility with the base LLM, as measured by CAR, rather than solely relying on benchmark performance, to maximize the effectiveness of instruction tuning.  |
| Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings (Read more on [arXiv](https://arxiv.org/abs/2411.08017) or [HuggingFace](https://huggingface.co/papers/2411.08017))| Derek Cheung, Arianna Rampini, Pradyumna Reddy, Aliasghar Khani, adityasanghi | WaLa introduces a novel framework for generating high-quality 3D shapes from various input modalities.  The objective is to address the computational challenges of large-scale 3D generative models while preserving fine details and complex geometries.  The key methodology involves encoding 3D shapes into compact wavelet-based latent representations using a VQ-VAE, achieving a 2,427x compression ratio, and training a billion-parameter diffusion model on this latent space.  On the Google Scanned Objects (GSO) dataset, WaLa achieved an Intersection over Union (IoU) of 0.978 for point cloud to mesh reconstructions.  WaLa offers AI practitioners a highly efficient and versatile method for generating high-resolution 3D shapes from various modalities, including text, sketches, and images, within seconds, which was previously computationally infeasible.  |
