

## Papers for 2024-11-14

| Title | Authors | Summary |
|-------|---------|---------|
| Large Language Models Can Self-Improve in Long-context Reasoning (Read more on [arXiv](https://arxiv.org/abs/2411.08147) or [HuggingFace](https://huggingface.co/papers/2411.08147))| Mo Yu, Lemao Liu, Zesen Cheng, Cheng Yang, Siheng99 | SEALONG, a novel self-improvement method for LLMs, enhances long-context reasoning.  The research investigates LLMs' capacity for self-improvement in reasoning over extended text.  The methodology involves sampling multiple output reasoning trajectories, scoring them using Minimum Bayes Risk (MBR), and fine-tuning via supervised learning or preference optimization.  Llama-3.1-8B-Instruct improved by 4.2 points using SEALONG, outperforming prior methods relying on expert-generated data. This self-improvement technique allows LLMs to enhance their long-context reasoning abilities without external annotations, offering a scalable path towards more advanced reasoning capabilities for AI practitioners.  |
| EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation (Read more on [arXiv](https://arxiv.org/abs/2411.08380) or [HuggingFace](https://huggingface.co/papers/2411.08380))| Guosheng Zhao, Jiayu Wang, Feng Liu, Kang Zhao, Xiaofeng Wang | EgoVid-5M is a 5-million-clip dataset designed for training egocentric video generation models.  The research aimed to create a high-quality dataset to address the challenges of generating egocentric videos due to dynamic viewpoints, action diversity, and scene complexity.  The researchers annotated EgoVid-5M with fine-grained kinematic control data using Visual Inertial Odometry and high-level textual descriptions via a multimodal large language model, and then implemented a data cleaning pipeline addressing text-video and frame-frame consistency, motion smoothness, and video clarity. Training a DynamiCrafter model on EgoVid-1M-3 (a subset of EgoVid-5M) resulted in an improved CD-FVD score compared to models trained on alternative cleaning strategies. AI practitioners can now leverage EgoVid-5M and its associated metadata to train and evaluate egocentric video generation models, potentially advancing applications in virtual/augmented reality and gaming.  |
| Direct Preference Optimization Using Sparse Feature-Level Constraints (Read more on [arXiv](https://arxiv.org/abs/2411.07618) or [HuggingFace](https://huggingface.co/papers/2411.07618))| Hanqi Yan, Minjun Zhu, Hongbo Zhang, Chak Tou Leong, Qingyu Yin | FPO (Feature-level constrained Preference Optimization) improves large language model (LLM) alignment by using sparse feature-level constraints.  The research aimed to develop a more efficient and controllable method for aligning LLMs to human preferences than existing methods like RLHF and DPO.  FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints within a Direct Preference Optimization (DPO) framework, minimizing mean squared error (MSE) between sparse activations. On the AlpacaEval-2 benchmark, FPO achieved a win rate improvement of up to 5.08% compared to baseline methods.  This provides AI practitioners with a more efficient and stable method for aligning LLMs, potentially reducing computational costs and improving generation quality.  |
| CamemBERT 2.0: A Smarter French Language Model Aged to Perfection (Read more on [arXiv](https://arxiv.org/abs/2411.08868) or [HuggingFace](https://huggingface.co/papers/2411.08868))| Benoît Sagot, Éric de la Clergerie, Rian Touchent, Francis Kulumba, Wissam Antoun | This paper introduces CamemBERT 2.0, two updated French language models: CamemBERTav2 (DeBERTaV3 architecture, Replaced Token Detection objective) and CamemBERTv2 (RoBERTa architecture, Masked Language Modeling objective).  The objective is to address temporal concept drift and improve performance on various natural language processing (NLP) tasks.  Both models were trained on a larger, more recent 275B token dataset with an updated tokenizer designed to better capture French linguistic nuances. CamemBERTav2 achieved an F1 score of 93.4% on named entity recognition (NER) using the FTB dataset, significantly outperforming the original CamemBERT (89.97%). AI practitioners can leverage these updated, open-source models for improved performance in various French NLP applications, including specialized domains like biomedicine, highlighting the importance of continuous model updates and data freshness in mitigating concept drift.  |
| Can sparse autoencoders be used to decompose and interpret steering vectors? (Read more on [arXiv](https://arxiv.org/abs/2411.08790) or [HuggingFace](https://huggingface.co/papers/2411.08790))| Adam Mahdi, Yushi Yang, Harry Mayne | This paper investigates why directly applying sparse autoencoders (SAEs) to steering vectors yields misleading decompositions. The research aims to understand why SAEs provide inaccurate interpretations of steering vectors, which are used to control the behavior of large language models.  The methodology involves decomposing steering vectors for "corrigibility" in a language model using SAEs and comparing them to decompositions of zero vectors and model activations. The primary results show that the L2-norm of the corrigibility steering vector is substantially smaller than that of typical model activations, and that 51.2% of relevant features show stronger activations on negative example prompts.  This implies that SAE interpretations of steering vectors are often dominated by the encoder bias and fail to capture meaningful negative projections in feature directions, hindering their direct use for interpreting how these vectors influence language model behavior.  |
