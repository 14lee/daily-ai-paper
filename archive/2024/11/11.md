

## Papers for 2024-11-11

| Title | Authors | Summary |
|-------|---------|---------|
| Balancing Pipeline Parallelism with Vocabulary Parallelism (Read more on [arXiv](https://arxiv.org/abs/2411.05288) or [HuggingFace](https://huggingface.co/papers/2411.05288))| Min Lin, Penghui Qi, Man Tsung Yeung, ufotalent | This paper proposes Vocabulary Parallelism to address computational and memory imbalances caused by vocabulary layers in pipeline parallel training of large language models.  The research aims to mitigate pipeline bubbles and memory bottlenecks arising from uneven workload distribution across pipeline stages due to vocabulary layers. The core methodology involves partitioning vocabulary layers across all pipeline devices, grouping computations into pipeline passes, and minimizing communication barriers within these layers.  Results show up to a 51% improvement in throughput compared to naive approaches, and near-perfect memory balance when combined with the V-Half scheduling strategy. This allows AI practitioners training large language models with pipeline parallelism to achieve significantly improved throughput and reduced memory consumption, particularly in large vocabulary scenarios, enabling training of larger models or using larger batch sizes.  |
| StdGEN: Semantic-Decomposed 3D Character Generation from Single Images (Read more on [arXiv](https://arxiv.org/abs/2411.05738) or [HuggingFace](https://huggingface.co/papers/2411.05738))| Kaiwen Xiao, Zhongkai Wu, Wang Zhao, Yanning Zhou, Yuze He | StdGEN is a novel pipeline for generating semantically decomposed 3D characters from single images.  The research aimed to create a method for generating high-quality, decomposable 3D characters from single images, addressing limitations of existing methods in decomposability, quality, and optimization time.  The pipeline utilizes a Semantic-aware Large Reconstruction Model (S-LRM), a multi-view diffusion model, and an iterative multi-layer surface refinement module.  On the Anime3D++ dataset, StdGEN achieved a CLIP similarity score of 0.935 for 3D character generation from arbitrary pose images. The decomposable nature of the generated 3D characters and the speed of generation (within minutes) offer AI practitioners a valuable tool for efficient character creation, editing, and animation in various 3D applications.  |
| DELIFT: Data Efficient Language model Instruction Fine Tuning (Read more on [arXiv](https://arxiv.org/abs/2411.04425) or [HuggingFace](https://huggingface.co/papers/2411.04425))| Marina Danilevksy, Lucian Popa, Krishna Killamsetty, ishikaa | DELIFT is a novel algorithm for optimizing data selection across different fine-tuning stages of Large Language Models (LLMs).  The research aimed to create a unified framework for efficient data selection across all fine-tuning stages of LLMs, optimizing performance and data efficiency.  DELIFT uses a pairwise utility metric combined with submodular optimization techniques to select data subsets. In experiments, DELIFT reduced fine-tuning data size by up to 70% without compromising performance, sometimes even exceeding full-dataset performance. This allows AI practitioners to significantly reduce computational costs and training time for LLMs without sacrificing performance, potentially increasing accessibility of LLM fine-tuning in resource-constrained environments.  |
| Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2411.02462) or [HuggingFace](https://huggingface.co/papers/2411.02462))| Jingyue Li, andstor | This paper investigates the effectiveness of parameter-efficient fine-tuning (PEFT) methods for training large language models (LLMs) to generate unit tests.  The primary research question is how well PEFT methods perform on unit test generation compared to full fine-tuning and in relation to resource utilization. The study evaluates LoRA, (IA)Â³, and prompt tuning against full fine-tuning across ten LLMs of varying sizes using the METHODS2TEST and HumanEval-X datasets, measuring syntactic correctness, CodeBLEU similarity, and code coverage. LoRA achieved the highest CodeBLEU scores in five out of ten models and was the only method to improve CodeBLEU for CodeLlama-7B.  AI practitioners can leverage PEFT, especially LoRA, to efficiently fine-tune LLMs for unit test generation, potentially matching or exceeding the performance of full fine-tuning while significantly reducing computational costs.  |
| LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation (Read more on [arXiv](https://arxiv.org/abs/2411.04997) or [HuggingFace](https://huggingface.co/papers/2411.04997))| Yuqing Yang, Xufang Luo, Aoqi Wu, Weiquan Huang, Yif29 | LLM2CLIP enhances visual representations by integrating large language models (LLMs) into CLIP training.  The research aimed to determine if LLMs could improve multimodal representation learning, addressing CLIP's limitations with complex and long text.  The key methodology involved caption contrastive fine-tuning of the LLM and a novel training process where the fine-tuned LLM guides CLIP's visual encoder.  LLM2CLIP boosted the performance of the SOTA EVA02 model by 16.5% on long and short-text retrieval tasks.  This implies that AI practitioners can leverage LLM2CLIP to significantly improve the performance of existing and future multimodal models relying on CLIP, especially in tasks involving complex or long textual descriptions.  |
| Improving the detection of technical debt in Java source code with an enriched dataset (Read more on [arXiv](https://arxiv.org/abs/2411.05457) or [HuggingFace](https://huggingface.co/papers/2411.05457))| Rick Kazman, Davide Di Ruscio, Phuong T. Nguyen, Anh M. T. Bui, Nam Le Hai | This paper presents a novel dataset and methods for improving the detection of technical debt (TD) in Java source code.  The research aimed to determine if manually classified comments and source code context enhance the detection of self-admitted technical debt (SATD). The authors curated a dataset, TESORO, by extracting SATD comments and corresponding source code from Java projects, then manually classifying TD types.  Experiments using pre-trained language models (PLMs) like CodeBERT and RoBERTa showed that adding TESORO to training data improved SATD detection F1-scores by up to 14.59%.  This suggests AI practitioners can significantly improve the performance of their TD detection models by incorporating source code context and leveraging datasets like TESORO for training.  |
