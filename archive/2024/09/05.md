## Papers for 2024-09-05

| Title | Authors | Summary | Link |
|-------|---------|---------|------|
| LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture | Benyou Wang, Chen Zhang, Shunian Chen, Xidong Wang, songdj | This research paper introduces LongLLaVA, a novel multimodal large language model (MLLM) architecture designed to process up to 1000 images efficiently while maintaining high performance. The key innovation lies in the hybrid architecture, which combines the strengths of both Transformer and Mamba models, enabling efficient processing of long sequences.  Evaluations demonstrate LongLLaVA's exceptional performance on various benchmarks, surpassing existing open-source MLLMs, and even exceeding the capabilities of commercial models. Its ability to process a large number of images with high efficiency and accuracy offers significant advantages for practitioners in domains like video understanding, high-resolution image processing, and multi-modal agents. This research underscores the growing trend of using hybrid architectures in multi-modal AI, offering a compelling solution for addressing the challenges of scaling up long-context understanding with MLLMs.   | [Read more](https://arxiv.org/abs/2409.02889) |
| LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA | LZDQ, Broccolito, davidlvxin, bys0318, NeoZ123 | This research proposes LongCite, a method for enabling LLMs to produce fine-grained citations within long-context question answering (LQAC) responses. To accomplish this, the researchers develop CoF, a novel pipeline for automatically constructing high-quality LQAC instances with precise sentence-level citations. Using CoF, they create LongCite-45K, a large-scale SFT dataset for LQAC, which is then used to train LongCite-8B and LongCite-9B. These models achieve state-of-the-art citation quality, surpassing even GPT-40, while also reducing hallucinations. These findings are valuable for practitioners as they demonstrate the potential for improving the reliability and trustworthiness of LLMs in long-context scenarios by providing fine-grained citations to support the generated responses.   | [Read more](https://arxiv.org/abs/2409.02897) |
| Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency | Gaojie Lin, Jiaqi Yang, Chao Liang, tianyumyum, janphu | The research paper "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency" presents a novel end-to-end audio-only conditioned diffusion model for generating realistic talking head videos. The model leverages long-term motion dependency through a temporal module design that captures both inter- and intra-clip temporal relationships. Additionally, it utilizes a dedicated audio-to-latents module that maps audio features to motion latents, further enhancing the correlation between audio and portrait movement. Extensive experiments demonstrate the effectiveness of Loopy, showing that it outperforms existing methods in generating high-quality, natural-looking, and temporally stable videos. This research is relevant to practitioners working on audio-driven animation and video generation, as it offers a more robust and efficient approach for producing realistic talking head avatars with minimal reliance on manual spatial constraints.   | [Read more](https://arxiv.org/abs/2409.02634) |
| MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark | btyu, jamessyx, yuanshengni, aaabiao, yuexiang96 | The paper introduces MMMU-Pro, an improved version of the MMMU benchmark, designed to more rigorously evaluate multimodal AI models' understanding and reasoning abilities.  MMMU-Pro introduces three key improvements: filtering out questions solvable by text-only models, augmenting candidate options to reduce guessing, and introducing a vision-only input setting where questions are embedded within images. Results show that models perform significantly worse on MMMU-Pro compared to MMMU, highlighting limitations in current multimodal AI. The paper also explores the impact of OCR and Chain of Thought reasoning, finding that OCR has minimal effect while CoT generally improves performance. This work provides a valuable tool for evaluating multimodal AI models and offers insights for researchers and practitioners in designing robust multimodal systems.   | [Read more](https://arxiv.org/abs/2409.02813) |
| Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining | rajhans-snowflake, stovecat, yuxiang630 | This paper introduces Arctic-SnowCoder-1.3B, a small code model trained with progressively higher-quality data.  The model outperforms other code models of similar size on several benchmarks, and even matches the performance of much larger models trained on trillions of tokens.  The paper demonstrates the importance of high-quality data in code pretraining and proposes a systematic method for selecting and using such data.  The paper also provides practical guidelines for practitioners, including optimal learning rate schedules, the use of repo-level data, and optimal repetitions of high-quality data. The work contributes significantly to the understanding of data quality in code pretraining and provides practical recommendations for AI engineers and data scientists.   | [Read more](https://arxiv.org/abs/2409.02326) |
| Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text | Rachel X. Peng, Ryan Yank Wang, Michael Burnham, kaylakahn | The paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. The models are completely open-source and are orders of magnitude more efficient than state-of-the-art large language models. By training the models on a small random sample of 10-25 documents, they outperform supervised classifiers trained on hundreds of thousands of documents and state-of-the-art generative models. The PolNLI dataset, consisting of 200,000 political documents with highly accurate labels across 800 classification tasks, is also released.  The paper highlights the efficiency and accuracy of the model while advocating for open-source models and ethical data practices. This work may be relevant to practitioners, such as AI engineers and data scientists, by demonstrating the effectiveness of few-shot learning in specific domains and promoting the use of open-source models for both research and production.   | [Read more](https://arxiv.org/abs/2409.02078) |
| FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation | Yuto Kondo, Hirokazu Kameoka, Takuhiro Kaneko, ououo | The authors of this paper propose FastVoiceGrad, a novel one-step diffusion-based voice conversion (VC) model that addresses the limitations of slow inference speeds inherent to multi-step diffusion-based VC models. This model utilizes adversarial conditional diffusion distillation (ACDD), which leverages the capabilities of generative adversarial networks (GANs) and diffusion models to distill a multi-step teacher diffusion model into a one-step student diffusion model. Experimental evaluations demonstrate that FastVoiceGrad achieves comparable VC performance to previous multi-step models while significantly reducing the number of iterations from dozens to one, thereby improving inference speed by a factor of 30. This approach offers practitioners a faster and more efficient model for VC, making it more suitable for real-time applications.   | [Read more](https://arxiv.org/abs/2409.02245) |
| Affordance-based Robot Manipulation with Flow Matching | Michael Gienger, Fanzhri | This paper presents a novel framework for robot manipulation that leverages visual affordances.  It utilizes prompt tuning to efficiently adapt large-scale vision-language models to downstream affordance understanding tasks. A Flow Matching method is then used to learn robot trajectories that are guided by the predicted affordances. This framework seamlessly integrates affordance modeling and trajectory generation for efficient and robust robot manipulation.  The proposed method is shown to be competitive with, and sometimes surpass, existing techniques in both data efficiency and performance, making it highly relevant to AI engineers and robotics practitioners seeking to build more intelligent and adaptable robotic systems.  The paper also provides valuable insights into the design choices and trade-offs involved in each stage of the framework, offering guidance for future research and development in the field.   | [Read more](https://arxiv.org/abs/2409.01083) |
