

## Papers for 2024-09-10

| Title | Authors | Summary |
|-------|---------|---------|
| Towards a Unified View of Preference Learning for Large Language Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2409.02795) or [HuggingFace](https://huggingface.co/papers/2409.02795))| hhhllan, ZefanCai, instro, songff, KbsdJames | This survey paper presents a unified framework for preference learning in large language models (LLMs), categorizing techniques based on data source, feedback mechanism, and optimization algorithm. The authors argue that existing categorizations based on reinforcement learning (RL) versus supervised fine-tuning (SFT) or online versus offline settings create artificial barriers, as core objectives are similar and algorithms can be decoupled from data acquisition strategies. The paper further details prevalent pointwise, pairwise, and listwise preference optimization methods, alongside training-free alignment approaches, highlighting their loss function designs. This comprehensive overview provides valuable insights for AI engineers and data scientists, facilitating understanding of the relationships between various alignment techniques and potentially enabling more effective development of human-aligned LLMs.   |
| MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct (Read more on [arXiv](https://arxiv.org/abs/2409.05840) or [HuggingFace](https://huggingface.co/papers/2409.05840))| Wa2erGo, iiiiwis, tnlin, lzchen2001, haonanzhang | MMEvol, a novel framework for evolving image-text instruction data, is introduced to enhance the capabilities of Multimodal Large Language Models (MLLMs). The authors identify data quality and diversity limitations in existing MLLM datasets and propose an iterative evolution process encompassing fine-grained perceptual, cognitive reasoning, and interactive evolutions, coupled with instruction elimination to filter inadequate samples. Experiments demonstrate that their MLLM trained on evolved data significantly surpasses open-source alternatives across 13 vision-language benchmarks. This work holds significant implications for AI practitioners, highlighting the importance of high-quality instruction data for developing robust MLLMs with improved reasoning, instruction following, and reduced hallucination susceptibility.   |
| OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs (Read more on [arXiv](https://arxiv.org/abs/2409.05152) or [HuggingFace](https://huggingface.co/papers/2409.05152))| huajunsir, square0083, xiangchen-dvi, sunmengshu, MikeDean | The research paper introduces OneGen, a novel framework designed to unify generation and retrieval tasks within a single Large Language Model (LLM). OneGen bridges the traditionally separate training paradigms of generation and retrieval by leveraging retrieval tokens generated autoregressively, enabling a single LLM to handle both tasks concurrently. Empirical evaluations across single-hop and multi-hop question answering, and entity linking demonstrate that OneGen outperforms pipeline solutions and, where applicable, prior single-model methods like GRIT. Moreover, the paper highlights OneGen's efficiency in training and inference, requiring less data and achieving faster inference speeds, particularly with increased retrieval frequency. Practitioners, including AI engineers and data scientists, can benefit from OneGen's simplified deployment, reduced computational costs, and improved efficiency, particularly in applications demanding seamless integration of retrieval and generation within LLMs.   |
| MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery (Read more on [arXiv](https://arxiv.org/abs/2409.05591) or [HuggingFace](https://huggingface.co/papers/2409.05591))| Zhicheng Dou, Kelong Mao, Zheng Liu, Hongjin Qian, namespace-Pt | This research paper introduces MemoRAG, a novel Retrieval-Augmented Generation (RAG) system designed to address challenges related to complex tasks involving extensive input contexts. MemoRAG leverages a memory module to create a global memory of the entire database and uses it to generate contextually relevant clues for accurate answer retrieval.  Experimental results demonstrate that MemoRAG surpasses existing RAG systems and other baselines across a range of tasks, including knowledge-intensive QA and summarization. MemoRAG's ability to effectively manage complex and lengthy texts, such as financial reports and legal contracts, by handling contexts of up to one million tokens and resolving intricate queries with high accuracy, makes it particularly valuable for AI practitioners working with large-scale text processing and retrieval applications.   |
| Benchmarking Chinese Knowledge Rectification in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2409.05806) or [HuggingFace](https://huggingface.co/papers/2409.05806))| huajunsir, Ningyu, cowTodd, JizhanFang, TianheLu | The authors introduce CKnowEdit, a novel dataset designed for evaluating and improving Chinese knowledge rectification in Large Language Models (LLMs). This dataset addresses a significant gap in the field, as prior knowledge editing research has primarily focused on English text and often fails to capture the nuances of the Chinese language.  Evaluations of existing knowledge editing methods on CKnowEdit reveal limitations in their ability to accurately and consistently rectify Chinese knowledge, highlighting the need for more sophisticated techniques. This work has significant implications for practitioners, as it provides a valuable resource for developing and evaluating Chinese-specific knowledge editing tools, ultimately leading to more reliable and culturally-sensitive LLMs for Chinese language applications.   |
| UniDet3D: Multi-dataset Indoor 3D Object Detection (Read more on [arXiv](https://arxiv.org/abs/2409.04234) or [HuggingFace](https://huggingface.co/papers/2409.04234))| Anna Vorontsova, ktoshik, filapro, barracuda049, maksimko123 | This paper introduces UniDet3D, a novel 3D object detection model trained on a mixture of indoor datasets to address the limitations of existing models trained on individual, insufficiently diverse datasets. UniDet3D leverages a unified label space across datasets and employs a simple yet effective architecture based on a vanilla transformer encoder without positional encoding or cross-attention.  The key innovation of UniDet3D lies in its ability to generalize to various indoor environments and achieve state-of-the-art results across six indoor benchmarks, outperforming existing methods in both accuracy and efficiency.  This advancement is particularly relevant to practitioners, such as AI engineers and data scientists, as UniDet3D offers a robust and customizable solution for indoor 3D object detection that can be readily adapted to various applications and computational constraints.   |
| POINTS: Improving Your Vision-language Model with Affordable Strategies (Read more on [arXiv](https://arxiv.org/abs/2409.04828) or [HuggingFace](https://huggingface.co/papers/2409.04828))| Xiao Zhou, Le Tian, Zeon-Zhuang, scyr, YuanLiuuuuuu | The authors introduce POINTS, a novel vision-language model that achieves state-of-the-art performance while utilizing a relatively small pre-training dataset and a publicly available visual instruction tuning dataset. Key innovations include the use of perplexity to filter the pre-training dataset, retaining only the top 20% of data with the lowest perplexity values, leading to significant performance improvements. Additionally, the authors propose "greedy model soup," a technique that averages the weights of models fine-tuned with varying dataset quantities and diversities, further enhancing performance. POINTS' effectiveness, coupled with its reliance on publicly available datasets, makes it a valuable tool for practitioners, including AI engineers and data scientists, seeking to develop and deploy robust vision-language models with constrained resources. The authors' meticulous ablation studies and detailed analysis of each component contribute to the model's transparency and ease of adoption.   |
| Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak (Read more on [arXiv](https://arxiv.org/abs/2409.04269) or [HuggingFace](https://huggingface.co/papers/2409.04269))| murodbek, mukhammadsaid | This research presents advancements in low-resource machine translation, specifically focusing on the Karakalpak language. The authors introduce a new FLORES+ devtest dataset translated into Karakalpak and develop parallel corpora for Uzbek-Karakalpak, Russian-Karakalpak, and English-Karakalpak language pairs. Utilizing these resources, they train and evaluate several neural machine translation models, demonstrating the effectiveness of incorporating data from related Turkic languages. The resulting models and datasets provide valuable resources for AI practitioners interested in developing NLP applications for Karakalpak and similar low-resource languages.   |
| Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance (Read more on [arXiv](https://arxiv.org/abs/2409.04593) or [HuggingFace](https://huggingface.co/papers/2409.04593))| Ge Liu, Pengrui Han, youjiaxuan, taofeng, cmulgy | This paper introduces Paper Copilot, a large language model (LLM) system designed to provide personalized and efficient academic research assistance. Paper Copilot employs thought retrieval, user profile generation, and high-performance optimization techniques to deliver its services. The system demonstrates a significant reduction in time required for information retrieval (69.92%) compared to traditional methods.  Moreover, user feedback indicates a strong preference for the self-evolving capabilities of the system, highlighting its potential as a valuable tool for researchers. This is highly relevant to AI practitioners, particularly those involved in natural language processing, as it showcases the application of advanced techniques like thought retrieval and efficient deployment strategies for real-world use cases in information retrieval and knowledge management.   |
| Insights from Benchmarking Frontier Language Models on Web App Code Generation (Read more on [arXiv](https://arxiv.org/abs/2409.05177) or [HuggingFace](https://huggingface.co/papers/2409.05177))| Yi Cui | This research paper presents an analysis of 16 large language models (LLMs) evaluated on WebApp1K, a benchmark designed to assess code generation capabilities for web applications. The key finding suggests that despite exhibiting similar knowledge levels, the performance difference among models stems from the varying frequency of errors. Notably, the study reveals that generating correct code exhibits higher complexity compared to producing incorrect code. Moreover, prompt engineering, while effective in specific scenarios, shows limited impact in overall error reduction. These insights are crucial for practitioners, particularly AI engineers and data scientists, highlighting the importance of prioritizing model reliability and minimizing mistakes during the development of coding LLMs.   |
| Evaluating Multiview Object Consistency in Humans and Image Models (Read more on [arXiv](https://arxiv.org/abs/2409.05862) or [HuggingFace](https://huggingface.co/papers/2409.05862))| Kanwisher, tgoconnell, Emma02, stephaniefu, tzler | The research introduces MOCHI, a novel benchmark for evaluating the alignment between human perception and computer vision models on 3D shape inference tasks. Using a "same/different" object identification task with varying viewpoints, the study reveals that while humans significantly outperform models like DINOv2, CLIP, and MAE, a correlation exists between human and model performance. Further analysis of human reaction time and gaze patterns suggests that humans achieve superior performance by dedicating more processing time and employing flexible attention mechanisms, which current models lack. This benchmark provides crucial insights for AI practitioners, highlighting the need for models to incorporate mechanisms for dynamic processing and flexible attention to achieve more human-like 3D shape understanding.   |
