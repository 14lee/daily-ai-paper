

## Papers for 2025-06-24

| Title | Authors | Summary |
|-------|---------|---------|
| Light of Normals: Unified Feature Representation for Universal
  Photometric Stereo (Read more on [arXiv](https://arxiv.org/abs/2506.18882) or [HuggingFace](https://huggingface.co/papers/2506.18882))| Bohan Li, Zhaoxi Chen, Chongjie Ye, Houyuan Chen, Hong Li | i) This paper introduces LINO-UniPS, a novel method for universal photometric stereo. ii) The research aims to improve surface normal recovery under complex lighting conditions by decoupling illumination and normal features and preserving high-frequency geometric details. iii) The methodology involves learnable light register tokens, a global cross-image attention mechanism, wavelet transform-based sampling, and a normal-gradient confidence loss. iv) LINO-UniPS demonstrates state-of-the-art performance on synthetic and real datasets; ablation studies showed improved CSIM and SSIM scores, indicating enhanced feature consistency. v) AI practitioners can leverage LINO-UniPS to develop more robust 3D reconstruction systems that are less sensitive to varying and uncalibrated lighting.  |
| OmniGen2: Exploration to Advanced Multimodal Generation (Read more on [arXiv](https://arxiv.org/abs/2506.18871) or [HuggingFace](https://huggingface.co/papers/2506.18871))| yzwang, sienna223, Shitao, Ruiran, wcyno23 | OmniGen2 is introduced as a versatile and open-source generative model for diverse generation tasks. The research aims to provide a unified solution for text-to-image, image editing, and in-context generation, employing distinct decoding pathways for text and image modalities. OmniGen2 uses comprehensive data construction pipelines and a reflection mechanism for image generation tasks, achieving competitive results with a relatively modest parameter size. On the OmniContext benchmark, OmniGen2 attains state-of-the-art consistency performance among open-source models, evaluated across eight task categories. The release of OmniGen2, including models, code, datasets, and pipelines, empowers AI practitioners with a unified generative model achieving competitive results on multiple benchmarks while maintaining strong text generation capabilities.  |
| LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2506.18841) or [HuggingFace](https://huggingface.co/papers/2506.18841))| Juanzi Li, Roy Ka-Wei Lee, Yushi Bai, Yuhao Wu, Zhiqiang007 | i) This paper introduces LongWriter-Zero, a reinforcement learning (RL) approach for mastering ultra-long text generation in large language models (LLMs). ii) The main objective is to develop an LLM capable of generating ultra-long, high-quality text without relying on supervised fine-tuning (SFT) on synthetic data. iii) The methodology involves training an LLM from scratch using RL with specialized reward models for length control, writing quality, and structural formatting, employing the Group Relative Policy Optimization (GRPO) algorithm. iv) Experimental results show LongWriter-Zero outperforms traditional SFT methods, achieving state-of-the-art results on WritingBench and Arena-Write benchmarks, surpassing even 100B+ models, and achieving an Elo rating of 1447 on Arena-Write. v) The implication for AI practitioners is a demonstration that RL can unlock ultra-long text generation capabilities in LLMs, offering an alternative to SFT that may lead to higher quality and more coherent long-form outputs, providing a training paradigm shift within LLM applications.  |
| Phantom-Data : Towards a General Subject-Consistent Video Generation
  Dataset (Read more on [arXiv](https://arxiv.org/abs/2506.18851) or [HuggingFace](https://huggingface.co/papers/2506.18851))| Crayon-Shinchan, onion-liu, TianxiangMa, lbc402, ZhuoweiChen | i) The paper introduces Phantom-Data, a large-scale dataset for subject-consistent video generation. ii) The research aims to address the copy-paste problem in subject-to-video generation by creating a dataset that disentangles subject identity from background and contextual attributes. iii) The dataset construction involves a three-stage pipeline: subject detection, cross-context subject retrieval from a large video and image database, and prior-guided identity verification. iv) The dataset comprises approximately one million identity-consistent pairs and the use of Phantom-Data in training demonstrates improvements in prompt alignment and visual quality, maintaining identity consistency comparable to in-pair baselines. v) AI practitioners can leverage Phantom-Data to train subject-to-video generation models with improved generalization and reduced copy-paste artifacts.  |
| ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought
  Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.18896) or [HuggingFace](https://huggingface.co/papers/2506.18896))| Ke Shen, Jiahao Qiu, Jingwen Gu, Ling Yang, Jiaru Zou | i) This paper introduces ReasonFlux-PRM, a trajectory-aware process reward model for evaluating chain-of-thought reasoning in LLMs. ii) The research aims to improve reward modeling for intermediate reasoning steps in trajectory-response outputs, specifically addressing limitations of existing PRMs. iii) The methodology involves training a PRM incorporating both step-level and trajectory-level supervision on a curated dataset of trajectory-response pairs, adapting it for offline data selection and online reward modeling. iv) Empirical results demonstrate that ReasonFlux-PRM-7B achieves an average gain of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling on downstream benchmarks. v) AI practitioners can leverage ReasonFlux-PRM to select higher quality distillation data and enhance reward signals for policy optimization, particularly in scenarios involving trajectory-response type outputs from frontier reasoning models.  |
| RLPR: Extrapolating RLVR to General Domains without Verifiers (Read more on [arXiv](https://arxiv.org/abs/2506.18254) or [HuggingFace](https://huggingface.co/papers/2506.18254))| Zefan Wang, Shu Yao, Shouli Wang, Bo Ji, Tianyu Yu | i) The paper introduces RLPR, a verifier-free framework to extrapolate Reinforcement Learning with Verifiable Rewards (RLVR) to general domains. ii) The research aims to overcome the reliance on domain-specific verifiers in RLVR by utilizing the intrinsic probability of Large Language Models (LLMs) for generating correct free-form answers as a reward signal. iii) The methodology involves replacing rule-based verifier rewards in RLVR with an intrinsic probability-based reward (PR), calculated from the average decoding probabilities of reference answer tokens, along with a debiasing technique and adaptive curriculum learning. iv) Experiments show that RLPR improves reasoning capabilities in both mathematical and general domains and outperforms VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva benchmarks. v) RLPR offers AI practitioners a simple, scalable approach to enhancing LLM reasoning without external verifiers, facilitating the utilization of general-domain data and broader application of RLVR. |
| Vision as a Dialect: Unifying Visual Understanding and Generation via
  Text-Aligned Representations (Read more on [arXiv](https://arxiv.org/abs/2506.18898) or [HuggingFace](https://huggingface.co/papers/2506.18898))| Qi Zhao, Yang Zhao, Hao Chen, hywang66, csuhan | i) The paper introduces Tar, a multimodal framework unifying visual understanding and generation through a shared, discrete, text-aligned representation. ii) The main objective is to create a multimodal LLM that can perform both visual understanding and generation tasks using a shared representation, eliminating the need for modality-specific designs. iii) The methodology involves a Text-Aligned Tokenizer (TA-Tok) that converts images into discrete tokens using a text-aligned codebook projected from a large language model's vocabulary and generative de-tokenizers for producing high-fidelity visual outputs. iv) Experiments show Tar matches or surpasses existing multimodal LLM methods and on the DPG Bench, Tar-1.5B achieves a score of 82.96. v) The principal implication is that AI practitioners can use Tar for faster convergence and greater training efficiency in multimodal tasks, benefiting from a shared, discrete representation for both visual understanding and generation.  |
| OAgents: An Empirical Study of Building Effective Agents (Read more on [arXiv](https://arxiv.org/abs/2506.15741) or [HuggingFace](https://huggingface.co/papers/2506.15741))| Yeyi Guan, Heyuan Huang, He Zhu, kangz, tianyue818 | i) This paper introduces OAGENTS, a new modular agent framework designed to achieve state-of-the-art performance in agentic AI tasks. ii) The main objective is to empirically analyze the impact of various agent component designs on overall effectiveness, addressing the lack of standardization in agent research. iii) The methodology involves a systematic study on the GAIA benchmark, comparing different designs for planning, tool use, memory, and test-time scaling within the OAGENTS framework. iv) The primary results show that OAGENTS achieves a 73.93% average score on the GAIA benchmark, outperforming existing open-source agent frameworks, and demonstrates a 74.07% cross-modal task accuracy. v) The principal implication for AI practitioners is a modular, open-source framework, OAGENTS, which standardizes agent building components and evaluation, enabling more reliable comparisons and advancements in agentic AI.  |
| VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed
  View Memory (Read more on [arXiv](https://arxiv.org/abs/2506.18903) or [HuggingFace](https://huggingface.co/papers/2506.18903))| Tomas Jakab, Andrea Vedaldi, Philip Torr, Runjia Li | i) The paper introduces Surfel-Indexed View Memory (VMem) for consistent, interactive video scene generation from a single image. ii) The main objective is to develop a memory mechanism that remembers and retrieves relevant past views geometrically to improve long-term consistency in autoregressive video generation. iii) The method indexes previous views using 3D surface elements (surfels) and retrieves relevant views based on the surfels visible from the new viewpoint to condition the generation. iv) Experiments on RealEstate10K demonstrate VMem outperforms existing methods in long-term scene consistency, with cycle trajectory translation distance reducing from 0.285 to 0.124. v) VMem provides AI practitioners with a plug-and-play module for geometrically indexing and retrieving relevant views, potentially enhancing the coherence of interactive video generation and scene exploration applications.  |
| LettinGo: Explore User Profile Generation for Recommendation System (Read more on [arXiv](https://arxiv.org/abs/2506.18309) or [HuggingFace](https://huggingface.co/papers/2506.18309))| Jianfeng Liu, Pu Zhao, Fangkai Yang, Di Zhang, Lu Wang | i) The paper introduces LettinGo, a novel framework for generating diverse and adaptive user profiles for recommendation systems using Large Language Models (LLMs). ii) The research aims to improve recommendation systems by generating diverse, adaptive, and high-quality user profiles by exploring and aligning profile generation with downstream task performance. iii) The proposed approach uses diverse LLMs for profile exploration, evaluates profile quality via downstream recommendation performance, and aligns profile generation through pairwise preference data using Direct Preference Optimization (DPO). iv) Experimental results demonstrate that LettinGo significantly enhances recommendation accuracy, adaptability, and contextual awareness, with an average increase of 20 percentage points in accuracy compared to the baseline when using LLaMA3 8B Instruct model. v) AI/ML engineers can leverage this framework to build recommendation systems with enhanced user profile generation capabilities and adapt profiles more effectively to diverse and evolving task requirements, potentially boosting recommendation accuracy and relevance.  |
| ReDit: Reward Dithering for Improved LLM Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2506.18631) or [HuggingFace](https://huggingface.co/papers/2506.18631))| Yao Shu, Hande Dong, Ying Tiffany He, Jiarui Yu, Chenxing Wei | i) This paper introduces ReDit, a reward dithering method to enhance LLM policy optimization. ii) The research aims to address gradient anomaly, optimization instability, and slow convergence issues associated with discrete reward functions in LLM training. iii) The method involves adding zero-mean random noise to discrete reward signals to facilitate smoother gradient updates and improve exploration. iv) Experiments show ReDit achieves performance comparable to vanilla GRPO with only 10% of the training steps, and a 4% improvement when trained for the same duration. v) ReDit mitigates gradient issues with discrete rewards, suggesting practitioners can improve LLM training by injecting random noise into discrete reward signals.  |
| FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.16123) or [HuggingFace](https://huggingface.co/papers/2506.16123))| Potsawee Manakul, Panop Pitchayarthorn, Warit Sirichotedumrong, pittawat, natnitaract | FinCoT introduces a structured chain-of-thought (CoT) prompting approach grounded in expert financial reasoning for large language models (LLMs). The research investigates standard, unstructured CoT, and structured CoT prompting styles for financial reasoning tasks. FinCoT incorporates domain-specific Mermaid blueprints into a structured CoT template to improve performance. Results on 1,032 CFA-style questions show FinCoT improves performance from 63.2% to 80.5% on Qwen-2.5-7B-Instruct and reduces generated tokens eight-fold compared to structured CoT prompting. AI practitioners can leverage FinCoT to enhance the accuracy and interpretability of LLMs in financial applications through structured, domain-aligned prompts. |
| ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs (Read more on [arXiv](https://arxiv.org/abs/2506.18792) or [HuggingFace](https://huggingface.co/papers/2506.18792))| Gregory Slabaugh, Zhensong Zhang, Thomas Tanay, Sibi Catley-Chandar, Michal Nazarczuk | ViDAR is a novel 4D reconstruction framework for monocular video using diffusion-aware techniques. The research aims to improve dynamic novel view synthesis from monocular video by leveraging personalized diffusion models to generate a pseudo multi-view supervision signal for training a Gaussian splatting representation. The methodology involves a personalized DreamBooth-style diffusion model for enhancing novel views and a diffusion-aware loss function combined with camera pose optimization. Experiments on the DyCheck dataset demonstrated improved visual quality and geometric consistency, outperforming state-of-the-art baselines, with an average improvement of 0.94dB in PSNR in dynamic masked regions compared to MoSca. This work suggests a method for AI practitioners to improve 4D reconstruction by integrating personalized diffusion models into existing frameworks.  |
| Auto-Regressively Generating Multi-View Consistent Images (Read more on [arXiv](https://arxiv.org/abs/2506.18527) or [HuggingFace](https://huggingface.co/papers/2506.18527))| Chen Zhao, Jinbo Wu, Jialun Liu, Yuxiao Yang, JiaKui Hu | i) This paper introduces the Multi-View Auto-Regressive (MV-AR) model for generating consistent multi-view images from diverse prompts. ii) The main objective is to develop a model capable of generating multi-view consistent images from various prompts, addressing the limitations of existing diffusion-based methods. iii) The methodology involves leveraging an auto-regressive model with condition injection modules for text, camera pose, image, and shape, along with a "Shuffle View" data augmentation technique and progressive training strategy. iv) Experiments demonstrate the performance of MV-AR, achieving a CLIP-Score of 29.49 on the Google Scanned Objects dataset in the text-to-multi-view task, indicating improved image-text consistency compared to diffusion-based methods. v) The principal implication is that the MV-AR framework provides AI practitioners with a robust baseline for multi-view image generation, enabling the development of unified models that handle diverse conditions synchronously and generate consistent images.  |
| SlimMoE: Structured Compression of Large MoE Models via Expert Slimming
  and Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.18349) or [HuggingFace](https://huggingface.co/papers/2506.18349))| Young Jin Kim, Ilgee Hong, Zixuan Zhang, Chen Liang, Pearush | SlimMoE presents a multi-stage compression framework for Mixture of Experts (MoE) models using expert slimming and distillation. The research aims to reduce the parameter count of large MoE models without extensive retraining by slimming experts and transferring knowledge through intermediate stages. The methodology involves structured pruning of neurons within experts and iterative knowledge distillation. SlimMoE compressed a Phi-3.5-MoE model, reducing total parameters to 7.6B (Phi-mini-MoE) and 3.8B (Phi-tiny-MoE) with activated parameters of 2.4B and 1.1B respectively, using only 400B tokens. The structured pruning and multi-stage distillation approach allows for the creation of high-quality compact MoE models, facilitating deployment in resource-constrained environments like single-GPU setups.  |
| Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.16962) or [HuggingFace](https://huggingface.co/papers/2506.16962))| Wenjie Li, Yujie Zhang, Wenjie Lou, Yankai Jiang, manglu3935 | i) This paper introduces a new approach for enhancing medical reasoning in multimodal large language models (MLLMs). ii) The primary objective is to develop a framework for generating effective chain-of-thought (CoT) data to improve the reasoning capabilities of medical MLLMs. iii) The methodology involves a novel reasoning-path searching scheme called Mentor-Intern Collaborative Search (MICS) and a curriculum learning strategy. iv) The resulting medical MLLM, Chiron-01, achieves state-of-the-art performance across several medical visual question answering and reasoning benchmarks, including improving its baseline model's performance by an average of 5.7% to 8.1% on VQA tasks. v) The development of MICS provides AI practitioners with a structured approach to creating high-quality CoT datasets for specialized domains, potentially improving the reasoning capabilities of MLLMs in tasks requiring complex, step-by-step analysis.  |
| ConsumerBench: Benchmarking Generative AI Applications on End-User
  Devices (Read more on [arXiv](https://arxiv.org/abs/2506.17538) or [HuggingFace](https://huggingface.co/papers/2506.17538))| Yiyu Liu, Hoang Nguyen, Rohan Kadekodi, Yile Gu, kamahori | i) CONSUMERBENCH is introduced as a comprehensive benchmark for evaluating GenAI applications' system efficiency and response time on end-user devices under realistic, concurrent execution scenarios. ii) The paper aims to address challenges in resource management, system efficiency, and user experience when deploying GenAI models on resource-constrained end-user devices, unlike cloud environments with dedicated GPUs. iii) The methodology involves developing a benchmarking framework to simulate multi-application workflows on end-user devices, capturing application-level (latency, SLO attainment) and system-level (CPU/GPU utilization, memory bandwidth) metrics under varying deployment strategies (GPU partitioning, shared model deployments). iv) Experiments reveal that greedy GPU resource allocation leads to severe starvation of lightweight applications, with decode phases in LiveCaptions running up to 30x slower, resulting in a 12.4x increase in average request latency; static GPU partitioning causes compute capacity to go unused despite the presence of unmet SLOs, while shared memory usage can lead to inefficient kernel implementations; model-sharing with an inference server incurs a 40% SLO miss for one application. v) The findings imply a need for dynamic, SLO-aware memory management and scheduling strategies, as well as GPU architecture-aware kernel designs, to optimize GenAI application performance on end-user devices.  |
| CommVQ: Commutative Vector Quantization for KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2506.18879) or [HuggingFace](https://huggingface.co/papers/2506.18879))| Tianle Cai, Talha Chafekar, Muhammad Yusuf Hassan, Yang Zhang, Junyan Li | i) This paper introduces Commutative Vector Quantization (CommVQ) to compress the key-value (KV) cache for long-context Large Language Model (LLM) inference. ii) The primary objective is to reduce the memory footprint of KV caches in LLMs without significant accuracy degradation. iii) The method involves additive quantization with a learned codebook designed to be commutative with Rotary Position Embedding (RoPE), integrated via an Expectation-Maximization (EM) algorithm. iv) Experiments demonstrate an 87.5% reduction in FP16 KV cache size using 2-bit quantization, with competitive performance, and the possibility of 1-bit quantization with minimal accuracy loss on the LLaMA-3.1 8B model, tested on LongBench, InfiniteBench, and GSM8K benchmarks. v) CommVQ provides AI practitioners a more memory-efficient method for deploying long-context LLMs, potentially enabling 128K context length on a single RTX 4090 GPU for a LLaMA-3.1 8B model, overcoming memory constraints.  |
| From Virtual Games to Real-World Play (Read more on [arXiv](https://arxiv.org/abs/2506.18901) or [HuggingFace](https://huggingface.co/papers/2506.18901))| Zilong Chen, Xi Chen, Jinjing Zhao, Fangyun Wei, Wenqiang Sun | i) The paper introduces RealPlay, a neural network-based real-world game engine enabling interactive video generation from user control signals. ii) The research aims to develop a photorealistic and temporally consistent video generation model that responds to user control, eliminating the need for annotated real-world data. iii) The methodology involves a mixed training paradigm combining labeled game data (Forza Horizon 5) with unlabeled real-world video data, adapting a pre-trained image-to-video generator (CogVideoX) for chunk-wise generation, and incorporating action control through adaptive LayerNorm. iv) Experimental results show RealPlay achieves a 90% control success rate and demonstrates control transfer from virtual to real-world entities (vehicles, bicycles, pedestrians). v) RealPlay presents a data-driven approach for creating interactive simulations, enabling AI practitioners to develop real-world game engines and interactive high-fidelity simulations using learned dynamics instead of traditional graphics engines.  |
| FaithfulSAE: Towards Capturing Faithful Features with Sparse
  Autoencoders without External Dataset Dependencies (Read more on [arXiv](https://arxiv.org/abs/2506.17673) or [HuggingFace](https://huggingface.co/papers/2506.17673))| Andrew Bermingham, Luis Eduardo Rodrigues Vieira, Donghyun Lee, Harryn Oh, seonglae | i) The paper introduces FaithfulSAE, a method for training sparse autoencoders (SAEs) on a model's self-generated synthetic dataset to improve the capture of model-internal features. ii) The research investigates whether training SAEs on faithful, self-generated datasets can mitigate issues of instability and hallucinated features arising from out-of-distribution data in external training datasets. iii) FaithfulSAE employs the LLM to generate a synthetic dataset reflecting its inherent distribution, and then trains a Top-K SAE on this dataset; "faithfulness" is then assessed using metrics such as reconstruction performance and shared feature ratio (SFR). iv) Results demonstrate that FaithfulSAEs outperform SAEs trained on web-based datasets in SAE probing tasks and exhibit a lower Fake Feature Ratio in 5 out of 7 models, with shared feature ratio analysis indicating increased stability across seeds compared to instruction datasets. v) The principal implication for AI practitioners is the recommendation to consider model-generated training datasets for SAEs, as this approach can reduce dependence on potentially noisy external datasets and improve the interpretability of learned features in LLMs.  |
| A deep learning and machine learning approach to predict neonatal death
  in the context of São Paulo (Read more on [arXiv](https://arxiv.org/abs/2506.16929) or [HuggingFace](https://huggingface.co/papers/2506.16929))| Afia Anjum Tamanna, A Z M Tahmidul Kabir, Plabon Kumar Saha, Mohon Raihan, rajandasgupta | i) This paper investigates machine learning and deep learning models for predicting neonatal mortality in São Paulo. ii) The primary research objective is to determine the most accurate model for identifying newborns at high mortality risk. iii) The methodology involves training and comparing various machine learning algorithms (Logistic Regression, KNN, Random Forest, XGboost) and deep learning models (CNN, LSTM) using a dataset of 1.4 million newborn child records. iv) The LSTM model achieved the highest accuracy (99%) compared to machine learning methods (XGboost and Random Forest at 94%). v) The LSTM model presents a potentially suitable solution for AI practitioners developing neonatal mortality risk prediction tools based on this dataset.  |
| Robust Reward Modeling via Causal Rubrics (Read more on [arXiv](https://arxiv.org/abs/2506.16507) or [HuggingFace](https://huggingface.co/papers/2506.16507))| Sravanti Addepalli, Gandharv Patil, Rahul Madhavan, Harman Singh, Pragya Srivastava | i) This paper introduces Causally Robust Reward Modeling (Crome) to mitigate reward hacking in Large Language Models (LLMs). ii) The main objective is to develop a reward model robust to superficial attributes and sensitive to true causal drivers of quality. iii) Crome employs synthetic targeted augmentations during training, including Causal Augmentations and Neutral Augmentations, guided by an oracle LLM based on identified causal rubrics. iv) Empirical results on RewardBench show that Crome improves average accuracy by up to 5.4%, with gains up to 13.2% and 7.2% in specific categories. v) The principal implication is that AI practitioners can use Crome's causal framework and augmentation techniques to develop more robust reward models that are less susceptible to reward hacking and more aligned with intended quality metrics.  |
| I Know Which LLM Wrote Your Code Last Summer: LLM generated Code
  Stylometry for Authorship Attribution (Read more on [arXiv](https://arxiv.org/abs/2506.17323) or [HuggingFace](https://huggingface.co/papers/2506.17323))| Bertalan Borsos, Nils Gruschka, Richard A. Dubniczky, Tamas Bisztray, Neo111x | i) This paper introduces LLM-AUTHORBENCH, a benchmark for LLM-generated C code authorship attribution and proposes a custom CodeT5-Authorship model. ii) The primary research question is to determine the feasibility and optimal methods for LLM authorship attribution in C code, comparing various ML and transformer models. iii) The methodology involves generating a dataset of 32,000 C programs from eight LLMs, training CodeT5-Authorship, and comparing it against traditional ML classifiers and other fine-tuned transformer models. iv) Results show that CodeT5-Authorship achieves 97.56% accuracy in binary classification of closely related LLMs, and 95.40% accuracy in multi-class attribution among five leading LLMs. v) AI practitioners can leverage the CodeT5-Authorship model and LLM-AUTHORBENCH benchmark to enhance accountability and security in software engineering, enabling better source code attribution.  |
| SoK: Evaluating Jailbreak Guardrails for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.10597) or [HuggingFace](https://huggingface.co/papers/2506.10597))| Daoyuan Wu, Zongjie Li, Wenxuan Wang, Zhenlan Ji, Xunguang Wang | i) This paper presents a systematization of knowledge (SoK) for evaluating jailbreak guardrails in large language models (LLMs). ii) The research aims to categorize existing LLM guardrails and evaluate their effectiveness against jailbreak attacks. iii) The methodology involves developing a multi-dimensional taxonomy along six key dimensions and a Security-Efficiency-Utility (SEU) evaluation framework. iv) The study found a significant vulnerability of current session-level guardrails against advanced multi-turn attacks with ASR exceeding 90% for some guardrails against adaptive attacks like X-Teaming. v) AI practitioners should be aware of the limitations of session-level guardrails against sophisticated multi-turn attacks and prioritize developing more robust defense methodologies.  |
