

## Papers for 2025-06-11

| Title | Authors | Summary |
|-------|---------|---------|
| Geopolitical biases in LLMs: what are the "good" and the "bad" countries
  according to contemporary language models (Read more on [arXiv](https://arxiv.org/abs/2506.06751) or [HuggingFace](https://huggingface.co/papers/2506.06751))| Dmitrii Korzh, tlenusik, apanc, IvanLazichny, msalnikov | This paper evaluates geopolitical biases in LLMs by analyzing their interpretation of historical events. The research question is: Do LLMs demonstrate geopolitical biases by showing a preference for specific national perspectives when interpreting controversial historical events? The methodology involves a structured framework with a manually collected dataset of neutral event descriptions and contrasting viewpoints from the USA, UK, USSR, and China, analyzed across GPT-4o-mini, llama-4-maverick, Qwen2.5 72B, and GigaChat-Max. The primary results show significant geopolitical biases, with models favoring specific national narratives (e.g., GPT-40-MINI favors USA in 76% of cases vs. USSR). The principal implication for AI practitioners is the need for advanced debiasing strategies to mitigate national narrative biases in LLMs, as simple debiasing prompts had limited effect.  |
| RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic
  Sampling (Read more on [arXiv](https://arxiv.org/abs/2506.08672) or [HuggingFace](https://huggingface.co/papers/2506.08672))| Jiaqi Li, Yang Liu, zlzheng | i) RuleReasoner enhances rule-based reasoning in small language models using reinforcement learning and domain-aware dynamic sampling. ii) The research investigates whether reinforcement learning can effectively enhance rule-based reasoning capabilities in small language models and generalize across diverse tasks. iii) The methodology involves a novel reinforcement learning with verifiable rewards (RLVR) framework and a domain-aware dynamic sampling (DADS) algorithm that dynamically reweights training domains based on historical rewards. iv) RuleReasoner achieves a 4.1% average points improvement on eight in-distribution tasks and a 10.4% average points improvement on three out-of-distribution tasks compared to OpenAI-01. v) Practitioners can leverage RuleReasoner to improve the reasoning performance of small language models with enhanced sample utilization, reducing the need for large-scale models or extensive human-engineered training recipes.  |
| Solving Inequality Proofs with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.07927) or [HuggingFace](https://huggingface.co/papers/2506.07927))| Alex Gu, Tony Xia, Jikai Jin, Luna Lyu, Jiayi Sheng | i) The paper introduces INEQMATH, a benchmark for evaluating LLMs on Olympiad-level inequality proofs. ii) The research aims to assess LLMs' ability to perform rigorous mathematical reasoning in the context of inequality proving. iii) It utilizes a novel LLM-as-judge evaluation framework that assesses both final-answer correctness and step-wise solution soundness. iv) Evaluation of 29 LLMs reveals that even advanced models like o1 achieve less than 10% overall accuracy under step-wise scrutiny, representing a drop of up to 65.5% compared to final-answer accuracy alone. v) The findings imply that current LLMs exhibit a significant gap between finding correct answers and constructing rigorous mathematical proofs, highlighting the need for future research into areas such as theorem-guided reasoning and self-refinement to improve proof correctness.  |
| Self Forcing: Bridging the Train-Test Gap in Autoregressive Video
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2506.08009) or [HuggingFace](https://huggingface.co/papers/2506.08009))| Eli Shechtman, Mingyuan Zhou, Zhengqi Li, Xun Huang, gdhe17 | i) The paper introduces Self Forcing, a training paradigm for autoregressive video diffusion models designed to mitigate exposure bias. ii) The research aims to bridge the train-test distribution gap in autoregressive video diffusion models to improve video generation quality and efficiency. iii) The methodology involves training the model through autoregressive self-rollout with KV caching, enabling supervision through a holistic video-level loss and employing a few-step diffusion model with stochastic gradient truncation. iv) Experiments show that Self Forcing enables real-time video generation at 17 FPS with sub-second latency on a single H100 GPU, achieving comparable or superior generation quality to existing diffusion models. v) The Self Forcing training paradigm allows for the development of lower latency video generation that is more suitable for real-time, interactive video generation, streaming and gaming applications.  |
| Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error
  Diagnosis in GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2506.04614) or [HuggingFace](https://huggingface.co/papers/2506.04614))| Junyang Wang, Haowei Liu, Haiyang Xu, Xi Zhang, Yuyang Wanyan | i) The paper introduces GUI-Critic-R1, a model for pre-operative error diagnosis in GUI automation. ii) The research aims to improve GUI automation by providing feedback before action execution, addressing issues of error accumulation and inefficiency. iii) A Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy was used to construct the model, along with a reasoning-bootstrapping pipeline to generate GUI-Critic-Train and GUI-Critic-Test datasets. iv) Experiments show that GUI-Critic-R1 improves the success rate of a baseline GUI automation system from 22.4% to 27.6% on the AndroidWorld benchmark. v) The GUI-Critic-R1 and S-GRPO are provided as a way to improve single-step accuracy in GUI agents which has low tolerance for decision-making errors at each step.  |
| Aligning Text, Images, and 3D Structure Token-by-Token (Read more on [arXiv](https://arxiv.org/abs/2506.08002) or [HuggingFace](https://huggingface.co/papers/2506.08002))| Georgia Gkioxari, Vansh Tibrewal, Aadarsh Sahoo | Kyvo is introduced as a decoder-only transformer model that aligns text, images, and structured 3D scenes token-by-token. The research investigates the potential of autoregressive models for structured 3D scene understanding and generation. The methodology involves designing and training an LLM with modality-specific tokenizers for images and 3D, evaluating across four core 3D tasks. The model achieved a Jaccard Index of 0.4784 on Objectron for real-world 3D object recognition, demonstrating competitive performance compared to specialized 3D object detectors. This unified LLM framework enables AI practitioners to tackle a variety of complex visual 3D tasks, such as 3D reconstruction and 3D-conditioned image generation. Some implementation details like training throughput (8,800 tokens/sec/GPU) are also mentioned.  |
| Frame Guidance: Training-Free Guidance for Frame-Level Control in Video
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.07177) or [HuggingFace](https://huggingface.co/papers/2506.07177))| Soo Ye Kim, Jaehyeong Jo, Sangwon Jang, jaehong31, tkkitkki | Frame Guidance is presented as a novel training-free method for controllable video generation using frame-level signals within video diffusion models (VDMs). The research aims to achieve fine-grained control over video generation without task-specific fine-tuning of large VDMs. The proposed method utilizes a latent processing technique called latent slicing to reduce memory usage and video latent optimization (VLO) for coherent video generation by applying deterministic optimization in early stages. Experiments demonstrate that Frame Guidance can produce high-quality controlled videos across diverse tasks and inputs, achieving an FID score of 55.60 and FVD score of 577.1 on the DAVIS dataset for keyframe guided generation when applied to CogX, outperforming training-required baselines. This training-free guidance method offers AI practitioners a flexible and efficient approach to control video generation using frame-level signals, potentially reducing computational costs and model retraining efforts.  |
| ECoRAG: Evidentiality-guided Compression for Long Context RAG (Read more on [arXiv](https://arxiv.org/abs/2506.05167) or [HuggingFace](https://huggingface.co/papers/2506.05167))| Seung-won Hwang, Dohyeon Lee, Jinsu Kim, yeonseokjeong | i) ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality in Retrieval-Augmented Generation (RAG). ii) The research aims to improve LLM performance on Open-Domain Question Answering (ODQA) tasks by filtering out non-evidential information in RAG. iii) The methodology involves compressing documents guided by evidentiality and reflecting on the evidentiality of compressed content to retrieve more if necessary. iv) Experiments on Natural Questions demonstrate ECoRAG achieves 36.48% exact match, outperforming standard RAG and RECOMP. v) Practitioners can utilize ECoRAG to improve LLM performance and reduce computational costs by filtering irrelevant content in RAG applications.  |
| DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for
  Parameter-Efficient Video-Text Retrieval (Read more on [arXiv](https://arxiv.org/abs/2506.08887) or [HuggingFace](https://huggingface.co/papers/2506.08887))| Yifeng Zhang, Tao He, Tianxiang Hao, Guoqiang Gong, lunar677 | i) DiscoVLA addresses discrepancies in vision, language, and alignment for parameter-efficient video-text retrieval. ii) The research objective is to mitigate vision, language, and alignment discrepancies that arise when adapting image-text pre-training models like CLIP to video-text retrieval. iii) The methodology involves an Image-Video Features Fusion module, Pseudo Image-level Alignment, and Image-to-Video Alignment Distillation. iv) On the MSRVTT dataset with CLIP (ViT-B/16), DiscoVLA achieves a 50.5% R@1, surpassing previous methods by 1.5%. v) DiscoVLA offers AI practitioners a novel approach to improve video-text retrieval by simultaneously addressing vision, language, and alignment discrepancies.  |
| Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural
  Compressor (Read more on [arXiv](https://arxiv.org/abs/2506.07932) or [HuggingFace](https://huggingface.co/papers/2506.07932))| Nandita Vijaykumar, Mohammadreza Mofayezi, Sankeerth Durvasula, Yushi Guan, rishitdagli | Squeeze3D proposes a novel framework for compressing 3D data by leveraging the implicit prior knowledge learned by pre-trained 3D generative models. The research aims to achieve high compression ratios for 3D data in various formats (meshes, point clouds, radiance fields) using existing pre-trained encoders and generators. The methodology involves training forward and reverse mapping networks to bridge the latent spaces between pre-trained encoders and generators using a synthetic 3D dataset generated from the pre-trained generator. Experiments demonstrate Squeeze3D achieves compression ratios of up to 2187× for textured meshes while retaining comparable visual quality. The principal implication is a method for AI practitioners to achieve extreme 3D data compression without training object-specific networks, enabling efficient storage and transmission of 3D content.  |
| MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient
  Fine-Tuning of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.05928) or [HuggingFace](https://huggingface.co/papers/2506.05928))| Wenqiao Zhang, Rolan Yan, Hongyang He, Tianwei Lin, cajie | i) The paper introduces Heterogeneous Mixture-of-Adapters (MoA), a parameter-efficient fine-tuning approach utilizing diverse PEFT adapter architectures. ii) The research aims to address representation collapse and load imbalance in MoE-LoRA methods by integrating diverse adapter structures for enhanced expert specialization. iii) MoA employs token-level dynamic routing to activate PEFT adapter experts with diverse structures, including LoRA, parallel adapters, and prompt tuning. iv) Experiments show MoA achieves 81.51% accuracy on math benchmarks, outperforming homogeneous MoE-LoRA with fewer trainable parameters (24.52M). v) AI practitioners can leverage MoA to achieve higher parameter efficiency and knowledge transfer in LLMs while reducing memory consumption and improving inference speed.  |
| Institutional Books 1.0: A 242B token dataset from Harvard Library's
  collections, refined for accuracy and usability (Read more on [arXiv](https://arxiv.org/abs/2506.08300) or [HuggingFace](https://huggingface.co/papers/2506.08300))| Kristi Mukk, Jack Cushman, John Hess, Catherine Brobston, Matteo Cargnelutti | i) Institutional Books 1.0, a 242B token dataset of public domain books from Harvard Library, is introduced to address the scarcity of high-quality training data for large language models. ii) The research objective was to create a usable and documented dataset of historic texts from Harvard Library’s digitized collections. iii) The methodology included extracting digitized books, analyzing temporal and language coverage, performing topic classification using a fine-tuned BERT model, collection-level deduplication, and OCR artifact analysis, followed by optional OCR post-processing. iv) The dataset comprises 983,004 volumes (242B tokens), with a topic classification model achieving 97.8% accuracy on a benchmark dataset, and 91.41% of the volumes being identified as public domain by HathiTrust. v) The curated dataset provides AI/ML practitioners with a large, publicly available resource of historical text to enhance long context comprehension and text generation models.  |
| Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction (Read more on [arXiv](https://arxiv.org/abs/2506.07976) or [HuggingFace](https://huggingface.co/papers/2506.07976))| Amrith Setlur, Yifei Zhou, Lunjun Zhang, Junhong Shen, JackBAI | i) This paper introduces interaction scaling as a new dimension for test-time scaling of agents that emphasizes acting more, rather than simply thinking more. ii) The main objective is to demonstrate that increasing the number of interaction steps during test-time improves agent performance in interactive environments, enabling behaviors like exploration and backtracking. iii) The methodology involves a curriculum-based online reinforcement learning (RL) approach called TTI (Test-Time Interaction), which trains agents by adaptively adjusting their rollout lengths, along with prompting to scale test-time interaction. iv) Experiments show that TTI, using a Gemma 3 12B model, achieves state-of-the-art open-source, open-data web agent performance on WebVoyager and WebArena, improving over a non-fine-tuned agent by 9% and 8%, respectively. v) The principal implication for AI practitioners is that interaction scaling is a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents capable of balancing exploration and exploitation in dynamic environments, suggesting a shift from purely reactive policies to adaptive policies that collect information on-the-fly.  |
| Mathesis: Towards Formal Theorem Proving from Natural Languages (Read more on [arXiv](https://arxiv.org/abs/2506.07047) or [HuggingFace](https://huggingface.co/papers/2506.07047))| Roozbeh Yousefzadeh, Pengyi Zhai, Zijin Feng, Yu Xuejun, Jianyuan1 | Mathesis introduces an end-to-end theorem proving pipeline, addressing the gap between natural language problem statements and formal reasoning systems. The research aims to automate formal theorem proving from informal problem statements. The methodology employs a Mathesis-Autoformalizer trained via reinforcement learning with a hierarchical preference optimization mechanism and introduces a novel LeanScorer for nuanced formalization quality assessment. The system achieves a 64% accuracy on MiniF2F with pass@32 and 18% on Gaokao-Formal. Mathesis provides AI practitioners with an automated system for formalizing and proving theorems directly from natural language, thus enhancing the applicability of formal methods to real-world problems.  |
| RKEFino1: A Regulation Knowledge-Enhanced Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2506.05700) or [HuggingFace](https://huggingface.co/papers/2506.05700))| Jeff Zhao, Ruoyu Xiang, Yueru He, YanAdjeNole | i) The paper introduces RKEFino1, a regulation knowledge-enhanced language model for improved accuracy and compliance in Digital Regulatory Reporting (DRR). ii) The research aims to enhance the interpretability, compliance accuracy, and reliability of financial language models in DRR tasks using domain-specific knowledge. iii) The methodology involves fine-tuning the Fino1 model with domain knowledge from XBRL, CDM, and MOF, and formulating knowledge-based QA, mathematical reasoning QA, and numerical NER tasks. iv) Experimental results show that RKEFino1 achieves a 26.62% F1-score on the numerical NER task, outperforming the baseline Fino1 model's 14.99%. v) RKEFino1 offers AI practitioners a fine-tuned language model that demonstrates improved generalization in DRR tasks, particularly in recognizing numerical entities within financial text and tables, indicating potential utility for compliance-critical applications.  |
| QQSUM: A Novel Task and Model of Quantitative Query-Focused
  Summarization for Review-based Product Question Answering (Read more on [arXiv](https://arxiv.org/abs/2506.04020) or [HuggingFace](https://huggingface.co/papers/2506.04020))| Zhuang Li, Minh Ngoc Dinh, Xiuzhen Zhang, An Quang Tang | QQSUM introduces a novel task and model for quantitatively summarizing diverse customer opinions in review-based product question answering (PQA). The main research question is how to summarize diverse customer opinions into representative key points (KPs) and quantify their prevalence to effectively answer user queries in review-based PQA. They propose QQSUM-RAG, an extension of the RAG framework using few-shot learning to jointly train a KP-oriented retriever and a KP summary generator. Experimental results demonstrate that QQSUM-RAG achieves superior performance in both textual quality and quantification accuracy, with up to 2.11 times improvement in textual similarity with ground-truth KPs and up to 67.12% improvement in quantification performance over a state-of-the-art KPA system. The principal implication for AI practitioners is a new approach to PQA that captures the diversity of customer opinions using KP-based summarization, which improves both the textual quality and the quantification accuracy of the responses.  |
