

## Papers for 2025-06-20

| Title | Authors | Summary |
|-------|---------|---------|
| Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.14965) or [HuggingFace](https://huggingface.co/papers/2506.14965))| Shibo Hao, Zhoujun Cheng, fengyao1909, koalazf99, tianyang | i) This paper introduces GURU, a reinforcement learning (RL) corpus for improving large language model (LLM) reasoning across six domains. ii) The research investigates the domain-specificity of RL mechanisms in LLM reasoning, particularly whether RL primarily elicits existing knowledge or facilitates genuine skill acquisition. iii) The methodology involves curating a 92K-example RL corpus (GURU) across Math, Code, Science, Logic, Simulation, and Tabular domains and performing RL fine-tuning on Qwen2.5-7B and 32B base models. iv) Results indicate that while pretrained-heavy domains benefit from cross-domain RL, others require in-domain training; GURU-7B/32B models achieve state-of-the-art open model performance with 7.9% and 6.7% improvements, respectively, on a 17-task evaluation suite. v) This work implies that AI practitioners need to consider domain-specific training data for effective RL fine-tuning to improve reasoning capabilities, as multi-domain RL can significantly enhance general reasoning.  |
| EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech
  Emotion Detection (Read more on [arXiv](https://arxiv.org/abs/2506.09827) or [HuggingFace](https://huggingface.co/papers/2506.09827))| Maurice Kraus, Gollam Rabby, Robert Kaczmarczyk, felfri, ChristophSchuhmann | i) The paper introduces EMONET-VOICE, a new speech emotion detection (SER) resource with a fine-grained taxonomy and expert validation. ii) The main objective is to provide robust benchmarks for evaluating the emotional understanding capabilities of AI systems in speech. iii) The methodology involves curating a large-scale synthetic speech corpus (EMONET-VOICE BIG) and creating a benchmark dataset (EMONET-VOICE BENCH) with expert annotations of 40 emotion categories at different intensity levels, followed by the development of EMPATHICINSIGHT-VOICE models. iv) EMPATHICINSIGHT-VOICE LARGE achieved the highest Pearson correlation of 0.421 and lowest RMSE of 3.756 when evaluated against expert human judgments. v) The principal implication for AI practitioners is the demonstration of systematic performance patterns: such as high-arousal emotions being more detectable than low-arousal states, providing valuable insights into the capabilities and limitations of current SER models that can aid in developing more nuanced and context-aware AI applications.  |
| SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning (Read more on [arXiv](https://arxiv.org/abs/2506.15154) or [HuggingFace](https://huggingface.co/papers/2506.15154))| Dorien Herremans, Abhinaba Roy, Anuradha Chopra | i) SonicVerse is a multi-task learning model for generating detailed music captions by integrating auxiliary music feature detection. ii) The main objective is to create a music captioning system capable of generating captions that incorporate both technical and general musical feature descriptions. iii) The methodology uses a projection-based architecture that transforms audio input into language tokens while simultaneously detecting music features through dedicated auxiliary heads and a Mistral-7B large language model for caption generation. iv) Experimental results demonstrate that incorporating music feature extractors within the token projection model leads to improvements in caption quality, with a BLEU score of 0.3484 achieved on the MusicBench dataset using the SonicVerse model compared to a baseline of 0.3456. v) The principal implication for AI practitioners is the demonstration of a multi-task learning framework for music captioning that leverages auxiliary supervision to improve performance on smaller, open-source datasets, integrating feature prediction directly into the captioning pipeline, thus eliminating the need for external music feature extractors.  |
| Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction (Read more on [arXiv](https://arxiv.org/abs/2506.14837) or [HuggingFace](https://huggingface.co/papers/2506.14837))| Weiran Huang, Lichao Sun, Yuyang Wang, Chengzhi Xu, WaltonFuture | i) The paper introduces ChartIR, a training-free iterative refinement method for improved chart-to-code generation. ii) The research aims to enhance MLLMs' ability to accurately translate visual charts into executable code. iii) The methodology involves structured instructions for visual understanding (description and difference) and an iterative refinement process for code generation. iv) Experiments on Plot2Code and ChartMimic datasets using Qwen2-VL and GPT-40 showed that ChartIR achieves superior performance, improving GPT-40 Score by 17% over direct generation on the Plot2Code dataset. v) ChartIR provides AI practitioners with a robust, model-agnostic framework for enhancing chart-to-code generation in MLLMs, improving visual and structural fidelity without task-specific training.  |
