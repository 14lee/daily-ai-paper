

## Papers for 2025-06-09

| Title | Authors | Summary |
|-------|---------|---------|
| Will It Still Be True Tomorrow? Multilingual Evergreen Question
  Classification to Improve Trustworthy QA (Read more on [arXiv](https://arxiv.org/abs/2505.21115) or [HuggingFace](https://huggingface.co/papers/2505.21115))| VityaVitalich, nakrayko, VirVen, zlatamaria, memyprokotow | i) This paper introduces EverGreenQA, a multilingual dataset for evergreen question classification to improve trustworthy question answering. ii) The primary objective is to assess whether large language models (LLMs) encode question temporality, either explicitly or implicitly, and to improve self-knowledge estimation in QA systems. iii) The methodology involves constructing a new multilingual QA dataset, EverGreenQA, benchmarking 12 LLMs, and training EG-E5, a lightweight multilingual classifier for identifying evergreen questions. iv) EG-E5 achieves SoTA performance on evergreen question classification, reaching a weighted F1 score of 0.906, and improves self-knowledge estimation in 16 out of 18 settings. v) The results demonstrate that incorporating evergreen question classification improves self-knowledge estimation, dataset curation, and explainability for GPT-40's retrieval behavior, which informs AI practitioners of the importance of considering question temporality in QA system design and evaluation.  |
| FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal
  Contextual Fusion (Read more on [arXiv](https://arxiv.org/abs/2506.01111) or [HuggingFace](https://huggingface.co/papers/2506.01111))| Owen Lee, Liyan Zhao, Zheshu Chen, Shunian Chen, SatsukiVie | The paper introduces FusionAudio-1.2M, a dataset and pipeline for fine-grained audio captioning using multimodal context. The research aims to improve caption detail and contextual accuracy by leveraging specialized pretrained models for extracting diverse contextual cues and a large language model (LLM) for synthesis. The methodology involves a two-stage automated pipeline with specialized models for speech, music, general sounds, and visual information extraction. FusionAudio-1.2M comprises 1.2 million detailed captions and 6 million QA pairs. Fine-tuning a CLAP-based audio encoder with FusionAudio shows enhanced audio-text alignment, indicating the dataset's potential for improving audio understanding and contextual caption generation; this is impactful for engineers needing higher-quality audio-text datasets and models.  |
| Is Extending Modality The Right Path Towards Omni-Modality? (Read more on [arXiv](https://arxiv.org/abs/2506.01872) or [HuggingFace](https://huggingface.co/papers/2506.01872))| Yu Su, Muhao Chen, Kai Zhang, DarthZhu | i) This paper analyzes the impact of extending modality on Large Language Models (LLMs), evaluating its effect on core language abilities and exploring techniques for omni-modality. ii) The research questions the trade-offs between extending modality in LLMs and preserving core language abilities, investigating whether model merging and omni-modality fine-tuning can effectively achieve true omni-modality. iii) The study involves fine-tuning LLMs with different modalities (image, video, audio), employing model merging techniques (average and weighted average), and evaluating performance across a range of textual and multimodal tasks. iv) Results indicate a performance decline in instruction following across all modality-extended models compared to the original base model, suggesting a trade-off; weighted model merging, however, achieves the best performance across both textual and multimodal tasks. v) AI practitioners should carefully consider the potential degradation of core language abilities when extending LLMs with new modalities and explore weighted average model merging as a promising strategy to maintain multimodal capabilities, although it still falls short of modality-specific models.  |
| Audio-Aware Large Language Models as Judges for Speaking Styles (Read more on [arXiv](https://arxiv.org/abs/2506.05984) or [HuggingFace](https://huggingface.co/papers/2506.05984))| Linjie Li, Kevin Lin, Chung-Ching Lin, xiaofei-wang, dcml0714 | i) The paper investigates the use of audio-aware large language models (ALLMs) as automatic judges for evaluating the speaking styles of spoken language models (SLMs). ii) The research objective is to assess whether ALLMs can effectively evaluate the style adherence and realism of speeches generated by SLMs in voice style instruction following and role-playing tasks. iii) The methodology involves using GPT-4o-audio and Gemini-2.5-Pro as ALLM judges to evaluate speech generated by GPT-4o-audio, GPT-4o-mini-audio, Step-Audio, and Qwen-2.5-Omni on two tasks: voice style instruction following and role-playing. iv) The primary result shows that Gemini-human judge agreement in evaluating speaking styles can be comparable to human-human agreement, with a Pearson's r of 0.640 between Gemini and human evaluators. v) The principal implication for AI practitioners is that ALLMs, specifically Gemini-2.5-Pro, can serve as a viable automatic evaluation metric for assessing the speaking style quality of SLMs, reducing the reliance on costly and variable human evaluations.  |
| Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05629) or [HuggingFace](https://huggingface.co/papers/2506.05629))| sambaran, abhi1nandy2, ananthmuppidi | i) This paper introduces Input Dependent Soft Prompting with a self-Attention Mechanism (ID-SPAM) for parameter-efficient fine-tuning of LLMs. ii) The research aims to improve LLM performance on domain-specific tasks while minimizing the number of trainable parameters through input-dependent soft prompts. iii) The methodology involves generating soft prompts based on input tokens and attending to these tokens with varying importance using a self-attention mechanism, prepended to a transformer layer. iv) Experimental results on the GLUE benchmark show that ID-SPAM outperforms parameter-efficient soft prompt baselines on 4 out of 6 tasks, achieving an average performance improvement and demonstrates improved zero-shot domain transfer capability. v) ID-SPAM offers AI practitioners a method for efficiently adapting pre-trained LLMs to downstream tasks with reduced computational cost and improved generalization, particularly in scenarios with limited data or domain shift, and ID-SPAM performs better than LoRA in 5/6 tasks when using ROBERTa-BASE.  |
| STARFlow: Scaling Latent Normalizing Flows for High-resolution Image
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.06276) or [HuggingFace](https://huggingface.co/papers/2506.06276))| Yuyang Wang, Huangjie Zheng, David Berthelot, Tianrong Chen, Jiatao Gu | STARFlow is presented as a scalable generative model using normalizing flows for high-resolution image synthesis. The research aims to develop a more scalable normalizing flow model for image synthesis that can compete with diffusion models. It introduces Transformer Autoregressive Flow (TARFlow) blocks and a deep-shallow architecture trained in the latent space of pretrained autoencoders and presents a guidance algorithm. The model achieves competitive sample quality in both class- and text-conditional image generation, with an FID of 2.40 on ImageNet-256. STARFlow demonstrates that normalizing flows can achieve competitive results at scale for image generation. It enables AI practitioners to utilize normalizing flows for high-resolution image synthesis tasks, providing an alternative to diffusion models.  |
| PartCrafter: Structured 3D Mesh Generation via Compositional Latent
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.05573) or [HuggingFace](https://huggingface.co/papers/2506.05573))| Yiqiang Feng, Honglei Yan, Panwang Pan, Yuchen Lin, chenguolin | PartCrafter presents a structured 3D generative model for compositional mesh generation from single RGB images. The research aims to generate semantically meaningful, geometrically distinct 3D meshes without requiring segmented image inputs. The methodology involves a compositional latent diffusion transformer (DiT) architecture, incorporating a compositional latent space and a hierarchical attention mechanism. Experiments demonstrate that PartCrafter outperforms existing methods in generating decomposable 3D meshes, achieving higher generation quality and efficiency, with a reduction in run time from 80s to 34s in some experiments. PartCrafter provides AI practitioners with a part-aware generative prior for improved 3D understanding and synthesis, enabling more effective 3D content creation pipelines.  |
| MORSE-500: A Programmatically Controllable Video Benchmark to
  Stress-Test Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05523) or [HuggingFace](https://huggingface.co/papers/2506.05523))| Hyunwoo Jae, Ankit Nakhawa, Anirudh Satheesh, Andrew Wang, Zikui | i) MORSE-500 is introduced as a new video benchmark for multimodal reasoning. ii) The research aims to address the limitations of current multimodal benchmarks by evaluating diverse reasoning skills within temporal contexts. iii) The benchmark utilizes programmatically generated videos and curated real footage across six reasoning categories: mathematical, abstract, spatial, temporal, physical, and planning. iv) Initial experiments reveal performance gaps in state-of-the-art VLMs, such as OpenAI-03 and Gemini 2.5 Pro, particularly in abstract reasoning and planning tasks, with overall model accuracy averaging below 25% compared to 55.4% for human performance. v) The controllable generation pipeline allows for stress-testing next-generation models by creating arbitrarily challenging new instances, offering a forward-looking evaluation tool for AI practitioners.  |
| Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence
  with Egocentric-Exocentric Vision (Read more on [arXiv](https://arxiv.org/abs/2506.06253) or [HuggingFace](https://huggingface.co/papers/2506.06253))| Baoqi Pei, Lidong Lu, Yifei Huang, Yuping He, cg1177 | This paper surveys research on video understanding using both egocentric (first-person) and exocentric (third-person) views. The main objective is to provide a comprehensive review of approaches that integrate these perspectives for enhanced video analysis. The methodology involves categorizing and reviewing recent advancements into three research directions: leveraging egocentric data to enhance exocentric understanding, utilizing exocentric data to improve egocentric analysis, and joint learning frameworks. The survey analyzes several tasks and datasets and finds a growing body of work exploring cross-view learning, with the number of citations to egocentric-exocentric related papers increasing from 14 in 2015 to 1642 in 2024. AI practitioners can leverage insights from this survey to develop advanced video understanding systems that combine complementary information from egocentric and exocentric views to improve performance.  |
| 3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World
  Model (Read more on [arXiv](https://arxiv.org/abs/2506.06199) or [HuggingFace](https://huggingface.co/papers/2506.06199))| Quanxi Wu, Yubo Dong, Siyuan Zhou, Peihao Chen, Hoyard | 3DFlowAction presents a novel approach to robot manipulation learning using 3D optical flow as a unified action representation. The research investigates learning a cross-embodiment manipulation policy transferable across different robotic systems without hardware-specific training. The method uses a 3D flow world model trained on a new dataset, ManiFlow-110k, to predict object motion, combined with a flow-guided rendering mechanism and GPT-4o for closed-loop planning. Experiments demonstrated a task success rate of 70.0% across different manipulation tasks, indicating strong generalization capabilities. The work provides AI practitioners with a data-efficient method for developing robot manipulation policies that can adapt to new robots and environments without extensive retraining.  |
| Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward (Read more on [arXiv](https://arxiv.org/abs/2506.05433) or [HuggingFace](https://huggingface.co/papers/2506.05433))| Junxian Cai, Longteng Guo, Yepeng Tang, Tongtian Yue, Zikang Liu | i) The paper introduces Prefix Grouper, an algorithm to improve the efficiency of Group Relative Policy Optimization (GRPO) by eliminating redundant prefix computation. ii) The research aims to reduce the computational overhead associated with encoding long shared prefixes in GRPO training. iii) The methodology involves restructuring self-attention to encode shared prefixes only once via a Shared-Prefix Forward strategy while maintaining differentiability. iv) The experiments show Prefix Grouper achieves equivalent performance to standard GRPO while reducing computational cost, particularly in long-prefix scenarios; theoretical computation analysis show Prefix Grouper reduces FLOPs to 1/G in long-prefix situations. v) Prefix Grouper allows AI practitioners to scale GRPO to larger group sizes and more complex tasks within the same computational budget by reducing redundant computations.  |
| CodeContests+: High-Quality Test Case Generation for Competitive
  Programming (Read more on [arXiv](https://arxiv.org/abs/2506.05817) or [HuggingFace](https://huggingface.co/papers/2506.05817))| Kai Shen, Hongyan Li, Yang Sun, Siyao Liu, zhwang01 | This paper introduces CodeContests+, an improved dataset for competitive programming via high-quality test case generation. The research aims to address the limitations of existing datasets by generating comprehensive and correct test cases for evaluating LLM reasoning. The methodology involves an LLM-based Generator-Validator (G-V) agent system for test case construction and validation, ensuring constraint satisfaction. Evaluation using 1.72 million submissions showed CodeContests+ achieves significantly higher evaluation accuracy, with nearly twice as many problems meeting TPR&TNR >= 0.9 compared to CodeContests. The implication is that CodeContests+ provides a higher-quality benchmark dataset that is advantageous for training reasoning models via reinforcement learning.  |
| Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot
  Data (Read more on [arXiv](https://arxiv.org/abs/2506.04120) or [HuggingFace](https://huggingface.co/papers/2506.04120))| Zhibin Li, Tom Erez, Steven Bohez, Mauro Comi, Ben Moran | SplatMesh: end-to-end real-to-sim framework for creating physical scenes from imperfect robot data. The research aims to create accurate physical simulations directly from real-world robot motion despite data imperfections. The methodology involves a hybrid scene representation combining 3D Gaussian Splatting with explicit object meshes suitable for MuJoCo physics simulation and an end-to-end optimization pipeline using differentiable rendering and physics. The framework achieves high-fidelity object mesh reconstruction, generates photorealistic novel views, and performs annotation-free robot pose calibration; the full framework obtains Chamfer Distance of 0.073 mm² for object reconstruction on the Simulated YCB dataset. The developed real-to-sim pipeline offers AI practitioners a practical approach for creating robust and scalable robotic simulations from real-world data, specifically from low cost hardware, enabling more effective robot learning and planning.  |
| HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource
  Utilization (Read more on [arXiv](https://arxiv.org/abs/2506.04255) or [HuggingFace](https://huggingface.co/papers/2506.04255))| Harshil Patel, helloparthshah, guineapig | HASHIRU is a novel MAS framework for enhanced flexibility, resource efficiency, and adaptability in AI systems. This paper addresses how to improve resource utilization and adaptability in multi-agent systems by incorporating hierarchical control, hybrid intelligence, and autonomous tool creation. The framework uses a hierarchical structure with a "CEO" agent dynamically managing specialized "employee" agents based on task needs and resource constraints, prioritizing smaller, local LLMs while integrating external APIs and larger models when justified. Evaluations on tasks like academic paper review, safety assessments, and complex reasoning demonstrate HASHIRU's capabilities, with HASHIRU outperforming Gemini 2.0 Flash on GSM8K (96% vs. 61%). The principal implication for AI practitioners is a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension.  |
| Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.04755) or [HuggingFace](https://huggingface.co/papers/2506.04755))| Chong Peng, Hao Yang, Lei Wang, Kaiyuan Deng, Shenshen Li | i) This paper introduces Reasoning Activation Potential (RAP), a novel data selection paradigm for efficient multi-modal reasoning in MLLMs. ii) The research addresses the question of whether smaller, high-value datasets can match or outperform full corpora for multi-modal reasoning in MLLMs, aiming to reduce data redundancy and computational costs. iii) The methodology involves a Causal Discrepancy Estimator (CDE) and an Attention Confidence Estimator (ACE) to identify cognitive samples and a Difficulty-aware Replacement Module (DRM) to ensure data complexity. iv) Experiments demonstrate superior performance using only 9.3% of the training data, reducing computational costs by over 43%. v) The principal implication for AI practitioners is the potential to significantly reduce training data requirements and computational costs for MLLMs by focusing on high-value cognitive samples identified through RAP, thereby enabling more efficient development and deployment of multi-modal reasoning systems.  |
| GuideX: Guided Synthetic Data Generation for Zero-Shot Information
  Extraction (Read more on [arXiv](https://arxiv.org/abs/2506.00649) or [HuggingFace](https://huggingface.co/papers/2506.00649))| Eneko Agirre, Iker García-Ferrero, OSainz, neildlf | GUIDEX introduces a novel method for generating synthetic data to improve zero-shot information extraction (IE). The paper addresses the challenge of domain-specific adaptation in IE systems, which typically requires expert schema design and data annotation. It aims to improve out-of-domain generalization by automatically defining schemas, inferring guidelines, and generating synthetically labeled instances. The method uses a large language model (LLM) to identify key information, structure it into a JSON format, and generate annotation schemas and guidelines. Fine-tuning Llama 3.1 with GUIDEX achieves state-of-the-art results across seven zero-shot Named Entity Recognition (NER) benchmarks, with gains up to 7 F1 points over previous methods without human-labeled data. The principal implication for AI practitioners is a low-noise strategy for robust zero-shot IE across diverse domains, reducing the need for manual annotation and schema creation, but some areas, such as Miscellaneous labels, need improvement.  |
