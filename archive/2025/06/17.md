

## Papers for 2025-06-17

| Title | Authors | Summary |
|-------|---------|---------|
| MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention (Read more on [arXiv](https://arxiv.org/abs/2506.13585) or [HuggingFace](https://huggingface.co/papers/2506.13585))| ManTle, windlx, LINMUJIE-judy, enochzhang, sheep33333 | i) MiniMax-M1 introduces a hybrid Mixture-of-Experts model with lightning attention for efficient, large-scale reasoning. ii) The objective is to scale test-time compute efficiently in large reasoning models, maintaining or improving performance on complex tasks. iii) The methodology involves combining a hybrid MoE architecture with lightning attention, continual pretraining, supervised fine-tuning, and reinforcement learning with a novel CISPO algorithm. iv) MiniMax-M1 achieves comparable or superior performance to models like DeepSeek-R1 and Qwen3-235B on complex tasks, while consuming 25% of the FLOPs of DeepSeek R1 at a 100K token generation length. v) MiniMax-M1 provides AI practitioners with an open-weight model designed to efficiently scale test-time compute for long-context reasoning, particularly beneficial for developing next-generation language model agents.  |
| Scientists' First Exam: Probing Cognitive Abilities of MLLM via
  Perception, Understanding, and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.10521) or [HuggingFace](https://huggingface.co/papers/2506.10521))| Ruoyao Xiao, Xuming He, Yiheng Wang, Yuhao Zhou, WilsonHwang | i) This paper introduces Scientists' First Exam (SFE), a benchmark for evaluating scientific cognitive abilities of Multimodal Large Language Models (MLLMs). ii) The research aims to address the limited assessment of perception and reasoning abilities in existing scientific benchmarks for MLLMs. iii) SFE utilizes 830 expert-verified VQA pairs across three cognitive levels: scientific signal perception, scientific attribute understanding, and scientific comparative reasoning, spanning 66 multimodal tasks across five disciplines. iv) Experiments show state-of-the-art models GPT-03 and InternVL-3 achieve 34.08% and 26.52% accuracy on SFE, respectively. v) The SFE benchmark identifies a significant performance gap, indicating potential for AI practitioners to improve MLLMs' capabilities in scientific data analysis, particularly regarding perception and reasoning, necessitating developments tailored for real-world scientific workflows.  |
| DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents (Read more on [arXiv](https://arxiv.org/abs/2506.11763) or [HuggingFace](https://huggingface.co/papers/2506.11763))| Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ, Ayanami0730 | i) This paper introduces DeepResearch Bench, a new benchmark for evaluating deep research agents (DRAs). ii) The main objective is to provide a comprehensive and standardized evaluation methodology for assessing the capabilities of LLM-based DRAs, addressing the current lack of such benchmarks. iii) The methodology includes two novel frameworks: RACE, a reference-based method with adaptive criteria for evaluating report quality, and FACT, a framework for assessing information retrieval and citation accuracy. iv) The evaluation of several early-released DRAs, including Gemini-2.5-Pro Deep Research, showed that Gemini-2.5-Pro Deep Research achieved an average of 111.21 effective citations in its final reports, significantly outperforming other models in comprehensiveness. v) The benchmark and evaluation frameworks offer AI practitioners a tool for systematic development and assessment of LLM-based agents designed for complex research tasks, enabling comparative analysis of DRA performance.  |
| DoTA-RAG: Dynamic of Thought Aggregation RAG (Read more on [arXiv](https://arxiv.org/abs/2506.12571) or [HuggingFace](https://huggingface.co/papers/2506.12571))| Peerawat Rojratchadakorn, Natthapath Rungseesiripak, natnitaract, montholscbx, saksornr | i) The paper introduces DoTA-RAG, a Retrieval-Augmented Generation (RAG) system optimized for large-scale web knowledge indexes. ii) The research aims to address the challenges of high latency and limited accuracy in traditional RAG pipelines when applied to massive, diverse datasets. iii) The methodology involves a three-stage pipeline consisting of query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking, using Falcon3-10B-Instruct as the base LLM. iv) DoTA-RAG improved the answer correctness score from 0.752 to 1.478 on an internal test while maintaining low latency, and achieved a correctness score of 0.929 on the Live Challenge Day. v) The implementation of DoTA-RAG offers AI practitioners a fast, reliable, and scalable RAG system for domains requiring access to large and evolving knowledge sources, with dynamic routing significantly enhancing retrieval efficiency, reducing latency by more than half compared to static top-k search.  |
| Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.13654) or [HuggingFace](https://huggingface.co/papers/2506.13654))| Yuhao Dong, Penghao Wu, Hongming Guo, ruiqiw, shulin16 | Ego-R1 introduces a framework for reasoning over ultra-long egocentric videos by employing a Chain-of-Tool-Thought (CoTT) process orchestrated by a reinforcement learning (RL) agent. The paper addresses the challenge of long-horizon reasoning in egocentric videos spanning days or weeks. A structured CoTT process with dynamic tool invocation (Hierarchical RAG, Video LLM, and VLM) is designed to decompose reasoning into modular steps. The agent is trained using supervised fine-tuning (SFT) on the Ego-CoTT-25K dataset and RL on the Ego-QA-4.4K dataset. Evaluated on the Ego-R1 Bench, the agent achieves 46.0% accuracy, demonstrating effective handling of week-long video understanding. This tool-augmented reasoning paradigm can effectively tackle ultra-long egocentric videos for problems requiring temporal awareness and precise analysis, potentially expanding the time coverage from hours to a week.  |
| Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves
  Reasoning Efficiency (Read more on [arXiv](https://arxiv.org/abs/2506.08343) or [HuggingFace](https://huggingface.co/papers/2506.08343))| Ranjay Krishna, Zhaoyang Chu, Dongping Chen, Yuanning Feng, Chenlong Wang | i) This paper introduces NOWAIT, a training-free inference-time method to improve the reasoning efficiency of large language models (LRMs). ii) The research investigates whether explicit self-reflection, signaled by tokens like "Wait" and "Hmm," is necessary for advanced reasoning in LRMs. iii) NOWAIT suppresses the generation of specific keyword tokens associated with self-reflection by adjusting their logits during inference. iv) Experiments on ten benchmarks show NOWAIT reduces chain-of-thought trajectory length by up to 27%-51% in R1-style model series across textual, visual, and video reasoning tasks. v) NOWAIT provides AI practitioners with a plug-and-play solution to reduce computational overhead and latency in multimodal reasoning applications without compromising model utility.  |
| Discrete Diffusion in Large Language and Multimodal Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2506.13759) or [HuggingFace](https://huggingface.co/papers/2506.13759))| Xinchao Wang, Qi Li, Runpeng Yu | i) This survey provides a systematic overview of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). ii) The paper aims to formalize the underlying mathematical frameworks, categorize representative models, analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. iii) The methodology involves tracing the historical development of dLLMs and dMLLMs, categorizing representative models and analyzing key techniques for training and inference. iv) dLLMs and dMLLMs achieve up to 10x acceleration in inference speed compared to autoregressive models. v) Industrial-scale proprietary d(M)LLMs as well as open-source academic d(M)LLMs have demonstrated performance comparable to their autoregressive counterparts, positioning discrete diffusion models as a promising alternative to intelligence based on traditional autoregressive approaches for AI practitioners seeking efficiency gains.  |
| TaskCraft: Automated Generation of Agentic Tasks (Read more on [arXiv](https://arxiv.org/abs/2506.10055) or [HuggingFace](https://huggingface.co/papers/2506.10055))| Weizhen Li, Weichen Sun, Qianben Chen, Jingyi Cao, Dingfeng Shi | TaskCraft introduces an automated workflow for generating agentic tasks involving multi-step problem solving, tool use, and adaptive reasoning. The paper addresses the scalability limitations of existing agentic benchmarks by automating the generation of difficulty-scalable tasks with verifiable execution trajectories. The methodology uses depth-based and width-based extensions of atomic tasks, combined with rejection sampling and linguistic analysis for verification. Empirical results demonstrate improved prompt optimization and enhanced supervised fine-tuning of agentic foundation models; specifically, SFT achieves average performance improvements of +14.0% (Qwen2.5-3B-Base). TaskCraft provides AI practitioners with a synthetic dataset of approximately 36,000 tasks to support agent tuning and evaluation.  |
| VGR: Visual Grounded Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.11991) or [HuggingFace](https://huggingface.co/papers/2506.11991))| Haiyong Jiang, Haochen Wang, Zijiang Kang, bongbohong, stormthunder | VGR introduces a visual grounded reasoning framework for multimodal large language models (MLLMs). The research aims to enhance MLLMs' visual reasoning capabilities by enabling selective attention to relevant image regions during inference. VGR employs a self-driven selective visual replay method, retrieving visual tokens from a feature pool based on a replay signal generated by the model. Experiments on LLaVA-NeXT-7B show VGR achieves +4.1 on MMStar, +7.1 on AI2D, and +12.9 on ChartQA compared to the baseline with only 30% of image token usage. The principal implication is that targeted visual analysis and selective replay can significantly improve MLLM performance and efficiency on tasks requiring detailed image understanding.  |
| PersonaFeedback: A Large-scale Human-annotated Benchmark For
  Personalization (Read more on [arXiv](https://arxiv.org/abs/2506.12915) or [HuggingFace](https://huggingface.co/papers/2506.12915))| Yuchen Eleanor Jiang, Tiannan Wang, Dongyi Ding, Chenghao Zhu, Meiling Tao | PersonaFeedback introduces a human-annotated benchmark for evaluating LLM personalization capabilities. The study investigates the ability of LLMs to provide personalized responses based on predefined user personas. The methodology involves creating 8298 human-annotated test cases categorized by difficulty and evaluating various LLMs. Empirical results show that while SOTA LLMs perform well on general tasks, their performance declines on the hard tier of PersonaFeedback, with top proprietary models exhibiting relatively low average accuracy. The research implies that explicitly providing user personas improves performance in personalized scenarios over relying solely on implicit persona inference for AI practitioners.  |
| From Real to Synthetic: Synthesizing Millions of Diversified and
  Complicated User Instructions with Attributed Grounding (Read more on [arXiv](https://arxiv.org/abs/2506.03968) or [HuggingFace](https://huggingface.co/papers/2506.03968))| Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ | i) This paper introduces a framework for synthesizing diverse and complex instruction data for aligning large language models (LLMs) using attributed grounding. ii) The main objective is to generate instruction data at scale that reflects real-world use cases and cognitive insights, overcoming the limitations of existing synthetic instruction generation methods. iii) The methodology involves a top-down attribution process, grounding real instructions to situated users, and a bottom-up synthesis process that leverages web documents to generate situations and meaningful instructions. iv) The study constructs a dataset of 1 million instructions, SYNTHQUESTIONS, and demonstrates that models trained on this dataset achieve leading performance on common benchmarks; one example is the models improved Alpaca Eval 2.0. win rate with their models trained with SYNTHQUESTIONS. v) The framework allows AI practitioners to generate pre-training-level instruction data with high complexity and diversity, improving the alignment and performance of LLMs.  |
| Test3R: Learning to Reconstruct 3D at Test Time (Read more on [arXiv](https://arxiv.org/abs/2506.13750) or [HuggingFace](https://huggingface.co/papers/2506.13750))| Xinchao Wang, Xingyi Yang, Shizun Wang, Yuheng Yuan, florinshum | i) The paper introduces Test3R, a test-time learning technique for enhancing 3D reconstruction by optimizing cross-pair consistency. ii) The research aims to improve the geometric accuracy of 3D reconstruction by addressing limitations inherent in pairwise prediction methods. iii) Test3R utilizes image triplets and a self-supervised objective to maximize the geometric consistency between reconstructions generated from different image pairs through prompt tuning at test time. iv) Experiments demonstrate that Test3R significantly outperforms state-of-the-art methods, achieving a 1.3 reduction in Absolute Relative Error and a 14.2 increase in Inlier Ratio on the DTU dataset for multi-view depth estimation compared to the vanilla DUSt3R v) AI practitioners can leverage Test3R as a universally applicable and cost-effective method to improve the accuracy and robustness of existing 3D reconstruction pipelines with minimal overhead.  |
| BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning
  with Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.07961) or [HuggingFace](https://huggingface.co/papers/2506.07961))| Xiangnan Wu, Xiao Ma, Hongtao Wu, Yixiang Chen, LPY | BridgeVLA aligns 3D manipulation learning with vision-language models via input-output alignment using 2D heatmaps. The research aims to develop a sample-efficient 3D vision-language-action model by leveraging 3D structural priors and vision-language models. The methodology involves projecting 3D point clouds into multiple 2D images and predicting 2D heatmaps for action prediction. BridgeVLA improves the average success rate in RLBench from 81.4% to 88.2%. BridgeVLA offers AI practitioners an efficient method for learning 3D robot manipulation by aligning inputs and outputs in a shared 2D space, resulting in better sample efficiency.  |
| Language Surgery in Multilingual Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.12450) or [HuggingFace](https://huggingface.co/papers/2506.12450))| Muhammad Ilham Ghozali, samuel-cahyawijaya, tackhwa, muhammadravi251001, joanitolopo | i) The paper investigates representation alignment in multilingual LLMs, proposing Inference-Time Language Control (ITLC) for cross-lingual tasks. ii) The research aims to understand and leverage the naturally emerging representation alignment in LLMs to enable precise language control and mitigate language confusion. iii) The methodology involves empirically confirming the existence of representation alignment, disentangling language-specific and language-agnostic information, and using latent injection for cross-lingual language control. iv) ITLC achieves strong cross-lingual control, retaining semantic integrity, and demonstrates ~30% performance retention compared to LLMs with explicitly designed alignment, while achieving almost >90% relative to other non-aligned layers v) ITLC presents a practical solution for AI practitioners to enhance cross-lingual performance in LLMs by leveraging latent injection for language-specific manipulation, improving consistency and mitigating language confusion.  |
| AI Agent Behavioral Science (Read more on [arXiv](https://arxiv.org/abs/2506.06366) or [HuggingFace](https://huggingface.co/papers/2506.06366))| Honglin Zhang, Haoye Chai, Yunke Zhang, Lin Chen, JJ-TMT | i) This paper introduces AI Agent Behavioral Science as a paradigm for systematically studying AI agent behavior within specific contexts. ii) The main objective is to shift the focus from internal model mechanisms to empirically observing and understanding AI agents' actions, adaptations, and social patterns. iii) The paper synthesizes existing research across individual, multi-agent, and human-agent interaction settings, drawing inspiration from human and animal behavioral research. iv) The study reveals LLM-powered agents exhibit human-like capabilities in cognitive reasoning, emotion recognition, and theory of mind, though often demonstrate limited rationality as well as remaining sensitive to task framing. v) The principal implication is for AI practitioners to consider behavioral properties like fairness, safety, and interpretability as dynamic, context-dependent attributes, informing the design and evaluation of responsible AI systems.  |
| ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm
  Engineering (Read more on [arXiv](https://arxiv.org/abs/2506.09050) or [HuggingFace](https://huggingface.co/papers/2506.09050))| Kensho Aoki, Yoichi Iwata, Kohki Horie, Yuki Imajuku, iwiwi | ALE-Bench is introduced as a benchmark for AI systems in long-horizon, score-based algorithmic programming contests, using tasks from the AtCoder Heuristic Contests. The research aims to evaluate AI's capabilities in algorithm engineering for hard optimization problems without known exact solutions. The methodology involves evaluating frontier LLMs in one-shot and iterative refinement settings, using a software framework that supports interactive agent architectures with test-run feedback and visualizations. Results indicate that current LLMs, while demonstrating high performance on specific problems, still exhibit a gap compared to humans in consistency across problems and long-horizon problem-solving; one LLM achieved a performance score of 2880 on AHC039, corresponding to 5th place in the original contest. This benchmark highlights the necessity for further AI advancements to foster consistent and sustained problem-solving, especially in algorithm design and iterative optimization.  |
| LETS Forecast: Learning Embedology for Time Series Forecasting (Read more on [arXiv](https://arxiv.org/abs/2506.06454) or [HuggingFace](https://huggingface.co/papers/2506.06454))| Yin Li, Nada Magdi Elkordi, Satya Sai Srinath Namburi GNVV, viswa-98, alphaomeaga | i) The paper introduces DeepEDM, a novel deep learning framework for time series forecasting that integrates empirical dynamic modeling with deep neural networks. ii) The research aims to develop a forecasting model that explicitly models the underlying dynamics of complex nonlinear time series while addressing limitations of traditional EDM and deep learning methods. iii) DeepEDM utilizes time-delayed embeddings, a learned latent space robust to noise, kernel regression via softmax attention, and a learned decoder for end-to-end training. iv) Experiments on synthetic data demonstrate that DeepEDM consistently outperforms baselines, achieving lower MSE in chaotic regimes and remaining robust to significant noise levels; on a chaotic Lorenz system at σ = 2.5 and H = 48, its MSE (17.267) is 40% lower than Koopa (28.804). v) AI practitioners can leverage DeepEDM to improve forecasting accuracy by integrating dynamical systems principles into deep learning models, particularly for time series exhibiting complex, nonlinear dynamics and sensitivity to noise.  |
| Supernova Event Dataset: Interpreting Large Language Model's Personality
  through Critical Event Analysis (Read more on [arXiv](https://arxiv.org/abs/2506.12189) or [HuggingFace](https://huggingface.co/papers/2506.12189))| Ioana Ciucă, pranavAL2109 | i) The paper introduces Supernova Event Dataset and uses critical event analysis to interpret LLM personalities. ii) The objective is to understand and benchmark the decision-making processes and underlying "personality" traits of LLMs when extracting key events from diverse texts. iii) The methodology involves using RAG and an LLM as a judge to analyze the top-ranked events extracted by target LLMs from articles covering biographies, historical/news events, and scientific discoveries. iv) The analysis reveals distinct personality traits: Orca 2 exhibited emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displayed a more strategic, analytical style; when evaluating scientific discoveries, Claude Sonnet 3.7 focused on conceptual framing. v) This work improves model interpretability by providing a method to understand and potentially align LLMs with desired human values.  |
| Forecasting Time Series with LLMs via Patch-Based Prompting and
  Decomposition (Read more on [arXiv](https://arxiv.org/abs/2506.12953) or [HuggingFace](https://huggingface.co/papers/2506.12953))| Anish Gupta, Sri Harsha Vardhan Prasad Jella, Anshul Vemulapalli, Mayank Bumb, Franck-Dernoncourt | i) This paper introduces PatchInstruct, a novel prompt-based framework to enable Large Language Models (LLMs) for time series forecasting without fine-tuning or complex architectures. ii) The main objective is to enhance LLM-based time series forecasting by addressing limitations in inference speed and generalization, maintaining predictive strength without extensive model retraining. iii) The methodology involves patch-based tokenization of time series data with structured natural language instructions to guide the LLM, comparing it against Zero-shot and Neighbor-based prompting on Weather and Traffic datasets. iv) PatchInstruct consistently outperforms baselines on small forecasting horizons, achieving top forecasting accuracy on horizons at most 12 steps, while reducing inference overhead by 10x-100x; for Weather dataset at horizon = 1, the Mean Squared Error (MSE) drops from 1.15 × 10^-2 to 2.6 × 10^-4. v) The principal implication for AI practitioners is that specialized prompting techniques such as PatchInstruct can effectively replace some architectural complexity, and improve the scalability and domain adaptability of LLM-based time series forecasting, enabling more efficient and accurate predictions with minimal preprocessing.  |
| MS4UI: A Dataset for Multi-modal Summarization of User Interface
  Instructional Videos (Read more on [arXiv](https://arxiv.org/abs/2506.12623) or [HuggingFace](https://huggingface.co/papers/2506.12623))| Jiuxiang Gu, Seunghyun Yoon, Hao Tan, Yuan Zang, Franck-Dernoncourt | i) The paper introduces MS4UI, a new dataset for multi-modal summarization of user interface (UI) instructional videos. ii) The main objective is to provide a benchmark for generating concise and executable step-by-step instructions for UI-related tasks. iii) The methodology involves collecting 2,413 UI instructional videos and annotating them for video segmentation, text summarization, and video summarization. iv) Experiments with existing multi-modal summarization methods on the MS4UI dataset revealed suboptimal performance, particularly in tasks requiring fine-grained understanding of UI elements and actions; baseline methods show unsatisfactory performance on the three core tasks of the proposed dataset. v) The creation of MS4UI and its associated evaluation tasks highlight the necessity of developing specialized methods that can effectively understand structured UI layouts and actions, crucial for AI practitioners developing UI-related educational resources or automated assistance tools.  |
| Profiling News Media for Factuality and Bias Using LLMs and the
  Fact-Checking Methodology of Human Experts (Read more on [arXiv](https://arxiv.org/abs/2506.12552) or [HuggingFace](https://huggingface.co/papers/2506.12552))| Preslav Nakov, Maha Tufail Agro, Dilshod Azizov, Zain Muhammad Mujahid | i) The paper introduces a methodology for profiling news media outlets for political bias and factuality using large language models (LLMs). ii) The main objective is to emulate the criteria used by professional fact-checkers to assess media bias and factuality. iii) The methodology involves crafting custom prompts for LLMs and aggregating their responses for classification, and it contrasts with zero-shot predictions. iv) The experiments demonstrated improved accuracy over baselines, with the best model achieving 80.6% accuracy and 0.206 MAE for factuality prediction, and 93.5% accuracy and 0.075 MAE for political bias prediction using expert guidelines on a 3 point scale. v) The principal implication is that LLMs, when guided by expert-driven prompts, can provide a systematic and more accurate assessment of news media outlets, which can be used for factuality of reporting and political bias. |
| SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification
  and LLM Assistance (Read more on [arXiv](https://arxiv.org/abs/2506.09968) or [HuggingFace](https://huggingface.co/papers/2506.09968))| Weiyang He, Haoyue Zheng, Ziyan Wang, Yuqing Sun, Owenngt | i) SRLAgent, an LLM-assisted system, enhances self-regulated learning (SRL) skills in college students through gamification. ii) The research investigates whether SRLAgent improves SRL skills compared to baseline systems and traditional learning resources. iii) A between-subjects study compared SRLAgent to a baseline system (SRL without Agent features) and multimedia learning, involving 59 college students. iv) Results showed significant SRL skill improvements in the SRLAgent group (p < .001, Cohen's d = 0.234), with higher engagement. v) Embedding SRL scaffolding and AI support within gamified environments can enhance metacognitive skill development, offering design implications for educational technologies.  |
| Incorporating Domain Knowledge into Materials Tokenization (Read more on [arXiv](https://arxiv.org/abs/2506.11115) or [HuggingFace](https://huggingface.co/papers/2506.11115))| SangKeun Lee, SungHo Kim, Junho Kim, Jun-Hyung Park, yerim0210 | i) The paper introduces MATTER, a domain-specific tokenization framework for materials science that integrates material knowledge. ii) The research aims to improve materials science language models by addressing the limitations of frequency-centric tokenization, which often fragments material concepts. iii) The methodology involves training a material concept identifier (MatDetector) on a curated materials knowledge base and re-ranking token merging to prioritize material-related subwords. iv) Experiments on generation tasks showed an average performance gain of 4% and classification tasks 2% relative to existing tokenization methods. v) MATTER provides AI practitioners with an enhanced tokenization method that better preserves the semantic integrity of material concepts, improving the performance of downstream materials science NLP tasks.  |
| Steering LLM Thinking with Budget Guidance (Read more on [arXiv](https://arxiv.org/abs/2506.13752) or [HuggingFace](https://huggingface.co/papers/2506.13752))| Chuang Gan, Yang Zhang, Wenshuo Zhao, Junyan Li | i) The paper introduces Budget Guidance, a method for controlling the reasoning length of Large Language Models (LLMs) without fine-tuning. ii) The research aims to control the reasoning length of LLMs at inference time without sacrificing performance, especially under tight thinking budgets. iii) The methodology involves training a lightweight predictor to model a Gamma distribution over the remaining thinking length at each token, using it to guide LLM generation. iv) Experiments on MATH-500 benchmark show up to a 26% accuracy gain under tight budgets compared to baseline methods, while maintaining accuracy with only 63% of the tokens used by the full-thinking model. v) Budget Guidance offers AI practitioners a way to improve the token efficiency of LLMs on challenging math benchmarks and generalizes to other task domains like GPQA, FOLIO, LiveCodeBench.  |
| Uncertainty-Aware Remaining Lifespan Prediction from Images (Read more on [arXiv](https://arxiv.org/abs/2506.13430) or [HuggingFace](https://huggingface.co/papers/2506.13430))| Barbara Hammer, Philip Kenneweg, TristanKe | i) The paper introduces a method for estimating remaining lifespan from facial and whole-body images, with uncertainty quantification. ii) The research objective is to accurately predict remaining lifespan from images while providing calibrated uncertainty estimates. iii) The methodology leverages pretrained vision transformer foundation models (DINOv2) and a regression head modeling prediction uncertainty as a Gaussian distribution, trained with the Gaussian negative log-likelihood loss. iv) The approach achieves a mean absolute error (MAE) of 7.48 years on an established dataset and improves to 4.79 and 5.07 years MAE on two new datasets; the bucketed expected calibration error is 0.62 years. v) AI practitioners can utilize the demonstrated uncertainty modeling to develop more robust and interpretable image-based prediction systems in healthcare and other domains where calibrated confidence estimates are crucial for decision-making.  |
| Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns (Read more on [arXiv](https://arxiv.org/abs/2506.13172) or [HuggingFace](https://huggingface.co/papers/2506.13172))| PChemGuy | i) This paper evaluates structured prompts for LLMs in analyzing scholarly manuscript summaries. ii) The research question is whether LLMs can identify unsubstantiated claims and ambiguous pronouns in academic abstracts and conclusions through structured prompting. iii) The methodology involves designing proof-of-concept (PoC) structured prompts and evaluating them on Gemini Pro 2.5 Pro and ChatGPT Plus 03 under varied context conditions. iv) Results indicate that ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier, while Gemini correctly flagged it (95% success); in linguistic clarity analysis, ChatGPT achieved 100% success with limited context, whereas Gemini's performance degraded. v) The principal implication for AI practitioners is that prompt performance is highly dependent on the interplay between the model, task type, and context, emphasizing the need for rigorous, model-specific testing when deploying structured prompting for complex textual analysis.  |
| QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety (Read more on [arXiv](https://arxiv.org/abs/2506.12299) or [HuggingFace](https://huggingface.co/papers/2506.12299))| Yunho Maeng, Soo Yong Kim, Hyoungseo Cho, Jeonghwa Yoo, Taegyeong Lee | QGuard is a zero-shot safety guard method that utilizes question prompting for multi-modal LLMs to block harmful prompts. The research aims to develop a simple, effective method for detecting both text-based and multi-modal harmful prompts without fine-tuning. The methodology involves categorizing harmful prompts into groups and generating guard questions that are combined with user input and fed to an MLLM to extract logits. Experimental results show the QGuard achieves an F1 score of 0.7438 on text-based harmful prompt detection and an F1 score of 0.8080 on multi-modal harmful prompt detection using InternVL-2.5. QGuard offers AI practitioners a practical safety guard applicable in real-world LLM services, adaptable to emerging threats with minimal computational resources.  |
| EgoPrivacy: What Your First-Person Camera Says About You? (Read more on [arXiv](https://arxiv.org/abs/2506.12258) or [HuggingFace](https://huggingface.co/papers/2506.12258))| Xiaojun Shan, Yi Li, Jiacheng Cheng, Genpei Zhang, Yijiang Li | i) The paper introduces EgoPrivacy, a benchmark for evaluating privacy risks inherent in egocentric video. ii) The research question investigates how much privacy information about a camera wearer can be inferred from their first-person view videos. iii) The methodology involves defining three types of privacy (demographic, individual, and situational), creating seven tasks, and developing a Retrieval-Augmented Attack (RAA) that utilizes ego-to-exo video retrieval. iv) Experiments show that foundation models can compromise wearer privacy, achieving 70-80% accuracy in recovering attributes such as identity, scene, gender, and race in zero-shot settings. v) AI practitioners should be aware that even zero-shot foundation models can compromise privacy in egocentric videos, necessitating robust privacy safeguards when utilizing or deploying such data or models.  |
| Hatevolution: What Static Benchmarks Don't Tell Us (Read more on [arXiv](https://arxiv.org/abs/2506.12148) or [HuggingFace](https://huggingface.co/papers/2506.12148))| Albert Meroño-Peñuela, Yulan He, Barbara McGillivray, Chiara Di Bonaventura | i) This paper examines the temporal robustness of language models on evolving hate speech detection tasks. ii) The research questions how static hate speech benchmarks correlate with evolving language in the hate speech domain. iii) The study uses two experiments involving time-sensitive shifts and vocabulary expansion with neologisms, evaluating 20 language models with time-sensitive macro F1 and counterfactual invariance. iv) Results indicate that models exhibit performance volatility over time; for example, 6 out of 20 models flip labels on counterfactual sentences containing neologisms more than 10% of the time, and static benchmark performance does not consistently translate to time-sensitive evaluations with negative correlations observed in certain cases. v) AI practitioners should be aware of the limitations of static hate speech benchmarks and consider time-sensitive evaluations to ensure reliable safety assessments of language models, as static evaluations may overestimate model safety due to language evolution.  |
