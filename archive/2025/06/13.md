

## Papers for 2025-06-13

| Title | Authors | Summary |
|-------|---------|---------|
| ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.09513) or [HuggingFace](https://huggingface.co/papers/2506.09513))| Weiwen Xu, Xingyu Qian, Swrooy, 26hzhang, YuSun-AI | i) The paper introduces ReasonMed, a 370K example medical reasoning dataset. ii) The primary objective is to advance knowledge-intensive medical question answering by providing a large, high-quality dataset. iii) The methodology involves a multi-agent system for generating and verifying reasoning paths, including an Error Refiner powered by GPT-4o. iv) Results show a ReasonMed-7B model achieving state-of-the-art performance among sub-10B models and exceeding LLaMA3.1-70B on PubMedQA by 4.60%. v) The implication for AI practitioners is a new benchmark dataset to train and evaluate medical reasoning models, demonstrating that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy.  |
| SWE-Factory: Your Automated Factory for Issue Resolution Training Data
  and Evaluation Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2506.10954) or [HuggingFace](https://huggingface.co/papers/2506.10954))| Pengyu Yang, Caihua Li, Yanlin Wang, Lianghong Guo, itaowe | i) SWE-Factory is an automated pipeline for constructing GitHub issue resolution datasets and benchmarks. ii) The main objective is to automate the construction of GitHub issue resolution benchmarks by reducing manual effort in evaluation environment setup, grading, and validation. iii) The methodology involves a multi-agent system (SWE-Builder) for environment construction, exit-code-based grading, and automated fail2pass validation. iv) Experiments show SWE-Builder, with GPT-4.1-mini, constructs 269 valid task instances out of 671, achieving a valid rate of 40.1% with an average cost of $0.045 per instance, and exit-code-based grading achieves 100% accuracy compared to manual inspection. v) The primary implication is that SWE-Factory provides AI practitioners with an automated tool for creating large-scale, high-quality datasets, facilitating the development and evaluation of LLMs for software engineering tasks.  |
| Text-Aware Image Restoration with Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.09993) or [HuggingFace](https://huggingface.co/papers/2506.09993))| Jihye Park, Jaeeun Lee, paulcho98, jinlovespho, Min-Jaewon | i) The paper introduces Text-Aware Image Restoration (TAIR), a novel task for simultaneously recovering visual content and textual fidelity using diffusion models. ii) The main research objective is to address the challenge of text-image hallucination in degraded images by improving the reconstruction of textual regions. iii) The methodology involves creating SA-Text, a 100K image dataset, and proposing TeReDiff, a diffusion framework that integrates internal diffusion features with a text-spotting module. iv) Experiments show TeReDiff achieves superior performance with a F1-score of 69.29% on HQ level using ABCNetv2 on the SA-Text dataset, outperforming existing state-of-the-art restoration methods in text recognition accuracy. v) The principal implication for AI practitioners is the provision of a benchmark dataset and an effective diffusion model architecture for restoring images containing degraded text, thereby enhancing applications requiring both visual and textual clarity.  |
| VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos (Read more on [arXiv](https://arxiv.org/abs/2506.10857) or [HuggingFace](https://huggingface.co/papers/2506.10857))| Meng Chu, Yue Wu, LarryLee, chupei, awojustin | VRBench is introduced as a new benchmark for evaluating multi-step reasoning in long narrative videos. The main objective is to address limitations in existing evaluations by incorporating temporal reasoning and procedural validity. The methodology involves curating 1,010 long videos, annotating 9,468 multi-step question-answering pairs with 30,292 reasoning steps, and using a multi-phase evaluation pipeline. Evaluations of 28 LLMs and VLMs showed that GPT-4o achieved 83.25% outcome accuracy but a lower 58.1% process rating. The principal implication is providing a new tool for assessing and improving the reasoning capabilities of vision language models in complex, narrative contexts.  |
| AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven
  Clip Generation (Read more on [arXiv](https://arxiv.org/abs/2506.10540) or [HuggingFace](https://huggingface.co/papers/2506.10540))| Baotian Hu, Longyue Wang, Xinyu Chen, YunxinLi, MrSunshy | i) AniMaker is a multi-agent framework for generating coherent, multi-character animated storytelling videos from text. ii) The research aims to automate the creation of storytelling animation using a multi-agent system. iii) The methodology employs a Monte Carlo Tree Search (MCTS)-inspired strategy (MCTS-Gen) for efficient clip generation and a novel evaluation framework (AniEval) for multi-shot animation assessment. iv) Experiments show AniMaker achieves superior performance with a 14.6% higher score in AniEval compared to the second-best model, with VBench results demonstrating the best average rank of 2.50. v) AniMaker offers AI practitioners an efficient framework for generating production-grade animated content, substantially improving the efficiency of multi-candidate generation.  |
| Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture
  without Training (Read more on [arXiv](https://arxiv.org/abs/2506.10952) or [HuggingFace](https://huggingface.co/papers/2506.10952))| Xipeng Qiu, Lu Wang, Howe77, mzzhang | Domain2Vec introduces a novel approach to vectorize datasets for optimizing data mixtures in language model pretraining. The research aims to identify the optimal data mixture for pretraining language models without extensive training. The methodology involves decomposing datasets into a linear combination of meta-domains using a classifier and applying the Distribution Alignment Assumption. Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset and improves downstream performance by 2.83% under equivalent compute budget. Domain2Vec provides AI practitioners a computationally efficient and scalable method for determining optimal data mixtures for language model pretraining, reducing the need for extensive experimentation.  |
| Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable
  Task Experts (Read more on [arXiv](https://arxiv.org/abs/2506.10357) or [HuggingFace](https://huggingface.co/papers/2506.10357))| Weili Guan, Gongwei Chen, Rui Shao, Yuquan Xie, Zaijing Li | Optimus-3 is presented as a generalist multimodal agent for Minecraft capable of perception, planning, action, grounding, and reflection. This work aims to develop a general-purpose agent in Minecraft that overcomes challenges such as insufficient domain-specific data, task interference, and visual diversity. The methodology incorporates a knowledge-enhanced data generation pipeline, a Mixture-of-Experts architecture with task-level routing, and multimodal reasoning-augmented reinforcement learning. Experimental results show Optimus-3 achieves improvements of 20% on Planning and 76% on Embodied QA, compared to previous SOTA agents. The implementation of task-level routing with a MoE architecture offers AI practitioners a scalable and extensible approach to managing heterogeneous task learning in complex environments.  |
| AutoMind: Adaptive Knowledgeable Agent for Automated Data Science (Read more on [arXiv](https://arxiv.org/abs/2506.10974) or [HuggingFace](https://huggingface.co/papers/2506.10974))| Lanning Wei, Jingsheng Zheng, Yujie Luo, Ningyu, OE-Heart | AutoMind is an adaptive LLM agent framework designed for automated data science. The research aims to improve LLM agents by incorporating expert knowledge, agentic knowledge tree search, and self-adaptive coding. The methodology involves curating an expert knowledge base, developing an agentic knowledgeable tree search algorithm, and implementing a self-adaptive coding strategy. Experimental results on automated data science benchmarks demonstrate that AutoMind outperforms state-of-the-art baselines, surpassing 56.8% of human participants on MLE-Bench. This adaptive and knowledgeable approach provides AI practitioners with a more efficient and robust method for automating data science tasks.  |
| Magistral (Read more on [arXiv](https://arxiv.org/abs/2506.10910) or [HuggingFace](https://huggingface.co/papers/2506.10910))| Gabrielle Berrada, Andy Lo, Albert Q. Jiang, Abhinav Rastogi, Mistral-AI | Magistral introduces Mistral's first reasoning model and reinforcement learning pipeline. The research aims to explore the limits of pure reinforcement learning (RL) training of Large Language Models (LLMs) without relying on existing distilled data. The methodology employs a ground-up approach, relying solely on internally-trained models and infrastructure with optimizations to the GRPO algorithm for training stability, multilingual consistency and reward shaping. The models achieved a nearly 50% increase in AIME-24 (pass@1) using pure RL and it also shows that multimodal reasoning capabilities emerge with online RL with textual data on top of a multimodal model. AI practitioners can leverage this scalable RL pipeline, for generating reasoning models from foundational models.  |
| VideoDeepResearch: Long Video Understanding With Agentic Tool Using (Read more on [arXiv](https://arxiv.org/abs/2506.10821) or [HuggingFace](https://huggingface.co/papers/2506.10821))| Zhicheng Dou, Ji-Rong Wen, Junjie Zhou, Zheng Liu, Huaying Yuan | VideoDeepResearch introduces an agentic framework for long video understanding (LVU). The paper aims to address LVU challenges by using a text-only large reasoning model (LRM) with a modular multi-modal toolkit instead of relying on large MLLMs with extended context windows. The methodology involves formulating problem-solving strategies via reasoning and selectively accessing video content using multimodal retrievers and visual perceivers. Results show VideoDeepResearch outperforms existing MLLMs, achieving a 9.6% improvement on MLVU (test). The work implies AI practitioners can overcome LVU challenges effectively through agentic systems leveraging readily available tools, suggesting a shift from monolithic models towards modular, tool-using approaches.  |
| PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a
  Unified Framework (Read more on [arXiv](https://arxiv.org/abs/2506.10741) or [HuggingFace](https://huggingface.co/papers/2506.10741))| Haoyu Chen, Tian Ye, Jialin Gao, Jianyu Lai, Ephemeral182 | PosterCraft introduces a unified framework for high-quality aesthetic poster generation, moving beyond modular design paradigms. The main objective is to create a holistic approach capable of generating visually coherent and artistically compelling posters directly from textual input. PosterCraft employs a cascaded workflow, including scalable text rendering optimization via the Text-Render-2M dataset, region-aware fine-tuning on HQ-Poster-100K, aesthetic-text reinforcement learning, and joint vision-language feedback refinement. Experiments demonstrate PosterCraft significantly outperforms open-source baselines, achieving competitive performance with commercial systems and improving text rendering accuracy. This unified framework provides AI practitioners with a method to generate high-quality posters that integrates content, layout, and style cohesively.  |
| ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark (Read more on [arXiv](https://arxiv.org/abs/2506.10960) or [HuggingFace](https://huggingface.co/papers/2506.10960))| Bozhong Tian, Siyuan Cheng, Kangwei Liu, Jasonchen123, Ningyu | ChineseHarm-Bench is introduced as a benchmark for detecting harmful content in Chinese. The research aims to provide a comprehensive resource for content harm detection, covering six categories of violations. The methodology involves constructing a dataset from real-world violation records, expert annotation, and a knowledge-augmented baseline model. Results show that even state-of-the-art LLMs achieve macro-F1 scores of no more than 0.8, demonstrating limitations in Chinese harmful content detection. The benchmark and knowledge-augmented baseline provide a means for AI practitioners to evaluate and improve model performance in Chinese content moderation tasks.  |
| CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic
  Design Generation (Read more on [arXiv](https://arxiv.org/abs/2506.10890) or [HuggingFace](https://huggingface.co/papers/2506.10890))| Yutao Cheng, ShiLayne, YangMaoke, hxxxl, zbrl | CreatiPoster is a framework for generating editable, multi-layer graphic compositions from natural-language instructions or assets. The research aims to generate high-quality, editable graphic designs automatically, addressing limitations in existing AI tools regarding user asset integration, editability, and professional visual appeal. The proposed framework utilizes a protocol model (RGBA large multimodal model) to produce a JSON specification detailing each layer (text or asset) and a conditional background model to synthesize a coherent background. The framework outperforms existing systems in a new benchmark with automated metrics and releases a copyright-free corpus of 100,000 multi-layer designs. AI practitioners can leverage the CreatiPoster framework to create editable graphic designs for diverse applications, including canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters.  |
| Resa: Transparent Reasoning Models via SAEs (Read more on [arXiv](https://arxiv.org/abs/2506.09967) or [HuggingFace](https://huggingface.co/papers/2506.09967))| Ömer Faruk Akgül, Julian Asilis, willieneis, deqing, upup-ashton-wang | i) Resa introduces a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure for training reasoning language models. ii) The research aims to elicit strong reasoning in language models cost-effectively by leveraging their underlying representations. iii) SAE-Tuning trains an SAE to capture reasoning abilities from a source model and then uses it to guide supervised fine-tuning of a target model, using verified question-answer data. iv) SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. v) Practitioners can leverage the SAE-Tuning procedure to efficiently elicit and transfer reasoning abilities between language models with reduced computational costs and greater transparency.  |
| Ming-Omni: A Unified Multimodal Model for Perception and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09344) or [HuggingFace](https://huggingface.co/papers/2506.09344))| Chunluan Zhou, Chuanyang Zheng, Cheng Zou, Biao Gong, Inclusion AI | Ming-Omni proposes a unified multimodal model for perception and generation across images, text, audio, and video. The research aims to develop a single model capable of processing and generating multiple modalities without task-specific fine-tuning or structural redesign. Ming-Omni utilizes dedicated modality encoders processed by a Mixture-of-Experts architecture (Ling) with modality-specific routers, combined with an audio decoder and a diffusion-based image generator (Ming-Lite-Uni). Ming-Omni achieves a GenEval score of 0.64 in image generation, outperforming models like SDXL, and attains comparable image perception performance to Qwen2.5-VL-7B while using only 2.8B parameters. Ming-Omni offers AI practitioners an open-source architecture and methodology for building unified multimodal models with strong generation capabilities across diverse data types.  |
| Eliciting Fine-Tuned Transformer Capabilities via Inference-Time
  Techniques (Read more on [arXiv](https://arxiv.org/abs/2506.08060) or [HuggingFace](https://huggingface.co/papers/2506.08060))| codelion | i) The paper investigates approximating capabilities acquired through supervised fine-tuning (SFT) of transformer models using inference-time techniques like in-context learning (ICL). ii) The main objective is to formally prove whether a base transformer model can elicit fine-tuned capabilities via ICL, under idealized and practical constraints. iii) The methodology involves theoretically constructing an inference technique TSFT utilizing ICL and quantifying minimal dataset sizes required for approximation, rooted in the Turing completeness of transformers. iv) For text generation tasks, a dataset of size O(mV log(V/ɛ)) or, with fixed context, O((1/ɛ²)log(V/δ)log(mV/ε²)) suffices to approximate fine-tuned distributions across m contexts; for linear classification, datasets of size O(d/ε²) or, with fixed context, O((1/ε²)log(1/δ)) are sufficient. v) AI practitioners can leverage these findings for resource-efficient LLM deployment by approximating SFT capabilities via ICL with minimal datasets, potentially enhancing real-world applications using techniques like retrieval-augmented generation (RAG).  |
| Attention, Please! Revisiting Attentive Probing for Masked Image
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2506.10178) or [HuggingFace](https://huggingface.co/papers/2506.10178))| Tilemachos Aravanis, Ioannis Kakogeorgiou, Eirini Baltzi, Dionysis Christopoulos, Bill Psomas | i) The paper introduces efficient probing (EP), a novel multi-query cross-attention mechanism for evaluating self-supervised learning models trained with masked image modeling (MIM). ii) The research aims to address the limitations of standard linear probing in assessing MIM models by improving the accuracy and efficiency of attentive probing. iii) The methodology involves revisiting existing attentive probing mechanisms, identifying key simplifications, and introducing EP, which eliminates redundant projections and reduces parameter count. iv) EP achieves up to a 10x speed-up over conventional multi-head attention while maintaining or surpassing state-of-the-art performance, reaching a top-1 accuracy of 75.6% on ImageNet-1k with MAE ViT-B using less than 1.4M parameters. v) AI practitioners can leverage EP as a computationally efficient and accurate evaluation method for SSL models, particularly for those trained with MIM, facilitating faster prototyping and model selection.  |
| UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2506.09952) or [HuggingFace](https://huggingface.co/papers/2506.09952))| Jiwen Lu, Jie Zhou, Yanran21, LavenderLA | i) UniPre3D is a novel unified 3D point cloud pre-training method utilizing cross-modal Gaussian splatting. ii) The primary objective is to develop a single pre-training approach applicable across varying scales of 3D point clouds and model architectures, addressing the current lack of a unified method effective for both object- and scene-level tasks. iii) The method predicts Gaussian primitives, renders images using differentiable Gaussian splatting for pixel-level supervision, and integrates 2D image features from pre-trained models through scale-adaptive fusion. iv) Experiments show UniPre3D outperforms existing methods on ScanObjectNN, achieving 87.93% accuracy on the PB_T50_RS benchmark with a standard Transformer backbone. v) The unified pre-training approach facilitates development of more generalizable 3D perception systems, potentially enabling AI practitioners to leverage a single model across diverse 3D data scales and tasks.  |
| VerIF: Verification Engineering for Reinforcement Learning in
  Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2506.09942) or [HuggingFace](https://huggingface.co/papers/2506.09942))| Lei Hou, Bin Xu, Xiaozhi Wang, Yunjia Qi, Wesleythu | i) The paper introduces VERIF, a verification method combining rule-based code verification and LLM-based verification for reinforcement learning (RL) in instruction following. ii) The research explores verification challenges in RL for instruction following, aiming to improve performance and generalization capabilities. iii) The methodology involves constructing a dataset (VERINSTRUCT) of approximately 22,000 instances with verification signals, followed by RL training using VERIF on SFT-trained models. iv) Results show significant improvements across instruction-following benchmarks; specifically, the TULU 3 SFT model trained with VERIF achieves state-of-the-art performance among comparable-sized models, with pass@64 showing over a 20% increase compared to pass@1 on IFEval. v) VERIF provides a practical approach for enhancing instruction-following capabilities in LLMs and can be integrated into existing RL pipelines, improving performance without compromising general capabilities; a smaller distilled LLM verifier (IF-Verifier-7B) is also explored to reduce computational costs for RL training.  |
| Build the web for agents, not agents for the web (Read more on [arXiv](https://arxiv.org/abs/2506.10953) or [HuggingFace](https://huggingface.co/papers/2506.10953))| Siva Reddy, Marius Mosbach, Gaurav Kamath, xhluca | i) This paper proposes a shift from adapting AI agents to existing web interfaces towards designing Agentic Web Interfaces (AWIs) specifically for agent interaction. ii) The main objective is to address the limitations of current web agent approaches caused by mismatches between human-designed interfaces and LLM capabilities. iii) The methodology involves introducing the concept of AWIs and outlining six guiding principles for their design, emphasizing safety, efficiency, and standardization. iv) The paper does not provide quantitative results but argues that AWIs can overcome fundamental interface limitations. v) The principal implication for AI practitioners is the need for a collaborative effort in designing AWIs to enable more efficient, reliable, and transparent web agent development.  |
| Compound AI Systems Optimization: A Survey of Methods, Challenges, and
  Future Directions (Read more on [arXiv](https://arxiv.org/abs/2506.08234) or [HuggingFace](https://huggingface.co/papers/2506.08234))| Guan-Bo Yang, Jui-Chao Lu, Mei-Yi Liu, Guan-Ting Yi, Yu-Ang Lee | This paper surveys methods for optimizing compound AI systems, which integrate multiple components such as LLMs, simulators, and retrieval modules. The research objective is to systematically review and classify recent progress in optimizing these complex AI systems, encompassing both numerical and language-based techniques. The survey classifies existing methods based on structural flexibility and learning signals and presents a 2x2 taxonomy covering 26 representative works. The principal implication is a structured understanding of compound AI system optimization methods, providing a foundation for AI practitioners to design and refine complex AI workflows, though specific quantitative performance improvements across surveyed techniques remain to be identified in the summary provided.  |
| LLM Unlearning Should Be Form-Independent (Read more on [arXiv](https://arxiv.org/abs/2506.07795) or [HuggingFace](https://huggingface.co/papers/2506.07795))| Shu Wu, Mengqi Zhang, Acruxos | i) This paper identifies and mitigates Form-Dependent Bias in LLM unlearning. ii) The research objective is to demonstrate the failure of existing LLM unlearning methods to generalize across different input formats and proposes a form-independent solution. iii) The methodology involves characterizing Form-Dependent Bias, developing a benchmark (ORT) to evaluate unlearning robustness across diverse formats, and introducing Rank-One Concept Redirection (ROCR), a training-free parameter modification method. iv) Experiments showed that the probability of correct answers can be reduced by 58.12% on QA tasks but only by 5% on MCP tasks in RT and ROCR completed unlearning tasks in just 21 seconds. v) The findings imply that AI practitioners should consider the form-dependent vulnerability when deploying LLM unlearning techniques in security-critical applications, and ROCR is proposed as a solution along this direction.  |
| What Makes a Good Natural Language Prompt? (Read more on [arXiv](https://arxiv.org/abs/2506.06950) or [HuggingFace](https://huggingface.co/papers/2506.06950))| Nancy F. Chen, Kenji Kawaguchi, Ngoc-Hai Nguyen, Duy Dinh, Do Xuan Long | i) This paper proposes a property- and human-centric framework for evaluating natural language prompt quality, identifying 21 properties across six dimensions. ii) The main objective is to address the limited conceptual consensus on what quantifies effective natural language prompts. iii) The methodology involves a meta-analysis of 150+ prompting-related papers and blogs, followed by empirical exploration of multi-property prompt enhancements in reasoning tasks. iv) The study found that instruction-tuning on property-enhanced prompts can result in better reasoning models and observed that single-property enhancements often have the greatest impact on model performance. v) AI practitioners can leverage the proposed property-centric prompt evaluation framework for systematic prompt optimization and instruction tuning, bridging the gap between human-AI communication and improving model reasoning capabilities.  |
| Breaking Data Silos: Towards Open and Scalable Mobility Foundation
  Models via Generative Continual Learning (Read more on [arXiv](https://arxiv.org/abs/2506.06694) or [HuggingFace](https://huggingface.co/papers/2506.06694))| Yong Li, Chonghua Han, Yukun Liu, Yuan Yuan, JJ-TMT | i) This paper introduces MoveGCL, a privacy-preserving framework for training mobility foundation models using generative continual learning across decentralized data silos. ii) The research aims to develop a scalable and privacy-conscious method for building generalizable mobility foundation models. iii) MoveGCL employs synthetic trajectory replay from a frozen teacher model, knowledge distillation, a Mixture-of-Experts Transformer with mobility-aware expert routing, and layer-wise progressive adaptation. iv) Experiments on six real-world urban datasets show MoveGCL achieves performance comparable to joint training and outperforms federated learning baselines, with 95% of generated trajectories not having a similarity score higher than 50% with real trajectories, showing limited data leakage. v) MoveGCL offers AI practitioners a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models, particularly in privacy-sensitive domains like human mobility, enabling collaborative model evolution without raw data sharing.  |
| Token Perturbation Guidance for Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.10036) or [HuggingFace](https://huggingface.co/papers/2506.10036))| Babak Taati, Soroush Mehraban, Javad Rajabi, msadat97 | i) The paper introduces Token Perturbation Guidance (TPG), a novel training-free guidance method for diffusion models. ii) The primary research objective is to improve the generation quality and semantic alignment of diffusion models without specific training procedures or architectural changes and to make them agnostic to input conditions. iii) The methodology involves applying perturbation matrices, specifically norm-preserving shuffling, directly to intermediate token representations within the diffusion network. iv) Experiments on SDXL show that TPG achieves nearly a 2× improvement in FID for unconditional generation compared to the SDXL baseline; TPG also mirrors CFG. v) TPG provides AI practitioners with a condition-agnostic guidance method that extends CFG-like benefits to a broader class of diffusion models and allows for both conditional and unconditional generation.  |
| Draft-based Approximate Inference for LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.08373) or [HuggingFace](https://huggingface.co/papers/2506.08373))| Hyung Il Koo, Minjae Lee, Wonjun Kang, Ethan Ewer, Kevin Galim | i) This paper introduces a novel framework, Draft-based Approximate Inference, for optimizing long-context Large Language Model (LLM) inference by leveraging smaller draft models to predict token and KV-pair importance. ii) The research aims to improve the accuracy of approximate LLM inference techniques, such as KV cache dropping and prompt compression, by using draft models to estimate token importance. iii) The methodology involves two instantiations: SpecKV for KV cache dropping and sparse prefilling, and SpecPC for prompt compression, both utilizing draft model outputs and attention activations to identify and discard less important tokens/KV pairs. iv) Experiments on long-context benchmarks demonstrate that the proposed methods achieve higher accuracy than existing baselines, with improvements up to 25 points on the RULER benchmark, while preserving memory usage, latency, and throughput. v) The work offers AI practitioners an effective strategy for accelerating LLM inference and improving resource efficiency, specifically highlighting the potential of draft models to enhance the performance of approximate inference techniques in scenarios with memory and computational constraints. |
| LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure
  Profiles (Read more on [arXiv](https://arxiv.org/abs/2506.06561) or [HuggingFace](https://huggingface.co/papers/2506.06561))| Branislav Kveton, Aashish Anantha Ramakrishnan, Ting-Yao Hsu, Ho Yin 'Sam' Ng, Franck-Dernoncourt | i) The paper introduces LAMP-CAP, a dataset for personalized figure caption generation using multimodal profiles. ii) The research aims to improve figure caption generation by incorporating personalization through multimodal profiles. iii) The methodology involves creating a dataset from scientific figures, figure-mentioning paragraphs, and related figures as profiles, and evaluating four LLMs on the caption generation task. iv) The primary result demonstrates that using multimodal profile information consistently improves the similarity of generated captions to ground-truth captions, with captions being the most critical profile element; experiments also reveal personalization is more effective (higher similarity) when profile figures share the same type as the target figure. v) LAMP-CAP provides AI practitioners with a new benchmark and a dataset to explore and implement personalized figure caption generation using multimodal profiles, improving the contextual relevance of generated captions. The effectiveness of multimodal profiles suggests that using figure images in personalization are preferred over text-only information.  |
| MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness
  Against VLM-based Attacks (Read more on [arXiv](https://arxiv.org/abs/2506.05982) or [HuggingFace](https://huggingface.co/papers/2506.05982))| Yiren Song, Xin Wei, Yule Xue, Zonglin Wu | i) MCA-Bench is introduced as a multimodal CAPTCHA benchmark to evaluate the robustness against VLM-based attacks. ii) The objective is to rigorously evaluate the security robustness of diverse CAPTCHA schemes. iii) The methodology involves fine-tuning specialized cracking agents for each CAPTCHA category using a shared vision-language model backbone. iv) Experiments reveal VLMs achieve over 96% accuracy on simple tasks but as low as 2.5% on complex tasks involving physical interaction or multi-step logic. v) The principal implication is providing actionable insights for CAPTCHA hardening and guidance for human-machine verification in the face of intelligent-agent attacks.  |
| Fine-Grained Perturbation Guidance via Attention Head Selection (Read more on [arXiv](https://arxiv.org/abs/2506.10978) or [HuggingFace](https://huggingface.co/papers/2506.10978))| Jaewon Min, Minjae Kim, Sanghyun Lee, Jiwon Kang, Donghoon Ahn | i) The paper introduces a novel approach, HeadHunter, for fine-grained control in diffusion models by selectively perturbing individual attention heads. ii) The research investigates how granular attention perturbations, down to the individual head level, can improve generation quality and visual attribute control in diffusion models, specifically Diffusion Transformers (DiT). iii) HeadHunter iteratively selects attention heads based on user-defined objectives and introduces SoftPAG, which linearly interpolates attention maps toward an identity matrix for continuous perturbation strength tuning. iv) Experiments on DiT-based models like Stable Diffusion 3 demonstrate that HeadHunter achieves superior performance in general quality enhancement and style-specific guidance compared to layer-level perturbation, with effective heads not concentrated in any single layer. v) AI practitioners can leverage HeadHunter for targeted manipulation of generation quality and visual attributes in diffusion models by employing a systematic head selection framework, mitigating oversmoothing and improving control.  |
| Discovering Hierarchical Latent Capabilities of Language Models via
  Causal Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2506.10378) or [HuggingFace](https://huggingface.co/papers/2506.10378))| Hanlin Zhang, Sham Kakade, Vasilis Syrgkanis, Jikai Jin | i) This paper proposes a causal representation learning framework to discover hierarchical latent capabilities in language models. ii) The main objective is to address challenges in rigorously evaluating language model capabilities due to confounding effects and computational costs. iii) The methodology involves modeling benchmark performance as a linear transformation of latent capability factors identified through causal representation learning, controlling for the base model as a confounder; Hierarchical Component Analysis (HCA) is used to recover latent capabilities. iv) The study identifies a three-node linear causal structure from over 1500 models on the Open LLM Leaderboard, indicating a causal flow from general problem-solving to instruction-following and mathematical reasoning; minimal MIC (maximum inexactness coefficient) of 0.04 achieved. v) AI practitioners can utilize this framework to gain actionable insights for targeted post-training of language models by understanding the underlying causal relationships between latent capabilities. The impact of scaling up pre-training compute for downstream task performance has also been demonstrated.  |
| StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated
  Video Streams (Read more on [arXiv](https://arxiv.org/abs/2506.08862) or [HuggingFace](https://huggingface.co/papers/2506.08862))| Renjie Liao, Lele Wang, Xuanyu Yi, Qi Yan, Zike Wu | StreamSplat introduces an online framework for dynamic 3D Gaussian Splatting (3DGS) reconstruction from uncalibrated video streams. The research aims to enable real-time dynamic 3D scene reconstruction without calibrated camera poses. It employs a probabilistic sampling mechanism in a static encoder for 3DGS position prediction and a bidirectional deformation field for dynamic modeling. Experiments on the RE10K dataset show StreamSplat achieves a PSNR of 41.60 on given views, outperforming existing methods. This provides AI practitioners with a feed-forward method to reconstruct dynamic scenes from video without camera calibration.  |
