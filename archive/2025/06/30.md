

## Papers for 2025-06-30

| Title | Authors | Summary |
|-------|---------|---------|
| BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing (Read more on [arXiv](https://arxiv.org/abs/2506.17450) or [HuggingFace](https://huggingface.co/papers/2506.17450))| Sanghyun Woo, Saining Xie, Xuhui Jia, Ramin Mehran, cccjc | BlenderFusion is a generative framework for visual scene creation by recomposing objects, camera perspectives, and backgrounds through layering, editing, and compositing. The research aims to enable precise 3D-aware visual compositing by integrating generative models with 3D graphics tools like Blender. It utilizes a dual-stream diffusion model fine-tuned on video frames with source masking and simulated object jittering to enhance object control. Experiments on MOVi-E, Objectron, and Waymo datasets show BlenderFusion improves object-level and image-level metrics (e.g., achieving a FID score of 9.11 on MOVi-E), demonstrating better foreground and background modeling. The framework's use of 3D grounding with Blender allows for flexible visual compositing tasks, offering AI practitioners a way to create and manipulate visual data more effectively.  |
| LLaVA-Scissor: Token Compression with Semantic Connected Components for
  Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.21862) or [HuggingFace](https://huggingface.co/papers/2506.21862))| Qibin Hou, Xihan Wei, Jiaxing Zhao, Boyuan Sun | i) LLaVA-Scissor is a training-free token compression strategy for video multimodal large language models (VLLMs) that leverages Semantic Connected Components (SCC). ii) The paper aims to develop a token compression strategy that effectively reduces redundancy and comprehensively represents semantic regions in video data for efficient VLLM processing. iii) The methodology involves a two-step spatio-temporal token compression utilizing SCC in both spatial and temporal domains, partitioning tokens into non-overlapping semantic regions based on pairwise similarity and a threshold, with a final average merge. iv) Experimental results show that LLaVA-Scissor outperforms other token compression methods, achieving superior performance on video understanding benchmarks; specifically, a retention ratio of 5% achieves performance close to 10% retention ratios of other methods. v) LLaVA-Scissor provides AI practitioners with an effective training-free inference method to process long videos with reduced computational cost, especially crucial for resource-constrained deployments of VLLMs, enabling processing of long-form videos more efficiently.  |
| XVerse: Consistent Multi-Subject Control of Identity and Semantic
  Attributes via DiT Modulation (Read more on [arXiv](https://arxiv.org/abs/2506.21416) or [HuggingFace](https://huggingface.co/papers/2506.21416))| Xu Wang, Li Chen, Haomiao Sun, Mengyi Zhao, chenbowen | i) XVerse introduces a DiT-based framework for consistent multi-subject image generation with control over identity and semantic attributes. ii) The research aims to improve multi-subject image generation by addressing attribute entanglement and maintaining individual identity fidelity. iii) The methodology involves transforming reference images into token-specific text-stream modulation offsets and integrating VAE-encoded image features. iv) XVerse achieves an overall score of 73.40 on a new benchmark XVerseBench, outperforming other methods in both single and multi-subject generation. v) AI practitioners can leverage XVerse's text-stream modulation and VAE integration to enhance fine-grained control and consistency in personalized and complex scene generation tasks.  |
| ShotBench: Expert-Level Cinematic Understanding in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.21356) or [HuggingFace](https://huggingface.co/papers/2506.21356))| Yuhao Dong, Dian Zheng, Yi Jin, Jingwen He, Hongbo Liu | i) ShotBench introduces a new benchmark for evaluating cinematic understanding in Vision-Language Models (VLMs). ii) The research aims to assess VLMs' proficiency in comprehending cinematographic language beyond basic visual understanding. iii) The methodology involves creating ShotBench, a dataset with over 3.5k expert-annotated QA pairs from over 200 films, and evaluating 24 leading VLMs. iv) The evaluation reveals limitations in current VLMs, with the top-performing model achieving less than 60% average accuracy, and introduces ShotVL, which gains 19.0% compared to the original Qwen2.5-VL-3B model. v) The study provides AI practitioners with ShotBench and ShotQA to enhance VLM capabilities in fine-grained visual comprehension and AI-assisted video generation by using ShotVL.  |
| From Ideal to Real: Unified and Data-Efficient Dense Prediction for
  Real-World Scenarios (Read more on [arXiv](https://arxiv.org/abs/2506.20279) or [HuggingFace](https://huggingface.co/papers/2506.20279))| Minnan Luo, Zhuohang Dang, Chengyou Jia, Changliang Xia | This paper introduces DenseDiT, a data-efficient framework for dense prediction across real-world scenarios. The research aims to address the limitations of existing methods in generalizing to complex, data-scarce real-world dense prediction tasks. They use a generative model-based approach with parameter-reuse and lightweight branches for multi-scale context integration. Evaluation on the introduced DenseWorld benchmark shows DenseDiT achieves superior performance using less than 0.01% of baseline training data, achieving an average D-Score of 0.944. This demonstrates the potential for practical real-world deployment using pre-trained generative models for diverse dense prediction tasks.  |
| MiCo: Multi-image Contrast for Reinforcement Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.22434) or [HuggingFace](https://huggingface.co/papers/2506.22434))| Xiaogang Xu, Xiaoyang Wu, Shaoteng Liu, Mingkang Zhu, Xi Chen | i) MiCo introduces a self-supervised contrastive learning framework to improve multi-image visual reasoning in VLMs through rule-based reinforcement learning. ii) The research aims to enable chain-of-thought reasoning for linking visual cues across multiple images in VLMs without relying on manually curated question-answer pairs. iii) The methodology involves constructing image triplets (two augmented views of the same image and a similar but distinct image), prompting the VLM to compare images, and optimizing the model using augmented GRPO. iv) Experiments show that MiCo achieves significant improvements on multi-image reasoning benchmarks, with a reported improvement of +12.93 on VLM2-Bench without human annotated question-answer pairs. v) The principal implication for AI practitioners is a method to enhance VLMs' reasoning abilities by leveraging inherent image constraints, reducing the reliance on manually constructed training data and improving generalization to complex visual reasoning tasks.  |
| Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs (Read more on [arXiv](https://arxiv.org/abs/2506.21656) or [HuggingFace](https://huggingface.co/papers/2506.21656))| Xiaofeng Zhang, Xu Cao, Jingyuan Zhu, Yuanzhe Liu, Yifan Shen | SpatialReasoner-R1, a novel VLM, addresses limitations in fine-grained spatial reasoning. The research investigates how to improve spatial reasoning in vision-language models, particularly with complex logic and precise alignment. They employ Multi-Model Monte Carlo Tree Search (M3CTS) to generate diverse LongCoT reasoning trajectories and introduce fine-grained Direct Preference Optimization (fDPO) with segment-specific preference granularity. The fDPO training achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1 outperforms the strongest baseline on SPATIALRGPT-BENCH by 9.8% in average accuracy and this new SOTA model provides AI practitioners with a more robust and accurate tool for spatial reasoning tasks.  |
| Ark: An Open-source Python-based Framework for Robot Learning (Read more on [arXiv](https://arxiv.org/abs/2506.21628) or [HuggingFace](https://huggingface.co/papers/2506.21628))| Jiacheng Qiu, Huang Helong, Sarthak Das, Christopher E. Mower, Magnus Dierking | Ark is introduced as a new open-source, Python-first robotics framework designed to bridge the gap between AI and robotics. The primary objective is to provide a Gym-style environment for data collection, preprocessing, and policy training with imitation learning algorithms, facilitating seamless transition between simulation and real-world robots. The methodology involves a client-server architecture, ROS interoperability, and reusable modules for control, SLAM, and visualization. Ark achieves rapid prototyping and hardware swapping, unifying robotics and AI practices. The framework demonstrates the capability of switching between simulated and real-world environments by toggling a single configuration flag. The principal implication is that Ark lowers the entry barrier for AI practitioners to develop and deploy autonomous robots by providing a unified Python interface and streamlined machine-learning workflows.  |
| Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity (Read more on [arXiv](https://arxiv.org/abs/2505.21411) or [HuggingFace](https://huggingface.co/papers/2505.21411))| Wei Guo, Xiaosong Li, MightyCrane, Fangcheng2, tangyehui | i) Pangu Pro MoE, a 72B parameter sparse language model, introduces a Mixture of Grouped Experts (MoGE) architecture for balanced computational load across distributed devices. ii) The research aims to improve training and inference throughput by mitigating expert load imbalance in Mixture of Experts (MoE) models. iii) MoGE partitions experts into groups, enforcing a fixed number of experts activated per group during token routing, and optimizes model configuration for Ascend NPUs through system simulation. iv) Inference performance achieves 1148 tokens/s per card on Ascend NPUs, and can be further improved to 1528 tokens/s with speculative decoding, and improves Model FLOPs Utilization (MFU) by 35%. v) AI practitioners can leverage MoGE to design and deploy sparse models with enhanced load balancing and improved throughput, particularly in distributed inference scenarios using Ascend NPUs.  |
| Noise Consistency Training: A Native Approach for One-Step Generator in
  Learning Additional Controls (Read more on [arXiv](https://arxiv.org/abs/2506.19741) or [HuggingFace](https://huggingface.co/papers/2506.19741))| Jing Tang, Tianyang Hu, Shuchen Xue, Yihong Luo | i) This paper introduces Noise Consistency Training (NCT), a lightweight approach for integrating new control signals into pre-trained one-step generators. ii) The research aims to adapt one-step generative models to new control conditions without requiring retraining of the base diffusion model or access to original training data. iii) NCT employs an adapter module and a noise consistency loss in the generator's noise space, aligning the adapted model's generation behavior across varying noise levels. iv) Experiments demonstrate NCT achieves state-of-the-art controllable generation in a single forward pass, reducing function evaluations (NFEs) from 50 to 1 while maintaining or surpassing baseline performance. v) NCT offers AI practitioners a modular, data-efficient, and easily deployable method to enhance pre-trained generators with new controls, improving computational efficiency in AIGC applications.  |
| The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT
  Improvements (Read more on [arXiv](https://arxiv.org/abs/2506.22419) or [HuggingFace](https://huggingface.co/papers/2506.22419))| Roberta Raileanu, Xian Li, Minqi Jiang, Despoina Magka, Bingchen Zhao | The paper introduces the Automated LLM Speedrunning Benchmark to assess LLMs' ability to reproduce existing LLM training improvements. The main research objective is to evaluate AI agents' capability to reproduce research results in the NanoGPT speedrun competition. The methodology involves providing agents with previous records' training scripts and hints of varying detail levels. The primary result showed recent reasoning LLMs, even with detailed hints, struggled to replicate known innovations, recovering less than 20% of speedups without hints, and around 40%-46% of speedup with certain hints. This benchmark provides a non-saturated measure of LLMs' automated scientific reproduction, highlighting the need for improved reproducibility skills for autonomous research agents.  |
| Gazal-R1: Achieving State-of-the-Art Medical Reasoning with
  Parameter-Efficient Two-Stage Training (Read more on [arXiv](https://arxiv.org/abs/2506.21594) or [HuggingFace](https://huggingface.co/papers/2506.21594))| Amr Fawzy, Mostafa Samy, Ahmed M. Adly | i) Gazal-R1, a 32B parameter language model, achieves state-of-the-art results in medical reasoning through parameter-efficient two-stage training. ii) The research aims to develop a medical LLM with superior reasoning capabilities compared to larger models, while maintaining transparency and explainability. iii) The methodology involves supervised fine-tuning (SFT) on a synthetic medical reasoning dataset, enhanced with DoRA and rsLORA, followed by reinforcement learning using Group Relative Policy Optimization (GRPO). iv) Gazal-R1 attains 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, outperforming models up to 12x larger. v) This demonstrates the effectiveness of strategic training and parameter-efficient techniques for developing high-performance domain-specific language models.  |
| Confucius3-Math: A Lightweight High-Performance Reasoning LLM for
  Chinese K-12 Mathematics Learning (Read more on [arXiv](https://arxiv.org/abs/2506.18330) or [HuggingFace](https://huggingface.co/papers/2506.18330))| Yitao Duan, Jiachen Wang, Qiao Cheng, Na Cai, nomadlx | i) Confucius3-Math, a 14B parameter open-source LLM, achieves state-of-the-art performance on Chinese K-12 mathematics reasoning tasks. ii) The research aims to develop a cost-effective and high-performance LLM specifically for Chinese K-12 mathematics education. iii) The methodology involves post-training a base model with reinforcement learning, incorporating techniques like Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting. iv) Confucius3-Math achieves SOTA performance on K-12 math benchmarks and obtains a 15.8x speedup in inference compared to DeepSeek-R1 on comparable hardware, all while reducing costs. v) The paper demonstrates that a low-cost, domain-specific reasoning model can outperform larger general models, implying that AI/ML practitioners can achieve substantial performance gains by focusing on specialized model training.  |
| RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.22149) or [HuggingFace](https://huggingface.co/papers/2506.22149))| Hrvoje Bogunović, Ursula Schmidt-Erfurth, José Morano, Ronald Fecso | RetFiner introduces a vision-language refinement scheme to enhance retinal foundation models (FMs). The research aims to improve the semantic understanding of existing retinal FMs by incorporating textual data from Electronic Health Records (EHRs). The methodology refines vision encoders of existing retinal FMs using a combination of image-text contrastive (ITC), image-text matching (ITM), masked language modeling (MLM), and generative modeling (GM) losses. The refined FMs showed an average increase of 5.8 percentage points in linear probing performance on seven OCT classification tasks. RetFiner offers AI practitioners an efficient SSL method to adapt existing FMs to specific medical imaging populations with limited annotation.  |
