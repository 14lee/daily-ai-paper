

## Papers for 2025-06-27

| Title | Authors | Summary |
|-------|---------|---------|
| MMSearch-R1: Incentivizing LMMs to Search (Read more on [arXiv](https://arxiv.org/abs/2506.20670) or [HuggingFace](https://huggingface.co/papers/2506.20670))| Bo You, Yiding Liu, Wei Li, Zihao Deng, kimingng | i) MMSearch-R1 is a reinforcement learning framework enabling LMMs to perform on-demand search in real-world internet environments. ii) The research aims to incentivize LMMs to learn when and how to utilize image and text search tools effectively for visual question answering (VQA). iii) The methodology involves an end-to-end reinforcement learning (RL) approach with a group relative policy optimization (GRPO) algorithm, incorporating an outcome-based reward with a search penalty to encourage efficient search behavior and using image and text search tools. iv) The model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. v) MMSearch-R1 provides AI practitioners with a framework for training LMMs to intelligently integrate external knowledge sources, reducing reliance on static knowledge bases and improving performance on knowledge-intensive tasks.  |
| MADrive: Memory-Augmented Driving Scene Modeling (Read more on [arXiv](https://arxiv.org/abs/2506.21520) or [HuggingFace](https://huggingface.co/papers/2506.21520))| Maria Golitsyna, Ruslan Musaev, Kirill Struminsky, Polina Karpikova, apryc1 | i) MADrive introduces a memory-augmented framework for photorealistic driving scene reconstruction and novel view synthesis by retrieving and integrating 3D vehicle assets from an external database. ii) The main objective is to enhance existing scene reconstruction methods by replacing partially observed vehicles with realistically reconstructed counterparts to support photorealistic synthesis of altered or novel driving scenarios. iii) The key methodology involves curating MAD-Cars, a dataset of ~70K 360° car videos, developing a retrieval module to find similar car instances, and integrating reconstructed 3D assets into the target scene through orientation alignment and relighting. iv) Primary results show MADrive achieves a MOTA score of 0.810 for multi-object tracking and a segmentation IoU of 0.822, demonstrating improved rendering quality for downstream perception tasks compared to existing methods. v) The principal implication for AI practitioners is a framework for generating realistic driving simulation data with altered configurations, potentially improving the training and robustness of autonomous driving systems.  |
| WorldVLA: Towards Autoregressive Action World Model (Read more on [arXiv](https://arxiv.org/abs/2506.21539) or [HuggingFace](https://huggingface.co/papers/2506.21539))| Siteng Huang, Yuming Jiang, Chaohui Yu, Jun Cen, JacobYuan | WorldVLA is presented as an autoregressive action world model for unified action and image understanding and generation. The research aims to integrate VLA models and world models into a single framework to improve action generation through environmental physics learning. The methodology involves employing three tokenizers for images, text, and actions within a single LLM architecture, using an attention mask strategy to address action prediction errors. Experiments on the LIBERO benchmark show WorldVLA outperforms action models with a 4% increase in grasping success rate and reduces Fréchet Video Distance by 10% compared to vanilla world models. The attention masking strategy improved the grasping success rate by 4% to 23% in action chunk generation, addressing performance degradation; the mutual enhancement of both world and action models in a unified framework offers AI practitioners a more robust approach to robotic tasks requiring visual and action understanding.  |
| Where to find Grokking in LLM Pretraining? Monitor
  Memorization-to-Generalization without Test (Read more on [arXiv](https://arxiv.org/abs/2506.21551) or [HuggingFace](https://huggingface.co/papers/2506.21551))| Ziyue Li, zhoutianyi, Fcr09 | i) This paper investigates grokking during large language model (LLM) pretraining, demonstrating asynchronous memorization and a transition to generalization. ii) The research aims to understand the dynamics of grokking in LLM pretraining and identify internal changes that enable generalization. iii) The study analyzes the routing pathways within a 7B parameter MoE LLM, introducing metrics for pathway similarity and consistency. iv) The research shows pathway consistency strongly correlates with test accuracy (often exceeding 0.97), indicating coherent routing is a key marker of generalization. v) AI practitioners can use pathway metrics to monitor and predict generalization during LLM pretraining without reliance on finetuning or test data.  |
| Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2506.21506) or [HuggingFace](https://huggingface.co/papers/2506.21506))| Yu Gu, Zanming Huang, yhshu, nnnyt, BoyuNLP | Mind2Web 2 introduces a benchmark for evaluating agentic web search systems. The research aims to address the limitations of existing benchmarks by creating realistic, long-horizon tasks and a novel Agent-as-a-Judge evaluation framework. It utilizes a tree-structured rubric to assess answer correctness and source attribution. Evaluation of nine agentic search systems, including OpenAI Deep Research, demonstrated varying performance, with the best system achieving 50-70% of human performance while halving the time; the task completion success rate is about 28% for agents and about 54% for humans. Mind2Web 2 offers a foundation for developing and benchmarking next-generation agentic search systems by providing a dataset with realistic web search tasks and an automated evaluation methodology.  |
| SAM4D: Segment Anything in Camera and LiDAR Streams (Read more on [arXiv](https://arxiv.org/abs/2506.21547) or [HuggingFace](https://huggingface.co/papers/2506.21547))| Sheng Yang, Chunyong Hu, Ziqian Ni, Jianyun Xu, songw-zju | SAM4D introduces a multi-modal, temporal foundation model for promptable segmentation across camera and LiDAR streams. The research objective is to enable 2D-3D joint segmentation with cross-modal prompting and temporal alignment using an architecture built upon a multi-modal transformer. The key methodology involves a Unified Multi-modal Positional Encoding (UMPE) and Motion-aware Cross-modal Memory Attention (MCMA) within a multi-modal transformer architecture. Experiments on the constructed Waymo-4DSeg dataset, containing over 300k camera-LiDAR associated masklets, demonstrated an average cross-modal IoU of 0.56 for the generated masklets. The primary implication for AI practitioners is the potential for significantly reduced annotation costs via the proposed automated data engine and enhanced cross-modal segmentation for autonomous driving applications. |
| FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient
  Multi-turn Image Editing (Read more on [arXiv](https://arxiv.org/abs/2506.20911) or [HuggingFace](https://huggingface.co/papers/2506.20911))| Dang Nguyen, Rishie Raj, Advait Gupta, zhoutianyi | i) The paper introduces FaSTA*, a cost-efficient neurosymbolic agent for multi-turn image editing that combines fast LLM planning with slow A* search and subroutine mining. ii) The research objective is to reduce the computational cost of toolpath search in multi-turn image editing by reusing knowledge from previously explored tasks. iii) FaSTA* employs LLMs for high-level subtask planning and inductive reasoning to extract reusable symbolic subroutines, coupled with a cost-sensitive A* search for individual subtasks triggered adaptively. iv) Experiments show FaSTA* reduces cost by 49.3% compared to COSTA* while maintaining competitive success rates with a 3.2% quality degradation. v) This work offers AI practitioners a method for significantly reducing the computational expense of complex image editing tasks by leveraging reusable subroutines, making large-scale applications more feasible. |
| Whole-Body Conditioned Egocentric Video Prediction (Read more on [arXiv](https://arxiv.org/abs/2506.21552) or [HuggingFace](https://huggingface.co/papers/2506.21552))| Trevor Darrell, Yann LeCun, Amir Bar, dans123, Emma02 | PEVA models egocentric video prediction conditioned on whole-body human motion represented by 3D pose trajectories. The research investigates how effectively such a model can simulate actions and their visual consequences from a first-person perspective. It trains a conditional diffusion transformer autoregressively using the Nymeria dataset of real-world egocentric video and body pose data, incorporating random timeskips and sequence-level training. PEVA demonstrates improved performance over baselines, achieving a LPIPS score of 0.303 on single-step prediction tasks, showing enhanced action consistency and generative quality. This whole-body-conditioned model offers AI practitioners a method for generating more realistic and controllable embodied simulations through capturing intricate relationships between physical movement and resulting visual changes. Some of the evaluation results have error bounds however.  |
| Arch-Router: Aligning LLM Routing with Human Preferences (Read more on [arXiv](https://arxiv.org/abs/2506.16655) or [HuggingFace](https://huggingface.co/papers/2506.16655))| Adil Hafeez, Co Tran, nehcgs, parachas | i) Arch-Router is introduced as a framework for aligning large language model (LLM) routing with human preferences. ii) The main research objective is to guide model selection by matching queries to user-defined domains or action types, encoding preferences in routing decisions. iii) The methodology involves training a compact 1.5B model, Arch-Router, to map queries to domain-action preferences for model routing decisions, alongside a data creation pipeline to generate labeled conversations. iv) Experiments on conversational datasets demonstrate that Arch-Router achieves state-of-the-art results in matching queries with human preferences, outperforming top proprietary models by 7.71% on average. v) The principal implication is that AI practitioners can use Arch-Router to implement more transparent and flexible LLM routing systems that align with subjective human evaluations, offering a practical mechanism for operationalizing diverse LLMs.  |
| FairyGen: Storied Cartoon Video from a Single Child-Drawn Character (Read more on [arXiv](https://arxiv.org/abs/2506.21272) or [HuggingFace](https://huggingface.co/papers/2506.21272))| Xiaodong Cun, Jiayi Zheng | i) FairyGen is presented as a framework for generating multi-shot cartoon videos from a single child-drawn character image. ii) The research aims to create stylistically consistent and narratively coherent animations that reflect the artistic style of the input character while achieving natural motion. iii) The method employs a multimodal large language model (MLLM) for storyboarding, a style propagation adapter for background generation, and a 3D proxy-based motion generation technique fine-tuned with an MMDiT-based image-to-video diffusion model. iv) Experiments demonstrate the system's ability to generate personalized animated stories, where the proposed method achieved a style alignment score of 0.6580 compared to baselines. v) AI practitioners can leverage FairyGen's framework for personalized content creation and engaging story animation by utilizing the proposed style propagation and motion customization techniques.  |
| DiLoCoX: A Low-Communication Large-Scale Training Framework for
  Decentralized Cluster (Read more on [arXiv](https://arxiv.org/abs/2506.21263) or [HuggingFace](https://huggingface.co/papers/2506.21263))| YingJun Wu, Ming Wu, Li Li, WenPeng Zhu, Ji Qi | DiLoCoX is a framework for training large language models on decentralized clusters with low communication bandwidth. The research investigates how to pre-train models exceeding 100 billion parameters on decentralized clusters while maintaining model convergence. The paper combines pipeline parallelism with dual optimizer policy, one-step-delay overlap of communication and local training, and an adaptive gradient compression scheme. The results show DiLoCoX achieves a 357x speedup compared to vanilla AllReduce when pre-training a 107B model over a 1Gbps network. The findings demonstrate a method for training large models on less powerful decentralized infrastructure and the efficiency of DiLoCoX for distributed training.  |
| An Agentic System for Rare Disease Diagnosis with Traceable Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.20430) or [HuggingFace](https://huggingface.co/papers/2506.20430))| Pengcheng Qiu, Xiaoman Zhang, Yanjie Fan, Chaoyi Wu, Weike Zhao | DeepRare, a novel agentic system, addresses the challenge of rare disease diagnosis. The research aims to create an LLM-powered system capable of processing heterogeneous clinical inputs (free-text, HPO terms, VCF files). DeepRare employs a three-tier architecture integrating a central host with specialized agent servers and curated knowledge sources. The system achieves a 57.18% average Recall@1 score on HPO-based evaluations, surpassing existing methods. The verified 95.40% agreement with clinical experts on reasoning chains suggests that DeepRare provides trustworthy decision support. The system provides AI practitioners with a framework for building interpretable and adaptable diagnostic tools.  |
| HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial
  Optimization Challenges (Read more on [arXiv](https://arxiv.org/abs/2506.15196) or [HuggingFace](https://huggingface.co/papers/2506.15196))| Jiang Bian, Lei Song, Haolong Qian, Ling Zhang, VictorYXL | i) The paper introduces HeurAgenix, a two-stage hyper-heuristic framework using large language models (LLMs) for solving combinatorial optimization (CO) problems. ii) The research aims to automate heuristic design and adaptive selection for complex CO problems, improving upon traditional methods that rely on manual expertise. iii) HeurAgenix employs a contrastive, data-driven approach for heuristic evolution, using an LLM to analyze solution tuples and extract reusable strategies, coupled with an adaptive selection mechanism integrating LLMs and Test-time Scaling (TTS). iv) Experiments show HeurAgenix outperforms existing LLM-based hyper-heuristics and matches or exceeds specialized solvers on canonical benchmarks and reduces the average optimality gap from 5.01% to 0.59% on average across several combinatorial optimization problems through dual reward fine tuning. v) AI practitioners can leverage HeurAgenix to automate the design and selection of heuristics for complex CO problems, potentially enabling scalable and generalizable solutions with increased adaptability and reduced reliance on manual rule design.  |
| Learning to Skip the Middle Layers of Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.21103) or [HuggingFace](https://huggingface.co/papers/2506.21103))| Laurence Aitchison, tim-lawson | i) The paper proposes a novel Transformer architecture with a gating mechanism to dynamically skip middle layers based on input token complexity. ii) The research investigates whether skipping redundant middle layers in Transformers, as suggested by interpretability research, can improve the trade-off between performance and computational cost. iii) The methodology involves learning a gating mechanism that bypasses a symmetric span of central blocks, combined with gated attention and sandwich/peri-layernorm schemes, plus adaptive regularization to encourage sparsity. iv) At the scales investigated, the proposed architecture does not improve the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. v) The implication for AI practitioners is that the specific middle-layer skipping strategy explored here does not demonstrably improve efficiency in Transformers within the tested configurations and scales, warranting exploration of different conditional computation strategies or scaling to larger models.  |
| MuseControlLite: Multifunctional Music Generation with Lightweight
  Conditioners (Read more on [arXiv](https://arxiv.org/abs/2506.18729) or [HuggingFace](https://huggingface.co/papers/2506.18729))| Bo-Rui Chen, Sheng-Ping Yang, Weijaw Lee, Shih-Lun Wu, fundwotsai2001 | i) MuseControlLite is introduced as a lightweight fine-tuning mechanism for controllable text-to-music generation. ii) The research objective is to enhance the control accuracy of text-to-music generation models using time-varying musical attributes and reference audio signals with reduced trainable parameters. iii) The methodology involves augmenting a decoupled cross-attention mechanism with positional embeddings in diffusion Transformers. iv) Results show that adding rotary positional embeddings increases control accuracy from 56.6% to 61.1% in melody control while using 6.75 times fewer trainable parameters, with 85M trainable parameters in total. v) MuseControlLite offers AI practitioners a parameter-efficient fine-tuning strategy for integrating time-varying musical conditions into pre-trained text-to-music models, facilitating creative applications like audio inpainting and outpainting with improved controllability.  |
