

## Papers for 2025-06-18

| Title | Authors | Summary |
|-------|---------|---------|
| Scaling Test-time Compute for LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2506.12928) or [HuggingFace](https://huggingface.co/papers/2506.12928))| Siwei Wu, Hanhao Li, King Zhu, Wangchunshu, zhangysk | i) This paper explores the application of test-time scaling (TTS) methods to enhance the performance of language agent frameworks. ii) The main objective is to systematically investigate and analyze the impact of various TTS strategies, including parallel sampling, sequential revision, verification/merging techniques, and diversified rollouts, on language agent effectiveness. iii) The study involves comparative ablation experiments using the SmoLAgents framework and GPT-4.1 as the base model, with the GAIA benchmark dataset. iv) Results show that Best-of-N (BoN) sampling achieved the best performance gains, with an eight-point improvement over the baseline, and list-wise methods outperformed other verification/merging approaches; multi-agent rollouts further improved performance. v) AI practitioners can improve agent performance by strategically scaling test-time compute, using methods like BoN sampling and list-wise verification, and benefit from diverse rollout strategies in agentic frameworks.  |
| LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14429) or [HuggingFace](https://huggingface.co/papers/2506.14429))| Ziwei He, Qipeng Guo, Zengfeng Huang, Zhigeng Liu, LiuXR | LongLLaDA explores long-context capabilities in diffusion LLMs, revealing stable perplexity and localized perception during context extrapolation. This research investigates whether diffusion LLMs maintain consistent performance on long documents. Comparing diffusion LLMs (LLaDA) and auto-regressive LLMs (LLaMA3) through perplexity and Needle-In-A-Haystack (NIAH) tasks reveals that LLaDA retrieves information from the nearest 4k window, demonstrating a local perception, in contrast to auto-regressive LLMs' performance collapse beyond 8k tokens. LongLLaDA, a training-free method integrating LLaDA with NTK-based RoPE extrapolation, extends the context window to 24k tokens while maintaining performance, although aggregation tasks show performance limitations. The study identifies task-dependent capabilities in diffusion LLMs, indicating diffusion LLMs are superior in QA but lag in aggregation tasks compared to auto-regressive LLMs, offering insights into diffusion LLM application boundaries.  |
| Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes
  Correct Reasoning in Base LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14245) or [HuggingFace](https://huggingface.co/papers/2506.14245))| yangwang92, MasterVito, VEWOXIC, shun-zheng, XumengWen | i) This paper introduces CoT-Pass@K as a novel metric to evaluate correct reasoning within reinforcement learning with verifiable rewards (RLVR) for LLMs. ii) The research aims to address the question of whether RLVR genuinely incentivizes correct reasoning in LLMs, beyond merely finding correct answers. iii) The methodology involves theoretical analysis of RLVR optimization dynamics and empirical validation using a DeepSeek-R1-0528-Qwen3-8B LLM as an automated verifier to evaluate chain-of-thought (CoT) correctness. iv) Results show that RLVR improves CoT-Pass@K for all values of K, indicating incentivization of correct reasoning, and early training stages exhibit increased P(CC|CA)(q) values. v) AI practitioners should consider CoT-Pass@K as a more reliable metric for evaluating reasoning progress in RLVR-tuned LLMs and focus on approaches that directly incentivize correct CoTs to improve reasoning capabilities.  |
| Stream-Omni: Simultaneous Multimodal Interactions with Large
  Language-Vision-Speech Model (Read more on [arXiv](https://arxiv.org/abs/2506.13642) or [HuggingFace](https://huggingface.co/papers/2506.13642))| Yang Feng, Yan Zhou, Qingkai Fang, Shoutao Guo, Shaolei Zhang | i) Stream-Omni is introduced, a large language-vision-speech model that uses efficient text-centric modality alignments for multimodal interactions. ii) The research addresses efficient and flexible modality alignment in large multimodal models (LMMs) to support text, vision, and speech interactions. iii) Stream-Omni employs an LLM backbone and aligns vision using sequence-dimension concatenation, and speech with a CTC-based layer-dimension mapping to the text modality. iv) Experiments show Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks, using only 23,000 hours of speech data. v) AI practitioners can utilize Stream-Omni's efficient alignment approach to build multimodal systems using smaller speech datasets by leveraging layer-dimension mapping.  |
| Efficient Medical VIE via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.13363) or [HuggingFace](https://huggingface.co/papers/2506.13363))| Chong Li, Chenglin Zhu, Lijun Liu, zhaocheng, lryyyy | i) This paper introduces Reinforcement Learning with Verifiable Rewards (RLVR) for efficient medical Visual Information Extraction (VIE) using limited annotated data. ii) The main objective is to improve the performance of medical VIE models with only 100 annotated samples by addressing domain-specific schemas and high annotation costs. iii) The methodology employs a diversified dataset, a balanced precision-recall reward mechanism, and innovative sampling strategies to fine-tune a Qwen2.5-VL-7B model. iv) The results show the RLVR model achieves state-of-the-art performance on medical VIE tasks, improving F1 scores, precision, and recall, but experiences performance degradation on dissimilar, general VIE datasets; specifically, the model achieved an F1 score of 77.81 on the medical VIE task. v) The principal implication for AI practitioners is that domain-specific optimization, including task-specific reward mechanisms and reasoning strategies within the RLVR framework, are critical for enhancing VIE performance in specialized fields, especially when annotation resources are scarce.  |
| Reasoning with Exploration: An Entropy Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.14758) or [HuggingFace](https://huggingface.co/papers/2506.14758))| Wayne Xin Zhao, Bo Dai, Xuekai Zhu, Shaohan Huang, daixuancheng | i) The paper introduces an entropy-augmented reinforcement learning method to improve language model reasoning. ii) The research aims to enhance language model reasoning capabilities by explicitly encouraging exploration through an entropy-based reward shaping. iii) The methodology augments the advantage function in policy gradient methods (PPO and GRPO) with a clipped, gradient-detached entropy term related to token prediction probabilities. iv) The method achieves significant gains on the Pass@K metric, an estimator of LM reasoning capabilities, improving performance by +6.2 on AIME25 Pass@K even with K=256. v) AI practitioners can leverage the entropy-based advantage shaping to improve exploratory reasoning in reinforcement learning fine-tuning of language models, particularly to mitigate performance plateaus and push boundaries on complex reasoning tasks.  |
| Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just
  Like an Olympiad Team (Read more on [arXiv](https://arxiv.org/abs/2506.14234) or [HuggingFace](https://huggingface.co/papers/2506.14234))| Md Rizwan Parvez, Md Kishor Morol, Salman Rahman, Md Tanzib Hosain | i) Xolver is a training-free, multi-agent framework designed to improve LLM reasoning by integrating diverse experiential modalities. ii) The research aims to enhance LLM problem-solving by enabling the accumulation and application of experiential knowledge, mirroring expert human problem solvers. iii) Xolver utilizes a multi-agent architecture incorporating external and self-retrieval, tool use, agent collaboration, agent-driven evaluation, and iterative refinement, leveraging both open-weight and proprietary models. iv) Xolver achieves a new best result of 98.1% on GSM8K and 94.4% on AIME'24, often surpassing existing specialized reasoning agents and larger LLMs even when using lightweight backbones like QWQ-32B. v) The results suggest that AI practitioners can improve reasoning performance by implementing holistic experience learning in LLMs.  |
| QFFT, Question-Free Fine-Tuning for Adaptive Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.12860) or [HuggingFace](https://huggingface.co/papers/2506.12860))| Ke Ji, Yukang Lin, Fei Yu, Junxiao Xu, lwl-uestc | QFFT is a novel fine-tuning technique enabling adaptive reasoning in large language models. This research aims to mitigate overthinking in chain-of-thought models by enabling adaptive selection between short and long reasoning patterns. The proposed Question-Free Fine-Tuning (QFFT) approach involves fine-tuning models solely on long chain-of-thought responses, discarding the input questions during training. Experiments on mathematical datasets demonstrate that QFFT reduces average response length by over 50% while maintaining performance comparable to supervised fine-tuning. QFFT enables AI practitioners to deploy more efficient reasoning models that dynamically adjust complexity based on task demands.  |
| Can LLMs Generate High-Quality Test Cases for Algorithm Problems?
  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure (Read more on [arXiv](https://arxiv.org/abs/2506.12278) or [HuggingFace](https://huggingface.co/papers/2506.12278))| Xue Xia, Zexi Kuang, Zheyuan Yang, yilunzhao | TestCase-Eval is introduced as a benchmark to evaluate LLMs' ability to generate test cases for algorithm problems. The research investigates whether LLMs can generate high-quality test cases that match or surpass those designed by human experts. The methodology involves assessing LLMs on two tasks: Fault Coverage, measuring the ability to explore input scenarios, and Fault Exposure, evaluating the capacity to expose flaws in incorrect code implementations. Experiments with 19 LLMs reveal that the best-performing model, Qwen3-32B, achieves only 43.8% on the Fault Exposure task, contrasting with 93.3% for human experts. The results indicate that current LLMs face significant challenges in generating targeted test inputs, a factor that can be crucial for testing and validation efforts in AI-driven code development.  |
| Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC
  Transpilation with Testing Guarantees (Read more on [arXiv](https://arxiv.org/abs/2506.14606) or [HuggingFace](https://huggingface.co/papers/2506.14606))| Abdulrahman Mahmoud, Celine Lee, Chaimaa Abi, Sarim-Hash, ahmedheakl | i) The paper introduces Guaranteed Guess (GG), a language model-based assembly transpiler with testing guarantees for CISC-to-RISC translation. ii) The research aims to develop an accurate and efficient method for translating x86 assembly to ARM or RISC-V assembly. iii) The methodology involves a custom-trained, architecture-aware language model, tokenizer extension, and integration with software testing constructs for validation. iv) GG achieves 99.39% accuracy on HumanEval programs when translating to ARMv8, with 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage than Rosetta 2 in real-world binaries. v) GG provides AI/ML practitioners with a potential solution for efficient cross-ISA binary translation that can avoid the overhead of traditional emulation or virtualization methods.  |
| Align Your Flow: Scaling Continuous-Time Flow Map Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.14603) or [HuggingFace](https://huggingface.co/papers/2506.14603))| Karsten Kreis, Sanja Fidler, Amirmojtaba Sabour | i) This paper introduces Align Your Flow (AYF), a novel distillation method for scaling continuous-time flow map generative models. ii) The research aims to improve few-step generative model performance by developing new training objectives and techniques for flow map distillation. iii) The methodology involves introducing two new continuous-time objectives, EMD and LMD, generalizing existing objectives and leveraging autoguidance and adversarial finetuning. iv) AYF achieves state-of-the-art few-step generation performance on ImageNet 64x64 and 512x512, using small neural networks, and outperforms existing methods in text-conditioned synthesis; for instance, it allows for 4-step sampling on ImageNet that is as fast or faster than previous works' single step generation, while retaining high diversity. v) AYF offers AI practitioners an improved method for distilling generative models into efficient few-step samplers, enabling faster image generation without sacrificing diversity, relevant for applications where real-time or low-compute inference is crucial.  |
| CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language
  Models in Tool-Calling Error Scenarios (Read more on [arXiv](https://arxiv.org/abs/2506.13977) or [HuggingFace](https://huggingface.co/papers/2506.13977))| Junjie Ye, Siyu Yuan, Zehui Chen, Shiting Huang, CostaliyA | i) This paper introduces CRITICTOOL, a new benchmark for evaluating the self-critique capabilities of Large Language Models (LLMs) in tool-calling scenarios. ii) The main objective is to provide a more nuanced evaluation of how LLMs detect, diagnose, and recover from errors during complex tool utilization. iii) The methodology involves an evolutionary strategy for dataset construction, generating diverse tool-use errors categorized by their source (internal model-driven vs. external environment). iv) Experiments on CRITICTOOL show that GPT-4 achieves an overall score of 69.01, indicating superior self-critique performance, while tool-use-finetuned models generally perform poorly. v) CRITICTOOL's fine-grained error categorization and analysis provide AI practitioners with insights into the limitations of current LLMs' tool-use capabilities, guiding the development of more robust tool-calling systems.  |
| xbench: Tracking Agents Productivity Scaling with Profession-Aligned
  Real-World Evaluations (Read more on [arXiv](https://arxiv.org/abs/2506.13651) or [HuggingFace](https://huggingface.co/papers/2506.13651))| Xiaobo Hu, Yang Liu, Yixin Ren, Kaiyuan Chen, Liuff23 | i) xbench is introduced as a novel evaluation suite for assessing AI agent productivity in real-world professional settings. ii) The main objective is to develop a dynamic, profession-aligned evaluation framework that bridges the gap between AI agent capabilities and real-world productivity, targeting commercially significant domains with evaluation tasks defined by industry professionals. iii) The methodology involves creating profession-aligned evaluation sets with metrics correlated to productivity value, and presenting two benchmarks: Recruitment (50 tasks) and Marketing (50 advertiser requirements, 836 influencers). iv) The primary result shows that o3 ranks first in both recruitment and marketing benchmarks, and o3 achieves a score of 78.5 on the recruitment benchmark. v) The principal implication for AI practitioners is that xbench provides a value-oriented framework for guiding and predicting the development of effective, domain-specific AI agents, enabling the tracking of product capabilities over time and predicting Technology-Market Fit (TMF).  |
| Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse
  Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2506.14002) or [HuggingFace](https://huggingface.co/papers/2506.14002))| Zhuoran Yang, Tianhao Wang, Xuyuan Xiong, Heejune Sheen, Siyu Chen | i) This paper presents a novel Sparse Autoencoder (SAE) training algorithm, Group Bias Adaptation (GBA), with provable feature recovery guarantees for Large Language Models (LLMs). ii) The primary objective is to achieve theoretically grounded feature recovery in LLMs using SAEs by addressing the limitations of existing methods. iii) GBA utilizes "bias adaptation" to directly control neuron sparsity, and the analysis involves a statistical framework formalizing polysemantic features as sparse mixtures of monosemantic concepts. iv) The research theoretically proves that GBA correctly recovers all monosemantic features under specific statistical model conditions and demonstrates superior empirical performance on LLMs up to 1.5B parameters, achieving the sparsity-loss frontier and learning more consistent features. v) AI practitioners can leverage GBA as a more robust and theoretically sound alternative to conventional regularization techniques for training SAEs, enabling enhanced mechanistic interpretability in LLMs.  |
| EfficientVLA: Training-Free Acceleration and Compression for
  Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2506.10100) or [HuggingFace](https://huggingface.co/papers/2506.10100))| Chang Zou, Luo Zhongwei, Zichen Wen, Yuhao Wang, Yantai Yang | i) The paper introduces EfficientVLA, a training-free framework for accelerating and compressing vision-language-action (VLA) models. ii) The research aims to reduce computational and memory demands of VLA models to improve their deployment feasibility. iii) The methodology involves pruning inconsequential layers from the language module, task-aware visual token selection, and caching intermediate features within the diffusion-based action head. iv) EfficientVLA achieves a 1.93× inference speedup and reduces FLOPs to 28.9% on CogACT with a 0.6% success rate drop in the SIMPLER benchmark. v) EfficientVLA offers AI practitioners a computationally efficient method for deploying large-scale VLA models on resource-constrained platforms without requiring retraining.  |
| VideoMolmo: Spatio-Temporal Grounding Meets Pointing (Read more on [arXiv](https://arxiv.org/abs/2506.05336) or [HuggingFace](https://huggingface.co/papers/2506.05336))| Zhiqiang Shen, Abdelrahman Shaker, Hanan Gani, Ghazi Shazan Ahmad, ahmedheakl | VideoMolmo is presented as a large multimodal model for spatio-temporal pointing conditioned on textual input in videos. The research aims to improve fine-grained localization and reasoning in video grounding tasks. It decomposes video grounding into pointing and mask generation, using a temporal module with attention and a mask fusion pipeline with SAM2 for temporal consistency. VideoMolmo achieves a 5.4 percentage point average improvement on the introduced VPoS-Bench benchmark compared to baselines, and a 9.5 pp improvement on MeViS referring segmentation. AI practitioners can leverage VideoMolmo for enhanced video understanding applications requiring precise spatio-temporal reasoning.  |
| Ambient Diffusion Omni: Training Good Models with Bad Data (Read more on [arXiv](https://arxiv.org/abs/2506.10038) or [HuggingFace](https://huggingface.co/papers/2506.10038))| Constantinos Daskalakis, Antonio Torralba, Adam Klivans, Giannis Daras, adrianrm | i) The paper introduces Ambient Diffusion Omni, a framework that improves diffusion model training by leveraging low-quality, synthetic, and out-of-distribution images. ii) The primary research objective is to extract useful signal from degraded and non-target data to enhance the image generation capabilities of diffusion models. iii) The methodology involves modulating the training process based on each sample's utility, exploiting spectral power law decay and locality properties of natural images, and employing time-conditional classifiers to distinguish between noised distributions. iv) The framework achieves state-of-the-art ImageNet FID, demonstrating its ability to train successfully with synthetically corrupted images and shows that with COCO dataset, Ambient-o achieved a remarkable FID of 10.61, significantly improving the baseline FID of 12.37. v) The principal implication for AI practitioners is a cost-effective and efficient strategy for expanding training datasets using readily available but often discarded data sources, improving generative model quality and diversity without extensive data curation.  |
| Optimizing Length Compression in Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2506.14755) or [HuggingFace](https://huggingface.co/papers/2506.14755))| Mingyang Fu, Dongping Chen, Zhengxiang Cheng, zhoutianyi | i) The paper introduces LC-R1, a post-training method to compress reasoning chains in large reasoning models (LRMs). ii) The main objective is to reduce redundant reasoning steps ("invalid thinking") in LRMs while maintaining accuracy. iii) LC-R1 uses Group Relative Policy Optimization (GRPO) with a length reward for conciseness and a compression reward to remove invalid reasoning. iv) Experiments show LC-R1 achieves a ~50% reduction in sequence length with only a ~2% drop in accuracy on reasoning benchmarks. v) LC-R1 provides AI practitioners a method to improve the computational efficiency of LRMs by reducing verbose reasoning chains without significantly sacrificing accuracy.  |
| Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning
  for LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14731) or [HuggingFace](https://huggingface.co/papers/2506.14731))| Ding Liu, Deng Zhao, Cai Chen, Bin Hu, Ring Team | i) Ring-lite is a Mixture-of-Experts (MoE) large language model optimized via reinforcement learning (RL) for efficient and robust reasoning. ii) The research aims to improve reasoning capabilities of large language models through stable RL training. iii) The methodology introduces a joint training pipeline integrating distillation with RL using Constrained Contextual Computation Policy Optimization (C3PO) and a two-stage training paradigm. iv) Ring-lite achieves 76.61% on AIME2024 and 69.11% on AIME2025 while activating only one-third of the parameters required by comparable models. v) C3PO enhances training stability and computational throughput in RL, potentially benefiting AI practitioners aiming to scale reasoning abilities in LLMs using MoE architectures.  |
| Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time
  Markers (Read more on [arXiv](https://arxiv.org/abs/2506.14702) or [HuggingFace](https://huggingface.co/papers/2506.14702))| Sara Hooker, Ahmet Üstün, Adrien Morisot, Julia Kreutzer, Daniel D'souza | This paper introduces a training protocol leveraging training-time markers to improve controllability and performance on underrepresented features (long tail) in machine learning models. The research question explores optimizing training to improve controllability and long-tail performance at inference time. The methodology involves creating a taxonomy of data characteristics and task provenance for explicit control of generation attributes and implicit conditioning at inference, fine-tuning a base model to infer these markers automatically. The primary results show an average lift of 5.7% win rates in open-ended generation quality, with over 9.1% gains in underrepresented domains, and relative lifts of up to 14.1% on tasks like CodeRepair with 35.3% absolute improvements in length instruction following evaluations. The principal implication for AI practitioners is a principled and flexible approach to improving performance on long-tail data while providing users with a set of control levers the model is trained to be responsive to which can be optionally used at inference.  |
| Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic
  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise
  Pooled Representations (Read more on [arXiv](https://arxiv.org/abs/2506.13901) or [HuggingFace](https://huggingface.co/papers/2506.13901))| Utkarsh Bhatt, Danush Khanna, Chhavi Sharma, Abhilekh Borah, amanchadha | i) The paper introduces the Alignment Quality Index (AQI), a novel geometric metric for assessing large language model (LLM) alignment. ii) The research aims to provide a decoding-invariant measure of LLM alignment by analyzing latent space separation between safe and unsafe activations, addressing limitations of behavioral proxies. iii) The methodology involves combining the Davies-Bouldin score, Dunn index, Xie-Beni index, and Calinski-Harabasz index across various formulations and introduces the LITMUS dataset for evaluation. iv) Empirical tests demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics; for example, a Delta AQI exceeding 10-20% has been observed to correlate with early-stage alignment erosion. v) AQI offers AI practitioners a behavior-agnostic safety auditing tool that provides early warning signals for alignment faking, promoting the development of more robustly aligned LLMs.  |
| CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2506.13599) or [HuggingFace](https://huggingface.co/papers/2506.13599))| Yong Li, Jian Yuan, Yuwei Du, JJ-TMT | i) CAMS is an agentic framework using a language-based urban foundation model for human mobility simulation. ii) The research objective is to improve the controllability, accuracy, and generalizability of human mobility simulation by incorporating urban spatial knowledge into LLMs. iii) The key methodology integrates MobExtractor to extract mobility patterns, GeoGenerator to generate geospatial knowledge with an enhanced CityGPT, and TrajEnhancer to refine trajectories using direct preference optimization (DPO). iv) Experiments on real-world datasets show CAMS achieves superior performance in mobility simulation, with 11 out of 16 metrics showing improvement and a highest CMRR score; however, specific quantitative values for those metrics are not provided in the summary. v) CAMS provides AI practitioners a new paradigm for integrating agentic frameworks with urban-knowledgeable LLMs for enhanced mobility simulation and spatial reasoning.  |
| Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.09033) or [HuggingFace](https://huggingface.co/papers/2506.09033))| Jiaxuan You, Tao Feng, Haozhen Zhang | i) The paper introduces Router-R1, a reinforcement learning framework for multi-round routing and aggregation of large language models (LLMs). ii) The main objective is to coordinate multiple LLMs in a sequential decision process to solve complex tasks, optimizing for both performance and cost. iii) Router-R1 instantiates the router itself as an LLM, interleaving "think" and "route" actions, and employs a rule-based reward function comprising format, outcome, and cost rewards to guide training. iv) Experiments on seven QA datasets demonstrate that Router-R1 outperforms several baselines, achieving a 0.416 average exact match score with the Qwen base model, while exhibiting strong generalization and cost management. v) Router-R1 enables AI practitioners to dynamically orchestrate diverse LLMs, balancing performance and computational efficiency for complex reasoning tasks through reinforcement learning.  |
| Mixture-of-Experts Meets In-Context Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.05426) or [HuggingFace](https://huggingface.co/papers/2506.05426))| Daoyi Dong, Zican Hu, Haoru Li, Fuhong Liu, Wenhao0 | i) This paper introduces T2MIR, a novel mixture-of-experts architecture for in-context reinforcement learning (ICRL). ii) The main objective is to improve ICRL adaptability by addressing the multi-modality of state-action-reward data and the heterogeneity of decision tasks. iii) The methodology involves replacing the feedforward layer in transformer-based decision models with a token-wise MoE and a task-wise MoE, coupled with a contrastive learning method for task routing. iv) Experiments demonstrate T2MIR significantly facilitates in-context learning, outperforming baselines; for example, T2MIR reduces Cheetah-Vel return from -86.1 to -68.9 compared to existing models. v) T2MIR offers AI practitioners a scalable architectural enhancement for advancing ICRL, potentially improving performance on tasks requiring complex input processing and task diversification, however, details on model training/validation remain unclear.  |
| TR2M: Transferring Monocular Relative Depth to Metric Depth with
  Language Descriptions and Scale-Oriented Contrast (Read more on [arXiv](https://arxiv.org/abs/2506.13387) or [HuggingFace](https://huggingface.co/papers/2506.13387))| Hongliang Ren, Long Bai, Yiming Huang, Beilei Cui | TR2M is a framework that transfers monocular relative depth to metric depth using language descriptions and scale-oriented contrast. The research aims to address scale uncertainty in monocular relative depth estimation to improve its practical applicability. TR2M fuses image and text features with cross-modality attention, constructs confident pseudo metric depth for supervision, and employs scale-oriented contrastive learning. Experiments demonstrate TR2M achieves strong performance on seen datasets and superior zero-shot capabilities on five unseen datasets, with 19M trainable parameters. TR2M can be used to develop lightweight and generalizable monocular metric depth estimation models utilizing text descriptions for improved performance across diverse domains. The improvement over DepthAnything with linear fit on NYUv2 is a decrease in AbsRel from 0.055 to 0.082.  |
| Universal Jailbreak Suffixes Are Strong Attention Hijackers (Read more on [arXiv](https://arxiv.org/abs/2506.12880) or [HuggingFace](https://huggingface.co/papers/2506.12880))| Mahmood Sharif, Mor Geva, MatanBT | i) This paper investigates the mechanics of suffix-based jailbreak attacks, specifically the GCG attack, against safety-aligned LLMs. ii) The research aims to understand the underlying mechanisms that drive the efficacy and universality of GCG suffix-based jailbreaks. iii) The study employs attention knockout, activation patching, and a novel dot-product-based dominance metric to analyze information flow and contextual hijacking in LLMs. iv) Results show that GCG jailbreaks are shallow, relying on the adv→chat flow, exhibiting irregular dominance in contextualization with universal suffixes demonstrating higher hijacking strength (Spearman correlation of p = 0.55 at layer 20), and that GCG universality can be enhanced by a factor of up to 5 through a hijacking-enhanced objective function. v) AI practitioners can use hijacking suppression as a training-free framework, with an up to 10x attack success rate reduction.  |
| EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction (Read more on [arXiv](https://arxiv.org/abs/2506.12015) or [HuggingFace](https://huggingface.co/papers/2506.12015))| Yu-Chiang Frank Wang, Kai-Po Chang, Yu-Chu Yu, Hsi-Che Lin | EMLOC enables memory-efficient fine-tuning by using a downstream-aware emulator. The research investigates reducing the memory overhead of fine-tuning large foundation models to match inference costs. EMLoC constructs a lightweight emulator via activation-aware SVD on a downstream calibration set, then fine-tunes it using LoRA with a novel correction algorithm. Experiments show EMLoC enables fine-tuning a 38B model on a single 24GB GPU and outperforms baselines on VQA, with WC-VQA results improving from 43.1 to 48.8 after fine-tuning. EMLoC provides AI practitioners with a method to fine-tune large models in resource-constrained environments, using a memory budget equivalent to inference.  |
