

## Papers for 2025-06-04

| Title | Authors | Summary |
|-------|---------|---------|
| CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning
  Capabilities of VLMs (Read more on [arXiv](https://arxiv.org/abs/2505.24120) or [HuggingFace](https://huggingface.co/papers/2505.24120))| xuchensong, rockman24, jiangbopei, shawn0wang, qiuwj | i) The paper introduces CSVQA, a Chinese multimodal benchmark for evaluating scientific reasoning in VLMs. ii) The research aims to comprehensively assess VLMs' ability to integrate domain knowledge and visual evidence for scientific reasoning. iii) The methodology involves constructing a dataset of 1,378 STEM questions with curated explanations and evaluating 15 VLMs using a rigorous evaluation protocol. iv) The top-performing model achieved 49.6% accuracy, indicating a significant performance gap in scientific reasoning for current VLMs. v) The principal implication highlights the need for AI practitioners to focus on improving scientific reasoning capabilities in VLMs, especially concerning domain-specific knowledge integration and visual reasoning for complex, multimodal scenarios. The paper also proposes a process-tracing method for rigorous evaluation of reasoning ability.  |
| UniWorld: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03147) or [HuggingFace](https://huggingface.co/papers/2506.03147))| Yuwei Niu, Xinhua Cheng, Zongjian Li, BestWishYsh, LanguageBind | i) UniWorld is a unified generative framework for image perception and manipulation based on semantic encoders. ii) The main research objective is to explore and develop a unified model for both image perception and manipulation tasks without relying on VAEs. iii) The methodology involves a unified architecture leveraging pre-trained visual-language models and contrastive semantic encoders. iv) UniWorld outperforms BAGEL on image editing benchmarks using only 1% of BAGEL's training data (2.7M samples vs. 2665M samples) and achieves comparable performance on text-to-image generation benchmarks. v) The principal implication for AI practitioners is the demonstration of a unified architecture using semantic encoders for image tasks, offering a data-efficient alternative to VAE-based approaches.  |
| VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in
  Multi-Agent Environments (Read more on [arXiv](https://arxiv.org/abs/2506.02387) or [HuggingFace](https://huggingface.co/papers/2506.02387))| Xinlei Chen, Xiangmin Yi, Zhexuan Xu, HuiningYuan, zelaix | i) The paper introduces VS-BENCH, a new multimodal benchmark for evaluating Vision Language Models (VLMs) in strategic reasoning and decision-making within multi-agent environments. ii) The primary objective is to assess VLMs' capabilities in strategic reasoning and decision-making in visually-rich, multi-agent interactive scenarios. iii) The methodology involves offline evaluation of strategic reasoning via next-action prediction accuracy and online evaluation of decision-making via normalized episode return across eight vision-grounded environments. iv) Experiments on fourteen leading VLMs revealed that the best models achieved a 47.8% next-action prediction accuracy and a 24.3% normalized episode return. v) The benchmark highlights a significant gap between existing VLMs and optimal performance in strategic multi-agent interactions, suggesting the need for improved visual information extraction and reasoning capabilities for AI practitioners developing multi-agent systems.  |
| OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for
  Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.03135) or [HuggingFace](https://huggingface.co/papers/2506.03135))| Xinqiang Yu, Wenyao Zhang, Shaochen Zhang, Mengdi Jia, qizekun | i) This paper introduces OmniSpatial, a comprehensive spatial reasoning benchmark for vision-language models (VLMs). ii) The research aims to evaluate the spatial reasoning capabilities of VLMs across dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking. iii) The methodology involves constructing a dataset of over 1.5K question-answer pairs derived from internet data, standardized tests, and driving exam questions, followed by manual annotation and state-of-the-art VLM evaluation. iv) Results show that state-of-the-art VLMs peak at 57% accuracy on OmniSpatial, significantly below human performance and existing benchmarks, particularly struggling with geometric reasoning and non-egocentric perspective-taking. v) The implication for AI practitioners is a clear need for developing VLMs with enhanced spatial reasoning capabilities, specifically addressing identified limitations in geometric understanding and perspective-taking for robust real-world applications.  |
| Visual Embodied Brain: Let Multimodal Large Language Models See, Think,
  and Control in Spaces (Read more on [arXiv](https://arxiv.org/abs/2506.00123) or [HuggingFace](https://huggingface.co/papers/2506.00123))| Guanzhou Chen, Gen Luo, robot-haonan, Cusyoung, ganlinyang | i) The paper introduces Visual Embodied Brain (VeBrain), a unified framework that enables Multimodal Large Language Models (MLLMs) to perceive, reason, and control robots in real-world environments. ii) The main objective is to unify multimodal understanding, visual-spatial reasoning, and physical interaction capabilities within a single MLLM for robotic control. iii) The methodology reformulates robotic control into text-based MLLM tasks in a 2D visual space and uses a robotic adapter to convert textual control signals into motion policies, training the system on a new VeBrain-600k dataset. iv) VeBrain achieves a +5.6% improvement on MMVet compared to Qwen2.5-VL and demonstrates a +50% average gain in legged robot tasks. v) The principal implication for AI practitioners is a framework to leverage MLLMs for enhanced adaptability, flexibility, and compositional capabilities in robotic applications, offering a practical architecture for integrating perception, reasoning, and control.  |
| SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.02096) or [HuggingFace](https://huggingface.co/papers/2506.02096))| Hang Yan, Zichen Liu, Xiangyan Liu, Jinjie Ni, Jakumetsu | i) SynthRL introduces a scalable pipeline for synthesizing verifiable data to enhance visual reasoning in VLMs trained with RLVR. ii) The research investigates whether synthesized RL data with correctness and distribution guarantees can improve the performance of VLMs. iii) SynthRL employs a three-stage process: seed question selection based on model difficulty, targeted variant synthesis using a powerful VLM, and a guaranteed verification step to ensure correctness and difficulty enhancement. iv) Experiments on the MMK12 dataset resulted in synthesizing over 3.3K additional verifiable questions from 8K seeds, achieving consistent gains across five out-of-domain visual math reasoning benchmarks, including a +1.9% improvement on MathVerse, reaching 53.5% average accuracy. v) AI practitioners can utilize SynthRL to automatically generate scalable, high-quality training data, improving the out-of-domain generalization and reasoning capabilities of VLMs, particularly on challenging examples.  |
| MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.01674) or [HuggingFace](https://huggingface.co/papers/2506.01674))| Rui Xie, Kepan Nan, Tiehan Fan, Yipeng Du, yingtai | i) MotionSight introduces a zero-shot prompting method and a large-scale dataset to improve fine-grained motion understanding in MLLMs. ii) The paper investigates how to unlock inherent capabilities and boost MLLMs' motion perception by decoupling object and camera motion cues. iii) The methodology involves object-centric visual spotlighting, motion blur prompting, and the curation of the MotionVid-QA dataset with SFT and preference data. iv) Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models, with a 3.4% improvement in category average on MotionBench. v) The MotionVid-QA dataset and MotionSight prompting techniques provide AI practitioners with resources and methods to enhance MLLM performance in tasks requiring nuanced motion understanding.  |
| Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate
  Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.03065) or [HuggingFace](https://huggingface.co/papers/2506.03065))| Maosen Zhao, Xianfang Zeng, skicy, wchengad, PengtaoChen | i) The paper introduces Sparse-vDiT, a framework designed to accelerate Video Diffusion Transformers (vDiTs) by exploiting attention map sparsity. ii) The research aims to mitigate the high computational cost associated with the quadratic complexity of attention mechanisms in vDiTs for long sequence video generation. iii) The methodology involves identifying recurring sparsity patterns in vDiDT attention maps, developing pattern-optimized sparse kernels, and implementing an offline sparse diffusion search algorithm for optimal configuration. iv) Sparse-vDiT achieves up to 2.38x theoretical FLOP reduction and 1.85x inference speedup on HunyuanVideo while maintaining comparable generation quality (PSNR reaching 27.09). v) AI practitioners can leverage Sparse-vDiT's sparsity exploitation techniques to improve the inference efficiency of vDiTs in video generation tasks without significantly compromising visual fidelity.  |
| GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2506.03143) or [HuggingFace](https://huggingface.co/papers/2506.03143))| Jianwei Yang, vyokky, Ray2333, cckevinn, qianhuiwu | GUI-Actor is a novel VLM-based method for GUI agent visual grounding that eliminates coordinate generation. The research aims to address limitations of coordinate-based visual grounding approaches in GUI agents by using a coordinate-free method that emphasizes alignment with visual patch tokens. GUI-Actor introduces an attention-based action head that learns to align an <ACTOR> token with relevant visual patch tokens, enabling the model to propose action regions in a single forward pass and employs a grounding verifier to select the most plausible region. GUI-Actor-7B achieves scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL, outperforming UI-TARS-72B (38.1) on ScreenSpot-Pro. The approach allows VLMs to endow effective GUI grounding capabilities without compromising their general-purpose strengths, making it relevant for AI engineers building GUI agents.  |
| AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03126) or [HuggingFace](https://huggingface.co/papers/2506.03126))| Ying Shan, Yixiao Ge, Yuying Ge, liyz, qiulu66 | i) The paper introduces AnimeShooter, a new reference-guided multi-shot animation dataset to improve coherent video generation with character consistency. ii) The primary objective is to provide a dataset that addresses the limitations of existing video datasets for animation generation, specifically the lack of reference images and hierarchical annotations. iii) The dataset was built using an automated pipeline involving YouTube content collection, hierarchical story script generation via Gemini, and character segmentation with Sa2VA and InternVL. iv) The AnimeShooterGen model, trained on the dataset, demonstrates improved cross-shot visual consistency and character adherence, achieving a CLIP score of 0.8022 on Shot-1 for generated content aligned with reference images. v) The dataset and the AnimeShooterGen model offer AI practitioners a structured resource for training and evaluating video generation models capable of maintaining character identity and narrative coherence across multiple shots, essential for creating engaging animated content.  |
| Native-Resolution Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.03131) or [HuggingFace](https://huggingface.co/papers/2506.03131))| Yiyuan Zhang, Wanli Ouyang, Xiangyu Yue, Lei Bai, GoodEnough | i) This paper introduces Native-resolution image synthesis, a generative modeling paradigm for synthesizing images at arbitrary resolutions and aspect ratios. ii) The main objective is to overcome the limitations of conventional fixed-resolution, square-image methods. iii) The methodology involves a Native-resolution diffusion Transformer (NiT) architecture trained on ImageNet using dynamic tokenization, variable-length sequence processing with Flash Attention, and axial 2D Rotary Positional Embedding. iv) A single NiT model achieves a Fréchet Inception Distance (FID) of 1.45 on the ImageNet 512 × 512 benchmark, demonstrating state-of-the-art performance and an FID of 4.11 on novel 9:16 aspect ratio images. v) AI practitioners can leverage the NiT architecture for applications requiring flexible image generation across diverse resolutions and aspect ratios, improving generalization and reducing the need for resolution-specific models.  |
| Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in
  Robotics (Read more on [arXiv](https://arxiv.org/abs/2506.00070) or [HuggingFace](https://huggingface.co/papers/2506.00070))| Jaehyung Kim, Jinwoo Shin, Huiwon Jang, Sumin Park, Dongyoung Kim | i) The paper introduces ROBOT-R1, a reinforcement learning framework to improve embodied reasoning for robot control in Large Vision-Language Models (LVLMs). ii) The main objective is to enhance LVLMs' embodied reasoning capabilities specifically for robotic control tasks, addressing limitations of Supervised Fine-Tuning (SFT). iii) ROBOT-R1 uses reinforcement learning to train LVLMs to predict the next keypoint state for task completion, conditioned on scene images and metadata, reformulating the problem as a multiple-choice question answering task. iv) Experiments show that models trained with ROBOT-R1 achieve over a 28% improvement in embodied reasoning for low-level action control compared to SFT methods; also showed a 31% improvement in task performance on EmbodiedBench Manipulation. v) The implication for AI practitioners is a novel method, ROBOT-R1, that enhances the reasoning of LVLMs for robotics-related tasks.  |
| Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.03136) or [HuggingFace](https://huggingface.co/papers/2506.03136))| Mengdi Wang, Ke Shen, Ye Tian, Ling Yang, Yinjie Wang | i) The paper introduces CURE, a reinforcement learning framework for co-evolving LLM coders and unit testers without ground-truth code supervision. ii) The research question focuses on whether a code generator and unit test generator can co-evolve without relying on ground-truth code solutions to improve LLM coding ability. iii) The methodology involves a self-play setup where a single model acts as both code generator and unit test generator, with a pairwise reward matrix based on the interactions between the generated code and unit tests. iv) After optimization on Qwen2.5-Instruct models, the ReasonFlux-Coder 7B and 14B models demonstrate a 5.3% improvement in code generation accuracy and a 9.0% improvement in Best of N accuracy. v) This research implies that AI practitioners can enhance code generation capabilities in LLMs through a co-evolutionary reinforcement learning approach that leverages unit tests for self-supervision, potentially reducing the need for labeled data.  |
| LumosFlow: Motion-Guided Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2506.02497) or [HuggingFace](https://huggingface.co/papers/2506.02497))| Jiazheng Xing, Jingyun Liang, Yichen Qian, Hangjie Yuan, Jiahao Chen | LumosFlow introduces a motion-guided framework for generating temporally coherent long videos. The research aims to improve long video generation by incorporating explicit motion guidance. The methodology involves generating keyframes using a Large Motion Text-to-Video Diffusion Model (LMTV-DM) and interpolating intermediate frames with a Latent Optical Flow Diffusion Model (LOF-DM) and Motion ControlNet. Experiments show the method achieves 15× interpolation while maintaining consistent motion and appearance. LumosFlow provides AI practitioners with a novel hierarchical long video generation pipeline leveraging motion guidance for enhanced temporal coherence, potentially reducing artifacts in synthesized videos.  |
| DINGO: Constrained Inference for Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.23061) or [HuggingFace](https://huggingface.co/papers/2505.23061))| Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Debangshu Banerjee, Tarun Suresh | i) The paper introduces DINGO, a dynamic programming-based constrained decoding algorithm for diffusion language models. ii) The main objective is to develop a decoding method for diffusion LLMs that can enforce user-specified regular expression constraints while preserving the output distribution. iii) DINGO utilizes dynamic programming to find the maximum probability output string that adheres to the constraints, modifying the DFA transition function to account for mask tokens. iv) Experiments on symbolic math and JSON generation tasks show that DINGO achieves up to a 68% improvement over unconstrained inference in terms of syntactic correctness. v) DINGO provides AI practitioners with a reliable method for generating structured outputs from diffusion LLMs, enabling the use of these models in applications requiring formal guarantees such as symbolic reasoning and schema-based data generation.  |
| RelationAdapter: Learning and Transferring Visual Relation with
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.02528) or [HuggingFace](https://huggingface.co/papers/2506.02528))| Yin Zhang, Chenglin Li, Yicheng Li, Yiren Song, Yan Gong | RelationAdapter: A new module for diffusion transformers to transfer visual relationships from paired images. The paper addresses the research question of how to effectively extract and transfer content-aware editing intent from exemplar image pairs to novel query images for visual prompt-based image editing. The proposed method uses a RelationAdapter module, a lightweight dual-branch adapter within a Diffusion Transformer (DiT), to explicitly model visual relationships between pre-edit and post-edit images. Experiments on a new Relation252K dataset show RelationAdapter significantly improves the model's ability to understand and transfer editing intent, achieving a lower MSE of 0.020 and a higher CLIP-I score of 0.905 compared to the Edit Transfer baseline. AI practitioners can use RelationAdapter to improve the performance and generalizability of visual prompt-based image editing systems by leveraging paired image examples.  |
| FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.24714) or [HuggingFace](https://huggingface.co/papers/2505.24714))| Jinsheng Huang, Xiao Luo, chunfenri, alan1027, luojunyu | i) FinMME introduces a benchmark dataset for evaluating financial multi-modal reasoning in large language models (MLLMs). ii) The research aims to address the lack of specialized evaluation datasets in the financial domain to advance MLLM development for financial applications. iii) The study involved curating a dataset of over 11,000 financial samples, developing a hierarchical evaluation framework, and introducing FinScore, an evaluation metric that penalizes hallucination. iv) Experiments showed that even advanced models like GPT-4o achieve performance of just over 50%, and FinMME demonstrated high robustness with prediction variations under different prompts remaining below 1%. v) AI practitioners should use FinMME to rigorously evaluate and improve MLLMs for financial tasks, emphasizing the importance of hallucination control for reliable financial analysis.  |
| PCoreSet: Effective Active Learning through Knowledge Distillation from
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.00910) or [HuggingFace](https://huggingface.co/papers/2506.00910))| Sung Ju Hwang, Dongseop Kim, Hyungjoon Jang, Dong Bok Lee, Seongjae Kang | i) This paper introduces ActiveKD, an active learning framework using knowledge distillation from vision-language models, and proposes Probabilistic CoreSet (PCoreSet) for sample selection. ii) The research investigates how to effectively integrate knowledge distillation with active learning by leveraging zero- and few-shot capabilities of VLMs in data-scarce scenarios. iii) The methodology uses PCoreSet, a selection strategy that maximizes coverage in the probability space of VLM predictions to select categorically diverse unlabeled samples. iv) Experiments on 11 datasets show that ActiveKD improves final-round accuracy and PCoreSet outperforms existing methods, achieving a 27.33% improvement on ImageNet with random selection using zero-shot distillation. v) ActiveKD and PCoreSet provide AI practitioners with a method to train compact, task-specific models more efficiently with limited labeled data by exploiting the inductive bias of VLMs.  |
| OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for
  Over-Reasoning Mitigation (Read more on [arXiv](https://arxiv.org/abs/2506.02397) or [HuggingFace](https://huggingface.co/papers/2506.02397))| Changwang Zhang, Jiawei Chen, Junjie Wu, jwanglux, Cynthia-1628 | OThink-R1 enables dynamic switching between fast and slow thinking modes in large reasoning models (LRMs) to mitigate over-reasoning. The research aims to address the computational inefficiency of LRMs on simple tasks by adaptively engaging explicit reasoning only when necessary. It analyzes reasoning trajectories to classify them as redundant or essential using an LLM-Judge and constructs a supervised fine-tuning (SFT) dataset. Experiments show OThink-R1 reduces token generation by 23.4% on average across QA and math datasets without compromising accuracy. This offers AI practitioners practical guidelines for developing more efficient and scalable reasoning models.  |
| FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.01144) or [HuggingFace](https://huggingface.co/papers/2506.01144))| Lior Wolf, Ariel Shaulov, Hila, itayhzn | i) FlowMo is introduced as a training-free guidance method for enhancing temporal coherence in text-to-video diffusion models. ii) The research aims to improve motion coherence in generated videos without additional training data or conditioning signals. iii) The method involves measuring patch-wise variance of appearance-debiased latent representations over time, guiding the model to reduce this variance during sampling. iv) Experiments on Wan2.1-1.3B and CogVideoX-5B show FlowMo improves the Final Score (representing overall video quality) by 6.20% and 5.26%, respectively, on the VBench benchmark. v) FlowMo offers AI practitioners a plug-and-play solution to enhance the temporal fidelity of pre-trained video diffusion models without retraining, enabling the generation of more coherent videos from existing models.  |
| Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and
  Accountability (Read more on [arXiv](https://arxiv.org/abs/2506.01789) or [HuggingFace](https://huggingface.co/papers/2506.01789))| David Anugraha, Genta Indra Winata, cryptexcode, seungone, patrickamadeus | This paper introduces DATARUBRICS, a structured framework for automated dataset quality assessment in machine learning. The research question addresses the need for systematic and quantifiable metrics to evaluate dataset quality, moving beyond descriptive datasheets. The authors propose DATARUBRICS, a rubric-based framework with ten dimensions of data quality assessed via human evaluation and LLM-as-a-judge approaches. The evaluation involved annotating 100 NeurIPS papers and analyzing data quality trends across NLP, CV, ML, and speech conferences, finding that human annotations still contain errors even after quality assurance with 26% of annotations remaining incorrect despite quality assurance. DATARUBRICS offers a reproducible, scalable solution for both dataset authors and reviewers, aiding in upholding higher data-centric research standards.  |
| ReFoCUS: Reinforcement-guided Frame Optimization for Contextual
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.01274) or [HuggingFace](https://huggingface.co/papers/2506.01274))| Yong Man Ro, Hyunjun Kim, arkimjh, lakelee | i) This paper introduces ReFoCUS, a reinforcement learning framework for optimizing frame selection in video-LLMs to improve contextual understanding. ii) The main objective is to develop a frame selection policy that aligns with a model’s intrinsic visual preferences for temporally grounded responses. iii) ReFoCUS utilizes a reference LMM to generate reward signals for frame subsets, training an autoregressive frame selection policy via reinforcement learning. iv) The experimental results demonstrate that ReFoCUS consistently improves reasoning performance across video QA benchmarks without frame-level supervision. For instance, it significantly enhances performance on the long subset of Video-MME. v) ReFoCUS offers AI practitioners a model-agnostic approach to enhance video-LLMs by optimizing visual input selection, improving reasoning capabilities, particularly in complex, multi-event scenarios where information is sparse.  |
| Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.24726) or [HuggingFace](https://huggingface.co/papers/2505.24726))| Kiran Kamble, Christopher Bryant, Umar Jamil, Shelly Bensal, melisa | i) The paper introduces a reinforcement learning framework for improving large language models (LLMs) through self-reflection on failed tasks. ii) The research investigates if LLMs can learn to generate better self-reflections to improve performance on downstream tasks using reinforcement learning without task-specific data. iii) The methodology involves prompting the LLM to generate a self-reflection upon failure, retrying the task with the reflection context, and rewarding tokens in the reflection using Group Relative Policy Optimization (GRPO) if the retry succeeds. iv) Experiments on the APIGen function calling dataset show a performance increase of up to 34.7% in math equation writing and 18.1% in function calling; smaller fine-tuned models outperformed 10x larger models. v) This work enables AI practitioners to improve LLM performance on complex tasks with binary success/failure feedback by optimizing self-reflection without requiring task-specific training datasets or synthetic data generation.  |
| ORV: 4D Occupancy-centric Robot Video Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03079) or [HuggingFace](https://huggingface.co/papers/2506.03079))| Chongjie Ye, Nan Wang, Shaocong Xu, Bohan Li, gzzyyxy | ORV is a novel framework for generating action-conditioned robot manipulation videos guided by 4D semantic occupancy. The paper addresses the challenge of generating high-fidelity, controllable robot manipulation videos. The methodology involves using 4D semantic occupancy sequences as a fine-grained representation to guide video generation, along with a curated high-quality occupancy dataset. Experiments show ORV achieves superior performance compared to existing methods, demonstrated by a PSNR increase from 25 to 28 when incorporating physical constraints. This occupancy-centric approach offers AI practitioners a more precise and controllable method for synthesizing realistic robot videos, potentially improving simulation and robot learning.  |
| FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens (Read more on [arXiv](https://arxiv.org/abs/2506.03096) or [HuggingFace](https://huggingface.co/papers/2506.03096))| Matthias Hein, Nicolas Flammarion, Francesco Croce, chs20 | FuseLIP constructs multimodal embeddings via early fusion of discrete text and image tokens using a single transformer encoder. The primary objective is to encode multimodal inputs while maintaining vision-language alignment and zero-shot capabilities. The methodology involves tokenizing images with a discrete image tokenizer and concatenating these with text tokens for processing by a single transformer model, trained with a contrastive loss and masked multimodal modeling. FuseLIP surpasses late fusion methods on tasks that involve encoding image-text pairs achieving an accuracy of 94.3% on text-guided image transformations while being comparable on unimodal tasks. FuseLIP's architecture and training approach offer AI practitioners an effective method for building multimodal embeddings that enhance performance on multimodal understanding tasks compared to traditional late fusion methods.  |
| Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
  From Scratch with Agentic Framework (Read more on [arXiv](https://arxiv.org/abs/2506.02454) or [HuggingFace](https://huggingface.co/papers/2506.02454))| Xingyu Liu, Yiyao Wang, Han Wang, Bo Pan, Zhaorui Yang | i) This paper introduces Multimodal DeepResearcher, an agentic framework for generating text-chart interleaved reports from scratch. ii) The main research objective is to automate the generation of comprehensive reports that integrate textual content with diverse, high-quality visualizations. iii) The methodology involves a four-stage agentic framework: researching, exemplar report textualization using Formal Description of Visualization (FDV), planning, and multimodal report generation with iterative chart refinement using actor-critic mechanism. iv) Experimental results demonstrate that Multimodal DeepResearcher, using the Claude 3.7 Sonnet model, achieves an 82% overall win rate compared to the DataNarrative baseline in automatic evaluations. v) The principal implication for AI practitioners is a new approach to automate the generation of informative, multimodal reports, which relies on structured visualization representations (FDV) and agentic workflows for iterative refinement of generated content.  |
| One Missing Piece for Open-Source Reasoning Models: A Dataset to
  Mitigate Cold-Starting Short CoT LLMs in RL (Read more on [arXiv](https://arxiv.org/abs/2506.02338) or [HuggingFace](https://huggingface.co/papers/2506.02338))| Sunghyun Park, Beong-woo Kwak, Jihyuk Kim, Dongjin Kang, hyungjoochae | i) The paper introduces the Long CoT Collection dataset to improve the reasoning capabilities of short chain-of-thought (CoT) language models (LLMs) in reinforcement learning (RL). ii) The research investigates the feasibility of constructing long CoT data using LLMs not trained for inference-time scaling to address the cold-start problem in RL. iii) The methodology involves a pipeline to induce novel reasoning strategies into short CoT LLMs, creating a 100K instance dataset annotated with existing short CoT LLMs. iv) Experiments show that models initialized on the Long CoT Collection achieve 2-3x larger performance gains with RL with verifiable reward (RLVR) on MATH500 and GPQA; the dataset's quality is comparable to R1. v) The Long CoT Collection offers AI practitioners a reliable foundation for initializing SFT models for reinforcement learning, accelerating and stabilizing downstream learning in reasoning tasks.  |
| Accelerating Diffusion LLMs via Adaptive Parallel Decoding (Read more on [arXiv](https://arxiv.org/abs/2506.00413) or [HuggingFace](https://huggingface.co/papers/2506.00413))| Aditya Grover, Guy Van den Broeck, danielmisrael | i) This paper introduces adaptive parallel decoding (APD) to accelerate diffusion large language models (dLLMs). ii) The research aims to improve the text generation speed of dLLMs without significantly sacrificing quality, addressing the bottleneck of autoregressive decoding. iii) APD dynamically adjusts the number of tokens sampled in parallel by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model, incorporating KV caching and limiting masked input size. iv) Experiments show that APD achieves markedly higher throughput with minimal quality degradations on downstream benchmarks; for example, a Dream 7B model using APD maintained ~80% accuracy on GSM8K while generating over 5 tokens per iteration. v) The APD method provides AI practitioners with tunable parameters to flexibly tradeoff throughput and quality in dLLM inference, offering a more efficient alternative for fast text generation.  |
| R^2ec: Towards Large Recommender Models with Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16994) or [HuggingFace](https://huggingface.co/papers/2505.16994))| Wenjie Wang, Xinyu Lin, izhx, tensorslow, dd101bb | i) The paper introduces R²ec, a unified large recommender model with interleaved reasoning and recommendation. ii) The main objective is to develop a unified architecture that intrinsically integrates reasoning capabilities within a large recommender model, avoiding decoupled designs. iii) The methodology involves an autoregressive decoder-only backbone with a language-modeling head for reasoning and a recommendation head for item prediction, optimized using a Reinforcement Learning framework, RecPO. iv) Experiments show R²ec achieves relative improvements of 68.67% in Hit@5 and 45.21% in NDCG@20 compared to baselines on three datasets. v) The principal implication is that interleaving reasoning and recommendation within a single model architecture, optimized with reinforcement learning, offers a path to significantly improve recommendation performance compared to decoupled or LLM-augmented approaches, potentially reducing resource costs and optimizing joint training.  |
| MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition
  Query (Read more on [arXiv](https://arxiv.org/abs/2506.03144) or [HuggingFace](https://huggingface.co/papers/2506.03144))| Qi Xu, Xian Wang, Linfeng Li, Yuan Gao, WeiChow | i) MERIT is introduced as a multilingual dataset for interleaved multi-condition semantic retrieval. ii) The research aims to address the underexplored area of semantic retrieval involving composite multi-condition queries with multiple images and multilingual text. iii) A novel fine-tuning framework, CORAL, is proposed, integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning for comprehensive global semantics. iv) Experiments demonstrate that CORAL achieves a 45.9% performance improvement over conventional approaches on MERIT. v) CORAL offers AI practitioners a method for enhancing MLLMs in semantic retrieval tasks by preserving conditional elements and extracting comprehensive global semantics, thus potentially improving the accuracy of retrieval systems.  |
| M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial
  Meeting Understanding Evaluation Dataset (Read more on [arXiv](https://arxiv.org/abs/2506.02510) or [HuggingFace](https://huggingface.co/papers/2506.02510))| Lifan Guo, Xiandong Li, Yalong Wen, Junhui Li, amazingj | i) The paper introduces M³FinMeeting, a new multilingual, multi-sector, multi-task dataset for evaluating LLMs in financial meeting understanding. ii) The main objective is to address the gap in existing financial benchmarks by providing real-world financial meeting data across multiple languages and industry sectors. iii) The key methodology involves curating financial meeting transcripts in English, Chinese, and Japanese, and annotating them for summarization, question-answer pair extraction, and question answering tasks, with annotation quality ensured by financial analysts. iv) Experimental results with seven LLMs, including OpenAI GPTs and open-sourced models, reveal that Qwen2.5-72B-Instruct achieves overall scores above 70 when evaluated by GPT-4, while recall of QA extraction is low at 45.65%, leaving significant room for improvement. v) The principal implication for AI practitioners is the availability of a challenging benchmark for assessing and improving LLMs' ability to process complex, long-context financial meeting data, facilitating more effective applications in financial decision-making.  |
| QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large
  Language Model Adaptation (Read more on [arXiv](https://arxiv.org/abs/2506.02295) or [HuggingFace](https://huggingface.co/papers/2506.02295))| Omar Elshehy, Mahmoud Reda, Abdelakreem Elkhateb, Omer Nacar, oddadmix | i) QARI-OCR is presented as a series of vision-language models optimized for Arabic text recognition via fine-tuning Qwen2-VL-2B-Instruct. ii) The primary objective is to improve the accuracy and efficiency of Arabic OCR, specifically in handling diacritics, diverse fonts, and complex layouts. iii) The methodology involves generating synthetic datasets and iteratively fine-tuning the Qwen2-VL-2B-Instruct model. iv) QARI v0.2 achieves a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. v) QARI-OCR provides AI practitioners with an improved Arabic OCR model exhibiting enhanced performance in recognizing intricate Arabic script, thus improving cultural heritage preservation, scholarly research, and information access.  |
| Knowing Before Saying: LLM Representations Encode Information About
  Chain-of-Thought Success Before Completion (Read more on [arXiv](https://arxiv.org/abs/2505.24362) or [HuggingFace](https://huggingface.co/papers/2505.24362))| Florian Matthes, yziser, galchechik, anumafzal94 | i) This paper explores predicting the success of Chain-of-Thought (CoT) reasoning in LLMs before completion by probing internal representations. ii) The main objective is to determine if LLMs implicitly encode information indicative of CoT success in their internal representations prior to generating the full reasoning chain. iii) The methodology involves training a probing classifier on the LLM's hidden states at different stages of CoT generation and comparing its performance to a BERT-based baseline relying on generated tokens alone. iv) The primary result is that a classifier using LLM internal representations can predict CoT success with 60% to 76.4% accuracy even before token generation, outperforming a BERT baseline, indicating crucial information is present in initial steps; however, the utility of later reasoning steps toward classification accuracy is variable. v) The principal implication for AI practitioners is that LLM representations contain early signals for CoT success, suggesting potential for early stopping or adaptive allocation of computational resources in CoT reasoning. Some parts of the methodology were unclear, such as the source for labeled training sets, and as such the details of this are unknown.  |
| How Much Backtracking is Enough? Exploring the Interplay of SFT and RL
  in Enhancing LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.24273) or [HuggingFace](https://huggingface.co/papers/2505.24273))| Bhuwan Dhingra, Junlin Wang, chenyn66, jamescai20 | i) This paper explores the interplay of supervised finetuning (SFT) and reinforcement learning (RL) in enhancing reasoning abilities of large language models (LLMs), focusing on the role of backtracking. ii) The research question is how the extent and structure of backtracking in SFT data impact subsequent RL training performance on reasoning tasks. iii) The methodology involves controlled experiments with synthetic datasets on eight reasoning tasks, systematically varying the number of backtracking steps in SFT data used to initialize RL training. iv) Results indicate that longer chain-of-thought (CoT) demonstrations with backtracks generally lead to better RL training, with more challenging problems requiring higher numbers of backtracks during SFT; SFT using correct or incorrect QwQ-32B distillation data converged in performance during RL, and RL with one backtrack initialization attained 69.7% accuracy (Figure 4d), outperforming QwQ-32B 51.5%. v) The primary implication for AI practitioners is that incorporating synthetic SFT data with a number of backtracks matched to the problem difficulty can improve RL training efficiency for LLM reasoning, while data correctness in SFT may be less critical. |
| Deep Video Discovery: Agentic Search with Tool Use for Long-form Video
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.18079) or [HuggingFace](https://huggingface.co/papers/2505.18079))| Bin Li, Jiahao Li, Zongyu Guo, Zhaoyang Jia, Xiaoyi Zhang | Deep Video Discovery introduces an agentic search strategy with tool use for long-form video understanding. The paper aims to address the limitations of LLMs in processing information-dense long videos by developing an agent that autonomously searches segmented video clips. The methodology involves creating a video database with multi-granular information and search-centric tools like Global Browse, Clip Search, and Frame Inspect. The DVD agent achieves state-of-the-art performance on LVBench, reaching an accuracy of 74.2% and improves to 76.0% with transcripts. This demonstrates that autonomous agentic search strategies with tool use can substantially improve long-form video understanding.  |
| Revisiting LRP: Positional Attribution as the Missing Ingredient for
  Transformer Explainability (Read more on [arXiv](https://arxiv.org/abs/2506.02138) or [HuggingFace](https://huggingface.co/papers/2506.02138))| Lior Wolf, Hila Chefer, Itamar Zimerman, Yarden Bakish | i) The paper introduces positional-aware LRP (PA-LRP) for transformer explainability, addressing the omission of positional encoding attribution in existing LRP methods. ii) The research aims to improve the fidelity and comprehensiveness of transformer explanations by incorporating positional encoding into LRP. iii) The methodology involves reformulating the input space as position-token pairs and developing specialized LRP rules for different positional encoding methods (Rotary, Learnable, Absolute). iv) Experiments on fine-tuned classifiers and zero-shot foundation models demonstrate that PA-LRP significantly outperforms state-of-the-art methods, achieving a 14.5% improvement in AU-MSE score on the generation task for LLaMa-2 7B finetuned on IMDB. v) The principal implication is that AI practitioners should consider positional encodings when using LRP for transformer explainability, as PA-LRP provides a more faithful representation of model reasoning, enabling improved model debugging and trust.  |
| Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural
  Understanding and Transcreation (Read more on [arXiv](https://arxiv.org/abs/2506.01565) or [HuggingFace](https://huggingface.co/papers/2506.01565))| Wenyan Li, Shaohuan Cheng, Dongchu Xie, Lutong Yu, Li Zhou | i) The paper introduces Hanfu-Bench, a new multimodal dataset for evaluating cultural understanding and creative adaptation of Vision-Language Models (VLMs) across temporal dimensions. ii) The main objective is to assess VLMs' ability to understand temporal-cultural features of traditional Chinese Hanfu and transcreate them into modern designs. iii) The methodology involves two core tasks: cultural visual understanding via multiple-choice VQA and cultural image transcreation evaluated through multi-faceted human assessment. iv) Results show that the best-performing model achieves a success rate of only 42% in the transcreation task, while closed VLMs perform comparably to non-experts in VQA but fall short of expert human performance by 10%. v) The principal implication is that current VLMs exhibit limitations in capturing and adapting temporal cultural nuances, requiring AI practitioners to develop models capable of more nuanced understanding and creative application in culturally-sensitive contexts.  |
