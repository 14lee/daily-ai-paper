

## Papers for 2025-06-23

| Title | Authors | Summary |
|-------|---------|---------|
| Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights (Read more on [arXiv](https://arxiv.org/abs/2506.16406) or [HuggingFace](https://huggingface.co/papers/2506.16406))| Xuanlei Zhao, Yuhao Zhou, Dongwen Tang, Zhiyuan Liang, VictorKai1996NUS | Drag-and-Drop LLMs (DnD) introduces a prompt-conditioned parameter generator to eliminate per-task training for specializing LLMs. The paper investigates whether task-specific LoRA weights can be directly generated from task prompts, bypassing gradient descent. DnD employs a text encoder to distill prompts into condition embeddings, which are then transformed into LoRA weights using a hyper-convolutional decoder trained on prompt-checkpoint pairs. Results show DnD achieves up to 30% average gains over trained LoRAs on unseen benchmarks and reduces overhead by up to 12,000x. DnD provides AI practitioners with a method for efficient LLM specialization without per-task fine-tuning, facilitating rapid deployment across diverse tasks.  |
| PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and
  Quantized Attention in Visual Generation Models (Read more on [arXiv](https://arxiv.org/abs/2506.16054) or [HuggingFace](https://huggingface.co/papers/2506.16054))| Huixia Li, Xuefeng Xiao, Xinhao Yang, Ke Hong, A-suozhang | PAROAttention proposes a pattern-aware token reordering technique to improve the efficiency of sparse and quantized attention mechanisms in visual generation models. The research aims to mitigate challenges in sparsification and quantization arising from dispersed and irregular attention patterns in visual data. The methodology involves reorganizing attention patterns into hardware-friendly block-wise patterns through token reordering, followed by specialized sparsification and quantization techniques. The paper demonstrates a 1.9~2.7× end-to-end latency speedup on video and image generation tasks with lossless metrics under lower density (20%-30%) and bitwidth (INT8/INT4). PAROAttention provides AI practitioners with a method to reduce computational costs associated with attention mechanisms, enabling faster inference and potentially reduced memory footprint in visual generative models.  |
| Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal
  Document Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.16035) or [HuggingFace](https://huggingface.co/papers/2506.16035))| Biddwan Ahmed, Indraneel Das, Tanmay Odapally, udayallu, vishesh-t27 | i) This paper introduces a multimodal document chunking approach to enhance Retrieval-Augmented Generation (RAG) systems. ii) The main objective is to improve the quality of document chunking in RAG pipelines using Large Multimodal Models (LMMs) to better handle complex document structures. iii) The methodology involves a multimodal batch processing framework using LMMs to process documents in configurable page batches with cross-batch context preservation, along with techniques for maintaining table structures, step-by-step procedures, and multi-page content relationships. iv) Results on an internal benchmark dataset demonstrate an improvement in accuracy from 0.78 to 0.89 compared to traditional fixed-size chunking in RAG systems. v) The principal implication for AI practitioners is the demonstration that vision-guided chunking significantly enhances RAG performance by improving semantic coherence and structural integrity, offering a novel approach for processing complex multimodal documents.  |
| VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2506.09049) or [HuggingFace](https://huggingface.co/papers/2506.09049))| Jie Yang, Yiran Qin, Heng Zhou, Xiufeng Song, FACEONG | VIKI-R introduces a benchmark and framework for embodied multi-agent cooperation. The research aims to evaluate and improve visual reasoning in multi-agent systems through hierarchical tasks. VIKI-Bench structures tasks into three levels: agent activation, task planning, and trajectory perception, and VIKI-R employs a two-stage approach of supervised fine-tuning (SFT) with Chain-of-Thought demonstrations followed by reinforcement learning (RL) using multi-level rewards. Experiments demonstrate that VIKI-R significantly outperforms baselines across all task levels, with RL enabling compositional cooperation; specifically, VIKI-R achieves a 74.1% accuracy on the agent activation task (VIKI-L1). The principal implication is a method for enhancing visual reasoning and coordination in embodied AI agents through structured learning and hierarchical rewards, offering AI practitioners a refined approach for multi-agent system development, particularly when diverse embodiment types are involved. |
| Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with
  Hybrid History Condition (Read more on [arXiv](https://arxiv.org/abs/2506.17201) or [HuggingFace](https://huggingface.co/papers/2506.17201))| Yuan Zhou, Longhuang Wu, Zhiyong Xu, Junshu Tang, Jiaqi Li | Hunyuan-GameCraft is presented as a novel framework for high-dynamic interactive video generation in game environments. The main objective is to create action-controllable game video synthesis by unifying keyboard and mouse inputs into a shared camera representation and using a hybrid history-conditioned training strategy. The methodology includes training on a large-scale dataset of over one million gameplay recordings, fine-tuning on synthetic data, and incorporating model distillation for efficiency. Experiments demonstrate that Hunyuan-GameCraft reduces interaction errors by 55% in cross-domain tests compared to existing models. The principal implication for AI practitioners is a method to generate more realistic and playable interactive game videos with improved action controllability and temporal consistency.  |
| DreamCube: 3D Panorama Generation via Multi-plane Synchronization (Read more on [arXiv](https://arxiv.org/abs/2506.17206) or [HuggingFace](https://huggingface.co/papers/2506.17206))| Xihui Liu, Kaiyi Huang, Jianan Wang, Yanning Zhou, Yukun Huang | DreamCube introduces a multi-plane synchronization strategy for 3D panorama generation, enhancing consistency in multi-plane omnidirectional representations. The main objective is to generalize 2D diffusion models to multi-plane representations for tasks like RGB-D panorama generation. The methodology involves adapting operators from 2D foundation models to be omnidirectionally translation-equivalent, and a multi-plane RGB-D diffusion model called DreamCube is introduced. Experiments show DreamCube reduces FID to 12.58 on the Structured3D dataset for RGB panorama generation, indicating improved visual quality, and achieves a depth estimation accuracy of 0.787 for δ-1.25, outperforming existing methods, and suggesting benefits of cube map representations for joint modelling of panoramic appearance and geometry. AI practitioners can leverage multi-plane synchronization for improved consistency in generating 3D omnidirectional content, which may enable single-view-to-3D scene generation.  |
| Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate
  Details (Read more on [arXiv](https://arxiv.org/abs/2506.16504) or [HuggingFace](https://huggingface.co/papers/2506.16504))| Qingxiang Lin, Zibo Zhao, Haolin Liu, Yunfei Zhao, Zeqiang Lai | i) Hunyuan3D 2.5 is presented as an enhanced suite of 3D diffusion models for generating high-fidelity textured 3D assets. ii) The main research objective is to improve both shape and texture generation in 3D asset creation compared to previous methods. iii) The key methodologies involve a new shape foundation model named LATTICE and an upgraded texture generation model incorporating physical-based rendering (PBR) through a multi-view architecture. iv) Hunyuan3D 2.5 achieves better image-shape and text-shape similarities and outperforms commercial models, with user studies showing a 72% win rate in image-to-3D tasks compared to Commercial Model 1. v) The implication for AI practitioners is a potentially improved tool for creating realistic and detailed 3D assets, outperforming state-of-the-art models in shape detail, surface smoothness and texture consistency.  |
| InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.15745) or [HuggingFace](https://huggingface.co/papers/2506.15745))| Simyung Chang, Jungwook Choi, Kyuhong Shim, Minsoo Kim | i) InfiniPot-V is a training-free framework for memory-constrained streaming video understanding using key-value (KV) cache compression. ii) The research aims to address the challenge of unbounded KV cache growth in streaming video understanding by enforcing a hard, length-independent memory cap. iii) The methodology involves a continual KV cache compression framework using Temporal-axis Redundancy (TaR) and Value-Norm (VaN) metrics. iv) InfiniPot-V achieves up to 94% reduction in peak GPU memory usage while matching or surpassing full-cache accuracy; it maintains real-time performance with only 0.5% compression overhead at 14 frames per second. v) By enabling memory-constrained streaming video understanding without retraining or query knowledge, InfiniPot-V facilitates the deployment of on-device multimodal assistants, implying practical, real-time memory management.  |
| Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with
  Production-Ready PBR Material (Read more on [arXiv](https://arxiv.org/abs/2506.15442) or [HuggingFace](https://huggingface.co/papers/2506.15442))| Xin Huang, Yifei Feng, Mingxin Yang, Shuhui Yang, Team Hunyuan3D | i) Hunyuan3D 2.1 is introduced as a comprehensive open-source system for generating high-fidelity, textured 3D assets from single-image inputs, featuring shape generation and PBR material synthesis. ii) The research aims to create a robust 3D asset generation pipeline accessible to a broader audience by addressing the complexities in 3D data processing and model training. iii) The methodology employs Hunyuan3D-DiT for shape generation, a flow-based diffusion architecture combined with Hunyuan3D-ShapeVAE, and Hunyuan3D-Paint for texture synthesis, utilizing a multi-view PBR diffusion model. iv) Quantitative evaluations for shape generation show that Hunyuan3D-DiT achieves a ULIP-I score of 0.1395 and Uni3D-I score of 0.3213, which presents the best performance. v) The open-sourced system and detailed tutorial enables AI practitioners to fine-tune and develop 3D generative models for applications in gaming, VR, and industrial design by offering a step-by-step guide on data processing, training, and evaluation.  |
| UniFork: Exploring Modality Alignment for Unified Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.17202) or [HuggingFace](https://huggingface.co/papers/2506.17202))| Xizhou Zhu, Hao Li, Lirui Zhao, Quanfeng Lu, Teng Li | This paper introduces UniFork, a novel Y-shaped architecture for unified image understanding and generation. The research investigates modality alignment in task-specific expert models to understand the different alignment behaviors required for understanding and generation tasks. The methodology involves analyzing text-image feature alignment across Transformer layers and introducing task-specific branches in deeper layers to mitigate task interference. Experiments show UniFork outperforms fully shared Transformer architectures and achieves performance comparable to or better than task-specific models; for example, UniFork achieved an overall 46% accuracy on GenEval, a 39% improvement over the ablation variant with smaller parameter scale. The key implication for AI practitioners is that task-specific branching in unified multimodal models improves performance by addressing divergent modality alignment requirements, offering a potential pathway for more efficient and effective unified architectures.  |
| Reranking-based Generation for Unbiased Perspective Summarization (Read more on [arXiv](https://arxiv.org/abs/2506.15925) or [HuggingFace](https://huggingface.co/papers/2506.15925))| Kathleen McKeown, Nicholas Deas, narutatsuri | i) This paper addresses generating unbiased summaries, specifically in political perspective summarization, and identifies metrics for evaluating summary quality. ii) The research question is to identify reliable metrics for measuring perspective summary quality and to investigate the efficacy of LLM-based methods beyond zero-shot inference. iii) The methodology involves building a test set using human annotations to benchmark metric reliability and evaluating various generation methods, including prompting, mechanistic approaches, and reranking-based methods, further utilizing preference tuning with synthetically generated and reranking-labeled data. iv) Results show that traditional metrics underperform compared to language model-based metrics in evaluating summary quality and reranking-based methods yield superior performance; Preference tuning on reranked generations further boosts performance, particularly improving faithfulness, achieving a coverage of 0.437 and faithfulness of 0.724 using DPO+RR in human evaluation. v) The primary implication for AI practitioners is the demonstration of reranking-based methods' efficacy in improving perspective summarization, indicating a need to shift away from zero-shot inference and prompting alone.  |
| Long-term Traffic Simulation with Interleaved Autoregressive Motion and
  Scenario Generation (Read more on [arXiv](https://arxiv.org/abs/2506.17213) or [HuggingFace](https://huggingface.co/papers/2506.17213))| Philipp Krähenbühl, Shuhan Tan, Xiuyu Yang | i) The paper introduces InfGen, a unified autoregressive model for long-term traffic simulation using interleaved motion simulation and scenario generation. ii) The research aims to achieve realistic trip-level driving simulations by dynamically managing the entry and exit of traffic agents over extended time horizons. iii) InfGen uses a transformer architecture with task-specific tokenizers to convert agent behaviors into discrete tokens and employs mode-control tokens to switch between motion simulation and scene generation. iv) InfGen outperforms prior state-of-the-art models in 30-second traffic simulation and achieves Mean ACE of 8.1 against the baselines that have scores of 12.0 and 12.2. v) AI practitioners can leverage InfGen for generating realistic traffic scenarios to train and evaluate self-driving systems, particularly in situations requiring long-term prediction and dynamic agent management, advancing simulation capabilities for autonomous driving development.  |
