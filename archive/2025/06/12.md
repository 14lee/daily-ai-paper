

## Papers for 2025-06-12

| Title | Authors | Summary |
|-------|---------|---------|
| Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.06395) or [HuggingFace](https://huggingface.co/papers/2506.06395))| Ivan Oseledets, Andrey Kuznetsov, Alexander Zubrey, Matvey Skripkin, LiPengyi29 | i) The paper introduces Reinforcement Learning via Self-Confidence (RLSC), a novel method for fine-tuning large language models (LLMs) using the model's own confidence as a reward signal. ii) The research aims to develop a post-training optimization method for LLMs that aligns model behavior with task-specific goals without relying on human annotations or external reward models. iii) RLSC involves generating multiple completions per input, then optimizing a self-confidence objective based on the model's probability assigned to its own responses. iv) Experiments on Qwen2.5-Math-7B, using only 16 samples per question, demonstrate improved accuracy of +13.4% on AIME2024 and +21.2% on MATH500. v) RLSC offers AI practitioners a simple and scalable post-training technique for enhancing LLM performance, requiring minimal data and computation by leveraging intrinsic model confidence.  |
| Seedance 1.0: Exploring the Boundaries of Video Generation Models (Read more on [arXiv](https://arxiv.org/abs/2506.09113) or [HuggingFace](https://huggingface.co/papers/2506.09113))| Lu Jiang, Weilin Huang, Tuyen Hoang, Haoyuan Guo, Yu Gao | Seedance 1.0 is introduced as a high-performance video generation model. The main objective is to address challenges in balancing prompt following, motion plausibility, and visual quality in video generation. The methodology comprises multi-source data curation with precision video captioning, an efficient architecture with decoupled spatial and temporal layers, and a video-tailored RLHF algorithm. Seedance 1.0 can generate a 5-second 1080p video in 41.4 seconds (NVIDIA-L20) and achieves first place on Artificial Analysis leaderboards for both text-to-video and image-to-video tasks. This model enables AI practitioners to generate high-quality videos with superior spatiotemporal fluidity and precise instruction adherence while maintaining efficient inference speeds.  |
| PlayerOne: Egocentric World Simulator (Read more on [arXiv](https://arxiv.org/abs/2506.09995) or [HuggingFace](https://huggingface.co/papers/2506.09995))| Fan Wang, Xiang Bai, Xi Chen, Hao Luo, Yuanpeng Tu | i) The paper introduces PlayerOne, a novel egocentric realistic world simulator. ii) The research aims to enable immersive and unrestricted exploration within dynamic virtual environments accurately aligned with real-scene human motion. iii) The method utilizes a coarse-to-fine training pipeline, including pretraining on egocentric text-video pairs and finetuning on synchronous motion-video data with a part-disentangled motion injection scheme and a joint reconstruction framework for 4D scene and video frame modeling. iv) Experimental results demonstrate PlayerOne's generalization ability, achieving accurate control of human movements and consistent world modeling across diverse scenarios, with the model achieving a DINO-Score of 67.8 and a CLIP-Score of 88.2 on a constructed benchmark. v) PlayerOne provides AI practitioners with a new platform for developing and testing AI systems in interactive and realistic egocentric environments, particularly beneficial for applications requiring high-degree-of-freedom motion control and scene consistency.  |
| Autoregressive Adversarial Post-Training for Real-Time Interactive Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09350) or [HuggingFace](https://huggingface.co/papers/2506.09350))| Yuxi Ren, Jianwen Jiang, Hao He, Ceyuan Yang, Shanchuan Lin | i) The paper introduces Autoregressive Adversarial Post-Training (AAPT) for real-time interactive video generation. ii) The main objective is to transform a pre-trained latent video diffusion model into an efficient autoregressive generator suitable for interactive applications. iii) The methodology involves adversarial post-training using a block causal transformer architecture and student-forcing training. iv) The 8B-parameter AAPT model achieves real-time 24fps video generation at 736×416 resolution on a single H100 GPU with a latency of 0.16 seconds, enabling continuous 60-second video streams. v) AAPT offers AI practitioners a computationally efficient method for deploying real-time video generation systems, improving upon diffusion forcing methods and demonstrating comparable or improved performance in terms of video quality, particularly concerning long-duration consistency.  |
| ComfyUI-R1: Exploring Reasoning Models for Workflow Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09790) or [HuggingFace](https://huggingface.co/papers/2506.09790))| Weihua Luo, Longyue Wang, Xue Yang, Yiyu Wang, Zhenran Xu | ComfyUI-R1 introduces a large reasoning model for automated ComfyUI workflow generation. The research aims to develop a model that automates the creation of complex ComfyUI workflows from user instructions. The methodology involves curating a dataset of 4K ComfyUI workflows and training a 7B-parameter model using a two-stage process: CoT fine-tuning and reinforcement learning with a rule-metric hybrid reward. The model achieves a 97% format validity rate and outperforms existing methods in node-level and graph-level F1 scores. The study suggests that large reasoning models with chain-of-thought reasoning can facilitate the creation of complex AI workflows, reducing the barrier to entry for AI art generation and conditional image/video processing tasks, although specific node-level and graph-level F1 scores are not stated.  |
| SeerAttention-R: Sparse Attention Adaptation for Long Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08889) or [HuggingFace](https://huggingface.co/papers/2506.08889))| Yu Cheng, Yuqing Xia, Shijie Cao, Shuming Guo, Yizhao Gao | SeerAttention-R is introduced as a sparse attention framework for long reasoning models. The research focuses on improving long decoding efficiency specifically. SeerAttention-R learns attention sparsity through a self-distilled gating mechanism and removes query pooling for auto-regressive decoding. The methodology involves training the gating module of SeerAttention-R on 0.4B tokens and evaluating its performance on reasoning benchmarks like AIME under a 4K token budget. It maintains near-lossless accuracy and achieves up to 9x speedup over FlashAttention-3 on H100 GPU at 90% sparsity utilizing TileLang for a highly optimized sparse decoding kernel. It improves the viability of sparse attention as reasoning models scale by demonstrating near-lossless performance and hardware efficiency.  |
| SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner (Read more on [arXiv](https://arxiv.org/abs/2506.09003) or [HuggingFace](https://huggingface.co/papers/2506.09003))| Mouxiang Chen, Jian Yang, Min Yang, Jiaxi Yang, Lei Zhang | i) The paper introduces SWE-Flow, a novel data synthesis framework grounded in Test-Driven Development (TDD) for generating software engineering data. ii) The main objective is to create a framework to automatically generate structured development tasks and high-quality training instances for LLMs in software engineering. iii) SWE-Flow constructs a Runtime Dependency Graph (RDG) from unit test executions to infer incremental development steps and synthesize code, unit tests, and code modifications. iv) The framework generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, demonstrating that fine-tuning improves performance in TDD-based coding. v) SWE-Flow offers AI practitioners a means to synthesize verifiable software engineering data, enhancing LLM capabilities in incremental development tasks and enabling integration into reinforcement learning workflows.  |
| InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio
  Conditions (Read more on [arXiv](https://arxiv.org/abs/2506.09984) or [HuggingFace](https://huggingface.co/papers/2506.09984))| Gaojie Lin, Chao Liang, Jianwen Jiang, Jiaqi Yang, Zhenzhi Wang | i) This paper introduces InterActHuman, a novel framework for multi-concept human animation with layout-aligned audio conditions. ii) The main research question is how to achieve spatial alignment of multi-modal conditions in multi-concept human video generation. iii) The methodology involves using a mask predictor to infer layout information and injecting local audio conditions into corresponding regions in an iterative manner. iv) Empirical results show state-of-the-art performance in lip synchronization, motion diversity, and subject appearance fidelity. v) The principal implication for AI practitioners is the introduction of an effective method for controllable multi-concept human-centric video generation, offering enhanced control over individual entities and their interactions in complex scenes.  |
| SAFE: Multitask Failure Detection for Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2506.09937) or [HuggingFace](https://huggingface.co/papers/2506.09937))| Haruki Nishimura, Igor Gilitschenski, Shengxiang Sun, Yuanliang Ju, Qiao Gu | i) The paper introduces SAFE, a multitask failure detection method for vision-language-action models (VLAs) designed to generalize to unseen tasks. ii) The main research objective is to develop a failure detector that can accurately identify potential failures of generalist robot policies, such as VLAs, across diverse tasks and environments. iii) The proposed method, SAFE, leverages internal features of VLAs and conformal prediction to estimate the likelihood of task failure, training on both successful and failed rollouts. iv) Experiments on OpenVLA, πο, and πο-FAST show that SAFE achieves state-of-the-art failure detection performance, achieving the best trade-off between accuracy and detection time, with ROC-AUC values up to 0.89 on held-out tasks in simulation. v) SAFE provides AI practitioners with a scalable and generalizable approach to robustly deploy VLAs in real-world robotic applications by promptly detecting potential failures without retraining or task-specific data.  |
| Reparameterized LLM Training via Orthogonal Equivalence Transformation (Read more on [arXiv](https://arxiv.org/abs/2506.08001) or [HuggingFace](https://huggingface.co/papers/2506.08001))| Bernhard Schölkopf, Maximilian Dax, Tim Z. Xiao, Simon Buchholz, Zeju Qiu | i) This paper introduces POET, a reparameterized training algorithm for LLMs using orthogonal equivalence transformations. ii) The research aims to improve the effectiveness and reliability of training large language models by controlling the spectral properties of weight matrices. iii) POET reparameterizes each neuron with learnable orthogonal matrices and a fixed random weight matrix, optimizing these matrices using stochastic primitive optimization and Cayley-Neumann parameterization. iv) Experiments show that POET achieves better performance than AdamW and GaLore, with POET-FS (b=1/2) yielding a validation perplexity of 13.70 on a 1.3B LLaMA model, surpassing AdamW's 14.73. v) The primary implication is that POET provides AI practitioners with a more parameter-efficient and stable method for training LLMs, offering improvements in generalization and potentially reducing computational costs.  |
| MIRAGE: Multimodal foundation model and benchmark for comprehensive
  retinal OCT image analysis (Read more on [arXiv](https://arxiv.org/abs/2506.08900) or [HuggingFace](https://huggingface.co/papers/2506.08900))| Taha Emre, Ronald Fecso, Emese Sükei, Botond Fazekas, José Morano | i) The paper introduces MIRAGE, a multimodal foundation model (FM) for retinal OCT and SLO image analysis, along with a corresponding benchmark for evaluation. ii) The research aims to develop a FM capable of robust performance across retinal image analysis tasks, particularly segmentation, and to provide a rigorous benchmark for validating such models. iii) A Vision Transformer (ViT) was pretrained on a multimodal dataset of paired OCT, SLO, and automatically generated retinal layer labels using a masked autoencoding (MAE) objective. iv) MIRAGE achieved an average AUROC of 95.59% on OCT classification tasks, outperforming the second-best model by 1.15 percentage points and showed significant improvements in cross-dataset evaluations and segmentation, achieving a Dice score of 78.46% for OCT tasks. v) MIRAGE offers AI practitioners a robust FM for retinal image analysis that can be adapted for classification and segmentation tasks, along with a benchmark for evaluating and comparing new models. |
| Branched Schrödinger Bridge Matching (Read more on [arXiv](https://arxiv.org/abs/2506.09007) or [HuggingFace](https://huggingface.co/papers/2506.09007))| Pranam Chatterjee, Alexander Tong, Yinuo Zhang, Sophia Tang | i) The paper introduces Branched Schrödinger Bridge Matching (BranchSBM) for modeling divergent transitions between probability distributions. ii) The research aims to learn branched trajectories from a unimodal initial distribution to multiple target distributions, addressing limitations of existing methods in capturing branching dynamics. iii) BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, formulating a branched Conditional Stochastic Optimal Control (CondSOC) problem and leveraging a multi-stage training algorithm. iv) Experiments show BranchSBM can accurately reconstruct endpoint distributions on a LiDAR manifold with Wasserstein distances W1 = 0.239 and W2 = 0.309, outperforming single-branch SBM. v) BranchSBM provides AI practitioners with a framework for modeling dynamic branching trajectories in tasks such as multi-path surface navigation, single-cell population dynamics, and predicting heterogeneous cell states after perturbation, which may improve generative modeling applications.  |
