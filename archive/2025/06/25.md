

## Papers for 2025-06-25

| Title | Authors | Summary |
|-------|---------|---------|
| AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.19851) or [HuggingFace](https://huggingface.co/papers/2506.19851))| lsheng2024, pookiefoof, Yang-Tian, fenghora, huanngzh | i) AnimaX is a feed-forward 3D animation framework transferring video diffusion model motion priors to skeleton-based animation for diverse meshes. ii) The research aims to efficiently animate articulated 3D meshes with arbitrary skeletal structures using video diffusion model motion priors. iii) The methodology involves a joint video-pose diffusion model conditioned on template renderings and textual motion prompts, representing 3D motion as multi-view 2D pose maps. iv) Evaluated on VBench, AnimaX demonstrates state-of-the-art results in generalization, motion fidelity, and efficiency, trained on a dataset of 160,000 rigged sequences. v) AnimaX offers AI practitioners a scalable, category-agnostic 3D animation solution, enabling efficient and versatile animation generation for diverse articulated meshes. |
| Matrix-Game: Interactive World Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2506.18701) or [HuggingFace](https://huggingface.co/papers/2506.18701))| Qingcheng Zhu, Puyi Wang, Boyang Wang, Chunli Peng, Vanint | i) Matrix-Game introduces a world foundation model for controllable game world generation trained on a two-stage pipeline. ii) The main objective is to develop an interactive image-to-world generation model that can be precisely controlled and maintains visual quality and temporal coherence. iii) The model uses a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions, trained on the newly curated Matrix-Game-MC dataset. iv) Experiments show Matrix-Game outperforms previous Minecraft world models across all metrics of the GameWorld Score benchmark, particularly in controllability and physical consistency, and a human evaluation confirmed its superiority in generating realistic and controllable videos. v) The release of Matrix-Game model weights and the GameWorld Score benchmark provides AI practitioners with a new interactive world generation framework and a standardized tool for evaluating world models.  |
| GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.16141) or [HuggingFace](https://huggingface.co/papers/2506.16141))| Junhao Cheng, Yixiao Ge, Rui Wang, Yuying Ge, Yi Chen | GRPO-CARE introduces a consistency-aware reinforcement learning framework for improving multimodal reasoning in large language models. The research aims to address limitations of outcome-supervised GRPO, where answer accuracy is prioritized over logical reasoning consistency. The methodology involves an adaptive, group-relative consistency bonus based on reference-likelihood calibration in addition to base rewards for answer correctness. Results demonstrate a 6.7% performance gain on the most challenging level of SEED-Bench-R1 and a 24.5% improvement in consistency rate compared to standard GRPO. The framework's enhanced reasoning coherence and improved interpretability offers AI practitioners a method for training more reliable and transparent multimodal reasoning systems.  |
| Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.19290) or [HuggingFace](https://huggingface.co/papers/2506.19290))| Changshi Li, Yuzhen Xiao, chrisliu298, lycfight, zengliangcs | i) The paper introduces Skywork-SWE, a large-scale dataset and model for software engineering tasks in LLMs. ii) The main objective is to systematically scale and analyze software engineering dataset volume and diversity to understand data scaling laws in LLMs. iii) The methodology involves an automated data curation pipeline to generate over 8,000 runtime-validated training trajectories and fine-tuning a Qwen2.5-Coder-32B-based model. iv) The Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark and improves to 47.0% with test-time scaling, surpassing previous SOTA results for models under 32B parameters. v) The identified data scaling laws suggest that increasing high-quality, execution-grounded data substantially improves LLM performance in software engineering, providing a practical guideline for AI practitioners to further enhance LLM capabilities.  |
| ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality
  Debiasing (Read more on [arXiv](https://arxiv.org/abs/2506.19848) or [HuggingFace](https://huggingface.co/papers/2506.19848))| Pan Zhang, Xiaoyi Dong, Long Xing, yuhangzang, shikiw | ScaleCap is introduced as an inference-time scalable image captioning strategy for generating detailed captions. The research addresses the challenge of multimodal and linguistic biases in LVLMs to enhance caption quality. ScaleCap employs heuristic question answering and contrastive sentence rating for caption enrichment and hallucination reduction, respectively. Experiments show ScaleCap-450K improves pretraining efficiency, achieving superior performance on 11 benchmarks; for example, it improves InfoVQA scores by 4.3% over ShareGPT4V-450k in Qwen2.5-7B. ScaleCap enables AI practitioners to generate higher-quality image captions for improved vision-language model training and downstream task performance.  |
| SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in
  Real-World Applications (Read more on [arXiv](https://arxiv.org/abs/2506.18951) or [HuggingFace](https://huggingface.co/papers/2506.18951))| Per Jacobsson, Ge Qu, Jinyang Li, Tebmer, xia01ongLi | i) This paper introduces a new benchmark and training environment for SQL issue debugging using Large Language Models (LLMs). ii) The main research objective is to address the gap in evaluating and improving LLMs' ability to debug SQL issues distilled from authentic user scenarios. iii) The paper presents BIRD-CRITIC, a benchmark of 530 PostgreSQL and 570 multi-dialect SQL debugging tasks, along with SIX-GYM, a training environment utilizing SQL-Rewind and f-Plan Boosting. iv) Baseline evaluations on BIRD-CRITIC reveal a 38.87% success rate for the leading reasoning model (O3-MINI) on the PostgreSQL subset; BIRD-FIXER, fine-tuned on Qwen-2.5-Coder-14B, achieves 38.11% success rate on BIRD-CRITIC-PG. v) The introduction of BIRD-CRITIC, SIX-GYM, and BIRD-FIXER enables AI practitioners to evaluate and improve LLMs' ability to debug SQL queries effectively, and the f-Plan Boosting demonstrates a mechanism for improving the effectiveness of LLM trajectory training.  |
| Can Large Language Models Capture Human Annotator Disagreements? (Read more on [arXiv](https://arxiv.org/abs/2506.19467) or [HuggingFace](https://huggingface.co/papers/2506.19467))| Alexander Hoyle, Donya Rooein, Vil√©m Zouhar, Yu Fan, JingweiNi | i) This paper evaluates LLMs' ability to predict human annotator disagreement in NLP tasks. ii) The central research question is whether LLMs can effectively model informative human annotation variance without access to repeated human labels. iii) The methodology involves evaluating various LLMs (8B-671B parameters) across different training paradigms (RLHF, RLVR) and prompting strategies on five NLP datasets using variance correlation and distributional alignment metrics. iv) Results indicate that RLVR-style reasoning significantly harms disagreement prediction, with the verbalized distribution approach outperforming the sampling-based approach in disagreement prediction. v) The implication is that AI practitioners should exercise caution when using LLMs (particularly RLVR-tuned models) as annotators for subjective tasks, as these models may overlook critical human disagreements and that more focus on evaluating LLMs on these types of tasks is needed.  |
| JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo
  Retouching Agent (Read more on [arXiv](https://arxiv.org/abs/2506.17612) or [HuggingFace](https://huggingface.co/papers/2506.17612))| Panwang Pan, Jinbin Bai, Kunjie Lin, Zixu Lin, LYL1015 | i) The paper introduces JarvisArt, an MLLM-driven intelligent agent for photo retouching. ii) The primary objective is to develop an AI agent that can understand user intent, mimic professional artists' reasoning, and orchestrate Lightroom's retouching tools. iii) The methodology involves a two-stage training process: Chain-of-Thought supervised fine-tuning followed by Group Relative Policy Optimization for Retouching (GRPO-R), and an Agent-to-Lightroom Protocol for seamless integration. iv) JarvisArt demonstrates improved content fidelity, outperforming GPT-40 with a 60% improvement in average pixel-level metrics on MMArt-Bench. v) The principal implication for AI practitioners is a new avenue for intelligent photo retouching with user-friendly interaction, superior generalization, and fine-grained control, which can inform the development of more sophisticated, user-guided AI editing tools.  |
| SRFT: A Single-Stage Method with Supervised and Reinforcement
  Fine-Tuning for Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.19767) or [HuggingFace](https://huggingface.co/papers/2506.19767))| Xihuai Wang, Jiajun Chai, Tinghong Chen, SONGJUNTU, Yuqian-Fu | i) This paper introduces Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method unifying supervised fine-tuning (SFT) and reinforcement learning (RL) for large language model (LLM) reasoning. ii) The research aims to address the challenge of optimally integrating SFT and RL in LLM fine-tuning to enhance reasoning capabilities. iii) The methodology involves an entropy-aware weighting mechanism to simultaneously apply SFT and RL, leveraging demonstrations and self-exploration rollouts in a single optimization stage. iv) Experimental results demonstrate that SRFT achieves 59.1% average accuracy on mathematical reasoning benchmarks, outperforming zero-RL methods by 9.0%. v) SRFT offers AI practitioners a method for effectively combining SFT and RL in a single training phase, improving LLM reasoning performance and generalization with entropy-aware weighting.  |
| SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2506.19838) or [HuggingFace](https://huggingface.co/papers/2506.19838))| Xintao Wang, Menghan Xia, Shian Du, Yu Li, Liangbin Xie | SimpleGVR presents a latent-cascaded video super-resolution (VSR) baseline for efficient high-resolution video generation from large text-to-video (T2V) models. The research aims to improve cascaded VSR models by studying key design principles, specifically degradation strategies and training configurations. The methodology includes flow-based and model-guided degradation to generate training pairs, along with innovations in timestep sampling and attention mechanisms. Experiments show that SimpleGVR achieves higher quality 1080p videos from 512p outputs of a base T2V model and reduces computational overhead by 80% using sparse local attention compared to full self-attention. The work offers a simple and effective baseline, providing practical insights for AI practitioners in designing efficient cascaded video synthesis systems.  |
| Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low
  CFG Scales (Read more on [arXiv](https://arxiv.org/abs/2506.19713) or [HuggingFace](https://huggingface.co/papers/2506.19713))| Farnood Salehi, Tobias Vontobel, RMW, msadat97 | i) The paper introduces Frequency-Decoupled Guidance (FDG) for conditional diffusion models, improving image quality at low classifier-free guidance (CFG) scales. ii) The research aims to enhance image quality and prompt alignment in CFG by analyzing and decoupling the effects of different frequency components. iii) FDG decomposes CFG into low- and high-frequency components, applying distinct guidance strengths to each, implemented using Laplacian pyramids as the frequency transform. iv) Experiments show FDG consistently improves FID and recall across datasets and models; for instance, EDM2-S achieved a FID of 5.44 with FDG compared to 9.77 with standard CFG. v) FDG provides AI practitioners a plug-and-play alternative to standard CFG, enhancing sample fidelity and diversity in conditional diffusion models without retraining, thus improving generative modeling for image synthesis.  |
| Unified Vision-Language-Action Model (Read more on [arXiv](https://arxiv.org/abs/2506.19850) or [HuggingFace](https://huggingface.co/papers/2506.19850))| Yingyan Li, Junbo Zhang, Wenxuan Wang, Xinghang Li, Yuqi Wang | i) The paper introduces UniVLA, a unified vision-language-action model that represents vision, language, and action as discrete tokens within an autoregressive framework. ii) The research aims to develop a unified model capable of multimodal outputs and supporting a wide range of tasks, including perception grounding, world modeling, and policy learning. iii) The methodology involves a unified token-based design, autoregressive sequence modeling, and world model integration during post-training using large-scale video data. iv) UniVLA achieves a 95.5% average success rate on the LIBERO benchmark and improves performance in downstream policy learning, particularly for long-horizon and out-of-distribution tasks. v) AI practitioners can leverage UniVLA‚Äôs architecture for more integrated cross-modal modeling and scalable video-based training, offering a potential direction for generalist embodied intelligence.  |
| Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic
  Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2506.19794) or [HuggingFace](https://huggingface.co/papers/2506.19794))| Ziheng Zhang, Jintian Zhang, Yi Zhong, Yuqi Zhu, Ningyu | Open-source LLMs underperform in data analysis tasks compared to proprietary models. The paper investigates methods to improve open-source LLMs for reasoning-intensive data analysis scenarios. The study evaluates models across data understanding, code generation, and strategic planning using a curated dataset. Strategic planning quality is identified as the primary determinant of model performance; high-quality training data proves more critical than data diversity for optimal performance; fine-tuning a 7B model with data synthesis methodology achieved comparable or superior results to GPT-4o, with the 14B model's gains diminished at larger scale. The findings imply that improvements to reasoning processes within data synthesis can significantly enhance the analytical capabilities of open-source LLMs, directly benefiting AI practitioners by enabling more effective use of smaller LLMs for complex data analysis.  |
| Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text (Read more on [arXiv](https://arxiv.org/abs/2506.14012) or [HuggingFace](https://huggingface.co/papers/2506.14012))| Michalis Vazirgiannis, Yang Zhang, guokan-shang, amr-mohamed | i) This paper evaluates Large Language Model (LLM) comprehension of code-switched text across various linguistic settings. ii) The research investigates how LLMs process and reason about mixed-language data, specifically focusing on reading comprehension, multi-domain knowledge, and natural language inference tasks. iii) The methodology involves generating code-switched variants of established benchmarks using both linguistically grounded and heuristic approaches and then evaluating LLM performance. iv) Results indicate that embedding non-English tokens in English matrix languages degrades performance, while embedding English tokens in other languages sometimes improves it; Llama 70B's weighted average accuracy declined from 0.70 (English) to 0.66 on EN‚ÜíAR/EN‚ÜíDE v) AI/ML engineers should be aware that LLMs exhibit vulnerabilities to code-switching, particularly when English is the primary language, and that fine-tuning is a more reliable solution. |
| USAD: Universal Speech and Audio Representation via Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.18843) or [HuggingFace](https://huggingface.co/papers/2506.18843))| Alexander H. Liu, James Glass, saurabhati, vectominist | USAD proposes a universal audio representation model leveraging distillation to integrate speech, sound, and music. The main objective is to create a unified audio encoder capable of generalizing across various audio domains. The methodology involves layer-to-layer distillation from domain-specific self-supervised learning (SSL) models using a mixed audio dataset. USAD achieves competitive performance across SUPERB and HEAR benchmarks, exhibiting a 35.7 SUPERB score with the base model and 37.4 average performance score on HEAR with the large model; and demonstrating unified embedding space. AI practitioners can utilize USAD as a general-purpose audio encoder for downstream tasks across diverse audio types.  |
| KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality (Read more on [arXiv](https://arxiv.org/abs/2506.19807) or [HuggingFace](https://huggingface.co/papers/2506.19807))| Huajun Chen, Wenhao Yu, Shuofei Qiao, Baochang Ren, Ningyu | KnowRL explores integrating knowledge into reinforcement learning to enhance the factuality of slow-thinking LLMs. The research investigates how to mitigate hallucinations in slow-thinking models by incorporating a factuality reward based on knowledge verification into the RL training process. KnowRL trains models using a composite reward signal combining format, correctness, and factuality, evaluated on hallucination and reasoning benchmark datasets. Experiments show KnowRL mitigates hallucinations and maintains reasoning ability, evidenced by a 16.23% accuracy achievement on ChineseSimpleQA for Skywork-OR1-7B-Preview model. This framework implies that directly supervising the thinking process with factuality rewards is more effective for building reliable LLMs than solely optimizing for outcome accuracy.  |
| Intelligent Operation and Maintenance and Prediction Model Optimization
  for Improving Wind Power Generation Efficiency (Read more on [arXiv](https://arxiv.org/abs/2506.16095) or [HuggingFace](https://huggingface.co/papers/2506.16095))| Jiaqi He, Xiaobin Wu, Xun Liu, rajandasgupta | i) This study examines predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&M) systems for improved wind power generation efficiency. ii) The main objective is to analyze the effectiveness of predictive maintenance models in reducing downtime and to explore optimization strategies for intelligent O&M systems. iii) Qualitative research was conducted using structured interviews with five wind farm engineers and maintenance managers, followed by thematic analysis. iv) The study found that predictive maintenance models can reduce downtime by 20% but struggle with minor, gradual failures and false positives; sensor malfunctions and difficulties in integrating new models with older turbines are also problems. v) AI practitioners must address challenges in sensor data reliability, false positives, and seamless integration with legacy systems to improve the efficacy and reliability of predictive maintenance in operational wind turbine environments.  |
| Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments
  with a Hierarchical Spatial-Cognition Long-Short Memory System (Read more on [arXiv](https://arxiv.org/abs/2506.19433) or [HuggingFace](https://huggingface.co/papers/2506.19433))| Jie Feng, Yangcheng Yu, Zhenxing Chen, Haoyu Dong, Lixuan He | Mem4Nav enhances vision-and-language navigation (VLN) in urban environments using a hierarchical spatial-cognition long-short memory system. The research objective is to improve embodied agents' ability to navigate complex urban scenes by incorporating both fine-grained spatial detail and high-level landmark semantics. A dual-structured 3D map combining sparse octree indexing and a semantic topology graph, along with a reversible Transformer memory and short-term cache, is used. Mem4Nav achieved a 7-13 percentage point increase in Task Completion on Touchdown and Map2Seq datasets. AI practitioners can leverage this hierarchical memory system to improve the performance of VLN agents in complex, large-scale environments by incorporating efficient, lossless storage and retrieval of spatial information.  |
