

## Papers for 2025-06-16

| Title | Authors | Summary |
|-------|---------|---------|
| Aligned Novel View Image and Geometry Synthesis via Cross-modal
  Attention Instillation (Read more on [arXiv](https://arxiv.org/abs/2506.11924) or [HuggingFace](https://huggingface.co/papers/2506.11924))| Taekyoung Kim, Dongyoon Han, Sangdoo Yun, Junho Kim, Min-Seop Kwak | i) This paper introduces a diffusion-based framework, MoAI, for generating aligned novel view images and geometry from unposed reference images. ii) The main objective is to achieve accurate novel view synthesis with geometrically robust consistency, even in extrapolative settings, by addressing the limitations of previous methods that require dense posed images. iii) The method utilizes off-the-shelf geometry predictors, formulates novel view synthesis as an inpainting task, and employs cross-modal attention instillation to transfer attention maps from the image branch to the geometry branch. iv) The model achieves state-of-the-art performance in extrapolative camera settings on RealEstate10K, with PSNR of 17.41, SSIM of 0.614, and LPIPS of 0.229. v) The principal implication is that the cross-modal attention instillation technique provides a mechanism to enhance geometrically-aware image generation, resulting in high-quality 3D scene completion for AI practitioners working on 3D reconstruction and novel view synthesis tasks.  |
| Effective Red-Teaming of Policy-Adherent Agents (Read more on [arXiv](https://arxiv.org/abs/2506.09600) or [HuggingFace](https://huggingface.co/papers/2506.09600))| Guy Uziel, Matan Vetzler, Koren Lazar, George Kour, Itay Nakash | i) The paper introduces CRAFT, a multi-agent red-teaming system for evaluating the robustness of policy-adherent LLM-based agents. ii) The research question is how to effectively assess and improve the resilience of policy-adherent agents against adversarial users attempting to bypass policy restrictions for personal gain. iii) The methodology involves a multi-agent system (CRAFT) with policy-aware persuasive strategies and the introduction of T-break, a benchmark for assessing agent robustness against manipulative behavior, used for red-teaming customer service scenarios. iv) CRAFT achieves a 70.0% attack success rate (ASR) in the airline domain, significantly outperforming generic attacks like DAN (35.0%) and emotional manipulation (50.0%), when targeting policy adherent agent. v) AI practitioners need to develop stronger safeguards to protect policy-adherent agents from adversarial attacks, as conventional red-teaming methods underestimate the true risk posed by malicious users.  |
| The Diffusion Duality (Read more on [arXiv](https://arxiv.org/abs/2506.10892) or [HuggingFace](https://huggingface.co/papers/2506.10892))| Justin Chiu, Guanghan Wang, Aaron Gokaslan, Justin Deschenaux, Subham Sekhar Sahoo | i) The paper introduces Duo, a framework that leverages Gaussian diffusion insights to improve Uniform-state Discrete Diffusion Models (USDMs). ii) It aims to bridge the performance gap between USDMs and other text generation models. iii) Duo incorporates a Gaussian-guided curriculum learning strategy and Discrete Consistency Distillation, adapting techniques from continuous diffusion to discrete settings. iv) Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks and Discrete Consistency Distillation reduces sampling steps from 1024 to 8, with minimal effect on sample quality. v) Duo provides AI practitioners with techniques to accelerate training of diffusion language models and unlock few-step generation by accelerating sampling by two orders of magnitude.  |
| LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
  Programming? (Read more on [arXiv](https://arxiv.org/abs/2506.11928) or [HuggingFace](https://huggingface.co/papers/2506.11928))| Kaiyuan Liu, Shang Zhou, Zeyu Shen, Zerui Cheng, Zihan Zheng | i) This paper introduces LiveCodeBench Pro, a competitive programming benchmark to evaluate LLMs' algorithmic reasoning. ii) The research investigates the extent to which current LLMs exhibit human-level algorithmic reasoning in competitive programming scenarios. iii) The study employs a benchmark of Codeforces, ICPC, and IOI problems annotated by Olympiad medalists, along with line-by-line analysis of LLM-generated code. iv) The best model achieves 53% pass@1 on medium-difficulty problems and 0% on hard problems without external tools. v) AI practitioners should note LLMs' significant limitations in nuanced algorithmic reasoning and the importance of implementation precision and tool augmentation, indicating areas for targeted improvement in code-centric LLM development.  |
| pLSTM: parallelizable Linear Source Transition Mark networks (Read more on [arXiv](https://arxiv.org/abs/2506.11997) or [HuggingFace](https://huggingface.co/papers/2506.11997))| Sepp Hochreiter, Wei Lin, Thomas Schmied, Richard Freinschlag, korbip | pLSTM introduces a parallelizable linear recurrent architecture for processing data structured as directed acyclic graphs (DAGs). The paper investigates how to extend linear RNNs to data with higher-level structures like 2D grids and trees. The core methodology translates Multi-Dimensional RNN principles to linear RNNs, introducing Source, Transition, and Mark gates. Experiments demonstrate pLSTM's strong extrapolation abilities on an arrow-pointing task, generalizing well to larger image sizes compared to Transformers; furthermore, pLSTM achieved a top-1 accuracy of 75.51 on ImageNet-1k. pLSTM's performance and scalability provide AI practitioners with an alternative recurrent architecture for modeling non-sequential data, addressing limitations of existing linear RNNs, though more data or experiments may be necessary to clearly demonstrate the full applicability of the research. |
| A High-Quality Dataset and Reliable Evaluation for Interleaved
  Image-Text Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09427) or [HuggingFace](https://huggingface.co/papers/2506.09427))| kpzhang, ZhangShenglin, fanrui00, cyrilli, finyorko | i) The paper introduces InterSyn, a high-quality dataset, and SynJudge, an evaluation model, for interleaved image-text generation. ii) The main objective is to address the limitations of existing datasets and evaluation metrics for training and assessing instruction-following, multi-turn, interleaved image-text generation models. iii) The methodology involves constructing InterSyn using a Self-Evaluation with Iterative Refinement (SEIR) pipeline and developing SynJudge, a multi-dimensional evaluation model for assessing text content, image content, image quality, and image-text synergy. iv) Experiments show a 32% improvement in question quality and up to 52.1% gain in image-text synergy (ITS) for fine-tuned models, while SynJudge achieves 5% deviation from human judgment. v) InterSyn enables AI practitioners to train LMMs to achieve improved multimodal understanding, instruction following, and generation of coherent, synergistic interleaved content, while SynJudge provides a benchmark for quantitative assessment.  |
| SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation
  via Skill Blending (Read more on [arXiv](https://arxiv.org/abs/2506.09366) or [HuggingFace](https://huggingface.co/papers/2506.09366))| Tan-Dzung Do, Haoran Geng, jitendra1995, AmineElhafsi, yxK | i) SkillBlender introduces a hierarchical reinforcement learning framework for versatile humanoid loco-manipulation through pre-trained skill blending. ii) The research objective is to develop a versatile and scalable humanoid control system capable of performing diverse loco-manipulation tasks with minimal task-specific reward engineering. iii) The methodology involves pre-training goal-conditioned task-agnostic primitive skills and dynamically blending these skills using a high-level controller that outputs subgoals and per-joint weight vectors. iv) Experiments show SkillBlender significantly outperforms baselines on a newly introduced SkillBench benchmark, exhibiting more accurate and feasible behaviors; for example, SkillBlender achieves a 0.007±0.004 error on the BoxTransfer task compared to 0.421±0.026 for MCP. v) SkillBlender's pretrain-then-blend paradigm offers AI practitioners a method to reduce task-specific reward engineering and improve the versatility of humanoid robots, facilitating the development of more adaptable and robust control systems.  |
| Detecting Harmful Memes with Decoupled Understanding and Guided CoT
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08477) or [HuggingFace](https://huggingface.co/papers/2506.08477))| Anh Tuan Luu, Fengjun Pan, bobxwu | i) This paper introduces U-CoT+, a framework for detecting harmful memes using decoupled understanding and guided reasoning. ii) The main objective is to improve resource efficiency, flexibility, and explainability in harmful meme detection. iii) The framework employs a meme-to-text pipeline to convert visual memes into textual descriptions, followed by guided CoT prompting using human-crafted guidelines. iv) Experiments on seven benchmark datasets show U-CoT+ achieves performance comparable to state-of-the-art supervised baselines, with small-scale LLMs achieving 72.90 (Acc) and 72.87 (F1) on the FHM dataset. v) The framework's decoupled approach and guided CoT prompting enable resource-efficient and adaptable harmful meme detection, offering a practical solution for content moderation systems and highlighting the potential of small LLMs.  |
| Beyond Homogeneous Attention: Memory-Efficient LLMs via
  Fourier-Approximated KV Cache (Read more on [arXiv](https://arxiv.org/abs/2506.11886) or [HuggingFace](https://huggingface.co/papers/2506.11886))| Yuerong Song, Ruixiao Li, Qiqi Wang, Siyang He, Xiaoran Liu | i) The paper introduces FourierAttention, a training-free KV cache compression framework for memory-efficient LLMs. ii) The research aims to reduce memory demands in LLMs by exploiting heterogeneous roles of transformer head dimensions. iii) The method projects long-context-insensitive head dimensions onto orthogonal Fourier bases, approximating temporal evolution with fixed-length spectral coefficients. iv) Evaluations on LLaMA models show FourierAttention achieves superior long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH) tasks compared to existing methods; the method achieves a 94.04 overall score, as shown in Figure 5. v) FourierAttention offers AI practitioners a method for deploying LLMs in resource-constrained environments by optimizing KV cache memory without sacrificing long-context performance; FlashFourierAttention enables streamlined read-write operations.  |
| SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement
  Learning for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08989) or [HuggingFace](https://huggingface.co/papers/2506.08989))| Yang Wang, Yeyun Gong, Zhong-Zhi Li, Xiao Liang, yegong | i) This paper introduces Self-aware Weakness-driven problem Synthesis (SwS), a reinforcement learning framework that addresses model deficiencies by synthesizing targeted training problems. ii) The research aims to improve the reasoning capabilities of large language models (LLMs) by identifying and augmenting training data based on self-aware weaknesses. iii) SwS identifies weaknesses during preliminary RL training, extracts core concepts from failure cases, and synthesizes new problems to target these weaknesses. iv) Experiments on 7B and 32B models show average performance gains of 10.0% and 7.7%, respectively, across eight mainstream reasoning benchmarks. v) SwS enables AI practitioners to enhance LLM reasoning by generating targeted synthetic data that addresses specific model weaknesses, leading to more efficient RL training.  |
| DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware
  Regressive GRPO (Read more on [arXiv](https://arxiv.org/abs/2506.07464) or [HuggingFace](https://huggingface.co/papers/2506.07464))| Hyunwoo J. Kim, Jinyoung Kim, Jeehye Na, Jinyoung Park | i) DeepVideo-R1 introduces a video reinforcement fine-tuning method using a regressive Group Relative Policy Optimization (Reg-GRPO) and difficulty-aware data augmentation for video LLMs. ii) The research aims to enhance the reasoning capabilities of video LLMs by addressing the limitations of GRPO, specifically safeguard reliance and the vanishing advantage problem. iii) The methodology involves reformulating the GRPO objective as a regression task to directly predict advantage, and employing a difficulty-aware data augmentation strategy to dynamically adjust training sample difficulty. iv) Experiments show DeepVideo-R1 achieves a 10.06 performance improvement compared to GRPO on the validation split of SEED-Bench-R1 dataset. v) The principal implication for AI practitioners is the demonstration of combining a regression-based RL objective with data augmentation to improve video reasoning performance in large-scale multimodal reasoning models.  |
| Configurable Preference Tuning with Rubric-Guided Synthetic Data (Read more on [arXiv](https://arxiv.org/abs/2506.11702) or [HuggingFace](https://huggingface.co/papers/2506.11702))| vicgalle | This paper introduces Configurable Preference Tuning (CPT), a framework for dynamically adjusting language model behavior using explicit directives. The main research objective is to endow LLMs with the ability to modulate outputs based on human-interpretable instructions without retraining. CPT leverages synthetically generated preference data conditioned on system prompts derived from structured rubrics defining attributes like writing style. Experiments showed that CPT-distilled models achieved an accuracy of 0.83 compared to a baseline of 0.60 using Mistral-Nemo-12B in matching target quality bins. These models can better align with specified quality categories, enabling AI practitioners to achieve fine-grained control over language model outputs for diverse applications.  |
| ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual
  Perception in VLMs (Read more on [arXiv](https://arxiv.org/abs/2506.10128) or [HuggingFace](https://huggingface.co/papers/2506.10128))| Yuhang Zhou, Yongyuan Liang, Chao Feng, Zhengyuan Yang, Xiyao Wang | i) The paper introduces ViCrit, a reinforcement learning proxy task for enhancing visual perception in vision-language models (VLMs) by training them to identify synthetically injected visual hallucinations in image captions. ii) The primary objective is to develop a challenging, verifiable task that improves VLMs' visual perception capabilities beyond object memorization. iii) The methodology involves training VLMs with a reinforcement learning framework, using a reward signal based on exact string matching to localize injected hallucinations in human-written image captions. iv) The results show that VLMs trained with ViCrit exhibit substantial gains across various VL benchmarks, including a 3.4% average accuracy improvement on general vision-language tasks for a 72B parameter model. v) The principal implication for AI practitioners is the provision of an effective, generalizable objective for enhancing visual perception in VLMs, enabling improvements in tasks such as abstract image reasoning and visual math.  |
| A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data (Read more on [arXiv](https://arxiv.org/abs/2506.11130) or [HuggingFace](https://huggingface.co/papers/2506.11130))| Hsi-Chun Cheng, Liang-Hsuan Tseng, Ho-Lam Chung, Chan-Jan Hsu, Cheng Kang Chou | i) This paper introduces a self-refining framework leveraging TTS-synthesized data to improve ASR performance using only unlabeled data. ii) The research aims to enhance ASR capabilities, specifically in low-resource languages and code-switching scenarios, by exploiting unlabeled data through a self-improvement cycle. iii) The methodology involves using an existing ASR model to generate pseudo-labels for unlabeled speech, training a TTS system on these pseudo-labels, and then bootstrapping the ASR model with synthesized speech-text pairs. iv) The resulting ASR model, Twister, reduces error rates by up to 20% on Mandarin and 55.88% on Mandarin-English code-switching benchmarks compared to the Whisper-large-v2 baseline. v) The framework provides AI practitioners a practical and data-efficient alternative to self-distillation approaches for improving ASR models in data-scarce scenarios, reducing the reliance on large volumes of real, labeled speech data.  |
| Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity
  Dilemma of Embeddings (Read more on [arXiv](https://arxiv.org/abs/2506.08592) or [HuggingFace](https://huggingface.co/papers/2506.08592))| Fandong Meng, Jiangnan Li, Mo Yu, Zhenlin Su, lxucs | i) The paper introduces CapRetrieval, a Chinese image caption retrieval dataset, to reveal limitations in dense retrievers' ability to encode fine-grained semantics. ii) The research investigates why dense retrievers fail on seemingly simple queries requiring fine-grained entity or event recognition. iii) The study constructs a new Chinese dataset, CapRetrieval, of image captions and entity/event queries and evaluates zero-shot and fine-tuned encoders. iv) Zero-shot evaluations show encoders struggle with fine-grained matching regardless of size (0.1B to 7B), while finetuning with data generation strategies improves performance, with a finetuned 0.1B model outperforming 7B baselines, although analysis reveals a granularity dilemma where fine-grained salience conflicts with overall semantic understanding. v) AI practitioners should consider the granularity dilemma when composing training data for dense retrievers, as emphasis on fine-grained details can compromise broader semantic encoding.  |
| JAFAR: Jack up Any Feature at Any Resolution (Read more on [arXiv](https://arxiv.org/abs/2506.11136) or [HuggingFace](https://huggingface.co/papers/2506.11136))| Matthieu Cord, Jean-Emmanuel Haugeard, Louis Serrano, Loick Chambon, Paul Couairon | i) The paper introduces JAFAR, a lightweight attention-based feature upsampler for foundation vision encoders. ii) The research aims to enhance the spatial resolution of visual features from any foundation vision encoder to an arbitrary target resolution. iii) JAFAR employs an attention-based module that promotes semantic alignment between high-resolution queries and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. iv) Experiments show JAFAR achieves a +1.63 mIoU improvement on average over existing methods across semantic segmentation benchmarks. v) JAFAR provides AI practitioners with a versatile drop-in module for improving feature resolution and performance in various downstream vision tasks without high-resolution supervision.  |
| Inherently Faithful Attention Maps for Vision Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.08915) or [HuggingFace](https://huggingface.co/papers/2506.08915))| Diego Marcos, Dino Ienco, Cassio F. Dantas, ananthu-aniraj | i) This paper introduces iFAM, an attention-based method leveraging learned binary attention masks to improve model robustness against spurious correlations by restricting the receptive field of vision transformers (ViTs) to task-relevant regions. ii) The main objective is to develop a method that ensures attention maps are inherently faithful to the model's reasoning, enhancing robustness against spurious correlations and out-of-distribution backgrounds. iii) iFAM uses a two-stage framework: the first stage discovers object parts and task-relevant regions using PDiscoFormer, and the second stage restricts the ViT's receptive field to these regions via input attention masking. iv) Experiments show iFAM improves worst group accuracy (WGA) on MetaShift from 81.0% to 88.6% and from 94.0% to 97.0% on Waterbirds, indicating better robustness against background shifts. v) iFAM provides AI practitioners with a technique to create more robust vision models, reducing reliance on spurious correlations and improving generalization in diverse deployment scenarios, especially where contextual biases are prevalent.  |
