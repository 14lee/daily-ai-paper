

## Papers for 2025-04-25

| Title | Authors | Summary |
|-------|---------|---------|
| Step1X-Edit: A Practical Framework for General Image Editing (Read more on [arXiv](https://arxiv.org/abs/2504.17761) or [HuggingFace](https://huggingface.co/papers/2504.17761))| Peng Xing, Yucheng Han, Shiyu Liu, skicy, wchengad | Step1X-Edit introduces an open-source framework for general, instruction-based image editing designed to rival proprietary models. The research aims to develop and evaluate a unified image editing model that effectively understands natural language instructions and performs diverse, high-fidelity edits, bridging the gap with closed-source systems. The methodology combines a Multimodal Large Language Model (MLLM) for instruction and image understanding with a DiT-style diffusion decoder for image generation, trained on a custom-generated dataset of over 1 million high-quality editing triplets across 11 categories. Evaluated on the introduced GEdit-Bench benchmark using GPT-4.1, Step1X-Edit achieves a G_O score of 6.701 (Full set, English instructions), significantly outperforming open-source baselines like OmniGen (5.061) and approaching proprietary models like Gemini2 Flash (6.315) and GPT-40 (7.534). For AI practitioners, this work provides a high-performing, open-source alternative for instruction-based image editing, along with a large-scale dataset and benchmark, facilitating the development and evaluation of general-purpose image manipulation capabilities. |
| RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.17502) or [HuggingFace](https://huggingface.co/papers/2504.17502))| Michal Sokolik, Brian Gordon, Yonatan Bitton, Hagai Taitelbaum, lovodkin93 | REFVNLI introduces a cost-effective, fine-tuned VLM metric for evaluating both subject preservation and textual alignment in subject-driven text-to-image generation. The objective is to develop a single, automatic metric that reliably assesses both subject identity fidelity and prompt adherence in subject-driven T2I outputs, overcoming the limitations of costly or single-aspect existing evaluators. The methodology involves fine-tuning a PaliGemma vision-language model on a large-scale dataset (1.2M triplets) automatically curated from video reasoning benchmarks and image perturbations, performing sequential binary classifications for textual alignment then subject preservation. Primary results show REFVNLI matches or outperforms baselines across multiple benchmarks, achieving up to 8.5-point ROC AUC gains in subject consistency (ImagenHub, Multi-subject) and aligning with human preferences on rare entities over 87% of the time. For AI practitioners, REFVNLI provides a scalable, more reliable automated evaluation method for subject-driven T2I models, enabling faster iteration without dependence on expensive API calls or separate metrics that may not capture both essential generation qualities. |
| Paper2Code: Automating Code Generation from Scientific Papers in Machine
  Learning (Read more on [arXiv](https://arxiv.org/abs/2504.17192) or [HuggingFace](https://huggingface.co/papers/2504.17192))| Sung Ju Hwang, Seongyun Lee, jinheon, iaminju | PaperCoder is a multi-agent LLM framework designed to automatically generate functional code repositories from machine learning research papers. The objective is to improve research reproducibility by automating the conversion of scientific papers into executable code, addressing the common issue of unavailable implementations. The methodology involves a three-stage pipeline using specialized LLM agents: Planning (roadmap, architecture design with diagrams, dependency/configuration generation), Analysis (interpreting file-specific implementation details), and Coding (generating modular, dependency-aware code sequentially). Evaluation on benchmarks like PaperBench showed PaperCoder achieved a 44.26% replication score, significantly outperforming strong baselines (e.g., 16.4% for Iterative Agent), and generated code required minimal manual modification (averaging 0.48% of lines changed) for execution in case studies. For AI practitioners, PaperCoder demonstrates a viable approach to automatically generate high-fidelity, nearly executable code directly from research papers, substantially reducing the effort required to reproduce results and build upon existing work when official code is missing. |
| Breaking the Modality Barrier: Universal Embedding Learning with
  Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17432) or [HuggingFace](https://huggingface.co/papers/2504.17432))| Yanzhao Zhang, Xingjun Wang, Ziyong Feng, Tiancheng Gu, Kaichengalex | This paper introduces UniME, a two-stage framework using Multimodal Large Language Models (MLLMs) for universal multimodal embedding learning. The primary objective is to overcome limitations of CLIP and standard MLLMs to learn discriminative and transferable representations for diverse downstream vision-language tasks. UniME utilizes a two-stage methodology: (1) textual discriminative knowledge distillation from an LLM teacher model to enhance the MLLM's language component, and (2) hard negative enhanced instruction tuning with false negative filtering and hard negative sampling. Experimental results demonstrate superior performance, with UniME based on LLaVA-1.6 achieving a 66.6% overall score on the MMEB benchmark, a 3.3% improvement over the VLM2Vec baseline. For AI practitioners, this framework offers a method to adapt MLLMs for generating highly discriminative universal embeddings applicable across tasks like retrieval, VQA, classification, and grounding, enhancing multimodal understanding capabilities. |
| Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2504.17207) or [HuggingFace](https://huggingface.co/papers/2504.17207))| Leonidas Guibas, Mikaela Angelina Uy, Chanho Park, Jihyeon Je, Phillip Y. Lee | This paper introduces Abstract Perspective Change (APC), a framework enabling vision-language models (VLMs) to perform spatial reasoning from arbitrary viewpoints by simulating mental imagery. The main objective is to overcome the inherent egocentric bias in VLMs and equip them with robust allocentric reasoning capabilities necessary for understanding scenes from perspectives other than the camera's. APC utilizes vision foundation models (object detection, segmentation, orientation estimation) to build a coarse 3D scene abstraction, transforms this abstraction into the reference viewer's egocentric coordinate frame via coordinate transformation, and then prompts the VLM with this transformed representation (either numerically or visually). Experiments show APC significantly outperforms existing VLMs and reconstruction-based approaches, achieving 72.78% accuracy on the challenging 3DSRBench left/right spatial reasoning task using its visual prompt variant (APC-Vis), compared to significantly lower scores for baselines on real images. For AI practitioners, APC provides a concrete methodology to enhance VLM spatial intelligence for tasks requiring perspective shifts (like robotics or embodied AI) by effectively converting allocentric problems into egocentric ones that VLMs can readily solve. |
| QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM
  Pretraining (Read more on [arXiv](https://arxiv.org/abs/2504.16511) or [HuggingFace](https://huggingface.co/papers/2504.16511))| Yifan Zhang, Zhimiao Yu, Binbin Liu, Weidong Zhou, Fengze Liu | This paper introduces QuaDMix, a unified framework to automatically optimize data selection for large language model (LLM) pretraining by jointly balancing data quality and diversity. The primary objective is to address the challenge of the inherent trade-off between quality and diversity, which prior methods often optimize separately. QuaDMix employs multiple quality scoring criteria and domain classification, feeding these features into a unified parameterized sampling function; optimal parameters are found efficiently using simulated experiments on small proxy models and a LightGBM regression model to predict performance. Experiments demonstrate QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks compared to methods optimizing quality or diversity independently, confirming the necessity of joint optimization. For AI practitioners, QuaDMix offers an automated approach to curate more effective pretraining datasets by systematically balancing quality and diversity, potentially improving LLM efficiency and downstream performance. |
| Token-Shuffle: Towards High-Resolution Image Generation with
  Autoregressive Models (Read more on [arXiv](https://arxiv.org/abs/2504.17789) or [HuggingFace](https://huggingface.co/papers/2504.17789))| Chih-Yao Ma, Hao Tang, Haoyu Ma, Peize Sun, Xu Ma | Token-Shuffle enables efficient high-resolution (up to 2048x2048) image generation with autoregressive models by reducing the computational load of visual tokens. The primary objective is to overcome the computational and resolution constraints imposed by the large number of image tokens required by standard autoregressive Multimodal Large Language Models (MLLMs) for image synthesis. Key methodology involves 'token-shuffle' to merge spatially local tokens via MLPs before Transformer blocks and 'token-unshuffle' to restore spatial arrangement after, exploiting visual vocabulary dimensional redundancy within a VQGAN-tokenized Llama framework. A 2.7B parameter Token-Shuffle model achieved a 0.77 overall score on the GenAI-benchmark hard prompts, outperforming prior AR models like LlamaGen by 0.18. This provides AI practitioners a plug-and-play method to build more efficient and higher-resolution autoregressive image generation MLLMs using existing architectures with minimal modification and without needing additional text encoders. |
| Distilling semantically aware orders for autoregressive image generation (Read more on [arXiv](https://arxiv.org/abs/2504.17069) or [HuggingFace](https://huggingface.co/papers/2504.17069))| David Vazquez, Masih Aminbeidokhti, Juan A. Rodriguez, Antoine Poupon, rishavpramanik | This paper introduces an autoregressive image generation method that learns a semantically aware order for generating image patches, improving quality over traditional raster-scan approaches. The main objective is to determine if learning an optimal, content-dependent patch generation order can enhance the quality of autoregressive image generation models without requiring additional annotations. The key methodology involves first training an any-given-order autoregressive model, using it to infer optimal generation orders for the training data via likelihood maximization at each step, and then fine-tuning the model using these self-supervised, distilled orders combined with relative positional encoding. On the Fashion Product dataset, the proposed fine-tuned ordered generation method achieved a Fréchet Inception Distance (FID) of 2.56, significantly improving upon the 4.58 FID of the baseline raster-scan model. For AI practitioners, this work demonstrates that the generation order in patch-based autoregressive models is a crucial factor impacting image quality, and learning this order dynamically based on content offers a viable path to enhance generation performance using self-supervision. |
| DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17040) or [HuggingFace](https://huggingface.co/papers/2504.17040))| Heng Ji, Silvio Savarese, Caiming Xiong, Senthil Purushwalkam, Zhenhailong Wang | The paper introduces DyMU, a training-free framework to enhance the efficiency of Vision-Language Models (VLMs) by dynamically adjusting visual token counts. The primary objective is to reduce the computational burden of VLMs, stemming from fixed-length visual token sequences, without sacrificing task performance or requiring model retraining. DyMU employs Dynamic Token Merging (DToMe) to merge redundant visual tokens based on image complexity and Virtual Token Unmerging (VTU) to efficiently reconstruct attention dynamics for the Large Language Model (LLM) using Rotary Position Embeddings (RoPE). Experiments demonstrate DyMU can reduce average visual token counts by 32%-85%; specifically, DyMU-low on LLaVA-1.5 achieved 97.7% of the baseline performance using only ~15% of the original visual tokens. For AI practitioners, DyMU offers a plug-and-play method to significantly decrease the inference costs and computational requirements of existing VLMs without fine-tuning, enabling more efficient deployment. |
| IberBench: LLM Evaluation on Iberian Languages (Read more on [arXiv](https://arxiv.org/abs/2504.16921) or [HuggingFace](https://huggingface.co/papers/2504.16921))| Areg Mikael Sarvazyan, Álvaro Romo Herrero, Ian Borrego Obrador, José Ángel González, mchinea | IberBench introduces a comprehensive, extensible benchmark for evaluating LLM performance across Iberian languages, including Spanish varieties, focusing on both fundamental and industry-relevant tasks. The main objective is to assess LLM capabilities beyond English, addressing the limited coverage of Iberian languages, linguistic varieties, and task types in existing static benchmarks. The methodology involves compiling 101 datasets covering 22 task categories, standardizing them, and evaluating 23 LLMs (100M to 14B parameters) using a custom, incremental evaluation pipeline built upon `lm-evaluation-harness`, primarily in a zero-shot setting. Results indicate that LLMs generally perform worse on industry-relevant tasks than fundamental ones, exhibit lower average performance in Galician and Basque, and the top-performing model (Qwen-2.5-7b-Instruct) achieved a mean score of 46.8% across tasks and languages. For AI practitioners, IberBench provides a standardized tool and public leaderboard to compare LLM suitability for specific Iberian language applications, highlighting current model limitations in areas like industry relevance and performance on less-resourced languages like Basque and Galician, guiding model selection and development focus. |
| Boosting Generative Image Modeling via Joint Image-Feature Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.16064) or [HuggingFace](https://huggingface.co/papers/2504.16064))| Nikos Komodakis, Spyros Gidaris, Ioannis Kakogeorgiou, Efstathios Karypidis, Theodoros Kouzelis | ReDi enhances generative image modeling by jointly synthesizing low-level VAE image latents and high-level DINOv2 semantic features within a unified diffusion framework. The primary objective is to integrate representation learning directly into the generative process to improve synthesis quality and training convergence speed, bypassing explicit distillation steps. The core methodology involves modifying standard Diffusion Transformer architectures (DiT/SiT) to operate on a combined input of noise-corrupted VAE latents and PCA-reduced DINOv2 features, learning the joint distribution via a shared denoising objective, and introducing Representation Guidance for inference refinement. Key results demonstrate substantial improvements, such as accelerating SiT-XL/2 convergence by >6x compared to prior representation alignment methods (REPA) and achieving a state-of-the-art FID of 1.64 on ImageNet 256x256 (CFG). For AI practitioners, ReDi offers a method to train high-fidelity generative models significantly faster and introduces Representation Guidance as a novel technique to steer image generation using learned semantic features. |
| ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting (Read more on [arXiv](https://arxiv.org/abs/2504.15921) or [HuggingFace](https://huggingface.co/papers/2504.15921))| Mariano Beguerisse-Diaz, Shaogang Gong, Dimitrios Korkinof, Jian Hu | ViSMaP presents an unsupervised method for summarizing hour-long videos by adapting models trained on short videos using meta-prompting. The objective is to generate coherent summaries for long, unannotated videos, bridging the semantic gap between short segment descriptions and holistic long video narratives without costly long-form annotations. Key methodology involves pre-training a model on short videos, using iterative meta-prompting with multiple LLMs (generator, evaluator, optimizer) to create pseudo-summaries for long videos, and fine-tuning the model using these pseudo-summaries with a noisy label loss. Primary results demonstrate performance comparable to supervised methods, achieving a 26.0 CIDEr score on the Ego4D-HCap dataset versus 29.3 for the fully supervised VideoReCap, despite requiring no human annotations for the long videos. This provides AI practitioners a framework to summarize large volumes of unlabelled long videos by leveraging existing short-video datasets and LLMs, drastically reducing the need for expensive manual annotation. |
| 3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.17414) or [HuggingFace](https://huggingface.co/papers/2504.17414))| Fan Wang, Jingkai Zhou, Chaohui Yu, Min Wei | 3DV-TON introduces a diffusion-based video try-on framework employing textured 3D guidance to enhance temporal consistency and fidelity. The primary objective is to generate high-quality, temporally coherent try-on videos by mitigating the appearance bias inherent in pixel-reconstruction objectives which often leads to motion artifacts. Its methodology utilizes adaptively generated, animatable textured 3D meshes as explicit frame-level guidance for a diffusion model, complemented by a rectangular masking strategy to prevent information leakage. Quantitatively, on the ViViD dataset, 3DV-TON* achieved a paired VFID_I3D score of 10.9680, surpassing the baseline ViViD method's score of 17.2924 (lower indicates better generation quality and temporal consistency). For AI practitioners, the key implication is that leveraging explicit, textured 3D spatio-temporal guidance within diffusion models can significantly improve motion coherence and detail preservation in complex video generation tasks like virtual try-on. |
| TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming
  Videos (Read more on [arXiv](https://arxiv.org/abs/2504.17343) or [HuggingFace](https://huggingface.co/papers/2504.17343))| Shuhuai Ren, Lei Li, Yuancheng Wei, Yicheng Li, Linli Yao | TimeChat-Online introduces the Differential Token Drop (DTD) module to achieve efficient streaming video understanding by exploiting visual redundancy. The primary objective is to enable real-time, interactive VideoLLMs capable of handling long, redundant streaming videos and performing proactive responses. The core methodology involves the DTD module, which adaptively drops visually similar tokens between consecutive frames based on pixel or feature-level similarity, preserving only significant temporal changes and associated spatial-temporal positions. Experiments show DTD removes 82.8% of video tokens on StreamingBench while retaining 98% of the original accuracy and achieving a 1.76x latency speedup. For AI practitioners, this demonstrates that leveraging natural video redundancy through query-agnostic token dropping can drastically reduce computational costs and improve efficiency for deploying VideoLLMs in real-time streaming applications, especially for long-duration videos. |
| Interpretable non-linear dimensionality reduction using gaussian
  weighted linear transformation (Read more on [arXiv](https://arxiv.org/abs/2504.17601) or [HuggingFace](https://huggingface.co/papers/2504.17601))| erikbergh | This paper introduces an interpretable non-linear dimensionality reduction method using Gaussian-weighted linear transformations. The objective is to combine the representational power of non-linear techniques with the interpretability of linear methods. The methodology involves constructing a non-linear mapping as a weighted sum of multiple linear transformations, where weights are derived from normalized Gaussian functions centered throughout the data space, optimized to preserve pairwise distances. Applied to a 3D S-curve dataset reduced to 2D, the algorithm achieved a reconstruction error of 0.45 and allowed for quantifying dimension influence (e.g., y-dimension contributed 0.25 overall influence) and visualizing spatial variations in transformation properties like influence skewness and space contraction. For AI practitioners, this offers a dimensionality reduction tool that provides non-linear expressiveness while enabling analysis of how original dimensions contribute to the reduced space and how geometric properties change locally. |
| Process Reward Models That Think (Read more on [arXiv](https://arxiv.org/abs/2504.16828) or [HuggingFace](https://huggingface.co/papers/2504.16828))| Hao Peng, Jaekyeom Kim, Lajanugen Logeswaran, Rishabh Agarwal, Muhammad Khalifa | This paper introduces THINKPRM, a data-efficient generative process reward model (PRM) that verifies reasoning steps using long Chain-of-Thought (CoT). The objective is to create PRMs that are both high-performing and require significantly less supervision data compared to traditional discriminative PRMs. The methodology involves lightweight fine-tuning of large reasoning models on a small dataset of filtered synthetic verification CoTs (using as few as 8K process labels). Key results show THINKPRM outperforms discriminative PRMs trained on ~100x more data and LLM-as-a-judge baselines, achieving an 8% improvement over a discriminative PRM on a GPQA-Diamond subset despite using far less training data. For AI practitioners, this demonstrates the potential to build powerful and scalable reasoning verifiers with minimal supervision by leveraging generative models and CoT verification, reducing reliance on large labeled process datasets. |
