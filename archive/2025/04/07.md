

## Papers for 2025-04-07

| Title | Authors | Summary |
|-------|---------|---------|
| Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving (Read more on [arXiv](https://arxiv.org/abs/2504.02605) or [HuggingFace](https://huggingface.co/papers/2504.02605))| Linhao Zhang, Hanwu Chen, Wei Liu, Zhirong Huang, Daoguang Zan | This paper introduces Multi-SWE-bench, a multilingual benchmark for evaluating Large Language Models (LLMs) on software issue resolving tasks across diverse programming languages. The main objective is to overcome the limitations of existing Python-centric benchmarks like SWE-bench by providing a comprehensive evaluation framework for Java, TypeScript, JavaScript, Go, Rust, C, and C++. The methodology involved a five-phase pipeline including repository selection, pull request crawling, environment determination, automated filtering based on test outcomes, and rigorous manual verification by 68 experts, resulting in 1,632 high-quality instances; state-of-the-art LLMs were then evaluated using Agentless, SWE-agent, and OpenHands methods. Primary results show existing models struggle to generalize beyond Python, with performance significantly decreasing on complex tasks; for instance, resolved rates drop sharply when fix patches exceed 600 tokens or involve multiple files, indicating weaknesses in long-context retention and multi-file reasoning. For AI practitioners, Multi-SWE-bench offers a robust tool for assessing LLM capabilities in realistic, multilingual software engineering scenarios, revealing current limitations and guiding future development, alongside releasing initial datasets and infrastructure for reinforcement learning (Multi-SWE-RL) in this domain. |
| Agentic Knowledgeable Self-awareness (Read more on [arXiv](https://arxiv.org/abs/2504.03553) or [HuggingFace](https://huggingface.co/papers/2504.03553))| Xiangyuan Ru, Xiaobin Wang, Baochang Ren, Zhisong Qiu, Shuofei Qiao | This paper introduces agentic knowledgeable self-awareness, enabling LLM agents to autonomously regulate knowledge utilization based on situational difficulty. The research objective is to overcome the limitations of traditional "flood irrigation" methods by allowing agents to decide when to use internal capabilities, reflect, or seek external knowledge. The proposed method, KnowSelf, employs a heuristic situation judgment criterion on self-explored trajectories and a two-stage (SFT + RPO) training process using special tokens to signify different cognitive states (fast, slow, knowledgeable thinking). Experiments demonstrate KnowSelf achieves superior performance with minimal knowledge; for instance, on ALFWorld using Llama-8B, it attained an 84.33% average reward while using external knowledge for only 15.01% of actions, outperforming baselines. For AI practitioners, this implies a method to train more efficient agents that dynamically manage computational resources (like reflection or knowledge retrieval) based on assessed task complexity, potentially reducing inference costs and improving robustness. |
| MegaMath: Pushing the Limits of Open Math Corpora (Read more on [arXiv](https://arxiv.org/abs/2504.02807) or [HuggingFace](https://huggingface.co/papers/2504.02807))| Liping Tang, Zhoujun Cheng, Nikhil Ranjan, Zengzhi Wang, Fan Zhou | MegaMath introduces a large-scale, 371B token open dataset specifically curated for math-centric LLM pre-training. The primary objective was to address the lack of open, high-quality, large-scale corpora tailored for mathematical reasoning in LLMs. Methodology involved re-extracting and filtering Common Crawl data with math-specific optimizations, recalling math-relevant code from Stack-V2, and synthesizing QA, translated code, and interleaved text-code data. Key results demonstrate MegaMath's scale and quality, with subsets like MegaMath-Web-Pro (15.1B tokens) outperforming existing open math corpora like FineMath-4+ by â‰¥ 4% in comparative pre-training evaluations, and boosting Llama-3 CoT performance by 15-20%. For AI practitioners, MegaMath provides a high-quality, large-scale open resource enabling the pre-training of more capable mathematical reasoning LLMs, previously hindered by the scarcity of suitable open datasets. |
| SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge
  Refinement (Read more on [arXiv](https://arxiv.org/abs/2504.03561) or [HuggingFace](https://huggingface.co/papers/2504.03561))| Jialong Wu, Shuofei Qiao, Yuan Liang, Xiaobin Wang, Runnan Fang | SynWorld introduces a framework for LLM-based agents to refine action knowledge by synthesizing virtual scenarios and using Monte Carlo Tree Search (MCTS) for exploration. The primary objective is to enable agents to autonomously enhance their understanding of actions and optimize workflows in novel or complex environments. The methodology involves synthesizing multi-step task scenarios conditioned on tool subsets and applying iterative MCTS optimization to refine action descriptions and cognitive workflows based on simulated environmental feedback. Key results demonstrate SynWorld's effectiveness, achieving a 59.33 PASS score on ToolBench using GPT-4-turbo, outperforming several baseline methods. For AI practitioners, this implies a viable approach to automatically adapt agents to new tools and environments, improving planning and execution capabilities through simulated experience, thereby reducing reliance on manual annotation for action knowledge refinement. |
| MME-Unify: A Comprehensive Benchmark for Unified Multimodal
  Understanding and Generation Models (Read more on [arXiv](https://arxiv.org/abs/2504.03641) or [HuggingFace](https://huggingface.co/papers/2504.03641))| Bingyan Nie, Yang Shi, Chaoyou Fu, Yi-Fan Zhang, Wulin Xie | This paper introduces MME-Unify (MME-U), a comprehensive benchmark to evaluate Unified Multimodal Large Language Models (U-MLLMs) across understanding, generation, and novel unified tasks. The primary objective was to create a standardized evaluation framework addressing the lack of unified standards and benchmarks for mixed-modality generation capabilities in U-MLLMs. The methodology involved curating tasks from 12 datasets, standardizing formats (e.g., multiple-choice QA, normalized scores), and designing five new 'unify' tasks (e.g., Visual CoT, Image Editing & Explaining) requiring synergistic understanding and generation. Evaluations of 12 U-MLLMs revealed significant room for improvement, especially in instruction following and unified tasks, with the top model Gemini2.0-flash-exp achieving an MME-U score of 45.57, while many models struggled significantly on complex unified tasks. For AI practitioners, this highlights current U-MLLM limitations in reliably performing complex, integrated multimodal reasoning and generation, underscoring the need for improved model architectures and training strategies for robust real-world deployment. |
| VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via
  Iterative Instruction Tuning and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.02949) or [HuggingFace](https://huggingface.co/papers/2504.02949))| Liming Liang, Dongchao Yang, Yufan Deng, Yuxin Xie, Xianwei Zhuang | VARGPT-v1.1 presents an improved unified visual autoregressive model for enhanced understanding and generation tasks. The objective is to advance the VARGPT framework by improving instruction-following, generation quality, and overall multimodal performance through enhanced training strategies and data scaling. Key methodology combines iterative visual instruction tuning (SFT) on an expanded 8.3M visual-generative instruction pair corpus with Direct Preference Optimization (DPO) reinforcement learning, upgrades the LLM backbone to Qwen2-7B, increases generation resolution, and enables editing capabilities via SFT. The model achieves state-of-the-art results on multimodal understanding benchmarks, such as 81.01 on MMBench, significantly improving comprehension and generation metrics over its predecessor and comparable models. For AI practitioners, this work demonstrates that iterative SFT and DPO-based RL within a purely visual autoregressive framework can yield highly capable unified multimodal systems, offering an alternative architecture to diffusion-based or separate component approaches. |
| APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated
  Agent-Human Interplay (Read more on [arXiv](https://arxiv.org/abs/2504.03601) or [HuggingFace](https://huggingface.co/papers/2504.03601))| Ming Zhu, Jianguo Zhang, Weiran Yao, Zuxin Liu, Akshara Prabhakar | This paper introduces APIGen-MT, a two-phase framework for generating verifiable multi-turn agent interaction data via simulated agent-human interplay. The primary objective was to overcome the scarcity of high-quality, realistic multi-turn data needed for training capable AI agents. The methodology involves first generating verified task blueprints using an agentic pipeline with LLM reviewers and feedback, followed by simulating human-agent interactions based on these blueprints to create full trajectories. Key results show models trained on this data (xLAM-2-fc-r series) outperform strong baselines; for instance, the 70B model achieved 78.19% accuracy on BFCL v3, surpassing GPT-40, with smaller models also demonstrating superior multi-turn consistency. For AI practitioners, this work provides open-source, high-quality synthetic data and models enabling the development of more reliable agents for complex, multi-turn interactions, potentially allowing smaller models to achieve performance comparable to larger ones. |
| HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction
  via Gaussian Restoration (Read more on [arXiv](https://arxiv.org/abs/2504.03536) or [HuggingFace](https://huggingface.co/papers/2504.03536))| Guosheng Zhao, Xiaofeng Wang, Runqi Ouyang, Boyuan Wang, ZhengZhu | HumanDreamer-X introduces a unified pipeline for photorealistic single-image 3D human avatar reconstruction by integrating multi-view generation and Gaussian restoration. The primary objective is to overcome geometric inconsistencies and visual artifacts like fragmented limbs common in decoupled generation-then-reconstruction approaches for single-view inputs. The methodology involves initial coarse avatar reconstruction using 3D Gaussian Splatting (3DGS), rendering multi-view video frames, refining these frames with a video restoration model named HumanFixer which incorporates an attention modulation strategy, and subsequently using the restored video to enhance the 3DGS model. Key results show significant improvements over existing methods, achieving up to 25.62 dB PSNR in reconstruction quality, a 12.65% increase compared to prior SOTA on CustomHumans. For AI practitioners, this work demonstrates a technique combining explicit 3D representation (3DGS) with generative video restoration and attention modulation to create higher-quality, consistent digital humans from minimal input, applicable to virtual avatar creation and animation. |
| TransMamba: Flexibly Switching between Transformer and Mamba (Read more on [arXiv](https://arxiv.org/abs/2503.24067) or [HuggingFace](https://huggingface.co/papers/2503.24067))| Shuaipeng Li, Xingwu Sun, Ruobing Xie, andyyang, Yixinglee | This paper proposes TransMamba, a framework unifying Transformer and Mamba using shared parameters to switch dynamically between attention and state space model (SSM) mechanisms. The objective is to leverage the strengths of both Transformer (short context efficiency) and Mamba (long context efficiency) within a single flexible architecture, overcoming static hybrid model limitations. TransMamba utilizes shared QKV/CBx parameters and introduces a "Memory Converter" for lossless state transfer at designated sequence positions ("TransPoints"), with a scheduling strategy determining the switch points across layers. Experiments show TransMamba achieves superior efficiency (e.g., 0.75 relative training time vs. 1.00 for Transformer at 1.5B parameters) and performance on benchmarks like LongBench-v2 (38.76 overall score vs. 31.61 for Transformer-1.5B) compared to baseline Transformer, Mamba2, and static Hybrid models. For AI practitioners, TransMamba presents a scalable architecture potentially offering improved training/inference efficiency and performance, especially for applications involving variable sequence lengths, by dynamically selecting the optimal computation mechanism (Attention or SSM) per token segment and layer. |
| Comprehensive Relighting: Generalizable and Consistent Monocular Human
  Relighting and Harmonization (Read more on [arXiv](https://arxiv.org/abs/2504.03011) or [HuggingFace](https://huggingface.co/papers/2504.03011))| Zhixin Shu, Krishna Kumar Singh, Xin Sun, Jingyuan Liu, Junying Wang | This paper presents Comprehensive Relighting, a novel diffusion-based framework for generalizable and temporally consistent monocular human relighting and background harmonization. The main objective is to develop a single model capable of controllably relighting humans in images/videos (using Spherical Harmonics or background scenes), ensuring harmonization and temporal coherence across arbitrary body parts and scenes without large-scale supervised video data. The methodology utilizes a pre-trained latent diffusion model in a coarse-to-fine framework conditioned via ControlNet on coarse shading and background inputs, combined with an unsupervisedly trained temporal module (using cycle consistency) integrated via spatio-temporal feature blending and followed by guided refinement. Results show superior performance over baselines, achieving, for example, the best temporal consistency score (tLPIPS of 0.026, lower is better) on a challenging synthetic video benchmark (Scenario 3), compared to the next best (0.028). For AI practitioners, this work demonstrates adapting diffusion priors with conditioning and unsupervised temporal learning offers a potent strategy for tackling complex, data-limited generative video tasks, enabling the development of more robust and controllable video editing/synthesis tools. |
| EvMic: Event-based Non-contact sound recovery from effective
  spatial-temporal modeling (Read more on [arXiv](https://arxiv.org/abs/2504.02402) or [HuggingFace](https://huggingface.co/papers/2504.02402))| Lu Zhang, Xudong XU, Xu Jia, Shi Guo, yyzqy | EvMic introduces a deep learning pipeline for non-contact sound recovery using event cameras, overcoming traditional camera limitations. The objective is to effectively recover sound signals from object vibrations captured by event cameras by modeling spatial-temporal event data. The methodology employs a laser matrix for enhanced gradient capture, a synthetic dataset (EvMic) for training, and a network combining sparse convolutions, Mamba for temporal modeling, and a spatial aggregation block (SAB) for fusing information from multiple locations. The proposed method achieves superior performance on synthetic data, yielding an average SNR of 1.214 dB, significantly outperforming the EvPhase baseline (-0.079 dB). For AI practitioners, this demonstrates the potential of event-based vision and tailored architectures (sparse ConvNets, SSMs like Mamba, attention) for recovering high-frequency signals from subtle physical phenomena, offering a new modality for sensor fusion and signal processing tasks. |
| MedSAM2: Segment Anything in 3D Medical Images and Videos (Read more on [arXiv](https://arxiv.org/abs/2504.03600) or [HuggingFace](https://huggingface.co/papers/2504.03600))| Mohammed Baharoon, Bihui Chen, Sumin Kim, Zongxin Yang, Jun Ma | MedSAM2 is a promptable foundation model for general-purpose 3D medical image and video segmentation. The objective was to create a versatile model capable of segmenting diverse structures across modalities by overcoming the 2D limitations of prior work and enabling efficient large-scale annotation. The methodology involved fine-tuning the lightweight SAM2.1-Tiny architecture on a large curated dataset (>455k 3D pairs, 76k video frames) using bounding box prompts and a human-in-the-loop iterative refinement process. Primary results demonstrate superior segmentation performance over baseline SAM2.1 models across CT, MRI, PET, ultrasound, and endoscopy data, alongside a user study showing an over 85% reduction in manual annotation time for 3D CT lesions. For AI practitioners, MedSAM2 provides an efficient, deployable tool integrated into common platforms (3D Slicer, Gradio, etc.) to significantly accelerate the creation of large-scale annotated medical datasets and streamline segmentation workflows. |
| BEATS: Bias Evaluation and Assessment Test Suite for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.24310) or [HuggingFace](https://huggingface.co/papers/2503.24310))| Lisa Erickson, tbandopa, alokabhishek | This research introduces BEATS, a framework and benchmark using 29 metrics to evaluate Bias, Ethics, Fairness, and Factuality (BEFF) in Large Language Models. The main objective was to develop a systematic framework and establish a standard benchmark for measuring and detecting BEFF metrics within LLMs. Key methodology involved using a curated dataset of 901 evaluation questions, performing inference on five major LLMs, and employing a consortium of three LLMs-as-judges to score responses based on the BEFF metrics, followed by statistical analysis including ANOVA. The primary result showed that 37.65% of generated outputs from tested industry-leading models contained some form of bias, indicating substantial risk. For AI practitioners, this implies a critical need for rigorous bias assessment using tools like BEATS before deploying LLMs, especially in sensitive applications, to inform necessary mitigation strategies. |
