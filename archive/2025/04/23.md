

## Papers for 2025-04-23

| Title | Authors | Summary |
|-------|---------|---------|
| Kuwain 1.5B: An Arabic SLM via Language Injection (Read more on [arXiv](https://arxiv.org/abs/2504.15120) or [HuggingFace](https://huggingface.co/papers/2504.15120))| Omar Hadid, Sara Chrouf, ZeinaD, Moatasem444, Hennara | This paper introduces Kuwain 1.5B, an Arabic-English Small Language Model created via language injection into an existing English model. The primary objective was to efficiently integrate Arabic into an English-centric LLM (TinyLlama 1.1B) without compromising its original knowledge or incurring high retraining costs. The methodology involved expanding the tokenizer with 26K Arabic tokens and inserting 8 new, trainable layers into the model architecture while freezing the original layers, using only 20% of the original English data alongside a large Arabic corpus. Results demonstrated an average 8% performance improvement on Arabic benchmarks compared to the base model, while maintaining comparable performance on English benchmarks (53.28 average score vs. 52.99 for the base model). For AI practitioners, this work presents a resource-efficient language injection technique to expand model capabilities to new languages, especially low-resource ones, without extensive retraining or significant degradation of existing knowledge. |
| TTRL: Test-Time Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.16084) or [HuggingFace](https://huggingface.co/papers/2504.16084))| Xuekai Zhu, Li Sheng, Shang Qu, Yuxin Zuo, iseesaw | This paper introduces Test-Time Reinforcement Learning (TTRL), a method for improving Large Language Models (LLMs) on reasoning tasks using unlabeled test data. The objective is to enable LLM self-evolution using Reinforcement Learning (RL) during inference without access to ground-truth labels, addressing the challenge of reward estimation in this setting. TTRL employs repeated sampling to generate multiple outputs, uses majority voting to estimate a consensus label, and computes rule-based rewards based on this estimate to drive RL training. Experiments show TTRL boosted Qwen-2.5-Math-7B pass@1 performance on AIME 2024 by approximately 159% using only unlabeled test data, and consistently surpassed the performance upper limit implied by the initial model's majority voting accuracy. For AI practitioners, TTRL demonstrates a method for adapting and improving LLMs on new tasks using unlabeled data alone, suggesting a potential pathway for continuous learning and reduced reliance on extensive labeled datasets for RL fine-tuning. |
| The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2504.15521) or [HuggingFace](https://huggingface.co/papers/2504.15521))| Huifeng Yin, Sinuo Liu, Weixuan Wang, Minghao Wu, ChenyangLyu | This paper analyzes over 2,000 multilingual (non-English) benchmarks (2021-2024) to evaluate past, present, and future multilingual benchmarking practices. The primary objective is to assess historical trends, the current alignment of benchmarks with human judgments, and future needs for multilingual evaluation. The methodology involved collecting and annotating 2,024 arXiv papers, analyzing language/task/domain distributions, and correlating LLM performance on benchmarks with human Elo rankings across five languages. Key findings reveal English overrepresentation despite exclusion, poor correlation for translated benchmarks (e.g., MMLU Chinese correlation 0.473 vs. localized CMMLU 0.682), and better alignment for STEM tasks (0.70-0.85 correlation) than traditional NLP tasks like QA (0.11-0.30). The principal implication for AI practitioners is that evaluating multilingual models requires moving beyond translated English benchmarks towards developing localized, culturally authentic, and human-aligned benchmarks for accurate capability assessment. |
| Describe Anything: Detailed Localized Image and Video Captioning (Read more on [arXiv](https://arxiv.org/abs/2504.16072) or [HuggingFace](https://huggingface.co/papers/2504.16072))| Yifan Ding, richardaecn, yala, Boyiliee, longlian | This paper introduces the Describe Anything Model (DAM) for generating detailed captions for specific regions in images and videos. The primary objective is to overcome limitations in existing VLMs regarding precise localization and the generation of detailed, context-aware regional descriptions. DAM employs a focal prompt for high-resolution encoding of target regions and a localized vision backbone that integrates global context with local details using gated cross-attention, trained via a novel semi-supervised data pipeline (DLC-SDP). The model achieves state-of-the-art results on 7 benchmarks, including a 67.3% average accuracy on the newly proposed DLC-Bench. For AI practitioners, DAM offers a robust method for fine-grained visual understanding, enabling applications requiring detailed descriptions of user-specified image or video regions without relying on reference captions for evaluation. |
| Learning Adaptive Parallel Reasoning with Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15466) or [HuggingFace](https://huggingface.co/papers/2504.15466))| Charlie Snell, Long Lian, Jiayi Pan, yala, xiuyul | This paper introduces Adaptive Parallel Reasoning (APR), a framework enabling language models to learn adaptive parallelization of reasoning tasks using parent-child threading. The research objective is to overcome limitations of serialized chain-of-thought (latency, context limits) and simple parallel methods (redundancy, poor coordination) by training models to dynamically orchestrate both serial and parallel computations. APR employs a multi-threading mechanism with `spawn()` and `join()` operations, integrated into the language model's decoding process and optimized end-to-end using reinforcement learning. On the Countdown reasoning task, APR achieved significantly higher accuracy within a fixed context window (83.4% vs. 60.0% for serialized search at 4k context) and better accuracy at equivalent latency (75.2% vs. 57.3% at ~5000ms). The principal implication for AI practitioners is that LMs can be trained to autonomously manage and parallelize their inference-time computation, potentially leading to more efficient and scalable reasoning systems under resource constraints. |
| IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning
  in Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.15415) or [HuggingFace](https://huggingface.co/papers/2504.15415))| Yifan Yao, Jarvis Guo, Yuanxing Zhang, JinChengRen, mdh98 | IV-Bench is introduced as the first comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) specifically on image-grounded video perception and reasoning tasks. The research objective is to assess how effectively MLLMs utilize external static images as indispensable context for video comprehension, a capability largely overlooked by existing benchmarks. The methodology involved creating a dataset of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception, 6 reasoning) using externally sourced, necessary images, followed by evaluating 27 state-of-the-art open and closed-source MLLMs. The primary result shows current MLLMs significantly underperform, with the best model achieving only 28.9% overall accuracy, and performance deteriorating further on reasoning tasks (best at 24.9%). For AI practitioners, this implies a critical need to develop advanced MLLMs with improved mechanisms for integrating external image context into video understanding, as current models struggle significantly with these tasks and simple data format alignment proves insufficient. |
| BookWorld: From Novels to Interactive Agent Societies for Creative Story
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.14538) or [HuggingFace](https://huggingface.co/papers/2504.14538))| Yanghua Xiao, Jiaqing Liang, Tian Qiu, Xintao Wang, Yiting Ran | BookWorld introduces a system for constructing and simulating multi-agent societies based on fictional novels for creative story generation. The primary objective is to explore simulating established fictional worlds and characters using book data, enabling character-driven storytelling and interactive experiences. The methodology involves extracting character profiles, worldview data, and map information from source texts to initialize role agents and a world agent, which orchestrate interactions, memory updates, and movements within scene-based simulations managed by LLMs. BookWorld demonstrated superior performance in generating high-quality, faithful narratives, surpassing previous methods with a win rate of 75.36% in comparative evaluations. For AI practitioners, this research provides a framework for leveraging existing literary works to create immersive, context-rich simulations and interactive story generation applications, reducing the need for manual world-building. |
| Efficient Pretraining Length Scaling (Read more on [arXiv](https://arxiv.org/abs/2504.14992) or [HuggingFace](https://huggingface.co/papers/2504.14992))| Jianqiao Lu, Sijun Zhang, Shen Yan, Taoer, bongbohong | This paper introduces the Parallel Hidden Decoding (PHD) Transformer framework to enable efficient length scaling during language model pre-training. The objective is to achieve the performance benefits of increased sequence length during pre-training without proportionally increasing KV cache size or inference latency. The core methodology involves repeating input tokens multiple times but employing a novel KV cache strategy where only the cache from original tokens is retained globally, while the cache from repeated ("hidden decoding") tokens is discarded or kept only within a local/chunk-wise window (PHD-SWA/PHD-CSWA). Results demonstrate consistent performance improvements; for instance, the PHD-CSWA-3-16-32 variant achieved a 2.0% average accuracy increase across evaluated benchmarks compared to a 1.2B parameter baseline, with minimal impact on inference efficiency. For AI practitioners, this work presents a method (PHD-CSWA) to enhance model reasoning capabilities through pre-training length scaling without the typical memory and latency penalties, offering a practical approach to scale computational depth efficiently. |
| CheXWorld: Exploring Image World Modeling for Radiograph Representation
  Learning (Read more on [arXiv](https://arxiv.org/abs/2504.13820) or [HuggingFace](https://huggingface.co/papers/2504.13820))| Shiji Song, Pan Liu, Chenxin Tao, Yulin Wang, yueyang2000 | CheXWorld introduces a self-supervised world modeling framework for learning robust radiograph representations by capturing anatomical knowledge and domain variations. The primary objective is to develop a unified framework that models local anatomical structures, global anatomical layouts, and domain appearance variations essential for radiograph interpretation. Key methodology involves integrating these three aspects through tailored prediction tasks within a joint-embedding predictive architecture, predicting target representations based on context and latent variables (relative position, augmentation parameters). CheXWorld significantly outperforms existing self-supervised learning methods on eight medical image classification and segmentation benchmarks, achieving 95.24±0.13 AUROC on VinDr-CXR classification. For AI practitioners, the principal implication is that this world modeling approach yields highly effective and transferable representations for diverse radiograph analysis tasks, potentially reducing the need for extensive labeled data. |
| Personalized Text-to-Image Generation with Auto-Regressive Models (Read more on [arXiv](https://arxiv.org/abs/2504.13162) or [HuggingFace](https://huggingface.co/papers/2504.13162))| Xihui Liu, Yao Teng, Xian Liu, Kaiyue Sun | This research investigates personalized text-to-image generation using auto-regressive (AR) models, adapting them for a task typically dominated by diffusion models. The primary objective is to evaluate the potential of optimizing AR models for personalized image synthesis by leveraging their unified architecture for text and image modeling. The methodology involves a two-stage training strategy: first optimizing text embeddings associated with a unique identifier for the subject, and second, fine-tuning the model's transformer layers using 3-5 reference images. Experiments on the Lumina-mGPT 7B model demonstrated comparable subject fidelity (DINO: 0.671) and prompt following (CLIP-T: 0.314) to the diffusion-based DreamBooth method (DINO: 0.668, CLIP-T: 0.305) on the Dreambench dataset. For AI practitioners, this work highlights that appropriately optimized AR models present a viable alternative architecture for personalized image generation, achieving competitive fidelity and prompt adherence compared to established diffusion techniques, although generation speed is noted as slower. |
| LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale (Read more on [arXiv](https://arxiv.org/abs/2504.16030) or [HuggingFace](https://huggingface.co/papers/2504.16030))| Zejun Ma, Wei Li, Yiqi Lin, Ziyun Zeng, Joya Chen | LiveCC introduces a Video LLM trained at scale using densely interleaved, timestamped automatic speech recognition (ASR) transcripts for real-time video commentary. The primary objective is to enable scalable Video LLM training leveraging cheap ASR data for fine-grained, temporally-aligned vision-language modeling and low-latency inference. Key methodology involves a novel streaming training approach on curated datasets (Live-CC-5M, Live-WhisperX-526K) derived from YouTube closed captions. The final LiveCC-7B-Instruct model surpasses 72B models in commentary quality on the LiveSports-3K benchmark (achieving a 41.5% win rate against LLaVA-Video-72B) and achieves state-of-the-art results on VideoMME/OVOBench QA benchmarks at the 7B scale, with commentary latency under 0.5 seconds per frame. For AI practitioners, this work demonstrates a cost-effective and scalable method using readily available ASR data to develop high-performance, real-time Video LLMs, reducing dependency on expensive annotations or APIs. |
| Vidi: Large Multimodal Models for Video Understanding and Editing (Read more on [arXiv](https://arxiv.org/abs/2504.15681) or [HuggingFace](https://huggingface.co/papers/2504.15681))| Fan Chen, Chia-Wen Kuo, Celong Liu, Vidi Team, daviddousa | Vidi is a family of Large Multimodal Models (LMMs) designed for long-duration video understanding and editing, initially focused on temporal retrieval using vision, audio, and text. The primary objective is to develop a multimodal AI model capable of accurately performing temporal retrieval (identifying specific time ranges based on text/audio queries) within hour-long videos by processing visual, auditory, and textual information simultaneously. Vidi employs modality-specific encoders (SigLIP, Whisper), adapter layers, and a Mistral-7B LLM core utilizing Decomposed Attention for efficient processing of densely sampled (1fps visual, 16kHz audio), long multimodal sequences, trained via multi-stage alignment on synthetic and real annotated video data. On the introduced VUE-TR benchmark designed for realistic, long-form video retrieval, Vidi significantly outperforms proprietary models, achieving an overall Intersection-over-Union Area Under Curve (IoU AUC) of 35.4% compared to 21.2% (Gemini-2.0-Flash), 15.2% (Gemini-2.5-Pro), and 13.6% (GPT-4o). For AI practitioners, Vidi demonstrates a viable architecture using Decomposed Attention for building LMMs that can efficiently process and temporally ground queries in hour-long multimodal videos, offering a strong foundation for developing advanced, scalable video editing and retrieval applications. |
| From Reflection to Perfection: Scaling Inference-Time Optimization for
  Text-to-Image Diffusion Models via Reflection Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.16080) or [HuggingFace](https://huggingface.co/papers/2504.16080))| Renrui Zhang, Yue Liao, Sayak Paul, Liangbing Zhao, Le Zhuo | This paper introduces ReflectionFlow, an inference-time optimization framework that enables text-to-image diffusion models to iteratively refine their outputs via self-reflection. The objective is to improve image generation quality for complex prompts by scaling inference-time computation rather than solely relying on larger pre-trained models. Key methodology involves proposing three scaling axes (noise, prompt, reflection), constructing the large-scale GenRef dataset (1 million reflection triplets plus 227K CoT annotations), and performing efficient reflection tuning on the FLUX.1-dev diffusion transformer by jointly modeling multimodal inputs (prompt, reflection, flawed image, target image) in a unified sequence. The primary result shows ReflectionFlow significantly improves performance, achieving a GenEval score of 0.91 with 32 samples, outperforming the FLUX.1-dev baseline (0.67) and naive noise scaling (0.85), and requiring 10x fewer samples than noise scaling for similar performance levels. For AI practitioners, this offers a scalable, compute-efficient inference-time technique to enhance the fidelity and detail of generated images for challenging prompts without modifying the underlying generative model architecture or extensive retraining. |
| LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making
  Abilities (Read more on [arXiv](https://arxiv.org/abs/2504.16078) or [HuggingFace](https://huggingface.co/papers/2504.16078))| Razvan Pascanu, Markus Wulfmeier, Jordi Grau-Moya, Jörg Bornschein, Thomas Schmied | This paper investigates why LLMs act sub-optimally as decision-making agents and evaluates Reinforcement Learning Fine-Tuning (RLFT) to improve their performance. The research aims to identify the causes of sub-optimal LLM decision-making, specifically greediness, frequency bias, and the knowing-doing gap, and to determine if RLFT on self-generated Chain-of-Thought (CoT) rationales can mitigate these issues. Methodology involved analyzing Gemma2 models (2B, 9B, 27B) on multi-armed/contextual bandits and Tic-tac-toe, quantifying failure modes, applying RLFT with a PPO-like objective on CoT outputs, and evaluating various exploration strategies. Primary results indicate LLMs exhibit a knowing-doing gap (e.g., 87% correct rationales but acting greedily 58% of that time) and poor exploration (e.g., 27B model covering only 45% of actions in 20-arm MABs); RLFT improved exploration (e.g., +12% action coverage for 2B model after 30k steps) and reduced regret, partially mitigating greediness and frequency bias. The principal implication for AI practitioners is that base LLMs require explicit mechanisms beyond CoT prompting for effective exploration; RLFT on CoT rationales, especially enhanced with exploration bonuses or reward shaping, significantly improves decision-making but does not eliminate the need for careful consideration of exploration strategies in agentic systems. |
| WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World
  Model-based LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2504.15785) or [HuggingFace](https://huggingface.co/papers/2504.15785))| Deheng Ye, Guodong Long, Yijun Yang, Siyu Zhou, zhoutianyi | WALL-E 2.0 enhances LLM agent performance by aligning LLM-based world models with environment dynamics through neurosymbolic learning of executable code rules. The primary objective is to bridge the gap between LLM prior knowledge and specific environment dynamics, creating more accurate world models for LLM agents without requiring RL fine-tuning or large memory buffers. The key methodology involves using LLMs for inductive reasoning on environment trajectories to extract symbolic knowledge (action rules, knowledge/scene graphs), translating this into executable code rules, pruning redundant rules, and integrating these into an LLM world model within a Model-Predictive Control (MPC) loop. Results show significant improvements over baselines, including reward increases of 16.1%-51.6% in the Mars environment and achieving a 98% success rate in ALFWorld after only 4 iterations. For AI practitioners, this work demonstrates a training-free method to enhance LLM agent reliability and planning efficiency in novel or dynamic environments by explicitly learning and enforcing environment-specific constraints as verifiable code rules within the agent's world model. |
| MR. Video: "MapReduce" is the Principle for Long Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.16082) or [HuggingFace](https://huggingface.co/papers/2504.16082))| Yu-Xiong Wang, Ziqi Pang | MR. Video proposes and validates the MapReduce principle for long video understanding, using an agentic framework to separate parallel short clip perception (Map) from joint information aggregation (Reduce). The objective is to overcome context length limitations of VLMs and the sequential, limited-context nature of existing video agents by applying this big data processing paradigm. The methodology involves a two-stage MapReduce workflow (Captioning and Analysis) implemented via an LLM agent controlling a VLM (Gemini-2.0-Flash) for perception and an LLM (GPT4o) for reasoning/reduction. MR. Video achieves 60.8% accuracy on the challenging LVBench dataset, demonstrating a >10% improvement over state-of-the-art VLMs and video agents, and correctly localizes relevant scenes for 68.8% of questions via its intention analysis step. For AI practitioners, this demonstrates that structuring long video analysis using the MapReduce principle enables scalable, parallel processing of local details and comprehensive global context aggregation, offering a practical method to improve performance on long-form video tasks. |
| Progent: Programmable Privilege Control for LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2504.11703) or [HuggingFace](https://huggingface.co/papers/2504.11703))| Hongwei Li, Linyu Wu, Zhun Wang, Jingxuan He, stneng | Progent introduces a programmable framework using a domain-specific language (DSL) for fine-grained privilege control over LLM agent tool calls. The primary objective is to mitigate security risks associated with LLM agents executing potentially harmful actions via tools by enforcing the principle of least privilege. Key methodology involves a DSL, implemented using the JSON ecosystem, to define policies specifying permissible tool calls, conditions, and fallbacks, with support for manual definition and LLM-based automated generation/updating. Experimental results show Progent significantly enhances security, reducing attack success rates on the AgentDojo benchmark from 41.2% to 2.2% using combined manual and LLM-managed policies, while maintaining high utility. For AI practitioners, Progent offers a modular, API-based mechanism to integrate deterministic security controls into LLM agents, restricting tool use to essential functions and reducing vulnerabilities with minimal code modification. |
| RealisDance-DiT: Simple yet Strong Baseline towards Controllable
  Character Animation in the Wild (Read more on [arXiv](https://arxiv.org/abs/2504.14977) or [HuggingFace](https://huggingface.co/papers/2504.14977))| Chao Fan, Min Wei, Shikai Li, Yifan Wu, Jingkai Zhou | RealisDance-DiT introduces a simple yet strong baseline for controllable character animation in the wild by leveraging a powerful video foundation model with minimal modifications. The main objective is to address challenges in character animation such as rare poses, stylized characters, object interactions, and complex scenes without relying on elaborate, task-specific networks like Reference Net. The methodology involves making minor adjustments to the Wan-2.1 DiT architecture (adding condition layers, modifying RoPE) and employing specific fine-tuning strategies, namely low-noise warmup and large-batch/small-iteration training, to preserve foundation model priors while adapting to the animation task. Primary results show state-of-the-art performance, achieving an FVD of 563.28 and FID of 24.79 on the proposed RealisDance-Val benchmark, significantly outperforming prior methods. The principal implication for AI practitioners is that adapting large pre-trained foundation models with straightforward modifications and tailored fine-tuning can yield superior results for complex generative tasks compared to designing complex, specialized architectures from scratch. |
| IPBench: Benchmarking the Knowledge of Large Language Models in
  Intellectual Property (Read more on [arXiv](https://arxiv.org/abs/2504.15524) or [HuggingFace](https://huggingface.co/papers/2504.15524))| Minghui Zhu, Huaren Liu, Hongbo Wang, Guhong Chen, QiYao-Wang | This paper introduces IPBench, a comprehensive, bilingual benchmark designed to evaluate Large Language Model (LLM) knowledge across the complex intellectual property (IP) domain. The primary objective is to assess LLM capabilities in real-world IP scenarios involving both technical and legal understanding, covering 8 IP mechanisms and 20 distinct tasks. Methodologically, the benchmark comprises 10,374 data points used to evaluate 16 different LLMs, ranging from general-purpose to domain-specific models, under various prompting strategies. The key finding indicates substantial limitations, as the top-performing model (DeepSeek-V3) achieved only 75.8% overall accuracy, with open-source IP/law-oriented models notably underperforming compared to closed-source general models. For AI practitioners, this highlights the current gap in LLM proficiency for specialized IP tasks and suggests a need for enhanced domain-specific adaptation or fine-tuning, particularly for open-source solutions, to handle the required blend of technical and legal reasoning effectively. |
| CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via
  Occluded Object Counting (Read more on [arXiv](https://arxiv.org/abs/2504.15485) or [HuggingFace](https://huggingface.co/papers/2504.15485))| Mohit Bansal, Jaemin Cho, Elias Stengel-Eskin, Atin Pothiraj | This paper introduces CAPTURE, a benchmark to evaluate Vision Language Models' (VLMs) spatial reasoning by counting objects in patterns, especially when occluded. The primary objective is to quantify VLMs' ability to perform amodal counting by inferring patterns hidden behind occluders, testing their world modeling and spatial understanding. Methodology involves evaluating four VLMs (GPT-4o, Intern-VL2, Molmo, Qwen2-VL) on the CAPTURE dataset (real and synthetic images with occluded patterns) using the symmetric mean absolute percentage error (sMAPE) metric, comparing performance on occluded versus unoccluded images and against human/object-detection baselines. Results show current VLMs struggle significantly, performing worse with occlusion (average sMAPE of 27.37% on CAPTUREreal occluded images versus 21.09% unoccluded), in stark contrast to near-perfect human performance (3.79% sMAPE occluded); providing oracle object coordinates substantially improves VLM performance, reducing error significantly (e.g., average VLM error dropped by 15.65% on CAPTUREreal when given all coordinates). For AI practitioners, this highlights that even strong VLMs lack robust spatial world modeling for occluded scenes, indicating their errors stem from both visual counting difficulties and an inability to infer missing information, suggesting limitations in current architectures for tasks requiring integrated visual reasoning and amodal completion. |
| DiffVox: A Differentiable Model for Capturing and Analysing Professional
  Effects Distributions (Read more on [arXiv](https://arxiv.org/abs/2504.14735) or [HuggingFace](https://huggingface.co/papers/2504.14735))| Wei-Hsiang Liao, Ben Hayes, Junghyun Koo, Marco A. Martínez-Ramírez, yoyolicoris | i) DiffVox presents a differentiable model for estimating and analysing professional vocal effects parameter distributions from audio data. ii) The research aims to capture real-world vocal processing configurations by reverse-engineering effects parameters using differentiable signal processing and analysing their statistical properties. iii) The methodology employs a differentiable audio effects chain (parametric EQ, dynamics, delay, FDN reverb) optimized via gradient descent using multi-resolution spectral (MRS) and loudness dynamic range (MLDR) losses on paired dry/wet vocal stems from two datasets. iv) Primary results demonstrate effective parameter fitting (e.g., DiffVox achieves MRS loss of 0.75/0.98 on left/right & mid/side channels for MedleyDB) and principal component analysis indicates the most significant variation (13.86% of variance on the internal dataset) corresponds to perceived spaciousness, with parameter distributions confirmed as non-Gaussian. v) For AI practitioners, this work offers a validated method and a public dataset of vocal presets to establish realistic priors for audio effects, potentially improving generative audio models and automatic mixing systems by replacing non-informative uniform or Gaussian assumptions. |
