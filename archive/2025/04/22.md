

## Papers for 2025-04-22

| Title | Authors | Summary |
|-------|---------|---------|
| Learning to Reason under Off-Policy Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.14945) or [HuggingFace](https://huggingface.co/papers/2504.14945))| Zhi Wang, ganqu, huzican, yaful, Elliott | LUFFY introduces an off-policy guidance framework for reinforcement learning to enhance large reasoning model capabilities beyond purely on-policy methods. The primary objective is to effectively integrate external, high-quality reasoning traces (off-policy) with a model's own exploration (on-policy) within the zero-RL paradigm, overcoming limitations where models fail to acquire abilities beyond their initial scope. Key methodologies include mixed-policy training combining off-policy demonstrations with on-policy rollouts, and policy shaping via regularized importance sampling to dynamically balance imitation and exploration while mitigating entropy collapse. LUFFY demonstrates significant improvements, achieving an average gain of over +7.0 points across six math benchmarks compared to previous zero-RL methods and a +6.2 point advantage on out-of-distribution tasks. For AI practitioners, this work presents a validated technique to leverage off-policy data within RL, offering a scalable path to train more generalizable and capable reasoning models compared to standard supervised fine-tuning or purely on-policy RL. |
| FlowReasoner: Reinforcing Query-Level Meta-Agents (Read more on [arXiv](https://arxiv.org/abs/2504.15257) or [HuggingFace](https://huggingface.co/papers/2504.15257))| P2333, bhooi, dreamerdeo, yueliu1999, HongchengGao | This paper proposes FLOWREASONER, a meta-agent that automatically generates query-specific multi-agent systems using reasoning reinforced by execution feedback. The primary objective is to create a meta-agent that designs a unique multi-agent workflow optimized for each individual user query, overcoming the rigidity of one-size-fits-all task-level systems. Key methodology involves initial supervised fine-tuning (SFT) on reasoning data distilled from a large model, followed by reinforcement learning (RL) using external code execution feedback and a multi-purpose reward signal encompassing performance, complexity, and efficiency. Results show FLOWREASONER-14B achieves 81.89% overall accuracy across three code benchmarks (BigCodeBench, HumanEval, MBPP), notably surpassing the o1-mini baseline by 10.52%. For AI practitioners, FLOWREASONER offers a method to automate the creation of adaptive multi-agent workflows tailored to specific user inputs, potentially improving performance and reducing manual engineering effort for complex, query-dependent tasks. |
| Eagle 2.5: Boosting Long-Context Post-Training for Frontier
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15271) or [HuggingFace](https://huggingface.co/papers/2504.15271))| WonminByeon, deahuang, lulidong, RealZhiqiLi, cg1177 | Eagle 2.5 is a vision-language model family improving long-context video and image understanding through specialized post-training and a new dataset. The research objective is to enhance vision-language models' capabilities for processing long-context multimodal inputs, specifically long videos and high-resolution images, without introducing specialized compression modules. Key methodologies include an information-first sampling strategy (combining Image Area Preservation tiling and Automatic Degradation Sampling for token budgeting), progressive post-training to scale context length (up to 128K), and the creation of the Eagle-Video-110K dataset using a dual (story-level and clip-level) annotation approach, built upon a SigLIP-Qwen2.5 architecture. Primary results show strong performance on long-context tasks; specifically, Eagle 2.5-8B achieves 72.4% accuracy on the Video-MME benchmark with 512 input frames, competitive with significantly larger proprietary and open-source models. For AI practitioners, this work provides validated techniques (information-first sampling, progressive training) and a dataset (Eagle-Video-110K) for developing smaller yet high-performing VLMs capable of processing extended visual contexts, crucial for applications involving long videos or detailed images. |
| ToolRL: Reward is All Tool Learning Needs (Read more on [arXiv](https://arxiv.org/abs/2504.13958) or [HuggingFace](https://huggingface.co/papers/2504.13958))| Cheng Qian, Gokhantur, XtremSup, Merlin-Hongru, emrecanacikgoz | This paper presents a comprehensive study on reward design for enhancing Large Language Model (LLM) tool use capabilities via Reinforcement Learning (RL). The main research objective is to systematically investigate and identify optimal reward strategies for tool selection and application tasks within the RL paradigm, assessing factors like reward type, scale, granularity, and temporal dynamics. The key methodology involves proposing a principled, fine-grained reward design tailored for tool use and training LLMs using Group Relative Policy Optimization (GRPO), alongside extensive ablation studies on reward components. Empirical evaluations show this approach yields robust training, achieving a 17% improvement over base models and a 15% gain over Supervised Fine-Tuning (SFT) models on tool use benchmarks; specifically, fine-grained reward decomposition proved more effective than coarser signals. For AI practitioners, the principal implication is that careful, decomposed reward engineering within an RL framework is critical for developing LLMs with significantly enhanced and more generalizable tool-using abilities compared to SFT alone. |
| SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video
  Generation via Spherical Latent Representation (Read more on [arXiv](https://arxiv.org/abs/2504.14396) or [HuggingFace](https://huggingface.co/papers/2504.14396))| joyfull78, sungwon95, YeolJoo, TaewoongKang, mpark | SphereDiff introduces a tuning-free framework for generating seamless 360-degree panoramic images and videos by leveraging spherical latent representations with pre-trained diffusion models. The primary objective is to overcome the severe distortions and discontinuities, particularly near the poles, associated with traditional equirectangular projection (ERP) methods without requiring model fine-tuning. The methodology involves defining a uniform spherical latent representation, extending MultiDiffusion to this space, employing dynamic latent sampling to map spherical latents to a 2D grid compatible with standard diffusion models, and using distortion-aware weighted averaging during projection. SphereDiff demonstrates superior performance over baselines, achieving significantly higher scores for distortion mitigation (e.g., 3.238 vs. 2.854 for DynamicScaler) and end-to-end continuity (e.g., 4.892 vs. 3.985) in image generation tasks. For AI practitioners, this provides a robust, tuning-free approach to generate high-quality omnidirectional content directly from existing perspective-view diffusion models, bypassing the need for ERP-specific datasets and mitigating common projection artifacts. |
| StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on
  3D Gaussians (Read more on [arXiv](https://arxiv.org/abs/2504.15281) or [HuggingFace](https://huggingface.co/papers/2504.15281))| Cailin Zhuang, Yiying12, unpackableorange, wchengad, xuanyangz | StyleMe3D introduces a multi-encoder framework using disentangled priors for high-quality artistic stylization of 3D Gaussian Splatting representations. The primary objective is to enable versatile and coherent style transfer onto pre-reconstructed 3D Gaussian Splatting models while preserving geometric integrity and overcoming limitations of prior methods in handling stylized aesthetics. Key methodology involves integrating four novel components—Dynamic Style Score Distillation (DSSD) using Stable Diffusion's latent space, Contrastive Style Descriptor (CSD), Simultaneously Optimized Scale (SOS) via VGG features, and a 3D Gaussian Quality Assessment (3DG-QA) prior—while optimizing only the RGB attributes of the Gaussians. StyleMe3D demonstrated superior performance over state-of-the-art methods, achieving higher quantitative metrics (e.g., PSNR 18.015, SSIM 0.830, LPIPS 0.174 on evaluated datasets) and preserving fine geometric details and stylistic consistency. For AI practitioners, this work provides a robust method to apply diverse artistic styles to existing 3D GS assets, significantly enhancing visual content for gaming, virtual worlds, and digital art by effectively bridging photorealistic reconstruction with artistic expression without altering underlying geometry. |
| X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents (Read more on [arXiv](https://arxiv.org/abs/2504.13203) or [HuggingFace](https://huggingface.co/papers/2504.13203))| hamidpalangi, mparvez, genglinliu, liweijiang, salmannyu | X-Teaming introduces an adaptive multi-agent framework for systematic multi-turn language model jailbreaking and defense generation. The main objective is to address the gap in multi-turn conversational AI safety by exploring how harmless interactions escalate into harmful outcomes and generating diverse attack scenarios. Key methodology involves a two-phase approach using collaborative agents: a Planner for strategy, an Attacker for execution, a Verifier for evaluation, and a Prompt Optimizer using TextGrad for refining failed attacks. Primary results show state-of-the-art multi-turn jailbreak effectiveness, achieving attack success rates up to 98.1% across various models, including 96.2% against Claude 3.7 Sonnet, and the creation of the 30K-example XGuard-Train dataset. For AI practitioners, this work provides the X-Teaming framework for scalable multi-turn red-teaming and the XGuard-Train dataset, enabling the development and training of more robust multi-turn safety alignment defenses for LMs. |
| UFO2: The Desktop AgentOS (Read more on [arXiv](https://arxiv.org/abs/2504.14603) or [HuggingFace](https://huggingface.co/papers/2504.14603))| rujiawang, liqul, duchao, shilhe, vyokky | UFO2 presents a multiagent AgentOS deeply integrated with Windows for robust LLM-driven desktop automation. The objective is to build a practical, system-level automation framework that overcomes the limitations of prior CUAs reliant on shallow OS integration and screenshot-based interaction. Methodology involves a `HOSTAGENT` for orchestration, application-specific `APPAGENTS` leveraging native APIs and domain knowledge, a hybrid UIA-vision control detection pipeline, a unified GUI-API action layer, speculative multi-action execution, and a Picture-in-Picture interface for non-disruptive operation. Key results demonstrate superior performance over existing CUAs, achieving up to 32.7% success rate on the OSWorld-W benchmark (o1 model); API integration improved completion rates by over 8%, and speculative execution reduced LLM inference calls by up to 51.5% on certain tasks without degrading success rate. For AI practitioners, this work highlights that deep OS integration and hybrid GUI-API interaction models are critical for moving desktop automation agents from conceptual prototypes to reliable, efficient, and scalable real-world applications. |
| LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient
  Training of Code LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.14655) or [HuggingFace](https://huggingface.co/papers/2504.14655))| Yan Wang, Yunhui Xia, chuyi777, jasonkleinlove, Swtheking | This paper introduces LeetCodeDataset, a high-quality temporal benchmark curated from LeetCode Python problems for robust code LLM evaluation and efficient training. The objective is to address the lack of reasoning-focused coding benchmarks and provide a self-contained, contamination-free testbed for training and evaluation. The methodology involved collecting 2,869 LeetCode problems with metadata, 100+ test cases per problem, canonical solutions, and applying a strict temporal split (pre/post-July 2024) for training and test sets. Results demonstrate that reasoning models significantly outperform non-reasoning ones (DeepSeek-R1 achieved 65.23% pass@1 on the test set), and supervised fine-tuning (SFT) using only 2.6K model-generated examples from the dataset achieved performance comparable to models trained on 110K examples. For AI practitioners, this dataset provides a reliable resource for evaluating code generation models without contamination and highlights the potential for highly data-efficient SFT using curated, high-quality problem-solution pairs. |
| Seeing from Another Perspective: Evaluating Multi-View Understanding in
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2504.15280) or [HuggingFace](https://huggingface.co/papers/2504.15280))| Shengbang Tong, yubei, chengtim, ch-chenyu, danielchyeh | Here is a 4-sentence summary of the research paper:  This paper introduces All-Angles Bench, a new benchmark with over 2,100 question-answer pairs across 90 scenes, designed to evaluate the multi-view scene understanding capabilities of Multi-Modal Large Language Models (MLLMs). The primary objective is to assess how well MLLMs reconcile geometric consistency and cross-view correspondence across diverse viewpoints using six defined tasks, including attribute identification and camera pose estimation. Experiments on 27 MLLMs (e.g., GPT-4o, Gemini-2.0-Flash, InternVL2.5-38B) reveal a significant performance gap compared to humans (human 82.0% vs. best MLLM 60.8% on a 250 Q&A subset), with particular weaknesses in handling partial occlusions and estimating coarse camera poses. For AI practitioners, this implies that current MLLMs require substantial improvements, likely through domain-specific training or architectural changes incorporating multi-view awareness, to be reliably deployed in applications demanding 3D scene comprehension like embodied agents. |
| InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to
  Deliberative Reasoners (Read more on [arXiv](https://arxiv.org/abs/2504.14239) or [HuggingFace](https://huggingface.co/papers/2504.14239))| Xavier Hu, Yuhang Liu, xiaotianhan, xieck13, pengxiang | This paper introduces InfiGUI-R1, an MLLM-based GUI agent designed to transition from reactive behavior to deliberate reasoning for complex GUI tasks. The main objective is to advance GUI agents beyond reactive execution by explicitly incorporating robust planning, cross-modal spatial reasoning, and error recovery capabilities. The core methodology is the Actor2Reasoner framework, employing Spatial Reasoning Distillation (SFT) for initial reasoning injection, followed by Deliberation Enhancement using Reinforcement Learning with novel Sub-goal Guidance and Error Recovery Scenario Construction techniques. Experimental results show InfiGUI-R1-3B achieves strong cross-platform GUI grounding (87.5% average accuracy on ScreenSpot) and task execution performance (71.1% success rate on AndroidControl-High), competitive against larger parameter models. For AI practitioners, this work provides a structured framework and specific training techniques (reasoning distillation, RL with targeted rewards for sub-goals and error recovery) to build more capable GUI agents that can handle complex, long-horizon tasks requiring planning and adaptation. |
| EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.15133) or [HuggingFace](https://huggingface.co/papers/2504.15133))| Linear-Matrix-Probability, HaomingXu, xukewei, Saberlve, xzwnlp | i) EasyEdit2 is a framework enabling adjustable, plug-and-play, test-time behavioral control of Large Language Models (LLMs) via steering interventions. ii) The main research objective is to create a unified, user-friendly framework for steering diverse LLM behaviors (e.g., safety, sentiment, factuality, reasoning) without altering the model's underlying parameters. iii) The methodology centers on a steering vector generator (supporting methods like CAA, STA, LM-Steer, Prompt Auto) and a steering vector applier, which integrate intervention vectors during the forward pass, facilitated by a vector library and merging capabilities. iv) Primary results show effectiveness across different LLMs; specifically, the Contrastive Activation Addition (CAA) method achieved a 64.72% safety defense rate (DR) on Gemma-2-9B, surpassing the 58.29% baseline DR. v) For AI practitioners, EasyEdit2 offers a modular system for applying fine-grained, test-time control over LLM outputs with minimal code, aiding in model alignment, debugging, and customization for specific application requirements. |
| LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration
  Benchmark (Read more on [arXiv](https://arxiv.org/abs/2504.13805) or [HuggingFace](https://huggingface.co/papers/2504.13805))| dkeeeee, Yuxiang007, zhimingc, Pengxiangzhao, lgy0404 | This paper presents LearnAct, a few-shot learning framework, and LearnGUI, a benchmark, to improve mobile GUI agent generalization using human demonstrations. The primary objective is to enhance mobile GUI agent capabilities in handling diverse, unseen scenarios and user-specific tasks by learning from a small number of examples, addressing limitations of traditional pre-training or large-scale fine-tuning. The key methodology involves the LearnAct multi-agent framework (DemoParser for knowledge extraction, KnowSeeker for retrieval, ActExecutor for execution) and the LearnGUI benchmark dataset containing offline/online tasks with human demonstrations and similarity metrics. Primary results demonstrate significant performance gains; notably, a single demonstration improved Gemini-1.5-Pro's offline accuracy from 19.3% to 51.7%, and LearnAct boosted UI-TARS-7B-SFT's online success rate from 18.1% to 32.8%. For AI practitioners, this work implies that incorporating few-shot demonstration-based learning is a viable strategy to create more adaptable and deployable mobile GUI agents, reducing reliance on extensive datasets for personalization and handling long-tail tasks. |
| LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping (Read more on [arXiv](https://arxiv.org/abs/2504.08902) or [HuggingFace](https://huggingface.co/papers/2504.08902))| Vinicius C. Azevedo, Jingwei Tang, coffeeweb2907, ssancho, pascalchang87 | This paper introduces LookingGlass, a method using latent rectified flow models and a novel Laplacian Pyramid Warping technique to generate anamorphic images that reveal hidden content via specific viewpoints while maintaining a valid direct interpretation. The objective is to extend generative optical illusions to latent space models and complex spatial transformations beyond simple orthogonal warps, using only text prompts. The core methodology involves synchronizing latent flow model predictions across views by decoding to image space, applying frequency-aware Laplacian Pyramid Warping (LPW) for robust transformation and blending, encoding back to latent space, and using residual correction. Primary results demonstrate high-quality anamorphosis generation for conic/cylindrical mirrors and Nicéron's lens, quantitatively outperforming prior methods on complex transforms (e.g., achieving FID 129.74 vs 166.03+ for 135° rotation). For AI practitioners, this work presents a feed-forward approach for creating intricate perceptual illusions with modern latent generative models and introduces LPW, a generally applicable technique for high-fidelity, frequency-aware image warping in generative tasks. |
| DRAGON: Distributional Rewards Optimize Diffusion Generative Models (Read more on [arXiv](https://arxiv.org/abs/2504.15217) or [HuggingFace](https://huggingface.co/papers/2504.15217))| Somayeh Sojoudi, Jonah Casebeer, Njb, Bai-YT | DRAGON introduces a versatile on-policy framework for fine-tuning diffusion models using distributional rewards beyond standard instance-level feedback. The objective is to enable optimization for a wider class of reward functions, including instance-wise, instance-to-distribution, and distribution-to-distribution metrics, such as FAD or Vendi diversity. DRAGON operates by generating on-policy samples, evaluating them with the target reward function to construct positive (D+) and negative (D-) demonstration sets, and then applying contrastive optimization losses (like Diffusion-DPO/KTO) to align the model's output distribution. Experiments fine-tuning a text-to-music model showed DRAGON achieved an 81.45% average win rate across 20 diverse reward functions, and significantly improved human-perceived quality (60.95% human-voted win rate) by optimizing FAD with an appropriate exemplar set, without needing human preference annotations. For AI practitioners, DRAGON provides a method to directly optimize generative models for complex distributional metrics like FAD and enables using easily obtainable reference data (even cross-modal, like text descriptions for music) to improve generation quality, reducing reliance on costly human feedback collection. |
| Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls
  for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2504.14899) or [HuggingFace](https://huggingface.co/papers/2504.14899))| Shikai Li, yanweifuture, Alex-snow, theFoxofSky, ewrfcas | Uni3C introduces a unified framework for precise 3D-enhanced camera and human motion control in video generation using foundational video diffusion models (VDMs). The objective is to enable joint, precise control over both camera trajectories and human motions in video generation, overcoming limitations of separate controls and reliance on jointly annotated data. Key methodologies include PCDController, a lightweight, plug-and-play module trained with a frozen VDM backbone using unprojected point clouds for camera control, and a global 3D world guidance system aligning scenic point clouds and SMPL-X characters for unified inference. Uni3C significantly improves joint control, achieving an Absolute Trajectory Error (ATE) of 0.251 on the unified benchmark, substantially outperforming the baseline RealisDance-DiT's ATE of 0.549 while maintaining visual quality. For AI practitioners, the PCDController offers a robust, parameter-efficient module for adding precise camera control to existing VDMs with minimal training overhead and without needing joint annotations, while the global alignment enables unified multi-modal control. |
| TAPIP3D: Tracking Any Point in Persistent 3D Geometry (Read more on [arXiv](https://arxiv.org/abs/2504.14717) or [HuggingFace](https://huggingface.co/papers/2504.14717))| Katerina Fragkiadaki, Bowei Zhang, aharley, lkeab | TAPIP3D introduces a method for long-term 3D point tracking by representing videos as camera-stabilized spatio-temporal 3D feature clouds. The objective is to improve long-term 3D point tracking accuracy and robustness, particularly under complex deformations and large camera movements, by leveraging persistent 3D world-space representations. The methodology involves lifting 2D video features using depth and optional camera pose into a 3D point feature cloud (world or camera coordinates), employing a novel Local Pair Attention mechanism for contextualization, and iteratively refining 3D trajectories via a transformer. Results show state-of-the-art performance, significantly outperforming prior methods; for example, on the LSFOdyssey benchmark using ground-truth depth and camera pose, TAPIP3D-world achieved 72.2 AJ3D compared to 37.7 AJ3D for the DELTA baseline. For AI practitioners, this work demonstrates that utilizing explicit 3D world-space coordinates and 3D-specific attention mechanisms can yield substantial improvements in tracking accuracy for applications requiring fine-grained motion understanding, especially when reliable depth and pose are accessible. |
| An LMM for Efficient Video Understanding via Reinforced Compression of
  Video Cubes (Read more on [arXiv](https://arxiv.org/abs/2504.15270) or [HuggingFace](https://huggingface.co/papers/2504.15270))| Yuan Yao, Ji Qi, chuats, acharkq, bys0318 | Quicksviewer introduces a Large Multimodal Model (LMM) employing nonuniform partitioning and resampling for efficient video understanding. The primary objective is to create an LMM that dynamically compresses videos based on temporal information density, reducing redundancy for efficient long-video processing. Key methodology involves a cubing network using Gumbel Softmax to partition videos into nonuniform cubes, followed by a unified 3D resampler compressing each cube into a fixed number of tokens, achieving an overall 45x compression rate. Quicksviewer outperformed a fixed partitioning baseline by up to 8.72 in accuracy and achieved SOTA on Video-MME using significantly fewer tokens per frame (up to 5% of baseline needs). For AI practitioners, this reinforced dynamic cubing approach offers a method to develop computationally efficient LMMs for long video analysis, drastically reducing token requirements while maintaining strong performance. |
| RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary
  Quality-Diversity Search (Read more on [arXiv](https://arxiv.org/abs/2504.15047) or [HuggingFace](https://huggingface.co/papers/2504.15047))| Truong-Son Hy, tnngo2, quyanh | RainbowPlus introduces a novel evolutionary quality-diversity (QD) framework to enhance adversarial prompt generation for Large Language Model (LLM) red-teaming. The primary objective is to improve the scalability, effectiveness, and diversity of attack strategies compared to existing methods. It employs an adaptive QD search based on MAP-Elites, featuring a multi-element archive storing multiple prompts per cell and a probabilistic fitness function for concurrent multi-prompt evaluation. RainbowPlus demonstrated superior performance, achieving an average Attack Success Rate (ASR) of 81.1% on the HarmBench dataset across twelve LLMs, surpassing AutoDAN-Turbo by 3.9% while being 9 times faster. For AI practitioners, RainbowPlus offers a more scalable and computationally efficient open-source tool for comprehensive LLM vulnerability assessment and safety enhancement. |
| NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.13941) or [HuggingFace](https://huggingface.co/papers/2504.13941))| yejinchoinka, ericnyberg, ekmb, shrimai19, SieraL | NEMOTRON-CROSSTHINK proposes a framework to scale reinforcement learning-based self-learning for LLMs beyond mathematics by systematically incorporating multi-domain, multi-format data. The primary objective is to generalize RL-enhanced reasoning capabilities to diverse non-math domains (STEM, humanities, social sciences) where verifiable reward structures are less defined than in mathematics. The methodology involves curating multi-source QA data, applying structured answer templates (MCQ/Open-Ended), filtering for verifiability, optimizing data blending ratios, and employing Group Relative Policy Optimization (GRPO) for RL training. This framework achieved substantial accuracy gains over baselines on both math (MATH-500: +30.1%) and non-math benchmarks (MMLU-PRO: +12.8%), with the multi-domain blend notably improving response efficiency by using 28% fewer tokens for correct general-purpose reasoning answers compared to a math-only RL model. For AI practitioners, the principal implication is that incorporating diverse, multi-domain data with appropriate formatting and filtering into RL pipelines is crucial for enhancing LLM reasoning generalization and inference efficiency, moving beyond math-centric training paradigms. |
| CoMotion: Concurrent Multi-person 3D Motion (Read more on [arXiv](https://arxiv.org/abs/2504.12186) or [HuggingFace](https://huggingface.co/papers/2504.12186))| Stephan R. Richter, Alejandro Newell, vkoltun, lahavl, peiyun-hu-apple | CoMotion introduces an online approach for concurrent 3D pose estimation and tracking of multiple people from a single monocular video stream. The primary objective is to maintain temporally coherent and accurate 3D pose tracks for multiple individuals, even in crowded scenes with occlusions, in a streaming fashion. The methodology employs a recurrent model using a tracking-by-attention paradigm, directly updating existing pose tracks from image features via cross-attention and a GRU, alongside a module for detecting new tracks, trained on a heterogeneous mix of real and synthetic datasets with pseudo-labels. CoMotion achieves state-of-the-art pose accuracy and significantly improves tracking, notably increasing MOTA by 14% and IDF1 by 12% on PoseTrack21 over prior methods while being substantially faster. For AI practitioners, this demonstrates that directly updating tracks from image features enables more robust and efficient online multi-person 3D motion tracking compared to traditional detect-and-associate methods. |
