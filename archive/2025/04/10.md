

## Papers for 2025-04-10

| Title | Authors | Summary |
|-------|---------|---------|
| DDT: Decoupled Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.05741) or [HuggingFace](https://huggingface.co/papers/2504.05741))| Weilin Huang, Zhi Tian, lmwang, wangsssssss | This paper introduces the Decoupled Diffusion Transformer (DDT), separating semantic encoding and high-frequency detail decoding. The objective is to resolve the inherent optimization conflict in standard diffusion transformers, thereby accelerating training convergence and improving generation quality. DDT utilizes a distinct condition encoder for semantic extraction and a velocity decoder for detail generation, incorporating representation alignment and trained via linear flow matching. Key results show DDT-XL/2 achieves a state-of-the-art 1.31 FID on ImageNet 256x256 in 256 epochs, indicating approximately 4x faster convergence than prior diffusion transformers like REPA. For AI practitioners, DDT offers a significantly more efficient architecture for training high-fidelity diffusion models and introduces a statistical dynamic programming approach to accelerate inference by sharing encoder computations between steps with minimal performance loss. |
| GenDoP: Auto-regressive Camera Trajectory Generation as a Director of
  Photography (Read more on [arXiv](https://arxiv.org/abs/2504.07083) or [HuggingFace](https://huggingface.co/papers/2504.07083))| lindahua, wetzste1, liuziwei7, jingtan, Dubhe-zmc | This paper introduces GenDoP, an auto-regressive model, and DataDoP, a large-scale dataset, for generating artistic camera trajectories. The research aims to generate controllable, expressive camera trajectories based on multi-modal inputs (text, optional RGBD), addressing limitations in existing methods lacking directorial intent alignment or suffering from instability. The methodology involves creating the DataDoP dataset (29K shots, 11M frames) with detailed motion/directorial captions and developing GenDoP, a decoder-only Transformer that tokenizes camera parameters and generates trajectories auto-regressively. GenDoP significantly outperforms prior methods in text-trajectory alignment, achieving a CLaTr-CLIP score of 36.179 compared to 31.689 for a retrained baseline (Director3D) on the Motion caption task, and also shows superior user-rated alignment, quality, and complexity. For AI practitioners, this work provides a method for generating complex, instruction-following camera paths, enhancing controllability in camera-controlled video generation systems for applications like filmmaking and virtual cinematography. |
| OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens (Read more on [arXiv](https://arxiv.org/abs/2504.07096) or [HuggingFace](https://huggingface.co/papers/2504.07096))| Yensung, sewon, yanaiela, taylorb, liujch1998 | OLMOTRACE is a system that traces language model (LM) outputs back to their training data to understand LM behavior. The research question is how to efficiently trace LM outputs to their full multi-trillion-token training data in real time. The methodology uses an extended version of infini-gram to index the training data and a parallel algorithm to compute matching spans. The system traces LM responses (average 450 tokens) to the training data in 4.5 seconds on average. OLMOTRACE enables AI practitioners to explore the relationship between LM outputs and training data for fact-checking, creativity analysis, and understanding math capabilities.  |
| A Unified Agentic Framework for Evaluating Conditional Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.07046) or [HuggingFace](https://huggingface.co/papers/2504.07046))| Yiyu Wang, Longyue Wang, Xue Yang, Jifang Wang, imryanxu | i) The paper introduces CIGEVAL, a unified agentic framework leveraging large multimodal models (LMMs) for evaluating conditional image generation tasks. ii) The research aims to develop a task-agnostic, reliable, and explainable evaluation metric for conditional image generation. iii) CIGEVAL employs LMMs with a multi-functional toolbox and a fine-grained evaluation framework, synthesizing evaluation trajectories for fine-tuning smaller LMMs. iv) Experiments across seven conditional image generation tasks show CIGEVAL (GPT-40 version) achieves a Spearman correlation of 0.4625 with human assessments. v) CIGEVAL offers AI practitioners a more human-aligned and explainable method for automated evaluation of conditional image generation models, especially in tasks involving multiple conditions, and a pathway for fine-tuning smaller LMMs using synthesized evaluation trajectories for improved performance.  |
| Missing Premise exacerbates Overthinking: Are Reasoning Models losing
  Critical Thinking Skill? (Read more on [arXiv](https://arxiv.org/abs/2504.06514) or [HuggingFace](https://huggingface.co/papers/2504.06514))| Ming Li, zhoutianyi, sunlichao137, Fcr09 | i) The paper investigates the effect of missing premises in questions on the response behavior of reasoning Large Language Models (LLMs). ii) The study aims to quantify and analyze the extent to which LLMs exhibit "MiP-Overthinking", characterized by increased response length and ineffective reasoning on ill-posed questions with missing premises. iii) The methodology involves curating MiP datasets across varying difficulty levels, evaluating LLMs' response length, accuracy, and abstain rate, and analyzing step-level similarities in reasoning chains. iv) Reasoning models generate responses 2x-4x longer for MiP questions compared to well-defined questions, contradicting test-time scaling law, while non-reasoning models generate responses of similar lengths for both. v) AI practitioners should be aware that current training paradigms for reasoning LLMs insufficiently promote efficient thinking, potentially resulting in resource inefficiencies and the abuse of reasoning patterns when faced with ambiguous input. It is unclear how in-process suspicion metrics are calculated in the paper.  |
| FantasyTalking: Realistic Talking Portrait Generation via Coherent
  Motion Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.04842) or [HuggingFace](https://huggingface.co/papers/2504.04842))| Yunpeng Zhang, Yaqi Fan, Mengchao Wang, fanjiang, wangqiang9 | i) FantasyTalking generates realistic talking portraits from a single image via a dual-stage audio-visual alignment strategy. ii) The research aims to generate high-fidelity and coherent talking portraits with controllable motion dynamics from a static image. iii) The method utilizes a video diffusion transformer model with clip-level and frame-level audio-visual alignment and a facial-focused cross-attention module for identity preservation. iv) The proposed approach achieves state-of-the-art performance, demonstrating improved video quality, temporal consistency, and motion diversity, and achieves an aesthetic score of 0.6183 on the wild talking head dataset. v) AI practitioners can leverage this method for creating more realistic and controllable avatar animations, enhancing applications in gaming, filmmaking, and virtual reality.  |
| A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths
  to Reproducibility (Read more on [arXiv](https://arxiv.org/abs/2504.07086) or [HuggingFace](https://huggingface.co/papers/2504.07086))| AmeyaPrabhu, albanie, vishaal27, hrdkbhatnagar, libeanim | i) This paper analyzes the reproducibility of recent advances in language model (LM) reasoning, identifying sensitivities to implementation choices and proposing a standardized evaluation framework. ii) The research investigates whether reported performance gains in mathematical reasoning benchmarks are robust to variations in decoding parameters, random seeds, prompt formatting, and hardware configurations. iii) The methodology involves a comprehensive empirical study re-evaluating recent methods using a standardized framework and assessing variance across multiple seeds and varying hyperparameters. iv) The study found reinforcement learning approaches yield only modest improvements and are prone to overfitting, while supervised finetuning shows consistently stronger generalization; Pass@1 values show standard deviations ranging from 5 to 15 percentage points across seeds. v) AI practitioners should adopt rigorous, multi-seed evaluation protocols and standardized testing frameworks to ensure the reliability and generalizability of LM reasoning enhancements before integrating them into applications.  |
| OmniCaptioner: One Captioner to Rule Them All (Read more on [arXiv](https://arxiv.org/abs/2504.07089) or [HuggingFace](https://huggingface.co/papers/2504.07089))| Cxxs, Wayne-lc, Dakerqi, JiakangYuan, yeeeeeyy | OmniCaptioner introduces a unified visual captioning framework for diverse domains. The main objective is to generate fine-grained textual descriptions for natural images, visual text (posters, UIs), and structured visuals (tables, charts, math) using a single model. The methodology involves a two-stage captioning pipeline (Seed-Caption Generation with GPT-40, Caption Extension with Qwen LLMs) trained on a 21M multi-domain dataset, initializing from Qwen2-VL-Instruct. Primary results show that integrating OmniCaptioner's detailed captions with LLMs (e.g., DS-R1-Distill-Qwen-7B) significantly improves visual reasoning, achieving 40.5 on MathVerse without MLLM fine-tuning, enhances text-to-image generation (+2.97 on GenEval for SANA-1.0), and enables more efficient SFT (reaching comparable performance to LLaVA-OV-7B with ~1/3 of the SFT data). The principal implication for AI practitioners is the ability to leverage a single, versatile captioner to generate rich, domain-specific descriptions that directly enhance downstream visual reasoning systems, improve text-to-image generation quality, and accelerate supervised fine-tuning for various multimodal tasks. |
| Are We Done with Object-Centric Learning? (Read more on [arXiv](https://arxiv.org/abs/2504.07092) or [HuggingFace](https://huggingface.co/papers/2504.07092))| Matthias Bethge, coallaoh, AmeyaPrabhu, arubique | i) This paper explores the limits of current object-centric learning (OCL) methods. ii) The main objective is to assess whether advances in OCL provide practical benefits beyond unsupervised object discovery, particularly in out-of-distribution (OOD) generalization scenarios. iii) The methodology involves introducing Object-Centric Classification with Applied Masks (OCCAM), a probe using sample-efficient segmentation models to generate object-centric representations and evaluate downstream classification tasks with spurious backgrounds. iv) The primary result shows that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods in robust zero-shot image classification, achieving up to 78.5% accuracy on ImageNet-D with HQES masks and SigLip models, which is superior to baseline LLAVA 1.5 (73.3%) and FT-Dinosaur (71.5%). v) The principal implication for AI practitioners is that utilizing foundational segmentation models for generating object-centric representations offers a more scalable and effective approach for robust classification tasks compared to traditional slot-centric OCL methods.  |
| Self-Steering Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.07081) or [HuggingFace](https://huggingface.co/papers/2504.07081))| Jacob Andreas, Vikash K. Mansinghka, Joshua B. Tenenbaum, Gabriel Grand, alexanderlew | i) This paper introduces DISCIPL, a self-steering framework for language models (LMs) that decouples planning from execution by generating task-specific inference programs. ii) The main research question is how to enable LMs to perform complex reasoning tasks more efficiently and verifiably without extensive fine-tuning. iii) The methodology involves using a Planner LM to generate an inference program, which is then executed by a population of Follower LMs via Sequential Monte Carlo (SMC). iv) Experiments on constrained generation tasks show that DISCIPL, with a 1B Follower, matches or outperforms GPT-40 and 01 models and achieves 0.81 pass@1 on COLLIE sentence-level tasks. v) DISCIPL offers AI practitioners a method to automate the creation of highly parallelized Monte Carlo inference strategies for LMs, improving performance on challenging generation tasks.  |
| RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts (Read more on [arXiv](https://arxiv.org/abs/2504.06947) or [HuggingFace](https://huggingface.co/papers/2504.06947))| Anna Lapanitsyna, Natalia Tkachenko, Natalia Loukachevitch, nicolay-r, RefalMachine | i) The paper introduces the RuOpinionNE-2024 shared task for extracting structured opinion tuples from Russian news texts. ii) The primary objective is to extract tuples composed of a sentiment holder, target, expression, and polarity for a given sentence. iii) The methodology involved participants experimenting with large language models using zero-shot, few-shot, and fine-tuning techniques. iv) The best result on the test set was achieved through fine-tuning a large language model with an F1 score of 0.41. v) The principal implication for AI practitioners is the benchmark dataset and performance metrics for structured opinion extraction in Russian, enabling development and evaluation of models for Russian sentiment analysis.  |
| Masked Scene Modeling: Narrowing the Gap Between Supervised and
  Self-Supervised Learning in 3D Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.06719) or [HuggingFace](https://huggingface.co/papers/2504.06719))| Leon Sick, Christian Stippel, phermosilla | i) The paper introduces a novel self-supervised approach, Masked Scene Modeling, for learning 3D scene representations. ii) The research aims to develop a self-supervised model for 3D scene understanding that can achieve performance comparable to supervised models when using off-the-shelf features. iii) The methodology involves a bottom-up hierarchical masking approach with a novel reconstruction objective tailored to hierarchical 3D models, reconstructing deep features of masked patches. iv) Experiments demonstrate that the proposed model achieves competitive performance in semantic segmentation (68.7 mIoU on ScanNet using linear probing) compared to supervised models, surpassing existing self-supervised methods. v) The principal implication is that the proposed self-supervised pre-training approach provides AI practitioners with a method to extract features from 3D scenes that perform comparably to supervised approaches, reducing the need for labeled data.  |
| DiTaiListener: Controllable High Fidelity Listener Video Generation with
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2504.04010) or [HuggingFace](https://huggingface.co/papers/2504.04010))| chaubeyG, hongkung, minhtran, Boese0601, havent-invented | DiTaiListener is a video generation model for synthesizing high-fidelity listener head portraits conditioned on speaker audio, facial motions, and optional text prompts. The paper aims to generate controllable and temporally consistent listener behavior in video by adapting Diffusion Transformer (DiT) architecture. The method introduces a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speaker audio and visual cues and DiTaiListener-Edit for refining transitional frames between video segments. DiTaiListener achieves a 73.8% improvement in FID score on RealTalk dataset and a 6.1% improvement on VICO dataset, signifying enhanced photorealism and motion representation. This work provides AI practitioners with an approach for generating realistic and customizable listener videos for applications in virtual avatars and human-computer interaction. |
| VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.06958) or [HuggingFace](https://huggingface.co/papers/2504.06958))| Lanxingxuan, donglu, desenmeng, Aurorana, xinhaoli | VideoChat-R1 enhances spatio-temporal perception in video MLLMs via reinforcement fine-tuning. The research aims to improve spatio-temporal perception in video MLLMs while preserving general capabilities. It employs Reinforcement Fine-Tuning (RFT) with Group Relative Policy Optimization (GRPO) on spatio-temporal objectives using limited data samples. VideoChat-R1 achieves state-of-the-art performance, improving temporal grounding by +31.8 and object tracking by +31.2 compared to Qwen2.5-VL-7B. RFT offers a data-efficient approach for specialized task enhancement in video MLLMs without sacrificing general capabilities, relevant to AI engineers developing video understanding systems.  |
| WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments (Read more on [arXiv](https://arxiv.org/abs/2504.03886) or [HuggingFace](https://huggingface.co/papers/2504.03886))| Songyou Peng, Marc Pollefeys, Valentin Bieri, Zihan Zhu, Jianhao Zheng | WildGS-SLAM is presented as a monocular SLAM system using 3D Gaussian Splatting robust to dynamic environments. The research aims to achieve accurate camera tracking and scene reconstruction in dynamic environments using only monocular RGB input. An uncertainty map derived from DINOv2 features is used to guide dynamic object removal within tracking and mapping pipelines. Evaluation on the Wild-SLAM MoCap dataset shows the system achieves an ATE RMSE of 0.46 cm, outperforming existing dynamic SLAM methods. Practitioners can leverage this method for improved SLAM performance in real-world applications with dynamically changing elements without explicit depth or semantic information.  |
| RobustDexGrasp: Robust Dexterous Grasping of General Objects from
  Single-view Perception (Read more on [arXiv](https://arxiv.org/abs/2504.05287) or [HuggingFace](https://huggingface.co/papers/2504.05287))| Jie Song, Sammy Christen, Linyi Huang, Zijian Wu, ethHuiZhang | i) This paper introduces a reinforcement-learning-based framework for robust, zero-shot dynamic dexterous grasping of unseen objects from single-view perception. ii) The main objective is to enable a robot to grasp a wide range of previously unseen objects with a dexterous hand using only a single-view camera while adapting to external disturbances. iii) The methodology involves a mixed curriculum learning strategy that combines imitation learning from a teacher policy trained with privileged information and reinforcement learning for adaptation to disturbances, utilizing a hand-centric object representation. iv) The primary result is a grasping success rate of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects without prior knowledge or object-specific training. v) The principal implication for AI practitioners is the demonstrated effectiveness of sparse hand-centric object representation and mixed curriculum learning for training robust dexterous grasping policies that generalize to unseen objects from limited observations, suggesting a path toward more adaptable and general-purpose robotic manipulation systems.  |
