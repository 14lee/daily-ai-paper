

## Papers for 2025-04-04

| Title | Authors | Summary |
|-------|---------|---------|
| Advances and Challenges in Foundation Agents: From Brain-Inspired
  Intelligence to Evolutionary, Collaborative, and Safe Systems (Read more on [arXiv](https://arxiv.org/abs/2504.01990) or [HuggingFace](https://huggingface.co/papers/2504.01990))| KaitaoSong, JinlinW, Peiyan, xinfeng1i, Bang-UdeM-Mila | This survey presents a comprehensive overview of LLM-powered Foundation Agents, proposing a modular, brain-inspired architecture integrating cognitive science and neuroscience principles. The main objective is to structure the understanding of advanced intelligent agents by exploring their modular foundations, self-enhancement mechanisms, collaborative/evolutionary dynamics, and safety aspects. The methodology involves a structured literature review and synthesis, mapping agent components (memory, world modeling, reward, emotion) to brain functions and analyzing self-optimization (AutoML, LLM-driven), multi-agent systems, and safety/ethical threats. As a survey, the paper synthesizes existing research across these four areas rather than presenting novel quantitative findings, identifying key research gaps, challenges, and opportunities. For AI practitioners, this work provides a unified framework for designing, evaluating, and ensuring the safety of complex Foundation Agents, emphasizing the need to harmonize modular design, adaptive capabilities, and collaborative potential with robust safety and ethical considerations. |
| Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual
  Editing (Read more on [arXiv](https://arxiv.org/abs/2504.02826) or [HuggingFace](https://huggingface.co/papers/2504.02826))| Rethinker, GTZhai, KexianTang, zpy777, PhoenixZ | This paper introduces RISEBench, the first benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) capabilities in Large Multi-modality Models (LMMs). The main objective is to systematically assess LMM performance on visual editing tasks requiring Temporal, Causal, Spatial, and Logical reasoning beyond simple pixel manipulation. The methodology involves curating image-instruction test cases for each reasoning type and evaluating model outputs (from models like GPT-4o-Native, Gemini-2.0-Flash, EMU2) using both human judges and an LMM-as-a-judge (GPT-4o) framework across dimensions of Instruction Reasoning, Appearance Consistency, and Visual Plausibility. Primary results indicate that while GPT-4o-Native significantly outperforms other models with a 35.9% overall accuracy, even this state-of-the-art model struggles notably with logical reasoning tasks (37.5% accuracy), and open-source models achieve near-zero accuracy on RISEBench. The principal implication for AI practitioners is that current SOTA LMMs exhibit significant deficiencies in integrating complex, especially logical, reasoning within visual editing, highlighting a critical area requiring further research and development before such capabilities can be reliably deployed. |
| GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02782) or [HuggingFace](https://huggingface.co/papers/2504.02782))| shawnxyh, BestWishYsh, SereinH, liweijia, Yejy53 | This paper introduces GPT-ImgEval, a benchmark for quantitatively and qualitatively evaluating OpenAI's GPT-4o model in image generation and editing tasks. The main objective was to assess GPT-4o's performance across generation quality, editing proficiency, and world knowledge-informed synthesis, while also investigating its potential underlying architecture. Methodology involved evaluating GPT-4o using the GenEval, Reason-Edit, and WISE datasets via custom automation scripts, and employing a classification model trained to distinguish between diffusion and auto-regressive outputs to infer GPT-4o's generation mechanism. Primary results indicate GPT-4o significantly surpasses prior models, achieving an overall score of 0.84 on GenEval, and empirical analysis suggests it likely uses a hybrid auto-regressive architecture combined with a diffusion-based head, contrary to VAR-like structures. For AI practitioners, this work provides a standardized evaluation framework, highlights GPT-4o's advanced capabilities and specific limitations (e.g., editing inconsistencies, non-English text issues), and notes its outputs are detectable by current forensic models, impacting considerations for deployment and safety. |
| Rethinking RL Scaling for Vision Language Models: A Transparent,
  From-Scratch Framework and Comprehensive Evaluation Scheme (Read more on [arXiv](https://arxiv.org/abs/2504.02587) or [HuggingFace](https://huggingface.co/papers/2504.02587))| Pengfei, IanZhong, Ryan1122, steffichern, ManTle | This paper introduces MAYE, a transparent, from-scratch Reinforcement Learning (RL) framework for Vision-Language Models (VLMs), alongside a comprehensive evaluation scheme. The main objective is to improve reproducibility and standardized assessment in RL for VLMs, addressing limitations of complex, opaque existing frameworks. Methodologically, it presents a minimal four-step RL pipeline (using Reinforce++ with KL penalty) built with standard libraries and introduces an evaluation scheme tracking dynamics like accuracy curves, response length, and reflection ratios. Key results show RL consistently surpasses Supervised Fine-Tuning (SFT) generalization, achieving a 1.35x average accuracy increase (peaking at 1.76x) on the mm\_math5k validation set compared to the baseline, even when SFT uses high-quality data; findings also indicate response length sensitivity to random seeds and correlation between reflection and output length. For AI practitioners, this provides a reproducible baseline framework (MAYE) for VLM RL experimentation and demonstrates RL's potential for superior generalization over SFT on visual reasoning tasks, suggesting its utility even with access to good supervised data. |
| SkyReels-A2: Compose Anything in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2504.02436) or [HuggingFace](https://huggingface.co/papers/2504.02436))| raul678, ruiwang, diqiu7, Debang, onion | This paper introduces SkyReels-A2, an open-source framework for composing videos from text prompts and multiple reference images (characters, objects, scenes). The primary research objective is to generate high-fidelity videos that maintain strict identity consistency for each specified element while coherently composing the scene according to the text prompt, defining this as the elements-to-video (E2V) task. Key methodologies include a comprehensive data pipeline for constructing prompt-reference-video triplets, a novel joint image-text embedding model integrated into a diffusion transformer architecture with distinct spatial and semantic feature branches, and inference acceleration strategies. Evaluated on the proposed A2-Bench benchmark, SkyReels-A2 achieves comparable quantitative results to closed-source models, notably scoring 0.809 in object consistency, slightly outperforming competitors like Vidu (0.796) and Keling (0.790). For AI practitioners, SkyReels-A2 provides a publicly available model and benchmark for controllable multi-element video generation, facilitating development in areas requiring precise visual element control and composition, such as virtual e-commerce or creative content production. |
| Scaling Analysis of Interleaved Speech-Text Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02398) or [HuggingFace](https://huggingface.co/papers/2504.02398))| adiyoss, MajoRoth, hassid, gallilmaimon | This paper analyzes the scaling behaviour of interleaved speech-text language models (SLMs), finding they scale more efficiently than textless SLMs. The main objective is to determine if SLMs trained with interleaved speech and text data scale more efficiently with compute compared to textless SLMs. The methodology involves training dozens of interleaved SLMs across various sizes (0.5B-7B), compute budgets (2e18-2e20 FLOPs), and TextLM initialisations (e.g., Qwen2.5, Llama3.2), evaluating performance on speech-only validation loss and semantic metrics (sSC, tSC) using an ISO-FLOP curve approach. Results show interleaved SLMs scale significantly better with compute, indicating compute budgets should favour larger model sizes over more training tokens; for a 2e20 FLOP budget, a 7B parameter model trained on 4.2B tokens outperformed smaller models on more tokens, contrasting with textless SLM scaling predictions. The principal implication for AI practitioners is that when training large interleaved SLMs (e.g., >4.5B tokens), allocating more compute towards larger, high-quality pre-trained TextLM-initialised models is more efficient than towards increasing training tokens alone for improving semantic speech abilities. |
| ShortV: Efficient Multimodal Large Language Models by Freezing Visual
  Tokens in Ineffective Layers (Read more on [arXiv](https://arxiv.org/abs/2504.00502) or [HuggingFace](https://huggingface.co/papers/2504.00502))| xphan, sanmusunrise, luyaojie, chenjiawei-icip, yuanqianhao | ShortV enhances Multimodal Large Language Model (MLLM) efficiency by identifying and freezing visual token computations in ineffective layers. The primary objective is to reduce the high computational overhead of MLLMs, specifically addressing redundancy in how different layers process visual tokens. A novel metric, Layer Contribution (LC), is introduced to quantify a layer's impact by measuring the KL divergence in model output logits when that layer's transformations on specific tokens (visual or text) are bypassed; ShortV uses LC to identify layers ineffective for visual tokens and replaces them with sparse layers where visual computations are frozen. Experiments demonstrate that ShortV can freeze visual token processing in approximately 60% of MLLM layers (e.g., achieving 50% FLOPs reduction on LLaVA-NeXT-13B with N=24 replaced layers) with negligible performance degradation. For AI practitioners, ShortV offers a training-free, parameter-free method to significantly decrease MLLM inference costs by exploiting layer-wise redundancy for visual tokens, and it is compatible with token pruning techniques. |
| Audio-visual Controlled Video Diffusion with Masked Selective State
  Spaces Modeling for Natural Talking Head Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02542) or [HuggingFace](https://huggingface.co/papers/2504.02542))| Jun Zhou, Zixiang Zhou, danxuhk, xuzn, HarlanHong | This paper introduces ACTalker, an end-to-end video diffusion framework for natural talking head generation controlled simultaneously by audio and facial motion signals without conflict. The primary objective is to enable fine-grained control using multiple driving signals while preventing conflicts and ensuring spatio-temporal coherence. Key methodologies involve a parallel-control mamba (PCM) layer leveraging Masked Selective State Space Models (Mask-SSM) and a mask-drop strategy to direct each signal's influence to specific facial regions within a stable video diffusion architecture. Experimental results demonstrate state-of-the-art performance, achieving a Sync-C score of 5.317 and an FVD-Inc score of 232.374 on the CelebV-HD dataset under audio-only control, surpassing previous methods. For AI practitioners, this work presents a novel application of Mamba (SSM) structures for efficient, conflict-free multi-modal conditioning in video generation, offering precise control over synthesized facial dynamics. |
| ZClip: Adaptive Spike Mitigation for LLM Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2504.02507) or [HuggingFace](https://huggingface.co/papers/2504.02507))| gueraf, nilabhra, louisowen6, akanyaani | ZClip introduces an adaptive gradient clipping method based on z-scores to enhance stability during large language model (LLM) pre-training. The primary objective is to mitigate gradient instability and malignant loss spikes that disrupt training, necessitating costly interventions like checkpoint restoration. ZClip dynamically adjusts the gradient clipping threshold by tracking the exponential moving average (EMA) of the gradient norm's mean and standard deviation, applying z-score-based anomaly detection to identify and scale down spikes. Experiments on a 1B LLaMA model demonstrated that ZClip enabled stable training at a high learning rate (3.0x10⁻³), reaching baseline validation loss using 18.6B fewer tokens (over 35% faster) compared to fixed clipping at a lower, stable rate (5.0x10⁻⁴). For AI practitioners, ZClip offers a method to improve LLM pre-training stability and efficiency, potentially reducing training time and compute costs by allowing for more aggressive learning rates without succumbing to catastrophic divergence. |
| Inference-Time Scaling for Generalist Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2504.02495) or [HuggingFace](https://huggingface.co/papers/2504.02495))| Chong Ruan, Shirong Ma, Runxin Xu, Peiyi Wang, Zijun Liu | This paper introduces Self-Principled Critique Tuning (SPCT) to enhance the inference-time scalability and performance of generalist generative reward models (GRMs). The main objective is to investigate if a specific learning method can enable effective inference-time scaling for GRMs, improving reward quality beyond standard model or compute scaling. The key methodology involves SPCT, which uses rejective fine-tuning and rule-based online RL to train GRMs to generate adaptive principles and critiques, combined with inference-time scaling via parallel sampling and voting, optionally guided by a meta RM. Primary results show DeepSeek-GRM-27B trained with SPCT achieves 69.9% overall accuracy on RM benchmarks, improving to 71.0% with voting@32, and further to 72.8% with meta RM guidance, demonstrating effective inference-time scaling compared to just increasing model size. For AI practitioners, this implies that using SPCT and inference-time sampling with GRMs can yield superior reward signals for aligning LLMs, potentially offering a more compute-efficient path to performance gains than solely relying on larger models. |
| Efficient Model Selection for Time Series Forecasting via LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.02119) or [HuggingFace](https://huggingface.co/papers/2504.02119))| Hongjie Chen, Franck-Dernoncourt, ryanrossi, tiankaiy, wwdd7718 | This paper investigates leveraging Large Language Models (LLMs) for efficient, zero-shot model selection in time series forecasting, eliminating the need for costly pre-computed performance matrices. The primary objective is to determine if LLMs can select optimal forecasting models and hyperparameters for unseen time series datasets solely through prompting. The methodology involves querying LLMs (Llama 3.2, GPT-4o, Gemini 2.0 flash) with prompts containing time series data and optionally meta-features or Chain-of-Thought (CoT) instructions to recommend a model configuration. Results demonstrate that the LLM approach, particularly Llama 3.2 using prompts with meta-features, outperforms traditional meta-learning (e.g., achieving 7.27% hit@10 accuracy vs. 4.51% for MLP) and heuristic baselines while reducing median inference time by up to 89x compared to naïve exhaustive evaluation. For AI practitioners, this suggests LLMs offer a computationally cheaper and faster alternative for selecting appropriate time series forecasting models without extensive prior model evaluations or meta-feature engineering, streamlining the model selection workflow. |
| Instruction-Guided Autoregressive Neural Network Parameter Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02012) or [HuggingFace](https://huggingface.co/papers/2504.02012))| Sung Ju Hwang, Song Chong, Bruno Andreis, bedio | This paper introduces IGPG, an instruction-guided autoregressive framework for generating neural network parameters conditioned on task and architecture specifications. The primary objective is to enable scalable and coherent parameter synthesis across diverse models and tasks, addressing limitations of prior methods like diffusion models. IGPG utilizes a VQ-VAE to tokenize parameters and an autoregressive transformer, conditioned on task/dataset embeddings and architecture descriptions, to generate weight tokens sequentially. Key results demonstrate competitive performance, including generating LoRA parameters that improve accuracy by up to 10% over baseline methods on vision benchmarks. For AI practitioners, IGPG offers a unified tool for rapid model initialization, efficient adaptation to new tasks, and potentially reduces the need for extensive fine-tuning by generating specialized weights on demand. |
| Interpreting Emergent Planning in Model-Free Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.01871) or [HuggingFace](https://huggingface.co/papers/2504.01871))| David Krueger, Usman Anwar, Stephen Chung, agaralon, tuphs | This paper provides the first mechanistic evidence that model-free reinforcement learning agents (DRC) learn internal planning mechanisms in Sokoban using concept-based interpretability. The primary research objective was to determine if a DRC agent internally formulates, evaluates, and utilizes plans based on predicted future consequences without an explicit world model. The methodology involved probing ConvLSTM cell states for planning-relevant concepts (Agent Approach Direction CA, Box Push Direction CB), analyzing iterative plan formation across internal ticks, and performing causal interventions on activations to verify behavioral dependence. Results show the agent linearly represents CA and CB (e.g., final layer 1x1 probe Macro F1 for CB ~0.8 vs <0.3 baseline), forms plans iteratively resembling parallelized bidirectional search which refine with extra compute (Fig 6), and interventions causally steer behavior (e.g., 98.8% success rate for Layer 3 Agent-Shortcut interventions). The principal implication for AI practitioners is that complex planning capabilities can emerge implicitly in model-free architectures, suggesting that internal state representations and iterative computation may be key mechanisms for such behaviors, influencing agent design and analysis beyond purely behavioral metrics. |
| GenPRM: Scaling Test-Time Compute of Process Reward Models via
  Generative Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.00891) or [HuggingFace](https://huggingface.co/papers/2504.00891))| Saputello, dmux, ChetKao, iseesaw, RyanLiu112 | GenPRM introduces a generative process reward model utilizing explicit reasoning and code verification to scale test-time compute for LLM verification. The objective is to overcome limitations of current Process Reward Models (PRMs) by enhancing their process supervision capabilities and enabling test-time scaling (TTS) through generative modeling. GenPRM achieves this by performing multi-step Chain-of-Thought (CoT) reasoning integrated with code generation and execution for verification, using Relative Progress Estimation (RPE) and rationale synthesis for training data generation. Experiments demonstrate that a 7B GenPRM significantly outperforms prior models, surpassing the much larger Qwen2.5-Math-PRM-72B on ProcessBench (achieving 80.5 F1 score with Maj@8 scaling). For AI practitioners, this work shows that smaller generative PRMs, when combined with test-time scaling, can serve as highly effective and potentially more compute-efficient verifiers or critics compared to larger models or traditional scalar-based PRMs, improving the evaluation and refinement of complex reasoning processes. |
| Scaling Laws in Scientific Discovery with AI and Robot Scientists (Read more on [arXiv](https://arxiv.org/abs/2503.22444) or [HuggingFace](https://huggingface.co/papers/2503.22444))| Zhenting Wang, Renjun Xu, Huazhe Xu, Heng Zhang, universea | This paper proposes the Autonomous Generalist Scientist (AGS) concept, integrating agentic AI and embodied robotics to automate the end-to-end scientific research lifecycle. The main objective is to outline a framework for AGS systems capable of independent, multi-domain scientific discovery by synergizing AI's cognitive abilities with robotics' physical interaction capabilities. The methodology involves proposing a conceptual framework featuring a five-module architecture (literature review, proposal generation, experimentation, manuscript writing, reflection/feedback) and defining five distinct levels of automation, ranging from Level 1 (Tool-Assisted) to Level 5 (Pioneer/ASIR). The paper hypothesizes new scaling laws for scientific discovery driven by AGS capabilities and number, rather than presenting empirical results; it details requirements for virtual (OS agents) and physical (embodied AI robots) task execution. For AI practitioners, the primary implication is the conceptual roadmap for developing integrated AI-robotic systems capable of complex, multi-stage, cross-domain automation, moving beyond specialized AI tools to handle tasks requiring both virtual reasoning and physical manipulation. |
| Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.02821) or [HuggingFace](https://huggingface.co/papers/2504.02821))| Zeynep Akata, Serge Belongie, Quentin Bouniot, Shyamgopal Karthik, Mateusz Pach | This work extends Sparse Autoencoders (SAEs) to Vision-Language Models (VLMs) like CLIP, demonstrating their ability to learn more interpretable, monosemantic features from vision representations. The primary objective is to quantitatively evaluate whether SAEs applied post-hoc to VLM activations enhance neuron monosemanticity and enable model control. Methodology involves training various SAE types on CLIP layer activations and introducing a Monosemanticity Score (MS) metric, calculating activation-weighted pairwise image embedding similarity for neurons. Results demonstrate SAE neurons achieve significantly higher monosemanticity (e.g., MS increased from 0.48 in the base VLM to 0.81 with an SAE for specific neurons shown) and reveal hierarchical concept structures, especially with Matryoshka SAEs. For AI practitioners, this research validates SAEs as an unsupervised method to interpret VLM representations and directly steer the output concepts of multimodal LLMs like LLaVA by intervening on SAE activations, without modifying the base model. |
| Whisper-LM: Improving ASR Models with Language Models for Low-Resource
  Languages (Read more on [arXiv](https://arxiv.org/abs/2503.23542) or [HuggingFace](https://huggingface.co/papers/2503.23542))| Ibon Saratxaga, Eva Navas, inmahernaez, zuazo | This research improves Whisper ASR models for low-resource languages by integrating external n-gram and large language models (LLMs) with fine-tuned models at inference time. The main objective was to enhance transcription accuracy and robustness, particularly in low-resource and out-of-distribution scenarios, by combining acoustic model probabilities with language model scores. Key methodology involved fine-tuning Whisper models per language, followed by integrating KenLM 5-gram models or language-specific LLMs by modifying beam search scores using optimized weighting parameters. Primary results demonstrate substantial Word Error Rate (WER) reductions, achieving up to 51% improvement for in-distribution Basque data with 5-gram models, while LLMs offered consistently robust, albeit more moderate, gains across languages. For AI practitioners, this indicates that integrating external LMs significantly boosts Whisper's performance for under-resourced languages, but optimal performance requires careful language model parameter tuning and attention to evaluation settings. |
