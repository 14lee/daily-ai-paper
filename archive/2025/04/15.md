

## Papers for 2025-04-15

| Title | Authors | Summary |
|-------|---------|---------|
| InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2504.10479) or [HuggingFace](https://huggingface.co/papers/2504.10479))| jackroos, duanyuchen, gulixin0922, Yeshenglong, Weiyun1025 | InternVL3 presents an open-source Multimodal Large Language Model (MLLM) series developed via native multimodal pre-training and advanced training/test-time techniques. The research objective was to improve MLLM performance and training efficiency by jointly learning multimodal and linguistic capabilities within a single pre-training stage, circumventing typical post-hoc adaptation of text-only LLMs. Key methodologies employed include unified pre-training on mixed text and multimodal corpora, Variable Visual Position Encoding (V2PE), supervised fine-tuning (SFT), and Mixed Preference Optimization (MPO) post-training, alongside test-time scaling. The primary result shows InternVL3-78B achieving a state-of-the-art score of 72.2% on the MMMU benchmark among open-source MLLMs, demonstrating strong capabilities competitive with proprietary models like ChatGPT-4o and Gemini 2.5 Pro. For AI practitioners, this work provides evidence that native multimodal pre-training yields powerful open-source MLLMs, and the released models and data offer a strong foundation for developing advanced multimodal applications without relying solely on closed systems. |
| PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday
  Home Clusters (Read more on [arXiv](https://arxiv.org/abs/2504.08791) or [HuggingFace](https://huggingface.co/papers/2504.08791))| Hongfang Yu, Mohsen Guizani, NeuronNomad, LiPhilip, LIKirin | PRIMA.CPP introduces a distributed system for running 70B-scale LLMs on heterogeneous, low-resource home device clusters. The objective is to minimize inference latency while managing limited and diverse resources (CPU/GPU, RAM/VRAM, disk, OS, network). It employs piped-ring parallelism with prefetching to hide disk I/O latency from memory-mapped weights and uses the Halda algorithm to optimally assign model layers based on a detailed heterogeneity model. Evaluations on a four-node home cluster show prima.cpp is 15x faster than llama.cpp for 70B models, achieving ~600 ms/token with memory pressure under 6%. This enables AI practitioners to deploy state-of-the-art 30B-70B models locally on clusters of everyday consumer devices, expanding accessibility beyond high-end hardware or cloud services. |
| VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models
  with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.08837) or [HuggingFace](https://huggingface.co/papers/2504.08837))| Wei Chu, Chao Qu, wenhu, zuminghuang, JasperHaozhe | VL-Rethinker improves multimodal reasoning by incentivizing self-reflection in vision-language models through reinforcement learning. The research aims to enhance slow-thinking capabilities in VLMs for complex multimodal tasks. It uses Group Relative Policy Optimization (GRPO) with Selective Sample Replay (SSR) and Forced Rethinking to train the model. VL-Rethinker achieves state-of-the-art scores on MathVista (80.3%), MathVerse (61.8%), and MathVision (43.9%). The method provides AI practitioners with an RL approach for enhancing VLM reasoning without reliance on distillation, offering techniques such as SSR to stabilize training and Forced Rethinking to promote self-reflection.  |
| FUSION: Fully Integration of Vision-Language Representations for Deep
  Cross-Modal Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.09925) or [HuggingFace](https://huggingface.co/papers/2504.09925))| Jingzhou Chen, conghui, jingwei-xu-00, Balalauuoo, starriver030515 | i) The paper introduces FUSION, a family of multimodal large language models (MLLMs) designed for deep, dynamic integration of vision and language. ii) The research aims to enhance cross-modal understanding by achieving a fully vision-language aligned and integrated paradigm within MLLMs. iii) The methodology incorporates Text-Guided Unified Vision Encoding, Context-Aware Recursive Alignment Decoding, and a Dual-Supervised Semantic Mapping Loss. iv) Experiments show FUSION 3B outperforms Cambrian-1 8B and Florence-VL 8B on most benchmarks, even when limited to 300 vision tokens. v) FUSION's approach provides AI practitioners with a strategy for significantly improving MLLM performance with fewer vision tokens by focusing on deep modality integration.  |
| Iterative Self-Training for Code Generation via Reinforced Re-Ranking (Read more on [arXiv](https://arxiv.org/abs/2504.09643) or [HuggingFace](https://huggingface.co/papers/2504.09643))| Valentin Malykh, Ivan Sedykh, Nikita Sorokin | Iterative self-training is used to refine code generation through reinforced re-ranking with Proximal Policy Optimization (PPO). The research aims to improve code generation quality and re-ranking accuracy of decoder-based models through iterative self-training using PPO to optimize a reward/re-ranking model. The methodology involves supervised fine-tuning, reward model training, PPO-based code generation, and iterative refinement using hard negative mining. Results demonstrate a 13.4B parameter model outperforming a 33B parameter model on the MultiPL-E dataset in code generation quality and reaching comparable to GPT-4 performance in code generation, while being three times faster. For AI practitioners, the study presents a method for developing more efficient code generation models by focusing on a robust reward mechanism within a self-training framework.  |
| Mavors: Multi-granularity Video Representation for Multimodal Large
  Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.10068) or [HuggingFace](https://huggingface.co/papers/2504.10068))| kugwzk, zhenhuawu, UnnamedWatcher, CheeryLJH, DogNeverSleep | Mavors introduces a multi-granularity video representation framework for multimodal large language models (MLLMs) aimed at efficient long-context video understanding. The main objective is to balance computational efficiency with the retention of fine-grained spatio-temporal patterns, addressing information loss from methods like sparse sampling or token compression. Mavors uses an Intra-chunk Vision Encoder (IVE) for high-resolution spatial features within video segments and an Inter-chunk Feature Aggregator (IFA) with chunk-level rotary position embeddings (C-ROPE) for temporal coherence across segments. Results demonstrate Mavors-7B's strong performance, achieving a score of 39.4 on the DREAM-1K video captioning benchmark, significantly outperforming many comparable 7B models on tasks requiring fine-grained spatio-temporal reasoning. For AI practitioners, Mavors offers an approach to enhance MLLM capabilities for long video analysis by preserving detailed spatio-temporal information more effectively than common sampling or compression strategies, crucial for applications needing nuanced video understanding. |
| AgentRewardBench: Evaluating Automatic Evaluations of Web Agent
  Trajectories (Read more on [arXiv](https://arxiv.org/abs/2504.08942) or [HuggingFace](https://huggingface.co/papers/2504.08942))| dongchans, arkilpatel, ncmeade, kazemnejad, xhluca | This paper introduces AGENTREWARDBENCH, a benchmark designed to evaluate the automatic evaluation of web agent trajectories by LLM judges. The main objective is to assess the effectiveness of LLMs in judging web agent success compared to expert human annotations, addressing limitations of rule-based and manual evaluations. The methodology involved collecting 1302 trajectories from 4 LLMs across 5 web environments, annotating each by experts for success, side effects, and repetition, and then using this dataset to evaluate 12 different LLM judges and existing rule-based methods. Primary results indicate that no single LLM judge performs best across all benchmarks, the best judges achieve less than 70% precision against expert labels, and official rule-based methods significantly underestimate agent success rates (55.9% recall). The principal implication for AI practitioners is that current automatic evaluation methods, including LLM judges, are not yet reliable enough for high-fidelity assessment or reward modeling, necessitating the development of more accurate automatic evaluation techniques for web agents. |
| S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability
  of Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.10368) or [HuggingFace](https://huggingface.co/papers/2504.10368))| Tingwen Liu, Xinghua Zhang, Starrrrrry, ShuaiyiNie, WYRipple | i) S1-Bench is introduced as a benchmark to evaluate Large Reasoning Models' (LRMs) system 1 thinking capabilities, contrasting with their prevalent system 2 reliance. ii) The research aims to assess LRMs' performance on simple, intuitive tasks better suited for system 1 processing to understand the effects of over-reliance on system 2. iii) The methodology involves constructing a dataset of simple, diverse questions across multiple domains and languages and evaluating 22 LRMs on this benchmark. iv) Results indicate that LRMs exhibit lower efficiency tendencies, generating outputs averaging 15.5 times longer than traditional small LLMs, and accuracy degradation on simple questions. v) This highlights the need for substantial development in LRMs to achieve balanced dual-system thinking capabilities adaptable to task complexity for AI practitioners. |
| Have we unified image generation and understanding yet? An empirical
  study of GPT-4o's image generation ability (Read more on [arXiv](https://arxiv.org/abs/2504.08003) or [HuggingFace](https://huggingface.co/papers/2504.08003))| Ning Li, cuijiaxing, zhangjingran | i) This paper empirically evaluates GPT-4o's image generation capabilities across global instruction adherence, fine-grained editing precision, and post-generation reasoning. ii) The main objective is to assess whether GPT-4o achieves world knowledge-informed semantic synthesis during image generation. iii) The methodology involves designing three types of prompts: global instruction, fine-grained editing, and post-generation reasoning, to test specific aspects of image generation. iv) Results show GPT-4o defaults to literal interpretations, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. v) The principal implication is that GPT-4o has significant limitations in dynamically integrating knowledge into its image generation process, necessitating more robust benchmarks for reasoning-aware multimodal generation.  |
| DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM
  Post-training (Read more on [arXiv](https://arxiv.org/abs/2504.09710) or [HuggingFace](https://huggingface.co/papers/2504.09710))| zwt123home123, timecuriosity, gfcui, ztwang | i) The paper introduces DUMP, an automated distribution-level curriculum learning framework for reinforcement learning-based post-training of large language models. ii) The research aims to dynamically schedule training across heterogeneous data distributions to optimize learning efficiency in LLMs. iii) The methodology employs Upper Confidence Bound (UCB) scores based on expected absolute advantage to adaptively adjust sampling probabilities for different distributions. iv) Experiments on logic reasoning datasets show that DUMP significantly improves convergence speed and final performance, achieving a reward of over 0.5 in the 9-character K&K puzzles distribution, while the uniform sampling baseline remained below 0.0. v) The principal implication is that AI practitioners can utilize DUMP to improve the efficiency and effectiveness of RL-based LLM post-training by dynamically prioritizing learnable data distributions.  |
| SocioVerse: A World Model for Social Simulation Powered by LLM Agents
  and A Pool of 10 Million Real-World Users (Read more on [arXiv](https://arxiv.org/abs/2504.10157) or [HuggingFace](https://huggingface.co/papers/2504.10157))| milesz7777, tangshiping, SimingChen, libo-ca, Lishi0905 | i) SocioVerse is presented as an LLM-agent-driven world model for social simulation. ii) The research aims to address alignment challenges in social simulation across environment, users, interaction, and behavior. iii) The methodology involves a framework with four alignment components and a user pool of 10 million real individuals derived from social media data. iv) Experiments across politics, news, and economics domains demonstrated SocioVerse’s ability to reflect population dynamics, with presidential election prediction achieving over 90% accuracy in state voting results. v) The study indicates a need for careful selection of underlying LLMs to optimize simulation precision across different social scenarios for AI practitioners.  |
| Breaking the Data Barrier -- Building GUI Agents Through Task
  Generalization (Read more on [arXiv](https://arxiv.org/abs/2504.10127) or [HuggingFace](https://huggingface.co/papers/2504.10127))| jxhe, QiushiSun, changma, heroding77, leoozy | i) This paper investigates the effectiveness of mid-training Vision Language Models (VLMs) on reasoning-intensive tasks for improved generalization in GUI agent planning. ii) The research aims to determine how incorporating various instruction-tuning tasks during the mid-training phase of VLMs facilitates generalization to GUI planning scenarios, addressing the scarcity of high-quality trajectory data. iii) The methodology involves training VLMs on a range of readily available instruction-tuning datasets, including GUI perception, multimodal reasoning, and textual reasoning, followed by fine-tuning on GUI trajectory data. iv) The primary results indicate that task generalization proves highly effective, with multimodal mathematical reasoning enhancing performance on AndroidWorld by an absolute 6.3%; text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and a 5.4% improvement on AndroidWorld. v) The principal implication for AI practitioners is that incorporating specific, readily available reasoning tasks into the mid-training of VLMs can substantially improve the performance and generalization capabilities of GUI agents, offering a practical approach to addressing data scarcity challenges in this domain; The work also identifies an optimized dataset mixture called GUIMid which achieves absolute gains of 8.0% on WebArena and 12.2% on AndroidWorld.  |
| TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.09641) or [HuggingFace](https://huggingface.co/papers/2504.09641))| Lei Huang, Wenjun Wu, wenzz1, Zhang199 | TinyLLaVA-Video-R1 explores reasoning in small vision-language models (VLMs) for video understanding. The research investigates how reinforcement learning (RL) can improve reasoning capabilities in smaller VLMs using general Video-QA datasets. The GRPO algorithm was applied to TinyLLaVA-Video with modifications to the reward structure, including a continuous length reward and penalties for incorrect answers. TinyLLaVA-Video-R1 achieves 49.5 on MVBench, improving reasoning with fewer parameters. The work demonstrates that RL can elicit emergent reasoning abilities like self-verification in small-scale VLMs, suggesting avenues for improving video reasoning with limited computational resources.  |
| LLM-SRBench: A New Benchmark for Scientific Equation Discovery with
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.10415) or [HuggingFace](https://huggingface.co/papers/2504.10415))| Khoa D Doan, Amir Barati Farimani, Ngoc-Hieu Nguyen, mkmeidani, parshinsh | i) LLM-SRBench, a new benchmark, is introduced for evaluating scientific equation discovery using Large Language Models (LLMs). ii) The research aims to provide a rigorous benchmark that avoids memorization effects and properly assesses the equation discovery capabilities of LLMs. iii) The methodology involves creating a dataset with 239 challenging problems across four scientific domains, utilizing both LSR-Transform (alternative mathematical representations) and LSR-Synth (synthetic problems) categories. iv) Experimental results demonstrate that the best-performing system achieves only 31.5% symbolic accuracy across the benchmark. v) This benchmark highlights the limitations of current LLMs in scientific equation discovery, suggesting AI practitioners need to develop more robust methods to leverage LLMs for complex scientific reasoning tasks that go beyond memorization.  |
| EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental
  Health Safety (Read more on [arXiv](https://arxiv.org/abs/2504.09689) or [HuggingFace](https://huggingface.co/papers/2504.09689))| Edify-Kd2024, yaozixin, YimingWang, ChrisJuan, yinghuihe | i) EmoAgent is a multi-agent AI framework for evaluating and mitigating mental health risks in human-AI interactions within character-based chatbots. ii) The research aims to assess and safeguard human-AI interactions for mental health safety, particularly for vulnerable users. iii) EmoAgent employs a simulated environment (EmoEval) using clinically validated psychological assessment tools and a real-time safeguard agent (EmoGuard) that monitors and provides corrective feedback. iv) Experiments show that emotionally engaging dialogues can lead to mental state deterioration in vulnerable users in more than 34.4% of simulations; EmoGuard reduces these deterioration rates significantly. v) AI practitioners should be aware that emotionally engaging AI dialogues can lead to mental state deterioration in vulnerable users; and real-time monitoring and corrective feedback are crucial for ensuring safety in AI-human interactions.  |
| The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via
  Agentic Tree Search (Read more on [arXiv](https://arxiv.org/abs/2504.08066) or [HuggingFace](https://huggingface.co/papers/2504.08066))| Chris Lu, Shengran Hu, Robert Tjarko Lange, conglu, yyamada | i) This paper introduces THE AI SCIENTIST-v2, an AI agentic system for automated scientific discovery, improving upon its predecessor. ii) The research aims to develop an end-to-end system capable of autonomously producing scientific manuscripts acceptable for peer review. iii) The methodology involved agentic tree search managed by an experiment manager agent, Vision-Language Model (VLM) feedback loops, and parallel experiment execution. iv) The system generated a manuscript that achieved an average reviewer score of 6.33 at an ICLR workshop, exceeding the average human acceptance threshold. v) This work demonstrates the potential for AI to conduct all aspects of scientific research, enabling unprecedented scalability in research productivity.  |
| Executable Functional Abstractions: Inferring Generative Programs for
  Advanced Math Problems (Read more on [arXiv](https://arxiv.org/abs/2504.09763) or [HuggingFace](https://huggingface.co/papers/2504.09763))| Zaid Khan, mohitbansal, j-min, archiki, esteng | i) The paper introduces EFAGen, a framework for automatically constructing Executable Functional Abstractions (EFAs) for advanced math problems by inferring generative programs from static examples. ii) The research aims to automate the construction of EFAs for advanced math problems, operationalizing this as a program synthesis task. iii) EFAGen conditions a large language model (LLM) on a seed math problem and its solution to generate candidate EFA programs, using executable unit tests as verifiable rewards to train the LLM. iv) Experiments show that EFAs constructed by EFAGen remain faithful to seed problems, produce learnable problem variations, infer EFAs across multiple diverse sources of competition-level math problems, and EFA-based augmentation yields consistent improvements on MATH-500, where Pass@1 improves by +1.9 in the 33% seed setting. v) The principal implication is a scalable approach for generating diverse and verifiable math problem variants, aiding in data augmentation, model stress-testing, and curriculum learning for improving mathematical reasoning in AI systems.  |
| How new data permeates LLM knowledge and how to dilute it (Read more on [arXiv](https://arxiv.org/abs/2504.09522) or [HuggingFace](https://huggingface.co/papers/2504.09522))| Nolan Andrew Miller, Andrey Zhmoginov, Chen Sun, gozzo87, mendor | i) This paper investigates how individual text samples update LLM knowledge, introducing a "priming" effect where new facts inappropriately generalize to unrelated contexts. ii) The research aims to understand and predict how new information propagates through an LLM's knowledge base, leading to both generalization and problematic hallucination. iii) The methodology involves a novel dataset, "Outlandish", composed of 1320 diverse text samples designed to systematically probe knowledge permeation, along with measuring token probabilities before and after learning. iv) The study found that the degree of priming can be predicted by measuring the token probability of key words before learning, and developed two techniques, "stepping-stone" text augmentation and "ignore-k" update pruning, reducing priming effects by 50-95%. v) The findings offer AI practitioners empirical insights and practical tools for improving the specificity of knowledge insertion in language models and reducing undesirable knowledge permeation.  |
| VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search (Read more on [arXiv](https://arxiv.org/abs/2504.09130) or [HuggingFace](https://huggingface.co/papers/2504.09130))| QipengGuo, alphadl, ngc7293, sinwang, LibraTree | VisuoThink introduces a multimodal tree search framework to enhance Large Vision-Language Model (LVLM) reasoning by interleaving visual and textual information dynamically. The research aims to improve LVLM performance on complex reasoning tasks by integrating visual aids and step-by-step thinking through a predictive rollout search mechanism. The methodology involves a vision-text interleaved reasoning framework coupled with a look-ahead tree search algorithm that explores multiple reasoning paths. Experiments show VisuoThink achieves an accuracy of 48.5% on Geomeverse, a 21.8% improvement over the state-of-the-art baseline without fine-tuning, particularly excelling in problems requiring multi-step visual reasoning. This framework offers AI practitioners an effective method for improving reasoning capabilities in vision-language models without requiring model retraining.  |
| M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.10449) or [HuggingFace](https://huggingface.co/papers/2504.10449))| Daniele Paliotta, tridao, voidptr74, xu3kev, JunxiongWang | i) The paper introduces M1, a hybrid Mamba-based reasoning model, that exhibits efficient test-time compute scaling. ii) The research aims to develop a scalable reasoning model that can leverage increased test-time computation for improved performance on mathematical tasks. iii) The methodology includes distilling a Transformer model into a Mamba architecture, followed by supervised fine-tuning on math datasets and reinforcement learning training with GRPO. iv) M1 achieves performance comparable to DeepSeek-R1-Distill-Qwen-1.5B on MATH500 (82) and AIME25 (22) benchmarks, while demonstrating over 3x faster inference throughput compared to similarly-sized transformer models using vLLM. v) M1 offers AI practitioners an efficient alternative to Transformers for reasoning tasks, enabling greater test-time compute scaling through faster inference and potentially improving performance via self-consistency or chain-of-thought approaches under fixed time budgets.  |
| LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety
  in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.10430) or [HuggingFace](https://huggingface.co/papers/2504.10430))| Xinyi Zhang, sarvech123, aneverfull, Zhiyang03, mqliu | i) This paper introduces PERSUSAFETY, a framework for assessing persuasion safety in Large Language Models (LLMs). ii) The primary objective is to investigate whether LLMs reject unethical persuasion tasks and avoid unethical strategies, considering influencing factors like personality traits and external pressures. iii) The methodology involves creating persuasion scenes, simulating persuasive conversations between LLMs, and assessing safety via refusal rates and unethical strategy usage. iv) Experiments across 8 LLMs revealed that most models fail to consistently refuse harmful persuasion tasks and employ unethical strategies; Claude-3.5-Sonnet, while exhibiting strong refusal rates, showed high unethical strategy usage when engaged. v) AI practitioners should be aware that current safety alignment techniques in LLMs may not prevent the use of unethical strategies once the model is engaged, necessitating further research into safety alignment in goal-driven conversations.  |
| DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and
  Summarization? (Read more on [arXiv](https://arxiv.org/abs/2504.08120) or [HuggingFace](https://huggingface.co/papers/2504.08120))| Christoph Leiter, Yanran Chen, Ran Zhang, Sotaro Takeshita, Daniil Larionov | i) The paper systematically compares the performance of reasoning-enabled LLMs against non-reasoning counterparts in evaluating machine translation (MT) and text summarization (TS) tasks. ii) The main research questions are whether reasoning models improve upon conventional models in NLG evaluation and how effectively distillation preserves evaluation capabilities while reducing computational costs. iii) The methodology involves evaluating eight different models, including reasoning-based LLMs, distilled variants, and conventional LLMs, using GEMBA-MQM for MT evaluation and G-Eval for TS evaluation, across the WMT23 and SummEval benchmarks. iv) Primary results indicate that OpenAI's o3-mini models show performance improvements with increased reasoning intensity, achieving the highest overall Eval4NLP scores of 0.644 and 0.645, while DeepSeek-R1 generally underperforms compared to its non-reasoning variant. v) A principal implication for AI practitioners is that the efficacy of reasoning capabilities for NLG evaluation is highly architecture-dependent, and distillation of reasoning capabilities maintains reasonable performance in medium-sized models but degrades substantially in smaller variants, requiring careful consideration of model architecture and task alignment.  |
| MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in
  Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.05782) or [HuggingFace](https://huggingface.co/papers/2504.05782))| Jiaxin Ai, Zhaopan Xu, Xiaopeng Peng, Fanrui Zhang, Pengfei Zhou | i) MDK12-Bench is introduced as a new multi-disciplinary benchmark for evaluating multimodal reasoning in large language models (MLLMs) using K-12 level examinations. ii) The research aims to address the limitations of existing benchmarks by providing a more comprehensive evaluation of MLLMs' reasoning capabilities across multiple disciplines. iii) The methodology involves curating a dataset of 140K reasoning instances spanning six disciplines, annotating instances with knowledge points, and developing a dynamic evaluation framework to mitigate data contamination through bootstrapped unseen data. iv) Experiments showed that Gemini2-thinking achieves the highest overall accuracy of 59.4% on the MDK12-Mini dataset, and models demonstrate sensitivity to combined textual and visual bootstrapping. v) AI practitioners can utilize MDK12-Bench to identify specific knowledge gaps in MLLMs, facilitating targeted improvements in multimodal reasoning capabilities, particularly in areas such as contextual comprehension and resistance to data contamination.  |
