

## Papers for 2025-04-01

| Title | Authors | Summary |
|-------|---------|---------|
| TextCrafter: Accurately Rendering Multiple Texts in Complex Visual
  Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.23461) or [HuggingFace](https://huggingface.co/papers/2503.23461))| Nikai Du, yingtai, jzzzzk, Chenzzzzzz, zhen-nan | TextCrafter is a training-free framework designed to accurately render multiple texts across different regions in complex visual scenes generated by diffusion models. The primary objective is to address limitations like text distortion, omission, and blurriness encountered in Complex Visual Text Generation (CVTG). The methodology involves a progressive three-stage approach: Instance Fusion to align text content with its visual carrier, Region Insulation to separate text prompts and initialize layout using pre-trained model priors, and Text Focus to enhance text token attention for improved fidelity. Experiments on the newly proposed CVTG-2K benchmark show TextCrafter achieves a 0.7370 average Word Accuracy, significantly improving OCR accuracy by over 45% compared to the baseline FLUX model it builds upon. For AI practitioners, this provides an effective method to enhance multi-text rendering capabilities in text-to-image systems without requiring additional model training or fine-tuning, improving performance on complex scene generation with detailed textual elements. |
| MoCha: Towards Movie-Grade Talking Character Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.23307) or [HuggingFace](https://huggingface.co/papers/2503.23307))| Luczzz, daixl1992, FelixXu, haoyum1997, lim142857 | MoCha introduces an end-to-end Diffusion Transformer model for generating movie-grade talking characters directly from speech and text inputs without auxiliary conditions. The primary objective is to create realistic characters with synchronized lip movements, natural facial expressions, coherent full-body actions, and support for multi-character, turn-based conversations, addressing limitations in prior work focused on talking heads or general video synthesis lacking speech control. Key methodologies include a speech-video window attention mechanism for improved lip-sync, a joint training strategy leveraging both speech-labeled and text-only video data for better generalization, and structured character-tagged prompts for multi-character dialogue. MoCha significantly outperforms baselines on the MoCha-Bench benchmark, achieving superior human evaluation scores across all five axes (e.g., +1.40 in Lip-Sync Quality over the next best) and better quantitative lip-sync metrics (Sync-C: 6.037 vs 4.866). For AI practitioners, MoCha offers a direct speech+text-to-video synthesis approach for controllable character animation, enabling richer narrative generation for applications like automated filmmaking and virtual avatars without reliance on intermediate representations like keypoints or explicit pose control. |
| What, How, Where, and How Well? A Survey on Test-Time Scaling in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.24235) or [HuggingFace](https://huggingface.co/papers/2503.24235))| nancy-zwx, demolei, RubinSun, silentspring2, DonJoey | This survey introduces a unified four-dimensional framework (what, how, where, how well) to systematically organize and analyze research on Test-Time Scaling (TTS) in Large Language Models. Its objective is to address the lack of a comprehensive overview by categorizing TTS methods, applications, and evaluation metrics, identifying trends, and outlining future directions. The paper proposes a multi-axis taxonomy and conducts an extensive literature review, decomposing techniques like parallel, sequential, hybrid, and internal scaling, alongside tuning-based (SFT, RL) and inference-based (stimulation, verification, search, aggregation) implementation strategies. The review confirms TTS significantly enhances LLM performance across various tasks, observing scaling-law-like improvements with increased compute, and highlights specific techniques like internal scaling via RL (e.g., DeepSeek-R1) or search methods yielding efficiency gains (e.g., ETS achieving 1.8x KV cache reduction). AI practitioners can utilize the taxonomy and guidelines (Section 7) to select, combine, and evaluate complementary TTS strategies (e.g., Self-Consistency, MCTS, STaR, internal scaling) for balancing performance, cost, and task-specific requirements in LLM deployment. |
| Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement
  Learning on the Base Model (Read more on [arXiv](https://arxiv.org/abs/2503.24290) or [HuggingFace](https://huggingface.co/papers/2503.24290))| Xiangyu Zhang, Qi Han, djiang, YinminZhang, reign12 | Open-Reasoner-Zero (ORZ) introduces an open-source, minimalist approach for large-scale reinforcement learning (RL) focused on enhancing reasoning in base language models. The primary objective was to determine if vanilla PPO with simple rule-based rewards and no KL regularization could scale LLM reasoning performance and response length effectively. The methodology involved applying PPO with GAE (λ=1, γ=1) and a binary correctness reward directly to Qwen2.5 base models (0.5B to 32B) using a curated reasoning dataset. Results showed that ORZ-32B surpassed the DeepSeek-R1-Zero-Qwen-32B model on benchmarks like MATH500 (92.2 vs 91.6) and GPQA Diamond (55.5 vs 55.0) using only 1/10th the training steps, demonstrating stable scaling without KL constraints. The principal implication for AI practitioners is that complex RLHF setups with KL regularization may not be necessary for scaling reasoning; a simpler, resource-efficient PPO configuration can yield strong results directly on base models. |
| RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy (Read more on [arXiv](https://arxiv.org/abs/2503.24388) or [HuggingFace](https://huggingface.co/papers/2503.24388))| Haian Huang, Zhonghan Zhao, GaoangWang, pppppM, ZwwWayne | RIG introduces an end-to-end generalist policy that synergizes reasoning and imagination for embodied agents. The research aims to improve sample efficiency and generalization by integrating reasoning and imagination into a single Transformer model. The methodology involves a progressive data collection strategy to generate reasoning-enriched and dream-review trajectories coupled with language model-based training. Experimental results in Minecraft demonstrate that RIG achieves state-of-the-art performance, showing more than 17× sample efficiency improvements compared to prior works, requiring only 111 hours of video data; also shown is the improvement of robustness and interoperability of generalist policy. RIG provides AI practitioners with an architecture that enhances the performance and scalability of embodied agents by combining reasoning and imagination, offering a pathway towards more efficient and robust policy learning in complex environments.  |
| Effectively Controlling Reasoning Models through Thinking Intervention (Read more on [arXiv](https://arxiv.org/abs/2503.24370) or [HuggingFace](https://huggingface.co/papers/2503.24370))| Prateek Mittal, Jiachen T. Wang, cxiang, tongwu2020 | Reasoning models can be controlled through Thinking Intervention, a paradigm for guiding internal reasoning processes via strategic token insertion or revision.  The research question explores fine-grained control over model behavior by guiding internal reasoning processes of LLMs.  The methodology involves comprehensive evaluations across instruction following, instruction hierarchy, and safety alignment tasks.  Results show that Thinking Intervention achieves up to a 6.7% accuracy gain in instruction-following, a 15.4% improvement in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Thinking Intervention enables fine-grained control over reasoning trajectories, aligning model behavior with specific task objectives, allowing for more reliable and aligned AI systems.  |
| Query and Conquer: Execution-Guided SQL Generation (Read more on [arXiv](https://arxiv.org/abs/2503.24364) or [HuggingFace](https://huggingface.co/papers/2503.24364))| sfc-mwydmuch, Borchmann | i) The paper introduces an execution-guided self-consistency approach for text-to-SQL generation. ii) The research aims to improve accuracy in complex text-to-SQL tasks by leveraging execution results for candidate query selection. iii) The methodology utilizes exact and approximate execution-based similarity metrics within the Minimum Bayes Risk (MBR) decoding framework. iv) The Qwen 2.5 Coder 7B model employing this method achieves nearly a 10% accuracy improvement, matching the performance of O1 while reducing inference cost by 30 times. v) AI practitioners can leverage execution-guided self-consistency to improve the performance of smaller, cost-effective models in text-to-SQL tasks.  |
| SketchVideo: Sketch-based Video Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.23284) or [HuggingFace](https://huggingface.co/papers/2503.23284))| dizhang, WeicaiYe, Xintao, fuhongbo, Okrin | SketchVideo presents a unified framework for generating and editing videos conditioned on sparse keyframe sketches and text prompts. The research objective is to achieve precise spatial layout and motion control in video synthesis and editing using temporally sparse user-drawn sketches. It utilizes a skip-residual sketch control structure for DiT models, an inter-frame attention mechanism for propagating sparse conditions, and a video insertion module with latent fusion for editing. Experiments show superior performance, with SketchVideo achieving the lowest LPIPS (27.56) and highest CLIP score (98.31) in generation benchmarks compared to methods like SparseCtrl. AI practitioners can implement this technique to provide users with fine-grained geometric and motion control in video creation/editing tools, enhancing controllability beyond text-only approaches. |
| TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud
  Detection (Read more on [arXiv](https://arxiv.org/abs/2503.24115) or [HuggingFace](https://huggingface.co/papers/2503.24115))| Kai Wu, Jingpeng Wang, HuangMinhua, WDong, JimmyMa99 | This paper introduces TeleAntiFraud-28k, an open-source audio-text dataset with slow-thinking annotations for telecom fraud detection. The research aims to overcome the lack of suitable multimodal training data by integrating audio signals with reasoning-oriented textual analysis for automated fraud identification. Methodology involves dataset construction via three strategies: processing real anonymized calls with ASR/TTS, semantic expansion using LLM self-instruction, and multi-agent adversarial simulation, followed by LLM-based annotation capturing reasoning steps. Key results include the creation of 28,511 audio-text pairs and the demonstration that fine-tuning Qwen2Audio on this dataset significantly boosted fraud detection F1 score to 84.78% (average F1 across tasks 83.00%) on the established TeleAntiFraud-Bench. For AI practitioners, this work provides a crucial dataset and benchmark for developing and evaluating multimodal, reasoning-capable audio language models specifically for the challenging task of telecom fraud detection. |
| Efficient Inference for Large Reasoning Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.23077) or [HuggingFace](https://huggingface.co/papers/2503.23077))| jiaheng233, Bibaolong, HongyuChen, HongchengGao, yueliu1999 | This survey reviews and categorizes methods for improving the inference token efficiency of Large Reasoning Models (LRMs) while maintaining reasoning quality. The primary objective is to analyze techniques mitigating high token consumption, memory overhead, and inference time inherent in LRM's deliberative reasoning processes. It introduces a taxonomy classifying approaches into explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping explicit structure, and implicit latent CoT, which encodes reasoning in hidden representations, alongside empirical analysis. Key findings categorize methods based on whether they maintain explicit reasoning steps or encode them latently; for instance, on GSM8K, explicit methods like TokenSkip (ratio=0.5) achieve 86.70% accuracy using 113.05 tokens with LLaMA-3.1-8B-Instruct, while implicit methods like SoftCoT reach 85.81% accuracy with Qwen2.5-7B-Instruct, though its specific token cost comparison is not fully detailed in the provided table excerpt. AI practitioners gain insights into the performance/efficiency trade-offs of LRM optimization techniques, informing the selection of methods (e.g., explicit CoT for interpretability, implicit CoT for token reduction) for developing cost-effective reasoning applications. |
| Classical Planning with LLM-Generated Heuristics: Challenging the State
  of the Art with Python Code (Read more on [arXiv](https://arxiv.org/abs/2503.18809) or [HuggingFace](https://huggingface.co/papers/2503.18809))| jendrikseipp, andregrahl, abcorrea | This paper demonstrates using Large Language Models (LLMs) to automatically generate domain-dependent heuristic functions as Python code for classical planning tasks. The objective was to determine if LLM-generated heuristics could outperform traditional domain-independent heuristics and compete with state-of-the-art learned heuristics. The methodology involved prompting an LLM (e.g., DeepSeek R1) multiple times for a given planning domain, evaluating the resulting pool of Python heuristic functions on training tasks using Greedy Best-First Search (GBFS), and selecting the best-performing one. Results show the selected LLM-generated heuristics significantly outperformed the widely used hFF heuristic (solving 373 vs. 243 test tasks in Pyperplan) and were competitive with state-of-the-art learned heuristics implemented in optimized C++, even when run in an unoptimized Python planner. For AI practitioners, this implies LLMs can automate the creation of highly effective, domain-specific heuristics for planning, potentially accelerating development and improving performance without requiring deep heuristic engineering expertise or specialized learning pipelines. |
| Expanding RL with Verifiable Rewards Across Diverse Domains (Read more on [arXiv](https://arxiv.org/abs/2503.23829) or [HuggingFace](https://huggingface.co/papers/2503.23829))| zptu, haitaominlp, douvleplus, freesunshine0316, yudian | This paper extends Reinforcement Learning with Verifiable Rewards (RLVR) to diverse domains like medicine and economics, using a distilled generative reward model. The main objective is to investigate RLVR's applicability beyond well-structured tasks and evaluate if a single, trained reward model can effectively provide cross-domain reward signals for free-form answers without domain-specific annotations. The methodology involves training a 7B parameter reward model using judgments distilled from a larger teacher LLM (Qwen2.5-72B-Instruct) and incorporating model-based soft scoring for RL fine-tuning (using REINFORCE, RLOO, etc.) of a base 7B policy model. Using RLOO with the distilled 7B reward model (RM-7B) and soft scoring yielded a 30.0% average accuracy on multi-subject tasks, outperforming the baseline rule-based reward (16.6%) and matching the performance using the much larger Qwen2.5-72B model directly for rewards (30.6%). For AI practitioners, this suggests that a smaller, distilled generative reward model can effectively guide RL fine-tuning across diverse domains with unstructured answers, offering a computationally efficient alternative to large teacher models or domain-specific reward engineering, enhancing RLVR's scalability and robustness. |
| Progressive Rendering Distillation: Adapting Stable Diffusion for
  Instant Text-to-Mesh Generation without 3D Data (Read more on [arXiv](https://arxiv.org/abs/2503.21694) or [HuggingFace](https://huggingface.co/papers/2503.21694))| Zhen Lei, Xiangyu Zhu, Rongyuan Wu, DarklordLeto, ZhiyuanthePony | This paper presents Progressive Rendering Distillation (PRD) to adapt Stable Diffusion (SD) for instant text-to-mesh generation without 3D ground-truth data. The objective is to overcome the 3D data scarcity problem by distilling knowledge from multi-view 2D diffusion models into an SD-based native 3D generator. PRD progressively denoises latent noise over a few steps, decoding intermediate results into Triplanes and using score distillation with SD, MVDream, and RichDreamer as teachers; Parameter-Efficient Triplane Adaptation (PETA) adds only 2.5% trainable parameters via LoRA. The resulting model, TriplaneTurbo, generates high-quality textured meshes in 1.2 seconds, achieving a CLIP Score of 68.2, outperforming prior methods in speed and quality without 3D training data. For AI practitioners, this work demonstrates an effective, data-efficient method to repurpose large 2D diffusion models for rapid 3D content creation, significantly reducing reliance on 3D datasets and accelerating generation pipelines. |
| TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through
  Task Tokenization (Read more on [arXiv](https://arxiv.org/abs/2503.19901) or [HuggingFace](https://huggingface.co/papers/2503.19901))| BoDai, WenjiaWang, frankzydou, Zeshi209, lianganimation | TokenHSI introduces a unified transformer-based policy using task tokenization to synthesize diverse, physically plausible human-scene interactions. The primary objective is to develop a single, versatile physics-based controller capable of learning multiple foundational HSI skills and efficiently adapting them to novel, complex scenarios like skill composition or environment variations. Key methodology involves separate tokenizers for shared humanoid proprioception and distinct task states, combined within a transformer encoder via a masking mechanism, enabling multi-task learning and flexible adaptation by adding new tokenizers and lightweight adapter layers. Primary results demonstrate successful unification of diverse skills (following, sitting, climbing, carrying) and superior adaptation compared to baselines, achieving a 99.2% success rate on the challenging Climb + Carry skill composition task. For AI practitioners, this provides an efficient and extensible framework for building versatile physics-based agents capable of complex interactions, reducing the need for separate controllers per skill and enabling rapid adaptation to new tasks with minimal parameter fine-tuning. |
| KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large
  Vision-Language Models in the Korean Language (Read more on [arXiv](https://arxiv.org/abs/2503.23730) or [HuggingFace](https://huggingface.co/papers/2503.23730))| lastdefiance20, yoonshik1205 | This paper introduces KOFFVQA, a novel Korean free-form Visual Question Answering benchmark designed for objective evaluation of Large Vision-Language Models (VLMs). The main objective is to overcome the limitations of existing VLM evaluation methods, namely the subjectivity of judge models and the lack of Korean-specific benchmarks, by providing a reliable framework for assessing open-ended VLM responses. The methodology involves a benchmark dataset of 275 curated image-question pairs, each accompanied by detailed, objective grading criteria, which guide an LLM judge (specifically Gemma 2 9B in testing) to score VLM responses on a scale of 0-10 across 10 performance categories. Results from evaluating 47 VLMs show this criteria-guided LLM-judge approach achieves significantly higher evaluation consistency (e.g., mean score standard deviation of 0.398 for Gemma 2 9B vs. 0.584 for ground-truth comparison) and accuracy (89.3% correct grading for Gemma 2 9B) compared to methods using ground-truth comparisons or VLM-as-a-judge, which was found prone to visual hallucinations. For AI practitioners, this work provides a robust benchmark and methodology for objectively evaluating the free-form reasoning and Korean language capabilities of VLMs, highlighting that explicit, objective criteria significantly improve judge model reliability over subjective or ground-truth-comparative approaches. |
| UPME: An Unsupervised Peer Review Framework for Multimodal Large
  Language Model Evaluation (Read more on [arXiv](https://arxiv.org/abs/2503.14941) or [HuggingFace](https://huggingface.co/papers/2503.14941))| Zheyuan Liu, Yibing, yuehuang, MunanNing, 77Hui | This paper introduces UPME, an unsupervised peer review framework for evaluating Multimodal Large Language Models (MLLMs) using only image data, eliminating the need for human QA annotations. The research objective is to develop an objective MLLM evaluation method that avoids the high cost of human annotation and mitigates biases found in MLLM-as-a-judge systems. UPME utilizes a peer review process where MLLMs generate questions for images and evaluate peer answers using a vision-language scoring system (assessing correctness, visual understanding/reasoning, image-text correlation) refined by dynamic weight optimization based on evaluation consistency. Experimental results show UPME achieves high alignment with human judgments, attaining a Pearson correlation of 0.944 on the MMstar dataset, while significantly reducing verbosity and self-preference biases compared to baseline peer review methods. For AI practitioners, UPME offers a scalable, automated, and less biased approach to evaluate MLLM performance, particularly for visual capabilities, without requiring extensive human-annotated datasets. |
| Easi3R: Estimating Disentangled Motion from DUSt3R Without Training (Read more on [arXiv](https://arxiv.org/abs/2503.24391) or [HuggingFace](https://huggingface.co/papers/2503.24391))| Anpei Chen, Andreas Geiger, Yuliang Xiu, faneggg, rover-xingyu | Easi3R introduces a training-free method to adapt the static 3D reconstruction model DUSt3R for dynamic 4D reconstruction by disentangling motion from its attention maps. The main objective is to extract and separate camera and object motion information implicitly encoded within DUSt3R's attention layers without requiring retraining or fine-tuning on dynamic datasets. The key methodology involves aggregating spatial and temporal cross-attention maps to derive dynamic object segmentations, which are then used for attention re-weighting during a second inference pass and optional segmentation-aware global alignment. Easi3R significantly outperforms previous methods trained or fine-tuned on dynamic data across camera pose estimation, dynamic object segmentation (e.g., achieving 53.0 JM on DAVIS-all without SAM2 using the MonST3R backbone), and 4D point map reconstruction. For AI practitioners, this implies that task adaptation of large pre-trained models can sometimes be achieved through careful analysis and manipulation of internal representations like attention maps during inference, reducing the need for costly retraining on specialized dynamic datasets. |
| MeshCraft: Exploring Efficient and Controllable Mesh Generation with
  Flow-based DiTs (Read more on [arXiv](https://arxiv.org/abs/2503.23022) or [HuggingFace](https://huggingface.co/papers/2503.23022))| Xiaoshui Huang, Zexiang Liu, Di Huang, Junyi Chen, Xianglong He | MeshCraft introduces a novel framework for efficient and controllable 3D mesh generation using flow-based diffusion transformers. The paper addresses the challenge of slow generation speeds and uncontrollable face numbers in existing mesh generation techniques. MeshCraft employs a transformer-based VAE to encode and decode meshes in a continuous latent space and a flow-based diffusion transformer conditioned on the number of faces. Experiments demonstrate MeshCraft achieves a 35x speed increase compared to MeshGPT while maintaining state-of-the-art mesh quality. The framework's efficient and controllable mesh generation capability enables AI practitioners to rapidly generate high-quality 3D assets with user-defined specifications.  |
| Bridging Evolutionary Multiobjective Optimization and GPU Acceleration
  via Tensorization (Read more on [arXiv](https://arxiv.org/abs/2503.20286) or [HuggingFace](https://huggingface.co/papers/2503.20286))| Ran Cheng, Kebin Sun, Naiwei Yu, Hao Li, ZhenyuLiang | i) This paper introduces a tensorization methodology to accelerate evolutionary multiobjective optimization (EMO) algorithms on GPUs. ii) The research aims to bridge the gap between EMO algorithms and GPU computing by transforming EMO data structures and operations into tensor representations. iii) The methodology involves tensorizing data structures and operations within EMO algorithms and applying this tensorization to NSGA-III, MOEA/D, and HypE. iv) Experiments show that tensorized EMO algorithms achieve speedups of up to 1113× compared to CPU-based counterparts on a multi-objective robot control benchmark. v) Tensorization enables AI practitioners to effectively utilize GPUs to significantly improve the computational efficiency and scalability of EMO algorithms for complex optimization problems.  |
| Decoupling Angles and Strength in Low-rank Adaptation (Read more on [arXiv](https://arxiv.org/abs/2503.18225) or [HuggingFace](https://huggingface.co/papers/2503.18225))| Zeynep Akata, Leander Girrbach, Massimo Bini | i) The paper introduces Decoupled Low-rank Adaptation (DeLoRA), a novel parameter-efficient finetuning method. ii) The research aims to enhance the robustness of low-rank adaptation methods like LoRA by decoupling angular learning from adaptation strength. iii) DeLoRA normalizes and scales learnable low-rank matrices, bounding the transformation distance through normalization. iv) Experiments on subject-driven image generation demonstrate that DeLoRA achieves a DINO score of 0.693 and CLIP-I score of 0.820 matching or surpassing LoRA's performance. v) AI practitioners can leverage DeLoRA to achieve more robust performance in adapting large-scale models to downstream tasks, particularly where hyperparameter tuning is challenging or extended training is required.  |
| Entropy-Based Adaptive Weighting for Self-Training (Read more on [arXiv](https://arxiv.org/abs/2503.23913) or [HuggingFace](https://huggingface.co/papers/2503.23913))| Wei Wang, Mingyu Derek Ma, Yihe Deng, Xiaoxuan Wang | i) This paper introduces Entropy-Based Adaptive Weighting for Self-Training (EAST), a novel method to improve mathematical reasoning in large language models (LLMs). ii) The research aims to address the challenge of effectively using self-generated data in self-training by prioritizing uncertain data points. iii) EAST assigns adaptive weights based on the entropy of the model's sample distribution, using a mapping function with a tunable sharpness parameter integrated with SFT, DPO, and KTO loss functions. iv) On the MATH benchmark, EAST achieves approximately a 1% gain over the backbone model, and on GSM8K, it attains a further 1-2% performance boost compared to the vanilla method using the Llama-3.2-1B and Llama-3.1-8B architectures. v) EAST provides AI practitioners with an improved self-training strategy by reweighting training data to leverage uncertainty information, potentially increasing reasoning capabilities and reducing overfitting on overconfident data.  |
