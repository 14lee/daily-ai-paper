

## Papers for 2025-04-30

| Title | Authors | Summary |
|-------|---------|---------|
| UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with
  Diverse Modalities and Granularities (Read more on [arXiv](https://arxiv.org/abs/2504.20734) or [HuggingFace](https://huggingface.co/papers/2504.20734))| Sung Ju Hwang, Soyeong Jeong, jinheon, KangsanKim71, wgcyeo | UniversalRAG introduces a retrieval-augmented generation framework handling diverse modalities and granularities by routing queries to modality-specific corpora. The main objective is to address the limitation of existing RAG systems, which typically handle only single modalities or suffer from modality gaps when unifying heterogeneous corpora. The key methodology involves a modality-aware router (either training-free using GPT-4o or trained using models like T5) that dynamically selects the optimal corpus (text, image, video) and granularity (e.g., paragraph, document, clip) for retrieval, avoiding direct cross-modal comparison in a unified space. Across 8 benchmarks using the InternVL-2.5 model, UniversalRAG with a T5-Large trained router achieved a 39.36 average score, significantly outperforming a unified corpus baseline (31.15), demonstrating the effectiveness of modality-specific routing. For AI practitioners, this implies that dynamically routing queries to specialized, modality-specific corpora, rather than relying on a single unified multimodal index, is crucial for building robust RAG systems that accurately leverage heterogeneous knowledge sources. |
| Reinforcement Learning for Reasoning in Large Language Models with One
  Training Example (Read more on [arXiv](https://arxiv.org/abs/2504.20571) or [HuggingFace](https://huggingface.co/papers/2504.20571))| Baolin, renll, ZhiyuanZeng, hushqyang, ypwang61 | This paper demonstrates that Reinforcement Learning with Verifiable Reward (RLVR) using just one training example (1-shot RLVR) can dramatically enhance LLM mathematical reasoning. The main objective was to investigate the extent to which the RLVR training dataset size can be reduced while maintaining performance comparable to using large datasets. The key methodology involved applying RL algorithms, primarily GRPO, to LLMs (e.g., Qwen2.5-Math-1.5B) using a single math problem-answer pair, duplicated to form the training batch. Primary results show that a carefully selected single example improved the Qwen2.5-Math-1.5B model's performance on MATH500 from 36.0% to 73.6%, matching the performance obtained using a 1.2k example dataset. The principal implication for AI practitioners is that substantial reasoning improvements may be achievable with extreme data efficiency using RLVR, suggesting base models possess strong latent capabilities that can be activated by minimal, targeted RL signals, potentially reducing the need for extensive data curation. |
| ReasonIR: Training Retrievers for Reasoning Tasks (Read more on [arXiv](https://arxiv.org/abs/2504.20595) or [HuggingFace](https://huggingface.co/papers/2504.20595))| pangwei, sewon, Muennighoff, volpato30, rulins | ReasonIR-8B is a novel bi-encoder retriever trained specifically for reasoning-intensive retrieval tasks using a synthetic data generation pipeline. The main objective is to improve retrieval quality for complex reasoning queries where existing fact-focused training data and retrievers show limited gains. The key methodology involves REASONIR-SYNTHESIZER, a pipeline generating challenging synthetic data including varied-length queries, reasoning-intensive hard queries, and corresponding plausible hard negatives from seed documents, used for contrastive training of a Llama3.1-8B model. REASONIR-8B achieved a new state-of-the-art 29.9 nDCG@10 on the BRIGHT benchmark without reranking, outperforming existing retrievers and improving downstream RAG task performance on MMLU by 6.4% over the closed-book baseline. For AI practitioners, this work provides a method (REASONIR-SYNTHESIZER) and a trained model (REASONIR-8B) to enhance RAG systems needing to retrieve diverse, contextually relevant information for complex reasoning, rather than just direct factual answers. |
| Toward Evaluative Thinking: Meta Policy Optimization with Evolving
  Reward Models (Read more on [arXiv](https://arxiv.org/abs/2504.20157) or [HuggingFace](https://huggingface.co/papers/2504.20157))| Chanwoo Park, dykang, machineteacher, zaemyung | Meta Policy Optimization (MPO) introduces a meta-reward model to dynamically refine evaluation rubrics during reinforcement learning alignment for LLMs. The main objective is to mitigate reward hacking and reduce the prompt engineering overhead associated with static reward models in RLAIF. The key methodology involves using a meta-reward model (MRM) to monitor the training context (policy outputs, reward scores, current rubric) and iteratively update the reward model's (RM) evaluation prompt via meta-analysis, meta-refinement, and meta-merging steps during PPO training. Primary results demonstrate that MPO-aligned models consistently outperform static-prompt PPO baselines across tasks; for instance, on essay writing, the 32B RM + 32B MRM configuration achieved an Elo rating of 1196, significantly higher than the 966 Elo of the PPO baseline using the initial prompt. The principal implication for AI practitioners is that MPO provides a framework for automating the development of effective evaluation rubrics and enhancing alignment stability, reducing reliance on brittle, manually engineered prompts and mitigating risks of reward hacking. |
| TesserAct: Learning 4D Embodied World Models (Read more on [arXiv](https://arxiv.org/abs/2504.20995) or [HuggingFace](https://huggingface.co/papers/2504.20995))| Junyan Li, Hongxin Zhang, Qiao Sun, yilundu, anyeZHY | TesserAct introduces a 4D embodied world model learned from RGB, Depth, and Normal (RGB-DN) videos to predict 3D scene evolution based on text instructions. The objective is to create a model generating temporally and spatially consistent 4D scene predictions for embodied agent actions, surpassing traditional 2D world models. Key methodology involves fine-tuning a video diffusion model on a curated RGB-DN dataset for joint prediction and using a novel algorithm with consistency/regularization losses to reconstruct coherent 4D point clouds from generated videos. TesserAct achieves superior 4D reconstruction quality, evidenced by a Chamfer L1 distance of 0.0811 on synthetic data (lower is better), and improves downstream robotic task success rates (e.g., 88% vs 81% for UniPi* on 'close box'). For AI practitioners, this provides a framework for building geometrically accurate world models from multi-modal video, enabling enhanced simulation, planning, and policy learning for robotics compared to video-only approaches. |
| YoChameleon: Personalized Vision and Language Generation (Read more on [arXiv](https://arxiv.org/abs/2504.20998) or [HuggingFace](https://huggingface.co/papers/2504.20998))| Jing Shi, Krishna Kumar Singh, Thao Nguyen, Yuheng, yjlee0222 | Yo'Chameleon personalizes Large Multimodal Models for joint vision and language generation using few-shot concept images. The primary objective is to enable LMMs to understand and generate content about specific user concepts provided via only 3-5 images, addressing the limitation of generic models. The methodology utilizes dual soft prompts (for understanding and generation) tuned via a self-prompting mechanism for task selection, combined with a "soft-positive" image training strategy using adaptive prompt lengths based on visual similarity. Results demonstrate superior performance over prompting baselines, achieving a recognition accuracy of 0.845 (vs. 0.727 baseline) and a CLIP image similarity of 0.783 (vs. 0.566 baseline) for generation using only 32 learnable tokens, while preserving general model capabilities. For AI practitioners, this provides a parameter-efficient method to infuse personalized knowledge into LMMs using minimal user data, enabling tailored multimodal generation and understanding without requiring full model retraining or suffering significant catastrophic forgetting. |
| The Leaderboard Illusion (Read more on [arXiv](https://arxiv.org/abs/2504.20879) or [HuggingFace](https://huggingface.co/papers/2504.20879))| sayashk, dsouzadaniel, AlexWang, olivernan, shivalikasingh | This research reveals systematic distortions in Chatbot Arena rankings caused by undisclosed private testing, selective score reporting, and significant data access disparities favoring proprietary models. The objective was to investigate whether Chatbot Arena rankings accurately reflect generative AI model capabilities or are skewed by preferential evaluation policies and practices. Methodology involved auditing 243 models from 42 providers using ~2M battle records (Jan 2024 - Apr 2025), combining public data, scraped statistics, proprietary API data, simulations, and real-world private variant experiments. Primary results show undisclosed private testing benefits select providers (e.g., Meta tested 27 private variants pre-Llama 4 release), enabling selective score reporting; proprietary models receive disproportionately more data (Google/OpenAI ~20% each vs. 83 open-weight models combined ~30%); and access to Arena data provides substantial performance gains (up to 112% relative win-rate increase on ArenaHard). For AI practitioners, this implies that Chatbot Arena rankings may significantly reflect leaderboard-specific dynamics and overfitting due to unequal data access and private testing advantages, rather than solely general model competence, potentially misguiding model development and selection. |
| Certified Mitigation of Worst-Case LLM Copyright Infringement (Read more on [arXiv](https://arxiv.org/abs/2504.16046) or [HuggingFace](https://huggingface.co/papers/2504.16046))| Daniel Khashabi, Benjamin Van Durme, Jiacan Yu, mmarone, jackzhang | This paper introduces BLOOMSCRUB, an inference-time method for certified mitigation of worst-case copyright infringement in LLMs by removing long verbatim quotes. The primary objective is to prevent LLMs from generating long verbatim excerpts (worst-case infringement) from copyrighted corpora while preserving text utility and factual knowledge. The key methodology involves iteratively applying a Bloom filter for efficient detection of verbatim quotes exceeding a length threshold (τ) and using an LLM to perform guided rewriting ("scrubbing") of the detected segments, with an option to abstain if removal fails. Experimental results demonstrate BLOOMSCRUB effectively reduces worst-case infringement, achieving near-zero generation of quotes longer than 100 characters (%R > Q(100) ≈ 0.0%) compared to ~20% in vanilla models, while maintaining information quality and utility. For AI practitioners, BLOOMSCRUB offers a scalable, plug-and-play, inference-only technique to demonstrably reduce the risk of generating infringing long verbatim text, adaptable to different risk thresholds without requiring model retraining or access to logits. |
| ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting (Read more on [arXiv](https://arxiv.org/abs/2504.20630) or [HuggingFace](https://huggingface.co/papers/2504.20630))| Tao Jin, Zhiyuan Zhu, Changhao Pan, Wenxiang Guo, AaronZ345 | ISDrama introduces a unified framework for generating continuous, multi-speaker binaural speech with dramatic prosody from multimodal prompts for immersive spatial drama applications. The research objective is to simultaneously model complex spatial audio cues (position, orientation, movement, Doppler effect) and dramatic prosody from diverse inputs (scripts, audio, video, poses), overcoming data scarcity and integration challenges. Key methodology includes the creation of the MRSDrama dataset and the ISDrama model, featuring a contrastive learning-based Multimodal Pose Encoder for unified pose features and a flow-based Mamba-Transformer (Immersive Drama Transformer) with Drama-MOE and context-consistent CFG for generation. Primary results show ISDrama outperforms baselines, achieving superior prosodic expressiveness (MOS-E 4.01 ± 0.09 vs. 3.86 ± 0.06 for F5-TTS) and better spatial consistency (MOS-P 4.18 ± 0.10 for geometric input). The principal implication for AI practitioners is the availability of a one-stage solution for generating high-fidelity, spatially accurate, and dramatically expressive binaural speech directly from multimodal sources, avoiding error accumulation typical of cascaded approaches and enhancing immersive VR/AR experiences. |
| X-Fusion: Introducing New Modality to Frozen Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.20996) or [HuggingFace](https://huggingface.co/papers/2504.20996))| Yijun Li, Siddharth Srinivasan Iyer, Xun Huang, Thao Nguyen, Sicheng Mo | X-Fusion introduces a framework for adding vision capabilities to frozen Large Language Models (LLMs) while preserving their text abilities. The primary objective is to develop an efficient method for unified multimodal understanding and generation without retraining the LLM from scratch or degrading its inherent language skills. The key methodology involves a Dual Tower architecture where the original LLM (text tower) is frozen, and a parallel, trainable vision tower processes visual information, with selective feature integration across layers. Experimental results demonstrate superior performance over alternative architectures, with the Dual Tower design achieving a 23% lower (better) FID score (14.20 vs 19.10) on text-to-image generation compared to a single-tower fine-tuning approach using a 1B parameter model. For AI practitioners, this work presents a computationally efficient strategy to extend powerful LLMs to multimodal domains, suggesting that modality-specific towers can effectively integrate new capabilities without compromising the core model's pretrained knowledge. |
| Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional
  Talking Portrait Generation (Read more on [arXiv](https://arxiv.org/abs/2504.18087) or [HuggingFace](https://huggingface.co/papers/2504.18087))| Xiaobin Hu, FeiFan Xu, Chuming Lin, Weipeng Tan, ChengmingX | DICE-Talk introduces a diffusion-based framework for generating emotionally expressive talking portraits while preserving identity by disentangling emotion/identity and leveraging inter-emotion correlations. The primary objective is to develop an emotional talking head generation model that overcomes limitations such as insufficient audio emotion utilization, identity leakage into emotion representations, and isolated emotion learning. Key methodologies include a cross-modal disentangled emotion embedder modeling emotions as identity-agnostic Gaussian distributions, a correlation-enhanced conditioning module using a learnable emotion bank, and an emotion discrimination objective during diffusion. DICE-Talk significantly outperforms prior methods in emotion accuracy, achieving a top Emo-Score of 0.4865 on MEAD and 0.5527 on an out-of-domain dataset, while maintaining competitive lip-sync and visual quality. AI practitioners can utilize this framework's disentangled audio-visual emotion modeling and correlation-aware conditioning via emotion banks to create more controllable, realistic, and emotionally expressive digital avatars for applications requiring nuanced human interaction. |
| TreeHop: Generate and Filter Next Query Embeddings Efficiently for
  Multi-hop Question Answering (Read more on [arXiv](https://arxiv.org/abs/2504.20114) or [HuggingFace](https://huggingface.co/papers/2504.20114))| Xuming Hu, Shuliang Liu, Jinghuai Ou, Zhonghao Li, kpzhang1028 | TreeHop introduces an efficient, embedding-level iterative retrieval framework for multi-hop question answering that bypasses LLM-based query rewriting. The main objective is to significantly reduce computational overhead and latency in multi-hop RAG systems compared to methods requiring iterative LLM calls, while preserving retrieval effectiveness. Key methodology involves dynamically generating next-step query embeddings by fusing prior query and retrieved document embeddings using a gated cross-attention mechanism (UpdateGate) and rule-based pruning strategies. Primary results demonstrate comparable recall performance to advanced RAG systems on three MHQA benchmarks but with approximately 99% lower query latency (e.g., achieving 61.6% Recall@5 on 2WikiMultiHop iter2 with 0.022s latency versus 59.2% recall and 4.690s latency for Iter-RetGen). For AI practitioners, TreeHop presents a substantially faster and more cost-effective approach for implementing multi-hop RAG, replacing computationally expensive iterative LLM query refinements with lightweight embedding-space operations. |
