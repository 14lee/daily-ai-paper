

## Papers for 2025-04-03

| Title | Authors | Summary |
|-------|---------|---------|
| MergeVQ: A Unified Framework for Visual Generation and Representation
  with Disentangled Token Merging and Quantization (Read more on [arXiv](https://arxiv.org/abs/2504.00999) or [HuggingFace](https://huggingface.co/papers/2504.00999))| Cheng Tan, Juanxi, ZedongWangAI, LuyuanZhang01, Lupin1998 | MergeVQ presents a unified framework integrating token merging into VQ-based models to balance visual representation learning and autoregressive generation. The primary objective is to overcome the trade-off between generation quality, representation learning, and efficiency inherent in existing VQ-MIM approaches. Key methodologies include disentangling semantics via token merging (ToMe) while preserving spatial details in a recoverable source matrix, employing Look-up Free Quantization (LFQ), using cross-attention for detail recovery, global alignment via self-distillation (DINO), and introducing MergeAR with KV Cache compression for efficient generation. Experiments on ImageNet-1K show the representation-focused variant achieves 79.8% linear probe accuracy using only 36 merged tokens, while the generative variant achieves a competitive class-conditional generation gFID of 3.05 using MergeAR. For AI practitioners, MergeVQ offers a pathway to build more computationally efficient unified vision models, as demonstrated by its ability to achieve strong representation learning performance with significantly reduced token counts (36 tokens), potentially lowering pre-training and inference costs. |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training (Read more on [arXiv](https://arxiv.org/abs/2504.00883) or [HuggingFace](https://huggingface.co/papers/2504.00883))| Zijian Kong, Yanhao Zhang, Qingsong Xie, Zhenyi Liao, zhijie3 | This work enhances visual-spatial reasoning in Multimodal Large Language Models (MLLMs) using R1-Zero-like GRPO training. The primary objective was to improve visual-spatial intelligence (VSI) capabilities, particularly in small- to medium-sized Qwen2-VL models where Chain of Thought (CoT) prompting proved ineffective. The key methodology involved constructing the VSI-100k dataset from ScanNet and applying Group Relative Policy Optimization (GRPO) while identifying the necessity of retaining the KL penalty. The resulting vsGRPO-2B model outperformed its Qwen2-VL-2B base by 12.1% on the VSI-bench benchmark and surpassed GPT-4o performance. For AI practitioners, this demonstrates that GRPO training with curated datasets is a potent technique to specifically boost MLLM reasoning faculties like VSI, offering substantial gains over base models and even surpassing larger or closed-source alternatives for targeted tasks. |
| AnimeGamer: Infinite Anime Life Simulation with Next Game State
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2504.01014) or [HuggingFace](https://huggingface.co/papers/2504.01014))| Ying Shan, Jing Liao, Yixiao Ge, Yuying Ge, Howe666 | AnimeGamer introduces an MLLM-based framework for generating infinite, interactive anime life simulation games featuring dynamic video outputs and character state updates from language instructions. The primary objective is to create contextually consistent and dynamic multi-turn game states, addressing limitations of prior static image or text-only methods. The key methodology involves using an MLLM to predict novel action-aware multimodal representations from historical context and instructions, which are then decoded into video clips using a fine-tuned video diffusion model alongside character state prediction. AnimeGamer significantly outperforms baselines in quantitative evaluations, achieving higher character consistency (CLIP-I 0.8132 vs. 0.7960) and superior motion quality (ACC-F 0.6744 vs. 0.4249). For AI practitioners, this work demonstrates an effective approach using MLLMs to generate coherent, dynamic video-based interactive experiences by bridging language and video synthesis via specialized multimodal representations, enhancing immersion in generative games. |
| VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in
  One Step (Read more on [arXiv](https://arxiv.org/abs/2504.01956) or [HuggingFace](https://huggingface.co/papers/2504.01956))| Yueqi Duan, Jiawei Chi, Fangfu Liu, hanyang-21 | VideoScene introduces a framework to distill video diffusion models for efficient, one-step 3D scene generation from only two input images. The main objective is to bridge the gap between slow, multi-step video diffusion methods and the need for fast, 3D-consistent scene generation from sparse views. The key methodology involves a 3D-aware leap flow distillation strategy, initialized using a coarse scene from a feed-forward 3DGS model (MVSplat), and a dynamic denoising policy network (DDPNet) trained via contextual bandits to optimize leap timesteps. Primarily, VideoScene achieves significantly faster inference (~3s) while maintaining high quality; its 1-step generation on RealEstate10K yields an FVD of 103.42, vastly outperforming 1-step baselines and remaining competitive with their 50-step versions (e.g., CogVideoX-5B 50-step FVD 521.04). For AI practitioners, this offers an efficient tool for generating temporally coherent and geometrically consistent 3D video sequences from minimal input, drastically reducing computational cost for sparse-view 3D reconstruction tasks. |
| Understanding R1-Zero-Like Training: A Critical Perspective (Read more on [arXiv](https://arxiv.org/abs/2503.20783) or [HuggingFace](https://huggingface.co/papers/2503.20783))| Tianyu Pang, Wenjun Li, QPHutu, Cameron-Chen, lkevinzc | This paper critically analyzes R1-Zero-like LLM training, examining base model properties and RL optimization biases, particularly in GRPO. The primary objective is to understand how base model pretraining affects RL outcomes and to identify and mitigate biases in the GRPO algorithm. Methodology includes evaluating various base models (e.g., Qwen2.5, DeepSeek-V3-Base) on math benchmarks with different templates and comparing GRPO against a proposed unbiased variant, Dr. GRPO, in RL experiments. Key findings demonstrate that some base models exhibit strong initial reasoning (Qwen2.5 improves ~60% without templates), GRPO introduces length and standard deviation normalization biases impacting token efficiency, and the proposed Dr. GRPO optimizer corrects these, enabling a 7B model to achieve 43.3% accuracy on AIME 2024. The principal implication for practitioners is that understanding base model capabilities and utilizing unbiased RL optimizers like Dr. GRPO are essential for efficient reasoning enhancement, avoiding artifactual response length increases from biased optimization. |
| DreamActor-M1: Holistic, Expressive and Robust Human Image Animation
  with Hybrid Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.01724) or [HuggingFace](https://huggingface.co/papers/2504.01724))| Tianshu Hu, Longhao Zhang, Lizhen Wang, Zhengkun Rong, Yuxuan Luo | This paper introduces DreamActor-M1, a Diffusion Transformer (DiT) based framework for robust human image animation. The primary objective is to overcome limitations in existing methods regarding fine-grained holistic control, multi-scale adaptability (portraits to full-body), and long-term temporal coherence, particularly for unseen regions. Key methodologies include using hybrid motion guidance signals (implicit facial latent representations, 3D head spheres, 3D body skeletons with bone length adjustment), complementary appearance guidance for unseen areas, and a progressive multi-scale training strategy. The proposed method achieved superior quantitative results, for instance, an FVD score of 122.0 on their collected body animation dataset, outperforming prior works like Animate Anyone (158.3) and MimicMotion (149.9). For AI practitioners, this work demonstrates a robust DiT-based approach with hybrid explicit/implicit controls and appearance guidance, enabling the generation of higher-fidelity, more expressive, and temporally consistent human animations across diverse scales and viewpoints. |
| PaperBench: Evaluating AI's Ability to Replicate AI Research (Read more on [arXiv](https://arxiv.org/abs/2504.01848) or [HuggingFace](https://huggingface.co/papers/2504.01848))| Jun Shern Chan, James Aung, Dane Sherburn, Oliver Jaffe, Giulio Starace | PaperBench introduces a benchmark to evaluate AI agents' ability to replicate state-of-the-art AI research papers from scratch. The objective is to assess how well AI agents can understand paper contributions, develop codebases, and execute experiments to reproduce empirical results. The methodology involves providing agents with 20 ICML 2024 papers and using detailed, author-approved hierarchical rubrics alongside an LLM-based judge to evaluate the agent-generated code repository and its execution outputs. Results show the best agent, Claude 3.5 Sonnet with scaffolding, achieved an average replication score of 21.0%, significantly lower than a human expert baseline (41.4% on a subset), indicating current models have limited autonomous AI R&D replication capabilities. For AI practitioners, this highlights that while agents show nascent ability, they are not yet proficient at the complex, long-horizon task of independently replicating and validating frontier AI research, requiring substantial human oversight for such tasks. |
| ScholarCopilot: Training Large Language Models for Academic Writing with
  Accurate Citations (Read more on [arXiv](https://arxiv.org/abs/2504.00824) or [HuggingFace](https://huggingface.co/papers/2504.00824))| Zhiheng Lyu, Huaye Zeng, Ping Nie, Xueguang Ma, Yubo Wang | ScholarCopilot introduces a unified framework for training LLMs to generate academic text with accurate, context-aware citations. The main objective is to overcome limitations of traditional RAG systems by integrating dynamic retrieval directly into the generation process for improved citation relevance and quality in academic writing. The methodology involves dynamically generating special retrieval tokens ([RET]) during text generation, using their representations for similarity search against a database, and feeding retrieved references back into the model, optimizing generation and retrieval jointly. ScholarCopilot achieved 40.1% top-1 retrieval accuracy, significantly outperforming E5-Mistral-7B-Instruct (15.0%), and obtained a generation quality score of 16.2/25, surpassing larger models like Qwen-2.5-72B-Instruct (15.8/25). For AI practitioners, this work demonstrates a unified, dynamic RAG approach that can enhance LLM factual accuracy and contextual relevance for specialized generation tasks requiring precise citations, offering a potentially more efficient alternative to separate retrieval/generation pipelines. |
| Towards Physically Plausible Video Generation via VLM Planning (Read more on [arXiv](https://arxiv.org/abs/2503.23368) or [HuggingFace](https://huggingface.co/papers/2503.23368))| Lei Bai, Zhenfei Yin, Yiming Zhang, Baolu Li, Xindi Yang | This paper proposes a two-stage framework using a Vision Language Model (VLM) planner and a Video Diffusion Model (VDM) synthesizer to generate physically plausible videos. The objective is to enhance physical plausibility in video generation by explicitly incorporating physics priors, addressing the limitations of standard VDMs in understanding physical laws. The methodology involves a VLM performing coarse-grained, physics-aware motion planning via chain-of-thought (CoT) reasoning to predict rough object trajectories, which then guide a VDM through injected structured noise derived from optical flow for fine-level motion synthesis. Quantitative results on the PhyGenBench benchmark show the proposed method achieved an average score of 0.60, outperforming the best compared image-to-video method (SG-I2V at 0.54) by 11.1% in physical plausibility assessment. For AI practitioners, this demonstrates a method to integrate explicit physical reasoning from VLMs into VDMs to improve the realism and physical consistency of generated video content, particularly for scenarios involving object interactions governed by physics. |
| ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and
  Diffusion Refinement (Read more on [arXiv](https://arxiv.org/abs/2504.01934) or [HuggingFace](https://huggingface.co/papers/2504.01934))| Yunlong Yuan, Guansong Lu, Junwei Yang, Chunwei Wang, Runhui Huang | ILLUME+ enhances unified Multimodal Large Language Models (MLLMs) by integrating dual visual tokenization and diffusion refinement for improved understanding, generation, and editing. The objective is to create a single MLLM that overcomes limitations of prior models, such as poor texture preservation in editing or weaker semantic understanding, by effectively unifying these three core capabilities. Key methodologies include the DualViTok tokenizer preserving both semantic and texture details, a diffusion model decoder for high-fidelity image reconstruction and super-resolution, and a coarse-to-fine image representation strategy within the MLLM. Primary results show the 3B parameter ILLUME+ achieves competitive performance across understanding, generation, and editing benchmarks, including an improved Fréchet Inception Distance (FID) of 6.00 on the MJHQ-30k generation benchmark compared to its predecessor. For AI practitioners, this work presents a unified model architecture that supports flexible resolution inputs/outputs and demonstrates strong performance in fine-grained editing tasks, potentially offering a more versatile foundation for complex, interactive multimodal applications. |
| Articulated Kinematics Distillation from Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.01204) or [HuggingFace](https://huggingface.co/papers/2504.01204))| Chenfanfu Jiang, Yongxin Chen, Tsung-Yi Lin, Qianli Ma, Xuan Li | Articulated Kinematics Distillation (AKD) synthesizes articulated motions for rigged 3D assets by leveraging video diffusion models. The objective is to generate high-fidelity, structurally consistent character animations from text prompts, addressing limitations of prior text-to-4D methods based on neural deformation fields. AKD utilizes a low-DoF skeleton-based representation optimized via Score Distillation Sampling (SDS) with a pre-trained video diffusion model, incorporating explicit ground rendering and optional physics-based motion tracking. Experiments show AKD outperforms TC4D, achieving higher automated scores (e.g., Semantic Adherence 0.81±0.26 vs 0.40±0.34) and preference in user studies for motion quality and physical plausibility. For AI practitioners, AKD offers a method to generate controllable, physically grounded 3D character animations from text by effectively combining generative video priors with explicit articulated structure, improving consistency over deformation field approaches. |
| Safeguarding Vision-Language Models: Mitigating Vulnerabilities to
  Gaussian Noise in Perturbation-based Attacks (Read more on [arXiv](https://arxiv.org/abs/2504.01308) or [HuggingFace](https://huggingface.co/papers/2504.01308))| Zhendong Liu, Yushen Zuo, sofyc, AllenChai, Jarvis1111 | This paper investigates Vision-Language Model (VLM) vulnerability to Gaussian noise perturbations and proposes noise-augmented fine-tuning and a diffusion-based defense (DiffPure-VLM) to mitigate these risks. The primary objective is to systematically analyze VLM robustness against visual Gaussian noise and develop effective defense strategies against both simple noise and optimization-based adversarial attacks while preserving model helpfulness. Key methodologies include creating the Robust-VLGuard dataset with aligned/misaligned safety pairs, employing Gaussian noise augmentation during safety fine-tuning, and proposing the DiffPure-VLM pipeline which uses diffusion models to transform adversarial perturbations into Gaussian-like noise manageable by the fine-tuned VLMs. Primary results demonstrate that while baseline VLMs degrade significantly under Gaussian noise, the proposed noise-augmented fine-tuning enhances robustness, and DiffPure-VLM substantially reduces optimization-based attack success rates; for example, with InternVL2-8B-RobustVLGuard under a €=32/255 attack, DiffPure-VLM (t*=50) lowered the attack success rate from 70.6% to 33.4%. For AI practitioners, this implies that incorporating noise-augmented safety fine-tuning and employing diffusion-based preprocessing defenses like DiffPure-VLM are practical strategies to significantly bolster VLM security against visual perturbation attacks without excessive computational overhead or loss of core functionality. |
| Boost Your Own Human Image Generation Model via Direct Preference
  Optimization with AI Feedback (Read more on [arXiv](https://arxiv.org/abs/2405.20216) or [HuggingFace](https://huggingface.co/papers/2405.20216))| Hyunjoon Lee, Yonggyu Kim, sanghyeonna | This paper introduces HG-DPO, a method enhancing human image generation realism by applying Direct Preference Optimization (DPO) with real images and curriculum learning. The main objective is to improve diffusion models for human image synthesis by overcoming the limitations of standard DPO, which typically relies only on generated images. HG-DPO utilizes a novel preference structure where real images serve as preferred (winning) examples and generated images as non-preferred (losing), combined with a three-stage curriculum learning pipeline (easy, normal, hard) and AI feedback for dataset construction. Results demonstrate HG-DPO significantly outperforms baseline models and prior DPO methods, achieving a lower FID of 29.41 compared to the base model's 37.34 and higher CI-S of 0.9858 versus 0.9573. For AI practitioners, this provides a framework to boost the quality and realism of text-to-human image generation models by effectively integrating real-world image data into the preference learning process without costly human annotation, and enhances personalized generation tasks. |
| DASH: Detection and Assessment of Systematic Hallucinations of VLMs (Read more on [arXiv](https://arxiv.org/abs/2503.23573) or [HuggingFace](https://huggingface.co/papers/2503.23573))| Matthias Hein, Maximilian Augustin, YanNeu | This paper introduces DASH, an automated pipeline for detecting systematic false-positive object hallucinations in Vision-Language Models (VLMs) using large-scale, real-world image data. The main objective is to systematically identify clusters of semantically similar real-world images that cause a VLM to incorrectly affirm the presence of an object not actually depicted. Key methodologies include DASH-LLM, which uses LLM-generated text queries for image retrieval, and DASH-OPT, which optimizes latent diffusion model inputs to generate misleading images, both followed by k-NN retrieval on ReLAION-5B and clustering. Applying DASH to PaliGemma and two LLaVA-NeXT models across 380 objects yielded over 19k hallucination clusters containing over 950k images; fine-tuning PaliGemma on DASH-identified images improved accuracy on the derived DASH-B benchmark by 11.6%. For AI practitioners, this work highlights that significant object hallucination issues persist beyond standard benchmarks, necessitating open-world testing methods like DASH for reliable VLM assessment and providing datasets (DASH-B) for more rigorous evaluation and potential mitigation fine-tuning. |
| LSNet: See Large, Focus Small (Read more on [arXiv](https://arxiv.org/abs/2503.23135) or [HuggingFace](https://huggingface.co/papers/2503.23135))| Guiguang Ding, Jungong Han, Zijia Lin, Hui Chen, jameslahm | LSNet introduces a lightweight vision network family leveraging a novel LS convolution inspired by the human vision system's "See Large, Focus Small" strategy. The primary objective is to enhance the performance and efficiency balance in lightweight models by improving the token mixing process, specifically perception and aggregation under limited computational budgets. The key methodology involves the proposed LS (Large-Small) convolution, which uses large-kernel static depth-wise convolution for broad perception and small-kernel grouped dynamic convolution for adaptive, focused aggregation. Results demonstrate state-of-the-art performance; for instance, LSNet-B achieves 80.3% top-1 accuracy on ImageNet-1K with 1.3G FLOPs, outperforming comparable models like AFFNet and RepViT-M1.1 in both accuracy and efficiency. For AI practitioners, LSNet provides a new efficient architectural block (LS convolution) and model series offering improved accuracy-efficiency trade-offs for vision tasks deployed on resource-constrained platforms. |
| VerifiAgent: a Unified Verification Agent in Language Model Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.00406) or [HuggingFace](https://huggingface.co/papers/2504.00406))| Ehsan Shareghi, Wray Buntine, Jiuzhou Han | This paper introduces VerifiAgent, a unified agent employing two verification levels (meta and tool-based adaptive) to enhance large language model (LLM) reasoning reliability. The main research objective is to develop a generalisable and efficient verification framework for diverse LLM reasoning tasks, overcoming the limitations of current methods. VerifiAgent utilizes a two-layer methodology involving meta-verification for completeness and consistency, followed by tool-based adaptive verification which autonomously selects external tools (e.g., Python interpreter, search engine, symbolic solver) based on the reasoning type. Experimental results show VerifiAgent outperforms baseline verification methods across mathematical, logical, commonsense, and hybrid reasoning tasks, achieving 0.96 accuracy on GSM8K compared to baselines like deductive verifier (0.95). For AI practitioners, VerifiAgent offers a plug-and-play framework to improve the reliability and accuracy of LLM reasoning outputs, particularly in inference scaling scenarios, achieving better results with fewer samples and lower cost than methods like PRMs. |
| Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations (Read more on [arXiv](https://arxiv.org/abs/2503.18817) or [HuggingFace](https://huggingface.co/papers/2503.18817))| Sangheum Hwang, mawjdgus | This paper introduces Cross-Modal Alignment (CMA), a multi-modal fine-tuning method to enhance Out-of-Distribution (OoD) detection in vision-language models. The primary objective is to improve OoD performance by mitigating the modality gap observed between image and text embeddings during standard fine-tuning. CMA employs a regularization loss during fine-tuning to explicitly align in-distribution image-text embedding pairs in the hyperspherical representation space, shown theoretically to correspond to maximizing the log-likelihood of a joint energy-based model. The proposed CMA method, when combined with the NegLabel scoring function, achieved state-of-the-art OoD performance on the MOS benchmark, attaining an average FPR95 of 19.93% and 95.13% AUROC, significantly outperforming existing zero-shot and fine-tuning approaches while maintaining high ID accuracy (82.64% on ImageNet-1k). For AI practitioners, this work demonstrates that explicitly regularizing for cross-modal alignment during fine-tuning can substantially improve model robustness by enhancing both OoD detection and in-distribution classification, thereby increasing the reliability of VLMs deployed in open environments. |
