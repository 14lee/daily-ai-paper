

## Papers for 2025-04-16

| Title | Authors | Summary |
|-------|---------|---------|
| Genius: A Generalizable and Purely Unsupervised Self-Training Framework
  For Advanced Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.08672) or [HuggingFace](https://huggingface.co/papers/2504.08672))| Haiteng Zhao, Chang Ma, Hang Yan, QiushiSun, xufangzhi | Genius is a generalizable, purely unsupervised self-training framework designed to enhance Large Language Model (LLM) reasoning capabilities without external supervision. The central research objective is to advance LLM reasoning ability using only general, unlabeled queries, bypassing the need for annotated data or auxiliary reward models. Genius employs a stepwise foresight re-sampling strategy to sample candidate reasoning steps and estimate their value by simulating future outcomes, coupled with an Advantage-Calibrated Optimization (ACO) loss function to handle estimation noise and ensure robust optimization. Using only 25K unsupervised general queries from the Magpie dataset, Genius improved the average reasoning performance of LLaMA3.1-8B-Instruct by over 7% (from 49.65% to 57.08%) across seven reasoning benchmarks. For AI practitioners, this demonstrates a promising approach to scale LLM reasoning performance by leveraging vast amounts of readily available unlabeled data, potentially reducing dependency on expensive annotations and specialized reward models. |
| xVerify: Efficient Answer Verifier for Reasoning Model Evaluations (Read more on [arXiv](https://arxiv.org/abs/2504.10481) or [HuggingFace](https://huggingface.co/papers/2504.10481))| Bo Tang, Wentao Zhang, Pengyuan Wang, Duguce, Hush-cd | This paper introduces xVerify, an efficient LLM-based answer verifier designed for evaluating reasoning models by accurately determining answer equivalence. The research aims to address the inadequacy of existing evaluation methods in extracting final answers and performing robust equivalence checks for complex, multi-step reasoning outputs from LLMs. Methodologically, the authors constructed the VAR dataset from 19 LLMs across 24 benchmarks, used multi-round GPT-4o and human annotation for labeling, and fine-tuned various xVerify models (0.5B-32B parameters) using QLoRA. Key results show all xVerify models achieving over 95% F1 score and accuracy on the test set, with the xVerify-3B-Ib model surpassing even GPT-4o (used as a CoT judge) in overall performance (97.27% vs 96.95% accuracy). For AI practitioners, the publicly available xVerify models offer a more reliable, efficient, and cost-effective method for automatically evaluating the correctness of reasoning model outputs compared to expensive API calls or less robust rule-based frameworks. |
| Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.10465) or [HuggingFace](https://huggingface.co/papers/2504.10465))| Weixian Lei, Yanwei Li, Zilong Huang, Tao Zhang, LXT | Pixel-SAIL introduces a single-transformer architecture for multimodal large language models (MLLMs) targeting fine-grained, pixel-level understanding tasks. The primary research objective is to develop a highly simplified MLLM architecture for pixel-grounded understanding, eliminating the need for separate vision encoders and segmentation expert modules. Key methodologies include integrating a learnable upsampling module for refining visual tokens, a novel visual prompt injection strategy using special vocabulary tokens fused early with vision tokens, and a vision expert distillation technique. Pixel-SAIL (3B) demonstrates superior performance on referring segmentation benchmarks, outperforming larger models like GLaMM (7B) by up to 3.0% cIoU on RefCOCOg with a significantly simpler pipeline. For AI practitioners, this work shows that effective pixel-level understanding can be achieved with reduced architectural complexity using a unified transformer, potentially simplifying model development, training, and deployment. |
| Heimdall: test-time scaling on the generative verification (Read more on [arXiv](https://arxiv.org/abs/2504.10337) or [HuggingFace](https://huggingface.co/papers/2504.10337))| Xing Jin, WesleyShi | This paper introduces Heimdall, an RL-trained long CoT verifier, and Pessimistic Verification to enhance LLM solution correctness judgment and problem-solving scaling. The main objective is to improve the weak verification capabilities of LLMs for complex reasoning tasks and leverage this improved verification to scale overall problem-solving accuracy. Key methodology involves training Heimdall via PPO reinforcement learning on filtered math problems and proposing Pessimistic Verification, an algorithm that selects solutions by balancing solver outputs and verifier judgments using a lower-confidence-bound approach. Primary results show Heimdall boosting verification accuracy from 62.5% to 94.5% on AIME2024 (97.5% with sampling), while Pessimistic Verification improved AIME2025 solving accuracy from 54.2% to 70.0% (16x compute budget with DeepSeek-R1-Distill-Qwen-32B). The principal implication for AI practitioners is that utilizing dedicated RL-trained verifiers and selection algorithms like Pessimistic Verification can significantly enhance the reliability and performance of LLMs on complex problem-solving by explicitly verifying and selecting trustworthy solutions. |
| Seedream 3.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.11346) or [HuggingFace](https://huggingface.co/papers/2504.11346))| Zhichao Lai, Xiaoxia Hou, Qiushan Guo, Lixue Gong, Yu Gao | Seedream 3.0 is presented as a high-performance Chinese-English bilingual text-to-image foundation model with significant improvements over its predecessor. The objective was to enhance alignment with complex prompts, fine-grained typography (especially Chinese text), visual aesthetics, fidelity, and native image resolution. Methodologies involved data augmentation (defect-aware training, dual-axis sampling), architectural improvements (mixed-resolution training, cross-modality RoPE, representation alignment loss), advanced post-training (aesthetic SFT, VLM reward model), and novel acceleration techniques (consistent noise expectation, importance-aware timestep sampling). Seedream 3.0 achieves superior performance, ranking first on the Artificial Analysis Leaderboard (ELO 1158), demonstrating a 94% text availability rate for Chinese characters, and enabling 4-8x inference speedup while supporting native 2K resolution. For AI practitioners, this model offers enhanced capabilities for high-fidelity, high-resolution bilingual image generation with strong text rendering and improved prompt adherence, suitable for applications demanding advanced typography and aesthetic quality. |
| How Instruction and Reasoning Data shape Post-Training: Data Quality
  through the Lens of Layer-wise Gradients (Read more on [arXiv](https://arxiv.org/abs/2504.10766) or [HuggingFace](https://huggingface.co/papers/2504.10766))| Ziyue Li, Yanhong Li, Ming Li, zhoutianyi | This paper analyzes how instruction and reasoning data quality impacts LLM post-training dynamics through the spectral properties of layer-wise gradients. The primary objective is to understand how low/high-quality instruction and reasoning data affect gradients and to unify different data quality evaluation metrics using gradient spectral characteristics. The study employs Singular Value Decomposition (SVD) on the layer-wise gradients (specifically Q, K, V, O projections) of various LLMs (Qwen2, Llama3, Gemma2 families) finetuned on datasets partitioned by quality metrics (IFD, InsTag, Difficulty, Reward) and compares instruction-following versus reasoning data. Results consistently show that higher-quality data, for both instruction and reasoning types, leads to lower nuclear norms and significantly higher effective ranks of the gradients; for instance, high-quality reasoning data (s1.1) yielded substantially higher effective ranks than high-quality instruction data across models (e.g., Table 2, Qwen2.5-7B K-projection high-quality reasoning rank 361.2 vs. instruction rank 153.3). The principal implication for AI practitioners is that the effective rank of layer-wise gradients offers a unified, robust metric to evaluate data quality, potentially guiding more effective data selection or synthesis strategies for stable LLM post-training, particularly for developing complex reasoning abilities. |
| TextArena (Read more on [arXiv](https://arxiv.org/abs/2504.11442) or [HuggingFace](https://huggingface.co/papers/2504.11442))| Leshem Choshen, Benjamin-eecs, simonycl, bobbycxy, LeonGuertler | TextArena introduces an open-source framework leveraging 74+ competitive text-based games for evaluating and training agentic capabilities in LLMs via a dynamic TrueSkill leaderboard. The objective is to provide a scalable, relative benchmark assessing LLM skills like strategic planning, theory of mind, and deception, often missed by static benchmarks, through competitive gameplay. Methodologically, TextArena employs diverse text-based games (single/two/multi-player) within a Gym-compatible interface, evaluating models online (model-vs-model/human) and tracking performance using TrueSkill ratings across 10 specific soft skills. Primary results include relative model rankings and granular skill profiles; preliminary data shows frontier models achieving TrueSkill scores in the 30-38 range in certain games, demonstrating capabilities relative to a collective human baseline, though performance varies significantly across tasks (Figure 2). For AI practitioners, TextArena offers a platform to benchmark complex agentic behaviors without human preference bias, diagnose specific model skill gaps (e.g., Persuasion vs. Spatial Thinking), and potentially generate diverse interaction data for RL-based agent training. |
| The Scalability of Simplicity: Empirical Analysis of Vision-Language
  Learning with a Single Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.10462) or [HuggingFace](https://huggingface.co/papers/2504.10462))| Jun Hao Liew, Haochen Wang, Jiacong Wang, Weixian Lei, LXT | This paper introduces and empirically analyzes SAIL, a single-transformer architecture for joint vision-language processing, comparing its properties to modular designs. The research objective is to evaluate the scalability, cross-modal information flow patterns, and visual representation capabilities of this unified approach against modular Multimodal Large Language Models (MLLMs) that use separate vision encoders. SAIL employs a single transformer with mixed attention (bidirectional for image patches, causal for text) and multimodal rotary position embeddings (M-RoPE) to process raw pixels and text, evaluated via scaling experiments and performance on vision-language/vision benchmarks. Key results show SAIL exhibits superior data scalability compared to modular models (Fig 1A) and achieves strong vision task performance, including 84.95% Top-1 accuracy on ImageNet-1K classification, demonstrating effective visual feature learning without a pre-trained encoder. For AI practitioners, this indicates that unified single-transformer architectures are a viable, potentially more scalable alternative to complex modular designs, simplifying the model stack while achieving competitive performance, especially with large datasets. |
| Efficient Process Reward Model Training via Active Learning (Read more on [arXiv](https://arxiv.org/abs/2504.10559) or [HuggingFace](https://huggingface.co/papers/2504.10559))| Tianyu Pang, Xin Mao, Zichen Liu, Keyu Duan, dreamerdeo | This paper proposes ACTPRM, an active learning framework to efficiently train Process Reward Models (PRMs) for large language models. The primary objective is to reduce the prohibitive annotation costs required for obtaining step-level supervision needed to train PRMs. ACTPRM employs an ensemble PRM to estimate both aleatoric and epistemic uncertainty at each reasoning step, selectively forwarding only the most uncertain samples to a capable reasoning LLM for annotation, and then training the PRM exclusively on this subset. ACTPRM achieved state-of-the-art performance (75.0% average F1) on ProcessBench while requiring only 20% of the estimated annotation cost compared to the prior SOTA model, UniversalPRM. For AI practitioners, this methodology offers a significantly more cost-effective approach to training PRMs, enabling scalable development of LLMs with improved reasoning capabilities through fine-grained process supervision. |
| Efficient Generative Model Training via Embedded Representation Warmup (Read more on [arXiv](https://arxiv.org/abs/2504.10188) or [HuggingFace](https://huggingface.co/papers/2504.10188))| Tao Lin, Xufeng Li, Peng Sun, SempraETY | This paper introduces Embedded Representation Warmup (ERW) to accelerate diffusion model training by initializing early layers with pretrained representations. The primary objective is to improve training efficiency and representation quality by decoupling the representation learning phase from the generation phase in diffusion models. ERW employs a two-phase training strategy: first, a warmup phase aligns the initial layers (Latent-to-Representation circuit) with features from a pretrained model (e.g., Dinov2) using an alignment loss; second, standard diffusion training proceeds with a decaying alignment guidance term. Empirically, ERW demonstrates a 40x acceleration in training speed compared to the REPA baseline, achieving an FID of 6.0 on ImageNet-1k (SiT-XL/2, no CFG) within 100k iterations. For AI practitioners, ERW offers a plug-and-play method to significantly reduce computational costs and training time for large diffusion models by leveraging existing pretrained representation encoders, making state-of-the-art generative modeling more accessible. |
| NormalCrafter: Learning Temporally Consistent Normals from Video
  Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2504.11427) or [HuggingFace](https://huggingface.co/papers/2504.11427))| Bing Wang, Xinya Chen, Haoyuan Wang, Yanrui Bin, wbhu-tc | NormalCrafter introduces a novel method leveraging video diffusion priors to generate temporally consistent and detailed surface normals from open-world videos. The main objective is to address the challenge of maintaining both high spatial fidelity and temporal coherence in video-based normal estimation, which existing methods often fail to achieve simultaneously. Key methodology includes adapting a pre-trained video diffusion model (SVD), proposing Semantic Feature Regularization (SFR) to align internal features with semantic representations (from DINO), and utilizing a two-stage training protocol optimizing first in latent space for temporal context and then in pixel space for spatial accuracy. Primary results demonstrate superior performance on video benchmarks, achieving a 1.6° reduction in mean angular error on the Sintel dataset compared to the prior state-of-the-art, alongside improved temporal consistency. For AI practitioners, this research provides a framework for adapting large video generative models for downstream perception tasks, showcasing how diffusion priors combined with specific regularization and training strategies can yield high-fidelity, temporally stable outputs for video understanding applications like 3D reconstruction or editing. |
| A Minimalist Approach to LLM Reasoning: from Rejection Sampling to
  Reinforce (Read more on [arXiv](https://arxiv.org/abs/2504.11343) or [HuggingFace](https://huggingface.co/papers/2504.11343))| Lei Wang, Bo Pang, Yuhui Xu, Jiarui Yao, Wei Xiong | This paper analyzes simplified reinforcement learning algorithms for fine-tuning large language models (LLMs) on reasoning tasks, demonstrating the strong performance of rejection sampling. The primary objective is to understand the sources of effectiveness in complex RL algorithms like GRPO and identify minimal yet performant alternatives. Key methodologies include empirical comparisons of RAFT (rejection sampling), vanilla Reinforce, GRPO, and PPO on mathematical reasoning benchmarks, alongside ablation studies isolating components like reward normalization and sample filtering, leading to a proposed variant, Reinforce-Rej. The primary result shows that RAFT achieves competitive performance (e.g., 49.9% average accuracy on Qwen2.5-Math-7B-base) compared to GRPO (53.9%) and PPO (51.8%), with GRPO's advantage largely attributed to filtering prompts with only incorrect responses, not reward normalization. The principal implication for AI practitioners is that simpler, computationally lighter methods like RAFT and the proposed Reinforce-Rej can be highly effective alternatives to complex RL algorithms for reward-based LLM fine-tuning, highlighting the crucial role of selective sample filtering over intricate algorithmic designs. |
| DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and
  Verifiable Mathematical Dataset for Advancing Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.11456) or [HuggingFace](https://huggingface.co/papers/2504.11456))| Xingyu Chen, Qiuzhi Liu, Jiahao Xu, Tian Liang, Zhiwei He | This paper introduces DeepMath-103K, a large-scale, challenging, decontaminated, and verifiable mathematical dataset designed for advancing AI reasoning via reinforcement learning. The primary objective was to create a dataset overcoming limitations of existing resources, namely insufficient difficulty, lack of verifiable answers for RL, benchmark contamination, and inadequate scale for highly challenging problems. The methodology involved a rigorous curation pipeline including source analysis, semantic decontamination against multiple benchmarks using LLM-judges, difficulty filtering focusing on levels 5-9, and answer verification through consistency checks across three distinct R1-generated solutions for each of the 103K problems. Models trained using RL-Zero on DeepMath-103K demonstrated significant performance improvements, with DeepMath-Zero-7B achieving 85.5% pass@1 accuracy on MATH500, substantially outperforming baseline and models trained on other RL datasets. For AI practitioners, DeepMath-103K provides a crucial, publicly available resource enabling the development and evaluation of more powerful reasoning systems, particularly through rule-based RL paradigms demanding verifiable answers and high problem complexity. |
| Diffusion Distillation With Direct Preference Optimization For Efficient
  3D LiDAR Scene Completion (Read more on [arXiv](https://arxiv.org/abs/2504.11447) or [HuggingFace](https://huggingface.co/papers/2504.11447))| Jiale Wu, Zejian Li, Ling Yang, Shengyuan Zhang, An Zhaol | This paper proposes Distillation-DPO, a novel framework integrating diffusion distillation with direct preference optimization for efficient and high-quality 3D LiDAR scene completion. The primary objective is to accelerate the slow sampling speed of diffusion models for LiDAR completion while mitigating performance degradation typically associated with distillation. Distillation-DPO generates paired completion samples using a student model with varied initial noise, constructs win/lose pairs based on non-differentiable LiDAR metrics (used as preference), and optimizes the student by minimizing the difference in score functions between teacher and student models on these pairs, facilitated by two teaching assistant models. Experiments demonstrate that Distillation-DPO achieves superior completion quality (e.g., 0.354 refined CD compared to the SOTA LiDiff's 0.375) while accelerating inference speed by over 5-fold (3.38s vs 17.87s). For AI practitioners, this method offers a way to significantly enhance the efficiency of diffusion models for 3D scene completion tasks, making them more viable for real-world applications by effectively using preference data to guide distillation without requiring differentiable reward functions. |
| PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of
  Complex Videos in the Wild (Read more on [arXiv](https://arxiv.org/abs/2504.11326) or [HuggingFace](https://huggingface.co/papers/2504.11326))| Shuting He, Nikhila Ravi, Chang Liu, LXT, HenghuiDing | This report summarizes the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, focusing on methods and results for complex video segmentation tasks. The primary objective was to benchmark and advance algorithms for complex video object segmentation (MOSE track) and motion/language-guided video segmentation (MeViS track) using new, challenging real-world datasets. Key methodologies employed by top teams included fine-tuning large foundation models like SAM2, utilizing multi-model ensembles, adaptive pseudo-labeling (e.g., PGMR), and integrating Large Multimodal Models (LMMs) like Sa2VA, evaluated via J&F scores on confidential test sets. The top team on the MOSE track achieved a J&F score of 87.26%, while the MeViS track winner reached 61.98%, showcasing the effectiveness of these advanced techniques. For AI practitioners, the principal implication is the demonstrated benefit of adapting large pre-trained vision and multimodal models (SAM2, LMMs) and using ensemble strategies to improve robustness and accuracy in complex, dynamic video understanding tasks. |
| ReZero: Enhancing LLM search ability by trying one-more-time (Read more on [arXiv](https://arxiv.org/abs/2504.11001) or [HuggingFace](https://huggingface.co/papers/2504.11001))| Thinh Le, alandao | ReZero introduces a reinforcement learning framework to enhance LLM search persistence within Retrieval-Augmented Generation (RAG) by rewarding query retries. The main objective is to improve LLM robustness in information retrieval by explicitly incentivizing the model to attempt subsequent searches if the initial one fails. The key methodology utilizes Group Relative Policy Optimization (GRPO) to fine-tune an LLM, incorporating a specific `reward_retry` function that rewards additional search attempts conditional on generating a correct final answer. The primary result showed the ReZero model achieved 46.88% peak accuracy on the evaluation dataset, nearly doubling the 25.00% peak accuracy of a baseline model trained without the retry incentive. For AI practitioners, this implies that designing RL rewards to explicitly encourage persistence can significantly improve RAG system performance, especially for tasks where initial information retrieval attempts are likely insufficient. |
| AI-University: An LLM-based platform for instructional alignment to
  scientific classrooms (Read more on [arXiv](https://arxiv.org/abs/2504.08846) or [HuggingFace](https://huggingface.co/papers/2504.08846))| Rahul Gulati, Mostafa Faghih Shojaei, garikipati, Dinzhenzhenzhu, simocimolato | This paper introduces AI-University (AI-U), a framework using fine-tuned LLMs and Retrieval-Augmented Generation (RAG) to generate instructor-aligned responses for scientific courses. The objective was to develop and evaluate a platform that adapts an LLM (Llama-3.2-11B) to a specific graduate-level Finite Element Method (FEM) course's content and teaching style using lecture transcripts, notes, and textbooks. Key methodology involved systematic question-answer pair generation for LoRA-based fine-tuning (creating LLaMA-TOMMI-1.0), followed by RAG synthesis for contextualized, referenced answers, evaluated via cosine similarity and LLM-as-a-judge. The fine-tuned LLaMA-TOMMI-1.0 model achieved higher cosine similarity to ground-truth answers than the base model on 86% of test cases and was preferred approximately four times more often by an LLM judge. The principal implication for AI practitioners is that this combined approach of systematic data generation for fine-tuning and RAG offers a robust method for developing domain-specific LLMs that exhibit strong alignment with specialized technical content and style, providing traceable and accurate AI assistance. |
| Adaptive Computation Pruning for the Forgetting Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.06949) or [HuggingFace](https://huggingface.co/papers/2504.06949))| Aaron Courville, Johan Obando-Ceron, Zhixuan Lin, littleowen | This paper proposes Adaptive Computation Pruning (ACP) to accelerate the Forgetting Transformer (FoX) by dynamically skipping computations based on forget gate decay. The objective is to determine if dynamically pruning FoX attention computations based on decay strength can improve training throughput without performance loss. ACP employs a dynamic pruning threshold, calculated based on attention logit bounds and sequence length, to identify and skip negligible input-output dependency computations within a modified FlashAttention framework. Results demonstrate that ACP consistently reduces FLOPs in softmax attention by ~70% across different model sizes (125M-760M) and context lengths (4k-16k), resulting in 10%-35% faster training throughput without performance degradation on language modeling or downstream tasks. For AI practitioners, ACP provides a technique to significantly decrease computational costs and improve training efficiency for FoX models, particularly those with long contexts, while maintaining accuracy. |
| Multimodal Long Video Modeling Based on Temporal Dynamic Context (Read more on [arXiv](https://arxiv.org/abs/2504.10443) or [HuggingFace](https://huggingface.co/papers/2504.10443))| Xiangyu Yue, Yiyuan Zhang, Jiaming Han, Hoar012 | This paper introduces Temporal Dynamic Context (TDC), a method for multimodal long video understanding integrating static features and dynamic context compression. The research aims to address MLLM context length limitations and suboptimal multimodal integration (vision, audio) in long video processing. TDC segments videos by inter-frame similarity, encodes static keyframes fully, and uses a Q-Former to compress subsequent visual/audio tokens based on temporal differences relative to the static frame; a Long Video Chain-of-Thought (LVCoT) strategy handles extremely long videos without training. TDC demonstrates strong performance, outperforming the audio-visual VideoLLaMA2 model by 15.6% on the long-video MLVU benchmark. For AI practitioners, TDC provides an effective technique for encoding dense multimodal video data more efficiently, enabling MLLMs to process longer videos by compressing dynamic context while preserving key static details, reducing information loss compared to sparse sampling or purely visual compression methods. |
| Summarization of Multimodal Presentations with Vision-Language Models:
  Study of the Effect of Modalities and Structure (Read more on [arXiv](https://arxiv.org/abs/2504.10049) or [HuggingFace](https://huggingface.co/papers/2504.10049))| Frédéric Dufaux, Camille Guinaudeau, gigant | This paper analyzes how input modality and structure affect Vision-Language Model (VLM) performance for summarizing multimodal presentations. The primary objective is to evaluate the cost-performance tradeoffs of various input representations (raw video, extracted slides, transcript, structured/unstructured combinations) and suggest effective strategies. Using Qwen2-VL and other VLMs on a benchmark derived from the TIB dataset, the study measured performance with metrics like ROUGE and Importance-based Relevance (IbR). Results demonstrate that a structured representation using interleaved slides and transcript yields the best performance (e.g., Qwen2-VL 2B achieved ROUGE-1 of 27.1 and overall IbR of 33.4), significantly outperforming raw video or unstructured inputs. For AI practitioners, the key implication is that preprocessing presentations into structured, interleaved slide-transcript sequences offers the most effective input for VLM summarization, balancing computational cost and summary quality, especially for inputs exceeding approximately 6k tokens. |
| D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.09454) or [HuggingFace](https://huggingface.co/papers/2504.09454))| Zhendong Mao, Lei Zhang, Nan Chen, Mengqi Huang, Weinan Jia | This paper introduces D²iT, a Diffusion Transformer using dynamic compression based on regional information density to improve image generation accuracy. The main objective is to overcome the limitations of fixed spatial compression in standard Diffusion Transformers (DiTs) which disregard varying information densities across image regions. The methodology employs a two-stage framework: first, a Dynamic VAE (DVAE) uses a hierarchical encoder and information density estimation (Shannon entropy) to create multi-grained latent codes; second, the Dynamic Diffusion Transformer (D²iT) predicts corresponding multi-grained noise using novel Dynamic Grain and Content Transformers. Primary results demonstrate a significant quality improvement, achieving a 1.73 FID score on class-conditional ImageNet 256x256 generation, a 23.8% improvement over the baseline DiT's 2.27 FID, using only 57.1% of the training resources. For AI practitioners, this research implies that dynamically adapting compression and computational effort based on input complexity, rather than using fixed approaches, can yield substantial gains in both the performance and efficiency of generative models like DiTs. |
| Change State Space Models for Remote Sensing Change Detection (Read more on [arXiv](https://arxiv.org/abs/2504.11080) or [HuggingFace](https://huggingface.co/papers/2504.11080))| Erchan Aptoula, ElmanGhazaei | This paper introduces the Change State Space Model (CSSM), a computationally efficient Mamba-based architecture tailored for remote sensing change detection. The research objective is to develop a specialized state-space model that focuses exclusively on relevant bi-temporal changes, improving efficiency and accuracy over existing ConvNet, ViT, and general Mamba approaches for change detection. CSSM utilizes a lightweight CNN encoder-decoder framework incorporating a modified state space model block that employs an L1 distance mechanism on projected inputs to isolate and process only changed features between pre- and post-event images. Evaluated on benchmark datasets like LEVIR-CD+, CSSM achieved state-of-the-art performance, attaining an F1-score of 92.39 while requiring only 4.34M parameters and 5.10 GFLOPs, significantly less than comparable models. For AI practitioners, CSSM presents a highly resource-efficient architecture delivering state-of-the-art accuracy in change detection, making it suitable for large-scale analysis or deployment in computationally constrained environments. |
| LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews (Read more on [arXiv](https://arxiv.org/abs/2504.11042) or [HuggingFace](https://huggingface.co/papers/2504.11042))| Iryna Gurevych, Lizhen Qu, Anne Lauscher, Zhuang Li, sukannya | This paper introduces LAZYREVIEW, a dataset annotated with fine-grained categories to detect 'lazy thinking' heuristics in NLP peer reviews. The primary objective was to create this resource and evaluate the ability of Large Language Models (LLMs) to automatically identify such instances. The methodology involved iteratively developing annotation guidelines over three rounds using ARR-22 reviews, annotating 500 expert and 1276 silver review segments, and evaluating LLMs using zero-shot, few-shot in-context learning, and instruction fine-tuning. Key results show that while LLMs struggle in zero-shot detection, instruction fine-tuning on LAZYREVIEW significantly boosts performance by 10-20 accuracy points (e.g., instruction-tuned Qwen achieved 59.4% string-matching accuracy for fine-grained classification). For AI practitioners, this provides a validated dataset and methodology for building automated tools to flag superficial review arguments, potentially improving review quality assessment systems and reviewer training. |
