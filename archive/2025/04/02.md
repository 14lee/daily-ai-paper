

## Papers for 2025-04-02

| Title | Authors | Summary |
|-------|---------|---------|
| Any2Caption:Interpreting Any Condition to Caption for Controllable Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.24379) or [HuggingFace](https://huggingface.co/papers/2503.24379))| shuicheng, dizhang, Xintao, WeicaiYe, ChocoWu | Any2Caption introduces an MLLM-based framework to interpret diverse multimodal conditions into structured captions for controllable video generation. The main objective is to accurately interpret complex user intent from various inputs (text, images, specialized cues like pose/camera) to improve video synthesis control and quality. The methodology involves decoupling interpretation from generation, using a Qwen2-LLM with dedicated encoders to generate detailed, structured captions, trained on the new Any2CapIns dataset (337K instances). Results show high caption fidelity (e.g., 91.95 BERTSCORE) and improved video quality and controllability when integrated with existing video generators across various conditions. For AI practitioners, the key implication is the ability to enhance control over existing video generation models using complex multimodal inputs by integrating this interpretation module, which outputs structured text captions, without needing to retrain the core video generator. |
| Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1 (Read more on [arXiv](https://arxiv.org/abs/2503.24376) or [HuggingFace](https://huggingface.co/papers/2503.24376))| yshan2u, yxgeee, ruiwang, tttoaster, ChenYi99 | SEED-Bench-R1 is introduced to systematically evaluate reinforcement learning (RL) post-training for multimodal large language model (MLLM) video understanding. The primary objective is to compare the effectiveness and generalization of RL (specifically GRPO) against supervised fine-tuning (SFT) for video tasks requiring both perception and logical reasoning. Using Qwen2-VL-Instruct-7B, the study compared GRPO trained with outcome-based rewards against SFT on the hierarchical SEED-Bench-R1 benchmark (L1: In-distribution, L2/L3: OOD). Results show GRPO significantly outperforms SFT in data efficiency and generalization, particularly in OOD scenarios (e.g., 44.89% vs 38.15% accuracy on Level-3), and extends generalization benefits to benchmarks like LongVideoBench (43.40% vs 40.00%). For AI practitioners, this implies RL, even with simple outcome rewards, is highly effective at enhancing MLLM visual perception and OOD generalization for video tasks compared to SFT, though analysis notes RL may compromise logical coherence in the reasoning chain. |
| CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive
  Program Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.23145) or [HuggingFace](https://huggingface.co/papers/2503.23145))| Naveen Kannan, Jiannan Cao, kaiyan289, tarsur909, anjiangwei | This paper introduces CodeARC, an interactive benchmark evaluating LLM agents on inductive program synthesis. The main objective is to assess LLMs' ability to infer hidden functions solely from input-output examples through interaction, departing from static evaluation protocols. Key methodology involves agents querying a hidden target function, synthesizing candidates, and using a differential testing oracle for feedback and iterative refinement under budget constraints on 1114 Python functions. Primary results indicate the task is challenging: the best-performing model, o3-mini, achieved a 52.7% success rate on the anonymized dataset, and fine-tuning LLaMA-3.1-8B-Instruct improved performance by up to 31% relatively. For AI practitioners, this work provides a more realistic benchmark revealing significant limitations in current LLMs' inductive reasoning for code synthesis and suggests interactive refinement and targeted fine-tuning as avenues for improvement. |
| JudgeLRM: Large Reasoning Models as a Judge (Read more on [arXiv](https://arxiv.org/abs/2504.00050) or [HuggingFace](https://huggingface.co/papers/2504.00050))| Jiaying Wu, Nuo Chen, bhooi, qingyunzou, zhiyuanhucs | This paper introduces JudgeLRM, a family of LLMs trained via reinforcement learning (RL) to serve as evaluators, specifically targeting complex reasoning tasks where SFT judges falter. The research investigates whether enhancing reasoning capabilities improves LLM judge performance and proposes an RL-based training approach using judge-wise, outcome-driven rewards. Key methodology involves training base LLMs (Qwen2.5) using Group Relative Policy Optimization (GRPO) with a custom reward function combining structural correctness and content alignment (relation, absolute, confidence metrics) against ground-truth judgments. Primary results show JudgeLRM models outperform SFT-tuned and state-of-the-art reasoning models; notably, JudgeLRM-7B surpasses DeepSeek-R1 by 2.79% in F1 score on the JudgeLM benchmark, excelling particularly on tasks requiring deep reasoning. For AI practitioners, this implies that RL with carefully designed, reasoning-focused rewards is a more effective method than SFT for developing robust LLM evaluators capable of handling nuanced, complex judgment tasks, suggesting RL should be considered for building reliable automated evaluation systems. |
| GeometryCrafter: Consistent Geometry Estimation for Open-world Videos
  with Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2504.01016) or [HuggingFace](https://huggingface.co/papers/2504.01016))| Xiaoyu Li, yshan2u, wbhu-tc, xiangjun0211, slothfulxtx | GeometryCrafter generates temporally consistent, metrically accurate point map sequences from open-world videos using diffusion priors. The main objective is to estimate high-fidelity, temporally coherent point maps with correct metric scale from videos, overcoming the affine ambiguity and temporal inconsistency limitations of prior diffusion-based depth and geometry estimation methods. The key methodology employs a novel point map Variational Autoencoder (VAE) with a dual-encoder design (using an inherited SVD encoder and a residual encoder) to encode unbounded point maps while maintaining latent compatibility, integrated with a video diffusion model finetuned using these latents and per-frame geometry priors. Primary results demonstrate state-of-the-art performance, achieving an average rank of 1.9 on point map estimation across seven diverse benchmark datasets, indicating superior 3D accuracy and temporal consistency compared to previous methods. For AI practitioners, this provides a framework to extract metrically accurate, temporally consistent geometry from videos, directly usable for applications like 3D/4D reconstruction or depth-conditioned video editing/generation without post-hoc scale recovery. |
| Agent S2: A Compositional Generalist-Specialist Framework for Computer
  Use Agents (Read more on [arXiv](https://arxiv.org/abs/2504.00906) or [HuggingFace](https://huggingface.co/papers/2504.00906))| Vincent Tu, Kyle Wong, xw-eric, jc-y42, saa1605 | Agent S2 introduces a compositional generalist-specialist framework enhancing computer use agent capabilities via specialized modules. The primary objective is to address limitations in GUI grounding precision, long-horizon task planning, and reliance on single generalist models for diverse cognitive tasks. Methodologically, Agent S2 employs a Mixture-of-Grounding technique routing actions to specialized grounding experts and Proactive Hierarchical Planning for dynamic plan refinement based on evolving observations. Agent S2 achieved new state-of-the-art results, notably a 34.5% success rate on the OSWorld 50-step evaluation, a 32.7% relative improvement over the leading Claude Computer Use baseline. For AI practitioners, this demonstrates the effectiveness of composing generalist planning with specialized grounding modules to overcome bottlenecks in monolithic models for complex GUI automation tasks. |
| Z1: Efficient Test-time Scaling with Code (Read more on [arXiv](https://arxiv.org/abs/2504.00810) or [HuggingFace](https://huggingface.co/papers/2504.00810))| Xiao-Ping Zhang, armanc, yilunzhao, yh1567, zjy2001 | Z1 proposes an efficient test-time compute scaling method for LLMs using code-related reasoning trajectories and a novel shifted thinking window. The research aims to reduce the excessive thinking token cost associated with test-time scaling in Large Reasoning Models (LRMs) while preserving performance. Key methodology involves training an LLM (Qwen2.5-Coder-7B-Instruct) on a curated dataset (Z1-Code-Reasoning-107K) containing both short and long code solution trajectories and employing a "Shifted Thinking Window" during inference that avoids fixed delimiters and caps reasoning tokens. The resulting model, Z1-7B, matches the performance of R1-Distill-Qwen-7B on three reasoning benchmarks while using only about 30% of its average thinking tokens, and notably generalizes to non-code tasks like GPQA Diamond (47.5%). For AI practitioners, this demonstrates a method to significantly improve the computational efficiency and reduce inference costs of LRMs for complex reasoning tasks by fine-tuning with varied-length code trajectories and adopting a flexible, adaptive thinking process during inference. |
| MixerMDM: Learnable Composition of Human Motion Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.01019) or [HuggingFace](https://huggingface.co/papers/2504.01019))| José García-Rodríguez, Sergio Escalera, Cristina Palmero, Germs96, pabloruizponce | MixerMDM introduces a learnable technique for composing pre-trained text-conditioned human motion diffusion models. The main research objective is to dynamically combine motions from specialized single-person and interaction models to achieve fine-grained control over individual movements within complex interactions. The key methodology involves a lightweight `Mixer` module, trained adversarially against multiple discriminators (one per pre-trained model), to predict dynamic, context-dependent mixing weights at each denoising step, using the pre-trained models' outputs as pseudo-ground truth. Primary results demonstrate superior performance over fixed-weight or scheduled methods, with MixerMDM achieving significantly better alignment and consistency, ranking first in 85.14% of user study comparisons based on motion alignment to textual descriptions. For AI practitioners, MixerMDM provides a modular framework to combine specialized, pre-trained diffusion models for generating nuanced, controllable human motion sequences without requiring retraining of the base models or explicit ground truth for the combined outputs. |
| Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal
  LLMs on Academic Resources (Read more on [arXiv](https://arxiv.org/abs/2504.00595) or [HuggingFace](https://huggingface.co/papers/2504.00595))| Heng Wang, Yu Tian, windwest, yanglj55, weizhiwang | Open-Qwen2VL details compute-efficient pre-training of a fully open-source 2B parameter Multimodal Large Language Model (MLLM) on academic-scale resources. The objective is to develop and openly release an efficient MLLM pre-training pipeline reproducible with limited compute, specifically using 8xA100-40G GPUs. Key methodologies include low-to-high dynamic image resolution (144 visual tokens in pre-training, 729 in SFT), multimodal sequence packing, and data filtering using both CLIP-based methods and MLLM-based techniques (MLM-Filter) on a 29M image-text pair dataset. The resulting instruction-tuned Open-Qwen2VL, pre-trained on 5B packed multimodal tokens (using 442 A100-40G GPU hours), outperforms the partially-open Qwen2-VL-2B on benchmarks such as MMBench (achieving 80.9), SEEDBench, MMStar, and MathVista, despite using only 0.36% of Qwen2-VL's reported pre-training tokens. For AI practitioners, this work provides a fully open-sourced blueprint—including codebase, data filtering/packing scripts, curated pre-training data, and model checkpoints—demonstrating that efficient, high-performance MLLM pre-training is attainable without extensive industrial-scale resources, enabled by optimized data curation and training techniques. |
| Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.24377) or [HuggingFace](https://huggingface.co/papers/2503.24377))| Sudanl, pangjh3, BeyondHsueh, Merlin-Hongru, Ray121381 | This survey systematically reviews strategies for achieving "Reasoning Economy" in Large Language Models (LLMs), balancing performance benefits against computational budgets. The primary objective is to analyze the causes of reasoning inefficiency (e.g., length bias, deceptive behaviors), understand different reasoning patterns, and survey potential solutions across post-training and test-time inference stages. It employs a comprehensive literature review, categorizing challenges stemming from post-training methods (like Superficial Alignment leading to length bias) and test-time usage (like unreasonable computation allocation) and corresponding optimization solutions (e.g., behavior regulation, usage improvement). Key findings identify specific inefficiencies like length bias (where RMs may prefer longer responses, e.g., 63.1% in RLCD) and overly cautious reasoning, while highlighting solutions such as long2short RL methods (e.g., SimPO reducing lengths by 30-40%) and adaptive computation allocation based on task complexity. For AI practitioners, the principal implication is the need to shift from static, one-size-fits-all inference approaches towards dynamic, adaptive strategies (e.g., adaptive budget allocation, algorithm selection) to optimize resource utilization and unlock LLMs' full potential efficiently. |
| OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming
  Video Contexts (Read more on [arXiv](https://arxiv.org/abs/2503.22952) or [HuggingFace](https://huggingface.co/papers/2503.22952))| Tong Wu, Bo Chen, Yueqian Wang, zlzheng, ColorfulAI | This paper introduces OmniMMI, a benchmark for evaluating MLLMs in streaming video interaction, and M4, a framework enhancing these capabilities. The primary objective is to evaluate and improve the real-world interactive performance of OmniLLMs in streaming video contexts, focusing on streaming understanding and proactive reasoning challenges underexplored by existing benchmarks. Methodology involved curating the OmniMMI dataset (1,121 videos, 2,290 questions across six subtasks including dynamic state grounding and proactive alerting) and developing the Multi-modal Multiplexing Modeling (M4) framework using multiplexing techniques and an attention-based inference method for efficient, proactive processing. Experimental results show existing MLLMs perform poorly on OmniMMI, particularly struggling with proactive tasks and multi-turn dependencies, while the proposed lightweight M4 model demonstrates significant improvement, achieving 68.5% accuracy on the Proactive Turn-taking task after audio adaptation (M4-a). For AI practitioners, this research underscores the inadequacy of current models for real-time interaction, provides OmniMMI as a necessary tool for evaluating streaming/proactive capabilities, and suggests the M4 architecture as a resource-efficient approach to develop models that can simultaneously perceive and generate responses in dynamic environments. |
| Command A: An Enterprise-Ready Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.00698) or [HuggingFace](https://huggingface.co/papers/2504.00698))| salthammer, yazeed7, jayalammar, ArashAhmadian, aakanksha | This report details Command A, a 111B parameter multilingual large language model optimized for enterprise RAG and agentic tasks, alongside the smaller Command R7B. The primary objective was to develop and evaluate Command A and R7B as efficient, high-performing LLMs tailored for real-world enterprise settings, focusing on multilingualism, Retrieval Augmented Generation (RAG), and tool use. Key methodologies include a decentralised post-training strategy combining supervised fine-tuning (SFT) and reinforcement learning (RL) across specialized expert models, followed by parameter merging (linear soup), and a polishing phase using algorithms like Self-improving Robust Preference Optimisation (SRPO). Command A achieves competitive results, scoring 80.0 on the MATH benchmark and 51.7 on Taubench, while maintaining efficiency by requiring only two A100/H100 GPUs for inference and delivering up to 156 tokens/sec. For AI practitioners, Command A offers an efficient foundation for enterprise applications needing strong RAG and agentic capabilities, while the reported decentralised training and merging approach presents a method for integrating diverse expert functionalities into a single model. |
| Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on
  Elementary School-Level Reasoning Problems? (Read more on [arXiv](https://arxiv.org/abs/2504.00509) or [HuggingFace](https://huggingface.co/papers/2504.00509))| Xuesong Yao, xmerge123, ALEXoDu, yfxu, kaiyan289 | This paper demonstrates that cutting-edge LLMs often recite solutions rather than genuinely reason, even on elementary problems. The research objective was to determine if LLMs possess true reasoning ability or merely replicate patterns seen during training, particularly when faced with subtly altered conditions. A novel multi-modal benchmark, RoR-Bench, was created featuring pairs of original problems and variants with minor but crucial condition shifts. Empirical analysis revealed severe recitation behavior, with top models like OpenAI-o1 and DeepSeek-R1 experiencing performance drops exceeding 60% on modified elementary arithmetic and reasoning problems compared to their original counterparts. For AI practitioners, this highlights a critical need to re-evaluate LLM intelligence claims and emphasizes that current models may lack robustness, potentially failing unexpectedly when encountering slight deviations from learned patterns. |
| AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models
  with Unsupervised Coefficient Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.23733) or [HuggingFace](https://huggingface.co/papers/2503.23733))| Yiru Wang, Jiabo Ye, Xiaochen Wang, Yiyang Du, carboncoo | AdaMMS introduces an unsupervised method for merging heterogeneous Multimodal Large Language Models (MLLMs) with differing architectures. The primary objective is to effectively combine capabilities from distinct MLLMs without requiring labeled data for optimizing the merging hyperparameters. The methodology involves parameter mapping to align weights, linear interpolation for merging, and an unsupervised search step that selects the optimal interpolation coefficient based on minimizing generation consistency differences across candidate merged models using a small unlabeled dataset. Experiments show AdaMMS outperforms supervised baselines; for example, merging LLaVA-OneVision-7B into Qwen2-VL-7B yielded a SUM score of 563.56, a +26.84 gain over the original models' average. AI practitioners can leverage AdaMMS to fuse heterogeneous MLLMs efficiently, creating enhanced models without supervised data by using generation consistency as a proxy for task performance during optimization. |
| When To Solve, When To Verify: Compute-Optimal Problem Solving and
  Generative Verification for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.01005) or [HuggingFace](https://huggingface.co/papers/2504.01005))| anna-rohrbach, kaiweichang, adityagrover, arianhosseini, hbXNov | This research compares the compute-efficiency of Self-Consistency (SC) and Generative Reward Models (GenRM) for LLM reasoning, revealing SC's superiority at lower budgets. The study investigates whether allocating a fixed inference budget towards generating more solutions (SC) or generating fewer solutions with multiple verifications (GenRM) yields better LLM reasoning performance, and how to optimally balance solutions and verifications for GenRM. A compute-matched analysis compared SC and GenRM across various models, tasks, and budgets, calculating FLOPs based on solution (S) and verification (V) generation; inference scaling laws were derived by fitting optimal solution (S_opt) and verification (V_opt) counts to compute budget C. Primary results show SC outperforms GenRM until high compute budgets are reached; for Llama-3.1-8B on MATH, GenRM required 8x the compute of SC to match its performance and 128x to achieve a 3.8% gain, while compute-optimal GenRM requires scaling solutions faster (S_opt ∝ C^0.57) than verifications (V_opt ∝ C^0.39). AI practitioners should prioritize SC for LLM reasoning under typical compute constraints; if using GenRM at high budgets, allocate compute preferentially towards increasing solution count over verification count per solution for optimal efficiency. |
| Scaling Language-Free Visual Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2504.01017) or [HuggingFace](https://huggingface.co/papers/2504.01017))| liuzhuang13, koustuvs, JiachenZhu, tsbpp, davidfan97 | This paper investigates scaling language-free visual self-supervised learning (SSL) on web-scale data, comparing its performance against Contrastive Language-Image Pretraining (CLIP) primarily on Visual Question Answering (VQA). The research aims to determine if visual SSL lags behind CLIP due to the absence of language supervision or disparities in training data. Key methodology involves training DINOv2 (SSL) and CLIP models (1B to 7B parameters) on the identical 2 billion sample MetaCLIP dataset and evaluating using the Cambrian-1 VQA suite and traditional vision benchmarks. Primary results indicate visual SSL scales better with model and data size than CLIP on VQA; specifically, a 7B parameter Web-DINO model trained on 8 billion examples outperforms a comparable CLIP model on average VQA performance across 16 tasks. The principal implication for AI practitioners is that appropriately scaled visual SSL can yield vision encoders competitive with language-supervised models for multimodal tasks like VQA, providing a strong vision-centric alternative without needing paired text data during pretraining. |
| Multi-Token Attention (Read more on [arXiv](https://arxiv.org/abs/2504.00927) or [HuggingFace](https://huggingface.co/papers/2504.00927))| sainbar, spermwhale, Tianlu, Golovneva | This paper introduces Multi-Token Attention (MTA), enhancing LLM attention by conditioning weights on multiple query and key vectors simultaneously via convolution operations. The primary objective is to overcome the "single token attention" bottleneck, allowing models to locate relevant context using richer, multi-token criteria rather than single vector similarity. MTA modifies standard attention by applying convolutions across query, key, and head dimensions (termed key-query convolution and head mixing convolution), often coupled with group normalization. Experiments demonstrate MTA achieves lower perplexity on language modeling (11.09 avg PPL vs 11.25 for an 880M Transformer baseline) and notably improves performance on long-context tasks like Needle-in-a-Haystack and BabiLong compared to baselines. For AI practitioners, MTA offers a method to improve model performance in scenarios requiring identification of context based on multiple simultaneous conditions, particularly beneficial for long-context reasoning, by incorporating these convolutional modifications into the attention mechanism. |
| Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features (Read more on [arXiv](https://arxiv.org/abs/2504.00557) or [HuggingFace](https://huggingface.co/papers/2504.00557))| Jaeyeon Kim, Donguk Lim, Seungmin Yang, Ki-Ung Song, Jewon Lee | This paper presents Trimmed Llama, a method for improving inference efficiency in cross-attention-based Large Vision-Language Models (LVLMs) by pruning visual features. The main objective is to mitigate the computational bottleneck caused by the large Key-Value (KV) cache size associated with image tokens in cross-attention layers. The key methodology involves exploiting the sparsity and inter-layer resemblance of cross-attention patterns, using head-wise attention scores from the first cross-attention layer to selectively prune redundant visual features for subsequent layers. Primary results show that Trimmed Llama can reduce visual feature usage by up to 50% (e.g., Kratio=0.15 retaining ~41.6% features for the 11B model) while maintaining performance parity with baseline Llama-3.2-Vision models on benchmarks like MME and LLaVA-Bench, alongside reduced inference latency (e.g., 14.2% reduction for batch size 16). For AI practitioners, this provides a training-free technique to decrease inference latency and memory consumption for cross-attention LVLMs with minimal impact on task performance. |
| Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies
  Ahead (Read more on [arXiv](https://arxiv.org/abs/2504.00294) or [HuggingFace](https://huggingface.co/papers/2504.00294))| Neel Joshi, Shivam Garg, Lingjiao Chen, Jingya Chen, Vidhisha Balachandran | This extensive empirical study evaluates the benefits and limitations of inference-time scaling methods across diverse complex tasks for large language models (LLMs). The main objective was to investigate how scaling performance, including accuracy and token usage tradeoffs, varies across nine state-of-the-art conventional and reasoning-tuned models on eight challenging benchmarks (e.g., math, NP-hard problems, planning, spatial reasoning). Key methodologies included evaluating models using standard Chain-of-Thought (CoT), parallel scaling (sampling N generations with aggregators like best-of-N), and sequential scaling (iterative refinement with self-critique), approximating performance bounds. Primary results show inference-time scaling benefits vary significantly by task and diminish with complexity; notably, increased token consumption does not reliably yield higher accuracy across models (e.g., on AIME 25, DeepSeek R1 used >5x more tokens than Claude 3.7 Sonnet for <3% accuracy difference). The principal implication for AI practitioners is that leveraging inference-time compute requires careful task-specific consideration and highlights the critical need for developing robust, efficient verifiers and adaptive scaling strategies, as current approaches show inconsistent gains and cost nondeterminism. |
| Discovering Knowledge Deficiencies of Language Models on Massive
  Knowledge Base (Read more on [arXiv](https://arxiv.org/abs/2503.23361) or [HuggingFace](https://huggingface.co/papers/2503.23361))| Ryotaro Shimizu, Jieyu Zhang, Xuwei Ding, MaksimSTW, linxinso | This paper introduces Stochastic Error Ascent (SEA), a scalable framework for efficiently discovering factual knowledge deficiencies in closed-weight LLMs against massive knowledge bases under budget constraints. The primary objective is to develop a scalable and budget-constrained method for automatically uncovering knowledge deficiencies (errors) in closed-weight LLMs by evaluating them against large knowledge bases. The core methodology, SEA, uses stochastic optimization to iteratively retrieve knowledge base paragraphs semantically similar to prior LLM failures, employing hierarchical retrieval and a relation DAG to guide the search efficiently. Empirically, SEA uncovered 40.7× more knowledge errors than the Automated Capability Discovery baseline and 26.7% more than AutoBencher, while significantly reducing the cost-per-error. For AI practitioners, SEA provides a cost-effective method to pinpoint specific factual weaknesses in LLMs, enabling targeted improvements through data curation or fine-tuning to enhance model reliability. |
| m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning
  with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.00869) or [HuggingFace](https://huggingface.co/papers/2504.00869))| Yuyin Zhou, Xianfeng Tang, Hui Liu, Juncheng Wu, Xiaoke Huang | This paper introduces m1, a method applying test-time scaling to enhance the medical reasoning capabilities of Large Language Models (LLMs). The primary objective was to investigate the effectiveness of test-time scaling for medical QA, contrasting it with mathematical reasoning tasks. The methodology involved curating medical QA datasets (m1K, m23K), fine-tuning Qwen2.5 models (7B, 32B) on these datasets using Supervised Fine-Tuning (SFT), and controlling the "thinking" token budget during inference. Results show that increasing the thinking budget improves accuracy (e.g., m1-7B-23K achieved 60.32% average accuracy), but plateaus around 4K tokens; budget forcing offered limited benefits, and performance gains were ultimately constrained by the model's inherent medical knowledge. For AI practitioners, this implies that while test-time scaling enhances medical reasoning, it is insufficient alone; complementing it with improved knowledge grounding via high-quality data curation and larger model capacity is essential for further performance gains, especially on complex medical tasks. |
| Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.00072) or [HuggingFace](https://huggingface.co/papers/2504.00072))| Gül Varol, Cordelia Schmid, Antoine Yang, Lucas Ventura | Chapter-Llama introduces an efficient LLM-based framework for automatic video chaptering in hour-long videos. The primary objective is to partition long videos into semantic chapters and generate corresponding titles automatically. The methodology involves finetuning a large language model (Llama-3.1-8B) using text inputs derived from ASR transcripts and descriptive captions of sparsely sampled keyframes, selected via a novel speech-guided strategy. Results show substantial improvement over the state-of-the-art on VidChapters-7M, achieving a 45.3 F1 score compared to the previous best of 26.7. For AI practitioners, this work presents a scalable, text-only approach leveraging LLMs and efficient frame sampling for indexing and structuring long-form video content without direct video feature processing. |
| Towards Trustworthy GUI Agents: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.23434) or [HuggingFace](https://huggingface.co/papers/2503.23434))| Ninghao Liu, Wenhu Chen, Wenlin Yao, Wenhao Yu, Yucheng Shi | This survey reviews the critical dimensions of trustworthiness for GUI agents interacting with digital interfaces via foundation models. The paper's objective is to systematically examine security vulnerabilities, reliability, explainability, ethical considerations, and evaluation methodologies pertinent to GUI agent trustworthiness. It employs a literature survey methodology, categorizing research into five key trustworthiness areas and summarizing existing attacks (e.g., WebPI, AEIA-MN), defenses (e.g., GuardAgent, CLEAR), and evaluation frameworks (e.g., ST-WebAgentBench, Agent-SafetyBench). Key findings identify significant security vulnerabilities, such as environmental injection attacks achieving up to 93% success rates (AEIA-MN), alongside challenges in reliability (hallucination) and privacy, while noting that current research often overlooks these aspects for functional performance. For AI practitioners, this necessitates a shift from solely optimizing task completion towards implementing holistic, multi-layered defenses, robust evaluation benchmarks incorporating safety metrics, and user-centric transparency mechanisms to ensure secure and responsible GUI agent deployment. |
| DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2503.24210) or [HuggingFace](https://huggingface.co/papers/2503.24210))| Gim Hee Lee, onandon | DiET-GS introduces a novel framework for motion deblurring in 3D Gaussian Splatting using event streams and diffusion priors. The research addresses the problem of reconstructing sharp 3D representations from blurry multi-view images. It leverages an event double integral prior and a pretrained diffusion model within a two-stage training strategy. DiET-GS outperforms existing methods, achieving a MUSIQ score of 51.71 on real-world datasets, but the DiET-GS++ has a longer training time compared to E2NeRF and Ev-DeblurNeRF. This provides AI practitioners with an approach for improving novel view synthesis from motion-blurred images.  |
| ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via
  Residual Learning (Read more on [arXiv](https://arxiv.org/abs/2503.21860) or [HuggingFace](https://huggingface.co/papers/2503.21860))| Siyuan Huang, Yuyang Li, Tengyu Liu, Puhao Li, Kailin Li | i) The paper introduces MANIPTRANS, a two-stage method for efficient transfer of human bimanual skills to dexterous robotic hands in simulation. ii) The primary research objective is to facilitate the transfer of human hand manipulation skills, especially bimanual actions, to dexterous robotic hands in simulation enabling accurate tracking of reference motions. iii) The method uses a pre-trained generalist trajectory imitator for hand motion mimicking followed by a fine-tuned residual module under interaction constraints. iv) MANIPTRANS achieves superior success rates (58.1/39.5 for single/bimanual respectively) compared to SOTA methods and constructs DEXMANIPNET, a dataset of 3.3K episodes of robotic manipulation, improving motion fidelity. v) The development of MANIPTRANS offers AI practitioners an efficient and generalizable framework for creating large-scale, high-quality datasets of dexterous manipulation enabling more effective training of robot control policies.  |
| MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote
  Sensing (Read more on [arXiv](https://arxiv.org/abs/2503.24219) or [HuggingFace](https://huggingface.co/papers/2503.24219))| Mustapha lebbah, Hanane Azzag, rdkarim | MB-ORES introduces a unified framework for object detection (OD) and visual grounding (VG) in remote sensing (RS) imagery. The paper aims to improve visual grounding in RS images by fine-tuning an open-set object detector with referring expression data and then processing outputs via a graph-based representation and a multi-branch, task-aware architecture. The methodology incorporates a multi-branch network for feature integration, an object reasoning network for proposal ranking, and a soft selection mechanism for object localization. Experiments on DIOR-RSVG show MB-ORES outperforms existing methods, increasing performance by +3.38% to +14.89% across threshold levels, while on the OPT-RSVG dataset, meanIoU increased by +6.98%. This implies a unified OD/VG approach, applicable by AI practitioners in the remote sensing domain, can achieve state-of-the-art performance while retaining OD capabilities, offering a more versatile tool.  |
