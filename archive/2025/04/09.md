

## Papers for 2025-04-09

| Title | Authors | Summary |
|-------|---------|---------|
| OmniSVG: A Unified Scalable Vector Graphics Generation Model (Read more on [arXiv](https://arxiv.org/abs/2504.06263) or [HuggingFace](https://huggingface.co/papers/2504.06263))| Jiaxu Zhang, Xianfang Zeng, Yiying Yang, CH3COOK, wchengad | OmniSVG is a unified framework leveraging pre-trained Vision-Language Models (VLMs) for end-to-end multimodal Scalable Vector Graphics (SVG) generation. The main objective is to produce high-quality, complex, and editable SVGs across diverse modalities (Text-to-SVG, Image-to-SVG, Character-Reference SVG), addressing the limitations of existing methods in handling complexity and structure. The key methodology involves parameterizing SVG commands and coordinates into discrete tokens using a dedicated SVG tokenizer and training a VLM (Qwen2.5-VL) on a large-scale dataset (MMSVG-2M) with a next-token prediction objective. Primary results demonstrate superior performance over existing methods; for instance, on the MMSVG-Illustration text-to-SVG task, OmniSVG(7B) achieved a FID score of 66.91, outperforming SVGDreamer (75.31 on MMSVG-Icon) and other baselines, while handling complex SVGs with token lengths up to 30k. For AI practitioners, OmniSVG offers a versatile, end-to-end solution for generating complex and editable vector graphics from multimodal inputs, potentially integrating into professional design workflows and overcoming the limitations of previous optimization-based or simpler auto-regressive approaches. |
| Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2504.05599) or [HuggingFace](https://huggingface.co/papers/2504.05599))| Jiangbo Pei, Yichen Wei, Xiaokun Wang, Chris, Yi Peng | This paper introduces Skywork R1V, a 38B parameter multimodal model enhancing LLM reasoning for visual tasks using Chain-of-Thought. The primary objective is to efficiently transfer the reasoning capabilities of the text-based R1-series LLM to handle multimodal inputs without retraining the base LLM or vision encoder. Key methodologies include an efficient multimodal transfer via a lightweight MLP visual projector, a hybrid optimization framework combining Iterative SFT and GRPO, and an Adaptive-Length Chain-of-Thought distillation for data generation. Skywork R1V achieves competitive performance, notably scoring 69.0 on the MMMU benchmark and 94.0 on the text-based MATH500 benchmark. For AI practitioners, this work presents an open-source model and methodology demonstrating how to effectively build capable multimodal reasoning systems by efficiently adapting existing strong LLMs, offering a practical approach to enhance VLM reasoning without prohibitive retraining costs. |
| An Empirical Study of GPT-4o Image Generation Capabilities (Read more on [arXiv](https://arxiv.org/abs/2504.05979) or [HuggingFace](https://huggingface.co/papers/2504.05979))| Zhuoran Zhao, Sixiang Chen, donghao-zhou, QingyuShi, BryanW | This paper empirically benchmarks GPT-4o's image generation, revealing strengths like text rendering but limitations like inconsistency. The objective is to assess GPT-4o's image generation capabilities by qualitatively benchmarking it against models like Gemini 2.0 Flash Experimental and domain-SOTA methods across >20 tasks (text-to-image, image-to-image, image-to-3D, image-to-X). Methodology relies on structured visual evaluation and error analysis (detailed qualitatively in Table 1) due to the lack of API access and unpublished architecture. Primary results show GPT-4o excels in exceptional text rendering, compositional prompt following, spatial reasoning, and image transformation, often surpassing benchmarks qualitatively, but exhibits limitations in inconsistent generation, hallucination, and data bias (e.g., non-Latin scripts); the study explicitly notes the qualitative nature and lack of quantitative metrics. For AI practitioners, GPT-4o's notably strong text rendering capability demonstrates potential for unified models requiring precise visual-textual alignment, although current reliability issues (inconsistency, bias) warrant caution for direct deployment. |
| Hogwild! Inference: Parallel LLM Generation via Concurrent Attention (Read more on [arXiv](https://arxiv.org/abs/2504.06261) or [HuggingFace](https://huggingface.co/papers/2504.06261))| Vage Egiazarian, George Yakushev, Alina Shutova, Roman Garipov, Gleb Rodionov | **Hogwild! Inference: Parallel LLM Generation via Concurrent Attention**  This paper proposes Hogwild! Inference, a method enabling multiple instances of the same LLM to generate text in parallel while sharing and concurrently updating a common Key-Value attention cache. The main objective is to explore if LLMs can develop dynamic collaboration strategies for problem-solving without pre-defined frameworks, leveraging immediate access to each other's partial progress. The key methodology involves running parallel LLM "workers" with a shared KV cache, utilizing Rotary Position Embeddings (RoPE) to efficiently manage positional information across workers and testing three cache layouts: contiguous, interleaved, and combined. Preliminary results on LIMO mathematical reasoning tasks show that the Hogwild! Combined layout allows multiple workers (e.g., 2 workers) to achieve higher accuracy faster than single-threaded baselines or independent parallel workers, reaching approximately 89% accuracy with an 8192 max forward pass budget, surpassing other methods at equivalent budgets. For AI practitioners, the principal implication is that existing reasoning-capable LLMs can potentially leverage shared KV caches for parallel, collaborative inference out-of-the-box to improve efficiency, without requiring model fine-tuning or explicit coordination protocols. |
| COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for
  Alignment with Human Values (Read more on [arXiv](https://arxiv.org/abs/2504.05535) or [HuggingFace](https://huggingface.co/papers/2504.05535))| Siwei Wu, M-A-P Team, Liam-Liu, aaabiao, JinChengRen | This paper introduces COIG-P, a large-scale (1,006k pairs), high-quality Chinese preference dataset generated via an LLM-based pipeline for human value alignment. The primary objective was to overcome limitations of existing Chinese preference datasets, such as small scale, narrow domains, lack of validation, and the scalability issues of human annotation. The methodology involved crawling and filtering 92k Chinese queries, using 15 LLMs to generate responses, and employing 8 LLMs to score and create chosen-rejected pairs without human intervention, alongside training an 8B Chinese Reward Model (CRM) and creating a Chinese Reward Benchmark (CRBench). Results show COIG-P significantly improves LLM performance on AlignBench, yielding gains of 2% to 12% for Qwen2/2.5 and Infinity-Instruct-3M-0625 models compared to training without it, and the developed CRM demonstrates scoring capabilities comparable to GPT-40 on a test split filtering task. For AI practitioners, COIG-P provides a valuable resource for aligning Chinese LLMs using methods like DPO, while the LLM-based annotation pipeline and the CRM offer scalable, cost-effective alternatives to manual annotation or reliance on expensive large models for data curation. |
| Less-to-More Generalization: Unlocking More Controllability by
  In-Context Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02160) or [HuggingFace](https://huggingface.co/papers/2504.02160))| Fei Ding, Yufeng Cheng, Mengqi Huang, wuwx, fenfan | i) This paper introduces UNO, a universal customization framework enabling less-to-more generalization for controllable single-to-multi-subject image generation using in-context generation. ii) The research aims to develop a stable and scalable paradigm for subject-driven image generation that enhances controllability and consistency, particularly for multi-subject scenarios, while overcoming data limitations. iii) The key methodology is a model-data co-evolution approach, featuring a progressive synthetic data curation pipeline leveraging diffusion transformers' in-context generation and the UNO model, which incorporates progressive cross-modal alignment and Universal Rotary Position Embedding (UnoPE) into a DiT architecture. iv) UNO demonstrates state-of-the-art results, achieving the highest DINO (0.760) and CLIP-I (0.835) scores on the DreamBench single-subject benchmark among tuning-free methods evaluated. v) For AI practitioners, UNO provides a tuning-free framework capable of generating high-fidelity images with strong subject similarity and text controllability for both single and multiple subjects, directly applicable to customization tasks without per-subject optimization. |
| Generative Evaluation of Complex Reasoning in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02810) or [HuggingFace](https://huggingface.co/papers/2504.02810))| Baizhou Huang, Ruilin Yan, Xiangyu Wang, YitaoLiang, pkuHaowei | This paper introduces KUMO, a generative evaluation framework combining LLMs and symbolic engines to dynamically create complex, contamination-resistant reasoning tasks for assessing large language models. The primary objective is to reliably evaluate genuine LLM reasoning capabilities, distinguishing it from memorization resulting from training data contamination of static benchmarks. KUMO employs a neural-symbolic pipeline utilizing LLMs for domain generation and SAT-based engines for task instantiation, creating partially observable, multi-turn reasoning games across numerous domains with adjustable difficulty, evaluated via success rate and relative action count. Key results from evaluating 23 LLMs on 5,000 tasks across 100 domains show reasoning-scaled models achieve university-level performance on complex tasks, and KUMO performance correlates strongly (Pearson correlation > 0.9 on hard setting vs MMLU-Pro/LiveBench-Reason) with recent real-world benchmarks, while experiments demonstrate resistance to overfitting. For AI practitioners, KUMO provides a scalable, dynamic, and contamination-resistant benchmark methodology for assessing the true reasoning progress of LLMs, facilitating more reliable model evaluation and development efforts compared to potentially saturated static datasets. |
| Tuning-Free Image Editing with Fidelity and Editability via Unified
  Latent Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2504.05594) or [HuggingFace](https://huggingface.co/papers/2504.05594))| Ming-Hsuan Yang, Mike Zheng Shou, Yuchao Gu, Lan Chen, Qi Mao | i) The paper introduces UnifyEdit, a tuning-free method for text-based image editing that balances fidelity and editability using a unified latent diffusion optimization framework. ii) The research aims to enable a balanced integration of fidelity and editability in text-based image editing without extensive retraining, addressing issues of over- or under-editing. iii) UnifyEdit employs self-attention preservation and cross-attention alignment constraints, along with an adaptive time-step scheduler, to guide diffusion latent optimization. iv) Experiments show UnifyEdit outperforms existing methods, demonstrating superior structure preservation and text alignment across various editing tasks, with user studies showing a 66%-84% preference for fidelity compared to baseline approaches. v) AI practitioners can utilize UnifyEdit for more robust and adaptable text-based image editing, achieving a better balance between preserving original image structure and accurately reflecting text-based modifications.  |
| V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric
  Capabilities in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.06148) or [HuggingFace](https://huggingface.co/papers/2504.06148))| Alex Jinpeng Wang, Ping Yu, Zhengyuan Yang, Linjie Li, Fengx1nn | i) V-MAGE is introduced as a game-based framework to evaluate the visual reasoning capabilities of multimodal large language models (MLLMs). ii) The research aims to address limitations in current game-based benchmarks by providing visually-centric tasks that assess diverse reasoning skills. iii) The methodology involves evaluating leading MLLMs across five games with 30+ levels, using an adaptive Elo-based ranking system for performance comparison. iv) Results show a substantial performance gap between top-performing MLLMs and humans, with GPT-40 scoring 1.93/10 versus a human score of ≈10/10 in FlappyBird Level 6, while Qwen2VL-72B achieved 0.61/10 on the same task. v) V-MAGE highlights limitations in MLLMs’ visual perception and reasoning, suggesting a need to refine agent strategies and address perceptual inaccuracies from an agent-centric perspective for AI improvement.  |
| CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs
  with Controllable Puzzle Generation (Read more on [arXiv](https://arxiv.org/abs/2504.00043) or [HuggingFace](https://huggingface.co/papers/2504.00043))| William W. Cohen, Bill Yuchen Lin, Langlin Huang, Chengsong Huang, Jixuan Leng | This paper introduces CrossWordBench, a benchmark using controllable crossword puzzles to evaluate the multimodal reasoning of LLMs and Large Vision-Language Models (LVLMs). The main objective is to assess model capabilities in handling tasks requiring simultaneous adherence to semantic constraints from text clues and structural constraints from visual grids. Methodologically, it utilizes a controllable puzzle generation framework creating text and image formats from diverse sources and evaluates over 20 models using zero-shot Chain-of-Thought and interactive modes. Results show reasoning LLMs significantly outperform non-reasoning models by leveraging crossing-letter constraints (achieving an 89% relative increase in Intersection Consistency Rate), while LVLMs perform poorly, with puzzle-solving performance strongly correlating (r=0.94) with grid-parsing accuracy. For AI practitioners, this highlights current LVLMs' limitations in integrating visual-structural information with textual reasoning for constrained tasks and suggests the benchmark's potential for developing and evaluating models with better spatial-textual grounding. |
| Accelerate Parallelizable Reasoning via Parallel Decoding within One
  Sequence (Read more on [arXiv](https://arxiv.org/abs/2503.20533) or [HuggingFace](https://huggingface.co/papers/2503.20533))| Yijiong Yu | The paper introduces a parallel decoding method, "Parallel Decoding in One Sequence," for accelerating reasoning in Large Language Models (LLMs). The research aims to address the inefficiency of autoregressive decoding for tasks with parallelizable steps. The methodology involves identifying parallelizable steps, decoding them in parallel using a modified attention mask and position IDs within a single sequence, and then concatenating the results. Experiments demonstrate over 100% speedup in decoding time on a retrieval task with a context of 10 items while maintaining answer quality. This method enables AI practitioners to accelerate LLM reasoning on parallelizable tasks without additional memory usage or KV cache recomputation.  |
| HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned
  Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.06232) or [HuggingFace](https://huggingface.co/papers/2504.06232))| Tong Wu, Pan Zhang, Yujie Zhou, Pengyang Ling, Jiazi Bu | HiFlow introduces a training-free, model-agnostic framework for high-resolution text-to-image generation using pre-trained rectified flow models. The research aims to enhance image quality in high-resolution synthesis by establishing a virtual reference flow and aligning it with the high-resolution sampling flow through initialization, direction, and acceleration alignment. HiFlow achieves superior high-resolution image quality over state-of-the-art methods, demonstrating, for example, a FID score of 52.55 for 4096x4096 image generation. The flow-aligned guidance approach offers AI practitioners a method for improving image fidelity and detail in high-resolution T2I tasks without requiring model retraining. The paper does not provide information about the compute resources required or its use with large language models.  |
| Leanabell-Prover: Posttraining Scaling in Formal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.06122) or [HuggingFace](https://huggingface.co/papers/2504.06122))| Yang Yue, Yahui Liu, Xingguang Ji, Qi Wang, Jingyuan Zhang | Leanabell-Prover improves automated theorem proving (ATP) through posttraining scaling of large language models using Lean 4 code. This research investigates posttraining techniques for ATP with the aim of achieving breakthroughs similar to those seen in natural language reasoning models. The study utilizes a hybrid dataset for continual training and GRPO for reinforcement learning, incorporating cognitive behaviors. Results show a 59.8% pass rate (pass@32) on the MiniF2F test after employing RL training, surpassing DeepSeek-Prover-v1.5-RL and Goedel-Prover-SFT. AI practitioners can leverage the proposed methods to enhance formal provers, leading to state-of-the-art performance in whole-proof generation.  |
