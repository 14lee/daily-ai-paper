

## Papers for 2025-04-28

| Title | Authors | Summary |
|-------|---------|---------|
| Towards Understanding Camera Motions in Any Video (Read more on [arXiv](https://arxiv.org/abs/2504.15376) or [HuggingFace](https://huggingface.co/papers/2504.15376))| Jay Karhade, Daniel Jiang, Stephen624, syCen, zhiqiulin | Introduces CameraBench, a large-scale dataset and benchmark for understanding camera motion primitives in diverse internet videos. The main objective is to evaluate how well current Structure-from-Motion (SfM) and Video-Language Models (VLMs) understand a comprehensive taxonomy of camera motions and to improve this capability. Methodology involves creating a detailed taxonomy with cinematographers, collecting and annotating ~3,000 videos, conducting human studies, and benchmarking 20 diverse models (SfM/SLAM and VLMs) on tasks like classification, VQA, captioning, and retrieval. Primary results show classic SfM struggles with semantic/dynamic content, while VLMs struggle with precise geometry; the best baseline method (MegaSAM) achieves ~50% overall Average Precision (AP) on primitive classification, while fine-tuning a generative VLM (Qwen2.5-VL-7B) boosts performance significantly (~2x improvement), reaching 59.3% AP. AI practitioners can utilize CameraBench's dataset and taxonomy to fine-tune VLMs, substantially improving their ability to interpret both geometric and semantic camera movements for enhanced video understanding applications. |
| Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.16656) or [HuggingFace](https://huggingface.co/papers/2504.16656))| Xiaokun Wang, Yi Peng, Yichen Wei, Chris, xuchensong | Skywork R1V2 is a multimodal reasoning model developed using a hybrid reinforcement learning strategy that eliminates the need for supervised fine-tuning or teacher model distillation. The primary objective is to enhance sophisticated reasoning capabilities in vision-language models (VLMs) while preserving broad generalization, directly addressing the challenge of balancing these competing demands via reinforcement learning. Methodologically, R1V2 combines Mixed Preference Optimization (MPO) with reward-model guidance and Group Relative Policy Optimization (GRPO), augmented by a Selective Sample Buffer (SSB) to counteract vanishing advantages and prioritize informative training samples. Key results demonstrate state-of-the-art open-source performance, achieving 78.9% on AIME2024 and 62.6% on OlympiadBench, significantly improving upon prior open-source models and reducing the gap with proprietary systems. For AI practitioners, this work presents a validated hybrid RL framework (MPO+GRPO+SSB) as an effective technique for building capable multimodal reasoning models without reliance on SFT, while highlighting the necessity of careful reward calibration to manage the trade-off between enhanced reasoning and potential visual hallucination. |
| BitNet v2: Native 4-bit Activations with Hadamard Transformation for
  1-bit LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.18415) or [HuggingFace](https://huggingface.co/papers/2504.18415))| Furu Wei, Shuming Ma, Hongyu Wang | BitNet v2 introduces a framework for 1-bit Large Language Models (LLMs) enabling native 4-bit activation quantization. The main objective is to overcome activation outliers that complicate low bit-width quantization in attention and feed-forward networks. Key methodology involves H-BitLinear, which applies an online Hadamard transformation to reshape activation distributions into more Gaussian-like forms prior to quantization, specifically targeting attention output (Wo) and FFN down-projection (Wdown). Experiments show BitNet v2 trained with native 4-bit activations achieves performance comparable to prior versions using 8-bit or hybrid activations; for instance, the 7B model BitNet v2 (a4) achieves an average downstream task accuracy of 58.30, close to BitNet b1.58's 58.12. For AI practitioners, this research offers a viable path towards deploying 1-bit LLMs with efficient native 4-bit activations, reducing memory and computational costs, particularly beneficial for hardware supporting low-bit computations and batched inference scenarios. |
| VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,
  Languages, and Domains in Video Comprehension (Read more on [arXiv](https://arxiv.org/abs/2504.17821) or [HuggingFace](https://huggingface.co/papers/2504.17821))| Wenhan Luo, Baotian Hu, Haoyuan Shi, Yunxin Li, Xinyu Chen | VideoVista-CulturalLingo introduces the first benchmark for evaluating video comprehension across diverse cultures (Chinese, Western), languages (Chinese, English), and domains. The primary objective is to assess the understanding and reasoning capabilities of large multimodal models (LMMs) beyond existing culturally and linguistically limited benchmarks. A dataset comprising 1,389 videos and 3,134 QA pairs was constructed using a hybrid annotation framework leveraging LLMs (Qwen2-VL, DeepSeek) and human review, followed by the evaluation of 24 LMMs. Experimental results indicate proprietary models like Gemini-2.0-Flash achieve the highest accuracy (76.3%), while open-source models show limitations, particularly on Chinese-centric questions and temporal understanding tasks like Event Localization (achieving only 45.2% maximum score). For AI practitioners, this benchmark provides a crucial tool for identifying weaknesses in LMMs' cross-cultural generalization and fine-grained temporal reasoning, highlighting areas needing improvement for developing more globally competent video understanding systems. |
| Can Large Language Models Help Multimodal Language Analysis? MMLA: A
  Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2504.16427) or [HuggingFace](https://huggingface.co/papers/2504.16427))| Peiwu Wang, Hua Xu, Yeshuang Zhu, Zhuohang Li, HanleiZhang | This paper introduces MMLA, a benchmark for evaluating large language models (LLMs) and multimodal large language models (MLLMs) on multimodal language analysis. The objective is to assess the capability of these foundation models to comprehend high-level, cognitive semantics (intent, emotion, dialogue act, sentiment, speaking style, communication behavior) in human utterances. Methodology involves evaluating eight model families across nine datasets (61K+ utterances) using zero-shot inference, supervised fine-tuning (SFT), and instruction tuning (IT). Results demonstrate that MLLMs significantly improve with SFT but still face challenges, achieving only about 60-70% average accuracy (best SFT model reaches 69.18%), indicating limitations in understanding complex human language nuances. For AI practitioners, this highlights that while fine-tuning substantially boosts performance and enables smaller models to rival larger ones, current MLLMs require further development to reliably decode complex multimodal semantics for applications like virtual assistants or social behavior analysis. |
| The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17768) or [HuggingFace](https://huggingface.co/papers/2504.17768))| Kelly Marchisio, Sebastian Ruder, Renjie Huang, Robert Li, Piotr Nawrot | This paper presents a large-scale empirical comparison of training-free sparse attention methods in Transformer LLMs across various model sizes, sequence lengths, and sparsity levels. The main objective was to systematically evaluate the viability, efficiency-accuracy trade-offs, and scaling properties of sparse attention for long-context processing. Researchers compared six representative sparse attention patterns on nine diverse long-sequence tasks using Qwen 2.5 models (7B-72B) up to 128k sequence length, employing isoFLOPS analysis, statistical tests, and scaling law fitting. Key findings include: 1) isoFLOPS analysis shows large, highly sparse models are preferable to smaller, dense ones for very long sequences; 2) maximum tolerable sparsity without performance degradation varies significantly, often exceeding 10x on average but dropping below 5x for at least one task in most configurations; 3) no single method excels universally, with optimal choices being task- and phase-dependent. For AI practitioners, this implies sparse attention is a valuable tool for scaling to longer sequences but is not a universally applicable solution and demands careful, application-specific evaluation of performance trade-offs, as even moderate sparsity can cause significant degradation on sensitive tasks. |
| Subject-driven Video Generation via Disentangled Identity and Motion (Read more on [arXiv](https://arxiv.org/abs/2504.17816) or [HuggingFace](https://huggingface.co/papers/2504.17816))| Wonjoon Jin, Jingxu Zhang, cluo-ms, daiqi, carpedkm | This paper presents a zero-shot method for subject-driven video generation by factorizing identity injection and temporal modeling using image customization and unpaired video datasets. The primary objective is to achieve high-fidelity subject consistency and temporal coherence without relying on large-scale annotated subject-video (S2V) datasets. Key methodologies include fine-tuning a pre-trained Multi-Modal Diffusion Transformer (MM-DiT) model via stochastically-switched optimization between identity learning (using an S2I dataset and LoRA) and temporal preservation (using unpaired videos and I2V fine-tuning), incorporating random frame selection and token dropping. The approach significantly outperforms baselines in zero-shot settings, achieving superior identity consistency (DINO-I: 59.29) and dynamic degree (60.19) on the VBench benchmark. For AI practitioners, this demonstrates a viable path to build scalable personalized video generation models using readily available image customization data, bypassing the significant cost and complexity of acquiring large annotated video datasets. |
| DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15716) or [HuggingFace](https://huggingface.co/papers/2504.15716))| Lifan Guo, Junhui Li, Huaixia Dou, Qian Chen, amazingj | The paper introduces DianJin-R1, an LLM framework enhancing financial reasoning via structured supervision and reinforcement learning using a curated dataset. The primary objective is to improve LLM performance on complex financial tasks requiring domain-specific knowledge, numerical calculation, and compliance adherence. Methodology involves fine-tuning Qwen2.5 models on the DianJin-R1-Data (derived from CFLUE, FinQA, CCC) to generate structured reasoning (<think>) and answers (<answer>), further refined using Group Relative Policy Optimization (GRPO) with format and accuracy rewards. Key results show DianJin-R1 models significantly outperform non-reasoning counterparts; DianJin-R1-32B achieved 96.00% accuracy on the proprietary CCC compliance benchmark with a single API call, exceeding a multi-agent baseline requiring 8.15 calls. For AI practitioners, this demonstrates that combining structured reasoning supervision with targeted reinforcement learning provides a scalable and computationally efficient approach to building specialized LLMs for complex, domain-specific reasoning tasks like financial compliance. |
| DC-SAM: In-Context Segment Anything in Images and Videos via Dual
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2504.12080) or [HuggingFace](https://huggingface.co/papers/2504.12080))| Lu Qi, Xiaoyang Bi, Xiangtai Li, Mengshi Qi, zaplm | DC-SAM adapts SAM/SAM2 for in-context image/video segmentation via prompt tuning and dual consistency. The objective is to enhance SAM's in-context segmentation ability by generating higher-quality visual prompts through better feature utilization and consistency, and to establish a benchmark for in-context video segmentation. The key methodology involves fusing SAM encoder features with backbone features, employing dual positive/negative prompt generation branches refined by cyclic consistent cross-attention, and using a mask-tube training strategy for video extension with SAM2. Primary results demonstrate state-of-the-art performance, achieving 55.5 mIoU (+1.4 improvement over VRP-SAM baseline) on COCO-20^2 and a J&F score of 71.52 on the proposed In-Context Video Object Segmentation (IC-VOS) benchmark. For AI practitioners, DC-SAM offers an efficient parameter-tuning approach to adapt foundation models like SAM/SAM2 for few-shot (specifically one-shot) segmentation tasks in both images and videos, improving prompt quality through explicit feature fusion and consistency enforcement. |
| Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing
  Efficiency Through Vocabulary Adaptation (Read more on [arXiv](https://arxiv.org/abs/2504.17025) or [HuggingFace](https://huggingface.co/papers/2504.17025))| Edoardo Barba, Andrei Stefan Bejgu, Pere-Lluis Huguet Cabot, Giovanni Puccetti, Luca Moroni | This paper introduces and evaluates vocabulary adaptation techniques, including the novel Semantic Alignment Vocabulary Adaptation (SAVA), for optimizing English Large Language Models (LLMs) for Italian. The primary objective is to compare different vocabulary adaptation methods against Language-Adaptive Pre-Training (LAPT) for adapting English LLMs (Mistral-7B-v0.1, Llama-3.1-8B) to Italian, focusing on token fertility reduction and downstream task performance. Methodology involves replacing the original tokenizer and embeddings using heuristics like Fast Vocabulary Transfer (FVT), Cross-Lingual Projection (CLP), the proposed SAVA (using neural mapping from a helper model, Minerva-3B), and random initialization, followed by continual training on Italian/English data. Results show that adapting Mistral-7B-v0.1 with the Minerva tokenizer reduced Italian token fertility by 25%, and adapting Llama-3.1-8B reduced its parameters by 1 billion (10% size reduction) due to vocabulary optimization; SAVA and FVT demonstrated competitive performance on downstream tasks, often converging faster during continual training than other methods. For AI practitioners, this indicates that vocabulary adaptation techniques like SAVA or FVT can significantly improve the efficiency (lower fertility, potentially smaller models) of deploying English-centric LLMs for other languages like Italian with relatively limited continual training. |
