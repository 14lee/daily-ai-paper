

## Papers for 2025-04-14

| Title | Authors | Summary |
|-------|---------|---------|
| Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2504.08685) or [HuggingFace](https://huggingface.co/papers/2504.08685))| Zhijie Lin, Ceyuan Yang, Team Seawead, zhenheny, lingff | This paper details a cost-effective strategy for training Seaweed-7B, a 7-billion parameter video generation foundation model using moderate compute. The primary objective was to demonstrate that a medium-sized video generation model can achieve competitive performance compared to much larger models trained with significantly greater computational resources. Key methodologies involved training a novel 64x compression Variational Autoencoder (VAE) and a hybrid-stream Diffusion Transformer (DiT) from scratch on curated data using 665,000 H100 GPU hours, employing multi-stage training, SFT, DPO, and infrastructure optimizations like 3D parallelism and Multi-Level Activation Checkpointing (MLAC). Seaweed-7B achieved competitive performance, ranking second in image-to-video generation Elo ratings (1047 Elo, 58% win rate) against models like Sora and Wan 2.1, and its VAE obtained state-of-the-art reconstruction (e.g., 0.0391 LPIPS on UCF-101). Its distilled version requires only 12 NFEs for inference, 62 times faster than Wan 2.1 (100 NFEs). For AI practitioners, this work implies that careful design choices in data curation, VAE/DiT architecture, and training/inference optimization enable the development of highly competitive, cost-effective video generation models without necessarily resorting to massive parameter counts. |
| GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for
  Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.08736) or [HuggingFace](https://huggingface.co/papers/2504.08736))| Jiashi Feng, Zilong Huang, Jun Hao Liew, XihuiLiu, YuuTennYi | GigaTok introduces a 3 billion parameter visual tokenizer for autoregressive image generation that improves reconstruction, generation, and representation quality simultaneously during scaling. The research aims to overcome the common dilemma where scaling visual tokenizers improves reconstruction but degrades downstream generation performance. Key methods involve semantic regularization using features from a pre-trained DINOv2 model, employing 1D Q-Former based tokenizers, prioritizing decoder scaling in an asymmetric architecture, and using entropy loss for billion-scale training stability. The proposed 2.9B parameter GigaTok, when paired with a 1.4B AR model, achieves state-of-the-art autoregressive generation performance with a gFID of 1.98* on ImageNet 256x256. AI practitioners can apply semantic regularization and the identified scaling practices (1D tokenizers, asymmetric scaling, entropy loss) to develop larger, more effective visual tokenizers for generative models without sacrificing downstream performance due to increased latent space complexity. |
| MineWorld: a Real-Time and Open-Source Interactive World Model on
  Minecraft (Read more on [arXiv](https://arxiv.org/abs/2504.08388) or [HuggingFace](https://huggingface.co/papers/2504.08388))| Yushu Jiang, Haoyu Wu, Tianyu He, Yang Ye, Junliang Guo | MineWorld introduces a real-time, open-source, interactive world model for Minecraft based on an autoregressive Transformer. The primary objective is to develop an efficient and controllable world model capable of real-time interaction by predicting future game states conditioned on past states and actions. Key methodology involves tokenizing visual game states and player actions, feeding them interleaved into a Transformer trained via next-token prediction, and employing a novel parallel decoding algorithm for inference acceleration. Results demonstrate the model's efficacy, with the 1.2B parameter version achieving 3.01 FPS, a discrete action F1 score of 0.73, and camera control L1 loss of 1.02, significantly outperforming diffusion-based baselines while the parallel decoding provides over 3x speedup. For AI practitioners, MineWorld offers a validated open-source framework and an efficient parallel decoding technique for building fast, interactive simulators essential for agent training and human-AI interaction in complex environments. |
| PixelFlow: Pixel-Space Generative Models with Flow (Read more on [arXiv](https://arxiv.org/abs/2504.07963) or [HuggingFace](https://huggingface.co/papers/2504.07963))| Ping Luo, Peize Sun, Shilong Zhang, Chongjian Ge, Shoufa Chen | i) PixelFlow, a novel image generation model, performs image generation directly in raw pixel space through cascade flow modeling. ii) The research aims to develop an end-to-end trainable image generation model operating directly in pixel space, avoiding the need for pre-trained VAEs and cascaded upsampling. iii) PixelFlow employs a cascade flow modeling strategy, operating on multi-scale samples across cascading resolutions and using Flow Matching for velocity prediction. iv) PixelFlow achieves an FID of 1.98 on the 256x256 ImageNet class-conditional image generation benchmark. v) The PixelFlow framework provides AI practitioners with a simpler, end-to-end trainable alternative to latent-space diffusion models, enabling efficient pixel-space image generation with competitive performance.  |
| SQL-R1: Training Natural Language to SQL Reasoning Model By
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.08600) or [HuggingFace](https://huggingface.co/papers/2504.08600))| Ran Chen, Xuhui Jiang, Chengjin Xu, Peixian Ma, ZhuangXialie | i) This paper introduces SQL-R1, an NL2SQL reasoning model trained via reinforcement learning to improve performance in complex scenarios. ii) The research aims to enhance NL2SQL inference performance in complex database scenarios using reinforcement learning. iii) The methodology involves training a NL2SQL model using reinforcement learning with a specialized reward function and a cold start strategy based on supervised fine-tuning. iv) SQL-R1 achieves execution accuracy of 88.6% on the Spider benchmark and 66.6% on the BIRD benchmark using a 7B base model. v) AI practitioners can leverage the SQL-R1 model to achieve competitive accuracy in NL2SQL tasks with limited data and improved reasoning capabilities, demonstrating the potential of RL in optimizing NL2SQL performance.  |
| FlexIP: Dynamic Control of Preservation and Personality for Customized
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.07405) or [HuggingFace](https://huggingface.co/papers/2504.07405))| Kaiwen Xiao, Yanning Zhou, Haonan Lin, DevLinyan | FlexIP is introduced as a novel framework for decoupling identity preservation and personalized editing in image generation. The research aims to enable flexible, parameterized control during inference through dynamic tuning of the weight adapter in generative models. FlexIP uses a dual-adapter architecture comprising a Personalization Adapter and a Preservation Adapter, coupled with a dynamic weight gating mechanism to balance identity retention and stylistic variation. Experiments demonstrate that FlexIP achieves a 61.4% controllability (Flex score) and 76.8% ID-Pres score. The framework offers AI practitioners a robust and flexible solution for subject-driven image generation by enabling continuous parametric control of the preservation-editability trade-off.  |
| In-2-4D: Inbetweening from Two Single-View Images to 4D Generation (Read more on [arXiv](https://arxiv.org/abs/2504.08366) or [HuggingFace](https://huggingface.co/papers/2504.08366))| Ali Mahdavi-Amiri, Hao Zhang, Daniel Cohen-Or, Sauradip Nag | i) This paper introduces In-2-4D, a method for generating 4D (3D object + motion) interpolations from two single-view images. ii) The primary objective is to generate and reconstruct a smooth 4D motion sequence given only start and end state images of an object. iii) The method uses a hierarchical approach involving video interpolation models, keyframe selection based on motion and appearance analysis, 3D Gaussian Splatting for static 3D representation, and dynamic Gaussian generation via a deformation field optimized with multi-view diffusion priors. iv) The method achieves improved performance on a newly introduced I4D-15 benchmark, outperforming baselines in terms of appearance (LPIPS: 0.103, FVD: 679.23) and geometry (SI-CD: 22.67, CD: 0.59), with user studies indicating a preference for the generated 4D motion quality (1.29 rating). v) The approach provides AI practitioners with a method for generating dynamic 3D content from minimal input, enabling applications in content creation and animation by requiring less data and allowing for diverse motion synthesis.  |
| ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on
  Transformer Encoder Models Performance (Read more on [arXiv](https://arxiv.org/abs/2504.08716) or [HuggingFace](https://huggingface.co/papers/2504.08716))| Djamé Seddah, Benoît Sagot, Wissam Antoun | This paper conducts a controlled comparison of ModernBERT and DeBERTaV3 architectures by pretraining them on identical French datasets. The objective is to disentangle architectural advantages from training data differences in explaining performance variations between these transformer encoder models. The methodology involves pretraining French ModernBERT on the same 275B token dataset as CamemBERTaV2 (a French DeBERTaV3 model) and evaluating on French NER, QA, and classification tasks. Results show DeBERTaV3 (CamemBERTaV2) achieves superior benchmark performance (e.g., 83.04 F1 QA vs. 81.34 F1 for ModernBERT-CV2) and sample efficiency when data is controlled, while ModernBERT offers faster training/inference speeds. For AI practitioners, this implies a trade-off: DeBERTaV3 yields higher accuracy, whereas ModernBERT provides better computational efficiency, highlighting the need to evaluate models under shared data conditions for fair comparison. |
| Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend
  NPUs (Read more on [arXiv](https://arxiv.org/abs/2504.07866) or [HuggingFace](https://huggingface.co/papers/2504.07866))| Xueyu Wu, Yehui Tang, Kaikai Song, Wenyong Huang, Yichun Yin | Pangu Ultra is a 135B parameter dense Transformer LLM trained on 13.2 trillion tokens using 8,192 Ascend NPUs. The primary objective was to explore the performance limits of large-scale dense LLMs and address the associated training stability and system efficiency challenges on Ascend hardware. Methodology involved proposing depth-scaled sandwich normalization and TinyInit for stable training of the 94-layer model, alongside system optimizations like NPU Fusion Attention (NFA) and MC2 for efficient training, achieving over 50% MFU. Results show Pangu Ultra significantly outperforms comparable dense models like Llama 3.1 405B (e.g., 90.3% vs 72.5% on C-Eval) and achieves competitive results against larger sparse MoE models such as DeepSeek-R1. For AI practitioners, this work validates the capability of Ascend NPUs for efficiently training >100B parameter dense models and demonstrates that optimized dense architectures can achieve state-of-the-art performance comparable to sparse models, potentially offering simpler inference deployment. |
| SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder
  Guardrails for Precision Unlearning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.08192) or [HuggingFace](https://huggingface.co/papers/2504.08192))| Virginia Smith, Mona Diab, Jacopo Bonato, Aashiq Muhamed | i) This paper introduces Dynamic SAE Guardrails (DSG), an activation-based method using Sparse Autoencoders (SAEs) that significantly improves precision unlearning in LLMs compared to gradient-based approaches. ii) The primary objective is to develop an unlearning technique that effectively removes targeted knowledge from LLMs while preserving general utility, addressing limitations of existing methods like high cost, instability, and poor data efficiency. iii) DSG employs principled feature selection using Fisher Information approximation via squared SAE activations to identify forget-relevant features and uses a dynamic, input-dependent classifier with a statistically determined threshold to conditionally clamp these features during inference. iv) Experiments demonstrate DSG substantially outperforms baseline methods, achieving a superior forget-utility trade-off by reducing WMDP-Bio accuracy to 29.64% (vs. 50.00% for the next best, RMU) while maintaining high MMLU accuracy (99.34%) and offering better computational efficiency, hyperparameter stability, and sequential unlearning performance. v) For AI practitioners, DSG provides a more computationally efficient, stable, interpretable, and data-efficient mechanism for targeted knowledge removal, enhancing LLM safety, privacy, and maintenance capabilities without requiring gradient computations during intervention. |
| Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning
  vs. Memorization in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.05262) or [HuggingFace](https://huggingface.co/papers/2504.05262))| Zhenzhong Lan, Renjun Xu, Yu Lu, Yang Yan | This paper probes whether Large Language Models genuinely understand elementary addition principles or rely on pattern memorization. The research investigates if LLMs learn generalizable arithmetic rules or merely exploit statistical patterns when performing two-integer addition. Methodology involves evaluating LLMs on addition tasks using standard digits versus isomorphic symbolic mappings, testing commutativity (A+B vs B+A), and analyzing performance scaling with digit count. Results show that while models achieve high numerical accuracy (73.8-99.8%), performance collapses to ≤7.5% under symbolic mapping, indicating a failure to generalize learned rules beyond familiar patterns. The principal implication for AI practitioners is that current LLMs heavily rely on memorization over true rule learning for arithmetic, necessitating more rigorous evaluation methods to assess genuine mathematical reasoning capabilities before deployment. |
| CoRAG: Collaborative Retrieval-Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2504.01883) or [HuggingFace](https://huggingface.co/papers/2504.01883))| Virginia Smith, Mona Diab, Aashiq Muhamed | i) The paper introduces CoRAG, a framework for collaborative retrieval-augmented generation. ii) The research investigates how to effectively train RAG models in collaborative settings with shared passage stores, addressing the challenges of data heterogeneity and client incentives. iii) The methodology involves developing a novel benchmark, CRAB, for homogeneous open-domain question answering and comparing CoRAG against parametric collaborative learning and local RAG baselines using FedAvg. iv) Experiments on CRAB show CoRAG consistently outperforms baselines in few-shot settings, achieving a 33.8% improvement over local RAG at 16-shot; further analysis reveals that relevant passages are crucial, hard negatives are detrimental, while irrelevant passages can even be beneficial for model performance. v) AI practitioners can leverage CoRAG to improve model performance in low-resource, collaborative knowledge-intensive tasks by careful curation of the shared passage store, balancing the inclusion of relevant and irrelevant passages while minimizing hard negatives.  |
| InteractVLM: 3D Interaction Reasoning from 2D Foundational Models (Read more on [arXiv](https://arxiv.org/abs/2504.05303) or [HuggingFace](https://huggingface.co/papers/2504.05303))| Cordelia Schmid, Omid Taheri, Shashank Tripathi, Dimitrije Antić, saidwivedi | i) InteractVLM estimates 3D human-object contact points from single images by leveraging 2D vision-language models. ii) The research objective is to accurately estimate 3D contact points between humans and objects from in-the-wild 2D images to improve joint reconstruction without relying on extensive 3D contact annotations. iii) The methodology involves a "Render-Localize-Lift" module using multi-view rendering, a novel multi-view localization model (MV-Loc), and fine-tuning a VLM with limited 3D contact data. iv) InteractVLM achieves a 20.6% improvement in F1 score over existing methods for binary human contact prediction on the DAMON dataset. v) InteractVLM enables AI practitioners to improve 3D human-object interaction reconstruction from 2D images using predicted contact points and minimal 3D annotation, improving the realism and accuracy of HOI reconstruction.  |
