

## Papers for 2025-04-21

| Title | Authors | Summary |
|-------|---------|---------|
| Does Reinforcement Learning Really Incentivize Reasoning Capacity in
  LLMs Beyond the Base Model? (Read more on [arXiv](https://arxiv.org/abs/2504.13837) or [HuggingFace](https://huggingface.co/papers/2504.13837))| Zhaokai Wang, Andrew Zhao, Rui Lu, Zhiqi Chen, Yang Yue | This paper demonstrates that Reinforcement Learning with Verifiable Rewards (RLVR) primarily enhances sampling efficiency for existing reasoning paths within LLMs, rather than fundamentally expanding reasoning capacity beyond the base model. The study critically investigates if RLVR enables LLMs to acquire novel reasoning abilities exceeding their base models' intrinsic capabilities. Using the `pass@k` metric with large `k` values across math, coding, and visual reasoning benchmarks, alongside perplexity analysis and manual Chain-of-Thought checks, the researchers compared the reasoning boundaries of base and RL-trained models. Key findings reveal that while RL models excel at low `k` (pass@1), base models consistently match or surpass RL models at high `k` (e.g., base Minerva 32B outperformed its RL counterpart by ~9% pass@128), indicating RL primarily learns to sample pre-existing correct reasoning paths more efficiently, rather than discovering new ones. For AI practitioners, this implies current RLVR mainly optimizes known reasoning patterns rather than fostering new skills, suggesting that achieving breakthroughs in reasoning might require complementary methods like distillation or fundamentally different training paradigms that overcome RL's observed limitation in narrowing exploration. |
| MIG: Automatic Data Selection for Instruction Tuning by Maximizing
  Information Gain in Semantic Space (Read more on [arXiv](https://arxiv.org/abs/2504.13835) or [HuggingFace](https://huggingface.co/papers/2504.13835))| Haochen Ye, Zerun Ma, Kai Hu, Yining Li, Yicheng Chen | This paper introduces MIG, an automatic method for selecting instruction-tuning data by maximizing information gain within a semantic label graph representation. Its objective is to unify the quantification of data quality and diversity for efficient subset selection from large pools, overcoming limitations of prior heuristic and embedding-based techniques. MIG models semantic relationships via a label graph, uses a submodular information function considering propagation effects, and iteratively selects samples via a greedy, gain-maximizing algorithm. Key results demonstrate MIG's superiority over baselines; notably, Llama3.1-8B tuned with just 5% of Tulu3 data selected by MIG improved performance over the model trained on the full dataset by +5.73% on AlpacaEval and +6.89% on Wildbench. This provides AI practitioners an efficient, automated approach to curate smaller yet highly effective instruction-tuning datasets, potentially reducing training costs while improving model alignment and outperforming methods relying solely on embedding distance or heuristics. |
| Could Thinking Multilingually Empower LLM Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2504.11833) or [HuggingFace](https://huggingface.co/papers/2504.11833))| Lei Li, Shujian Huang, Wenhao Zhu, Xu Huang, Changjiang Gao | This paper investigates harnessing multilingualism to enhance Large Language Model (LLM) reasoning capabilities. The primary objective is to quantify the potential performance upper-bound of multilingual reasoning compared to English-only approaches. Key methodology involves aggregating LLM responses (LLaMA3.1-70B, Qwen2.5-72B, R1-distill-LLaMA3.1-70B) to parallel inputs translated into 17 languages on GPQA and MGSM datasets, measuring the Acc@k upper bound. Results demonstrate that multilingual thinking significantly surpasses English-only baselines (Repeat/Paraphrase), boosting the Acc@k upper bound by nearly 10 points (e.g., GPQA from ~45 to ~90 Acc@17). For AI practitioners, this highlights substantial untapped potential in leveraging diverse languages for reasoning, though current answer selection methods (majority voting, prompt-based, LLM-as-judge) fail to fully realize this potential gain. |
| AerialMegaDepth: Learning Aerial-Ground Reconstruction and View
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.13157) or [HuggingFace](https://huggingface.co/papers/2504.13157))| Shubham Tulsiani, Srinivasa Narasimhan, Deva Ramanan, Anurag Ghosh, kvuong2711 | This paper introduces AerialMegaDepth, a hybrid dataset for improving aerial-ground 3D reconstruction and view synthesis. The objective is to overcome the failure of learning-based methods to handle extreme viewpoint variations between aerial and ground images due to a lack of suitable training data. The methodology combines pseudo-synthetic aerial renderings from 3D city meshes (Google Earth) with co-registered real ground-level images (MegaDepth) into a unified coordinate system. Fine-tuning the DUSt3R model on AerialMegaDepth significantly improved ground-aerial camera registration, increasing the Relative Rotation Accuracy @5Â° from under 5% to nearly 56%. AI practitioners can utilize this framework and dataset to develop models robust to drastic viewpoint differences for tasks like cross-view 3D reconstruction and novel view synthesis, addressing a key limitation in existing large-scale datasets. |
| HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation (Read more on [arXiv](https://arxiv.org/abs/2504.13072) or [HuggingFace](https://huggingface.co/papers/2504.13072))| Tao Hu, Yuan Li, Zesong Yang, Bangbang Yang, Wenqi Dong | HiScene introduces a hierarchical framework for generating editable, compositional 3D scenes from text prompts by leveraging isometric view generation. The main objective is to create high-fidelity 3D scenes with natural layouts, complete object instances, and interactive editing capabilities, overcoming limitations of prior methods. Its methodology involves initializing a scene from an isometric view using a native 3D generator, performing hierarchical scene parsing with 3D segmentation, applying video-diffusion-based amodal completion to handle occlusions, and using spatial alignment with shape prior injection for object regeneration. Experimental results show HiScene outperforms methods like GALA3D and DreamScene in user studies (Overall Quality score: 2.76 vs 1.75/1.73) and metrics, with its amodal completion achieving 83.84 mIoU on COCO-A, surpassing prior zero-shot methods. This research provides AI practitioners with a method to generate complex, editable 3D scenes from text, improving workflow for interactive applications, and introduces a robust video-diffusion technique for amodal completion relevant to 3D perception. |
| NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes (Read more on [arXiv](https://arxiv.org/abs/2504.11544) or [HuggingFace](https://huggingface.co/papers/2504.11544))| Yixin Liu, Haoxiang Chen, Chengze Li, Haojie Zheng, Tianyang Xu | This paper introduces NodeRAG, a framework optimizing retrieval-augmented generation by structuring knowledge into a carefully designed heterogeneous graph with distinct node types. The main objective is to demonstrate how this specific graph structure enhances graph-based RAG performance, particularly for multi-hop reasoning and summary-level queries, compared to methods with less considered structures. NodeRAG utilizes LLMs to decompose text into seven distinct node types (Entity, Relationship, Semantic Unit, Attribute, High-Level, Overview, Text), augments the graph via community detection and node importance metrics, and employs graph algorithms like shallow Personalized PageRank with a dual (exact + vector) search for retrieval. Primarily, NodeRAG achieves superior accuracy on multi-hop benchmarks, such as 46.29% on MuSiQue (compared to 41.71% for GraphRAG), while using fewer retrieval tokens. For AI practitioners, NodeRAG provides a concrete methodology for designing graph indices that yield more precise, efficient, and explainable retrieval, significantly improving RAG system performance and reducing operational costs associated with token usage for complex information synthesis tasks. |
| It's All Connected: A Journey Through Test-Time Memorization,
  Attentional Bias, Retention, and Online Optimization (Read more on [arXiv](https://arxiv.org/abs/2504.13173) or [HuggingFace](https://huggingface.co/papers/2504.13173))| Vahab Mirrokni, Peilin Zhong, Meisam Razaviyayn, Ali Behrouz | This paper reconceptualizes sequence models like Transformers and linear RNNs as associative memory modules optimizing an internal "attentional bias" objective, introducing the MIRAS framework. The main objective is to define an underlying design framework for these models by integrating concepts of associative memory, attentional bias, retention mechanisms, and online optimization. Key methodology involves proposing MIRAS, characterized by four design choices (memory architecture, attentional bias, retention gate, memory algorithm), and developing three new models (MONETA, YAAD, MEMORA) using novel biases (e.g., lp-loss, Huber) and retention gates (e.g., Lq, KL divergence). Primary results demonstrate that these MIRAS variants outperform state-of-the-art baselines on language modeling, commonsense reasoning, and recall tasks, with the 1.3B parameter YAAD model achieving 15.18 perplexity on Wikitext compared to 18.53 for Transformer++. For AI practitioners, the principal implication is that MIRAS offers a structured method to design sequence backbones by explicitly selecting attentional bias and retention mechanisms beyond standard L2/dot-product approaches, enabling targeted optimization for tasks demanding specific capabilities like long-context handling or robustness. |
| Tokenize Image Patches: Global Context Fusion for Effective Haze Removal
  in Large Images (Read more on [arXiv](https://arxiv.org/abs/2504.09621) or [HuggingFace](https://huggingface.co/papers/2504.09621))| Kaiqi Li, Qizhi Xu, Jiuchen Chen, fengyanzi | This paper introduces DehazeXL, an efficient end-to-end method for removing haze from large, high-resolution images by balancing global context and local features. The main objective is to overcome GPU memory limitations typically encountered when processing large images for dehazing, without resorting to performance-degrading slicing or downsampling. DehazeXL partitions the input image into patches, encodes them locally, fuses global context using an efficient global attention bottleneck inspired by large language models, and decodes patches asynchronously in mini-batches. Key results demonstrate that DehazeXL can process images up to 10240x10240 pixels using only 21 GB of GPU memory (FP16 inference), achieving state-of-the-art PSNR (32.35) and SSIM (0.9863) on the introduced 8KDehaze dataset. For AI practitioners, the primary implication is a validated, memory-efficient architecture enabling the application of complex image restoration models to ultra-high-resolution inputs on mainstream hardware, crucial for remote sensing or surveillance applications. |
| Thought Manipulation: External Thought Can Be Efficient for Large
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.13626) or [HuggingFace](https://huggingface.co/papers/2504.13626))| Wenhan Dong, Zifan Peng, Zhen Sun, Jingyi Zheng, Yule Liu | This paper introduces ThoughtMani, a training-free pipeline using external Chain-of-Thought (CoT) from smaller models to improve the inference efficiency of Large Reasoning Models (LRMs). The main objective is to mitigate LRM "overthinking" and reduce computational costs by bypassing unnecessary reasoning steps without fine-tuning. The core methodology involves inserting CoTs generated by a smaller model between specific thinking tokens (`<think>`, `</think>`) in the LRM's prompt to guide its generation. Key results demonstrate significant efficiency gains; for example, applying ThoughtMani to QwQ-32B on LiveBench/Code reduced output token counts by approximately 30% while maintaining performance and improving safety alignment by an average of 10%. For AI practitioners, ThoughtMani provides a practical, low-overhead method to make LRMs more computationally efficient and accessible for real-world applications, particularly when deploying different model sizes concurrently. |
