

## Papers for 2025-05-02

| Title | Authors | Summary |
|-------|---------|---------|
| A Survey of Interactive Generative Video (Read more on [arXiv](https://arxiv.org/abs/2504.21853) or [HuggingFace](https://huggingface.co/papers/2504.21853))| Xintao Wang, Quande Liu, Haoxuan Che, Yiran Qin, Jiwen Yu | This paper surveys the emerging field of Interactive Generative Video (IGV), defining it and outlining its applications. The main objective is to provide a comprehensive overview of IGV technology, survey its application landscape (gaming, embodied AI, autonomous driving), and propose a systematic framework to guide future development. The methodology involves synthesizing existing literature on video generation and interactive systems, classifying current IGV models (shown evolving since 2020 in Fig 1), and proposing a novel five-module framework (Generation, Control, Memory, Dynamics, Intelligence). Primary results include the proposed five-module IGV framework, an analysis identifying key technical challenges such as achieving real-time generation and ensuring long-term coherence, and a categorization of existing IGV methods across different application domains (Tables 1-3). The principal implication for AI practitioners is the provision of a structured framework to decompose the complex problem of IGV, enabling systematic development and targeted research into specific module challenges like control generalization or physics simulation for interactive AI systems. |
| DeepCritic: Deliberate Critique with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.00662) or [HuggingFace](https://huggingface.co/papers/2505.00662))| Ji-Rong Wen, Yankai Lin, Jingwen Chen, Keven16 | This paper introduces DeepCritic, a two-stage framework enhancing LLM critique abilities for mathematical reasoning. The objective is to develop LLM critics capable of deliberate, step-wise critiques with multi-perspective verification and meta-critiquing, addressing the superficiality of existing critics. The methodology involves supervised fine-tuning on 4.5K generated long-form critiques, followed by reinforcement learning using PRM800K data or Monte Carlo sampling-based annotations. The resulting DeepCritic-7B-RL-PRM800K model achieves a 67.1 average F1 score on error identification benchmarks, outperforming models like GPT-4o and same-sized DeepSeek-R1-distill models. For AI practitioners, this demonstrates a method to create more accurate automated supervision models that provide detailed feedback, improving LLM generator refinement and potentially enabling weak-to-strong supervision. |
| T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level
  and Token-level CoT (Read more on [arXiv](https://arxiv.org/abs/2505.00703) or [HuggingFace](https://huggingface.co/papers/2505.00703))| Hao Li, Zhuofan Zong, Renrui Zhang, Ziyu Guo, Dongzhi Jiang | T2I-R1 introduces a reasoning-enhanced text-to-image generation model using reinforcement learning with a bi-level Chain-of-Thought (CoT) process. The primary objective is to improve generation quality and prompt alignment by explicitly coordinating high-level semantic planning (semantic-level CoT) and low-level pixel processing (token-level CoT). The core methodology employs BiCoT-GRPO, a novel reinforcement learning framework that jointly optimizes both CoT levels within a Unified Large Multimodal Model (ULM) using group-relative rewards from an ensemble of vision expert models. T2I-R1 demonstrates superior performance, achieving a 13% improvement over its baseline on T2I-CompBench and surpassing the state-of-the-art FLUX.1 model. For AI practitioners, this work highlights that integrating and explicitly optimizing multi-level reasoning processes (planning and step-by-step generation) via RL within unified models can significantly boost performance and robustness in complex generative tasks. |
| AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2504.21659) or [HuggingFace](https://huggingface.co/papers/2504.21659))| Rui Liu, Jinluan Yang, Yibo Wang, Haiying He, Haotian Luo | AdaR1 introduces a bi-level optimization method for LLMs to adaptively switch between Long-CoT and Short-CoT reasoning, enhancing efficiency without sacrificing performance. The main objective is to overcome the high inference cost of Long-CoT by tailoring reasoning depth to input problem complexity. Key methodology involves merging long and short CoT models and then applying Bi-Level Preference Training (using DPO) to optimize reasoning path selection at both group (style) and instance (conciseness) levels. Primary results demonstrate a significant reduction in reasoning length (over 50% on average across five math datasets) while largely maintaining accuracy compared to Long-CoT baselines (-1.65% accuracy change with -50.93% length reduction for the 7B model). For AI practitioners, this approach offers a way to deploy powerful reasoning models more efficiently by dynamically allocating computational resources based on task demands, improving feasibility in latency-sensitive or resource-constrained environments. |
| KeySync: A Robust Approach for Leakage-free Lip Synchronization in High
  Resolution (Read more on [arXiv](https://arxiv.org/abs/2505.00497) or [HuggingFace](https://huggingface.co/papers/2505.00497))| Konstantinos Vougioukas, Michał Stypułkowski, Stella Bounareli, Rodrigo Mira, Antoni Bigata | KeySync introduces a two-stage latent diffusion framework for high-resolution, leakage-free, and occlusion-robust lip synchronization. The research objective is to overcome common lip-sync limitations including temporal inconsistency, expression leakage from source video, and poor occlusion handling, especially in cross-synchronization tasks. Key methodology involves keyframe generation and interpolation via a diffusion model conditioned on audio embeddings and carefully masked video latents, augmented by an inference-time occlusion handling pipeline using video segmentation. KeySync achieves state-of-the-art cross-synchronization performance, notably obtaining a LipScore of 0.48 while significantly reducing expression leakage to a LipLeak score of 0.16, outperforming existing methods quantitatively (Elo: 1145). For AI practitioners, this provides a robust model for high-fidelity applications like automated dubbing, offering temporally coherent output that minimizes leakage and handles occlusions effectively. |
| TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.20605) or [HuggingFace](https://huggingface.co/papers/2504.20605))| Laura Diosan, andreeatomescu, andreiPiscoran, mihainadas | This paper presents TF1-EN-3M, a dataset of 3 million synthetic English moral fables generated using open-weight language models under 8B parameters. The research investigates the effectiveness of combinatorial prompt expansion for generating diverse, high-quality fables with resource-constrained LLMs and identifies optimal models. Methodology involved a 6-slot prompt template (character, trait, setting, conflict, resolution, moral) and a hybrid evaluation pipeline combining GPT-based critique with reference-free diversity/readability metrics. Results show Llama-3.1-8B-Instruct achieved the highest composite score (0.891), producing high-quality fables at a cost of approximately $0.135 per 1000 on consumer hardware. The principal implication for AI practitioners is that large-scale, structured narrative datasets for tasks like moral reasoning can be efficiently created using small, open models, reducing reliance on proprietary systems. |
| LLMs for Engineering: Teaching Models to Design High Powered Rockets (Read more on [arXiv](https://arxiv.org/abs/2504.19394) or [HuggingFace](https://huggingface.co/papers/2504.19394))| Toby Simonds | This research evaluates Large Language Models' (LLMs) effectiveness in high-powered rocket design, showing reinforcement learning (RL) significantly enhances their capabilities. The study's objective was to determine if LLMs can function as effective tools for physical engineering tasks, using high-powered rocketry as a test domain. Methodology involved creating RocketBench, an interface to the RocketPy simulator, evaluating foundation LLMs (Claude 3.7, o1, etc.) via iterative prompting on altitude and landing tasks, and training a 7B parameter Qwen-2.5 model with Group Relative Policy Optimization (GRPO). Key results show that while foundation models plateaued below human expert performance (e.g., max human score 76.57 on altitude), the RL-trained 7B model surpassed both humans and foundation models, achieving a peak score of 95.6 on the precision landing task (vs. 91.6 human expert) and landing within 12 meters accuracy. The principal implication for AI practitioners is that integrating RL with LLMs, leveraging their domain knowledge alongside structured exploration, enables performance exceeding human experts in complex engineering optimization, indicating a promising approach for AI-driven design provided effective simulation interfaces and reward functions exist. |
| MediAug: Exploring Visual Augmentation in Medical Imaging (Read more on [arXiv](https://arxiv.org/abs/2504.18983) or [HuggingFace](https://huggingface.co/papers/2504.18983))| Lei Zhang, Hao Zhang, Canxuan Gang, Zeyu Zhang, Xuyin Qi | MediAug introduces a benchmark evaluating six mix-based data augmentation methods on medical image classification using CNN and Transformer backbones. The objective was to systematically assess the performance of MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix on brain tumor MRI and eye disease fundus datasets to address the domain gap and fragmented prior research. A unified framework applied these augmentations to train and evaluate ResNet-50 and ViT-B models on the two datasets. Results demonstrated significant variability: MixUp achieved the highest accuracy for ResNet-50 on brain tumors (79.19%), while SnapMix was best for ViT-B (99.44%); YOCO (ResNet-50, 91.60%) and CutMix (ViT-B, 97.94%) excelled on eye diseases. The principal implication for AI practitioners is that the optimal mix-based augmentation strategy is highly dependent on the specific combination of backbone architecture (CNN vs. Transformer) and the medical imaging task/dataset. |
