

## Papers for 2025-05-09

| Title | Authors | Summary |
|-------|---------|---------|
| On Path to Multimodal Generalist: General-Level and General-Bench (Read more on [arXiv](https://arxiv.org/abs/2505.04620) or [HuggingFace](https://huggingface.co/papers/2505.04620))| Gh0stAR, ChocoWu, LXT, JunchengLi, scofield7419 | This paper introduces General-Level, a 5-level taxonomy, and General-Bench, a massive benchmark, to evaluate multimodal large language models (MLLMs) based on their synergistic capabilities across comprehension, generation, and modalities. The research aims to establish a sophisticated evaluation framework that assesses MLLMs not just on task performance but on their "synergy effect"—the ability for knowledge in one modality/task to enhance others—as a truer measure of generalist intelligence towards Artificial General Intelligence (AGI). The General-Level framework classifies MLLMs into five levels, with progression requiring increasing synergy, defined by outperforming State-of-the-Art (SoTA) specialists on tasks within the General-Bench, which comprises over 700 tasks and 325,800 instances across image, video, audio, 3D, and language. Evaluation of over 100 MLLMs on General-Bench revealed that most lack the cross-task or cross-modal synergy for higher General-Level classifications; for instance, only 3 models (Mini-Gemini, Emu2-37B, Vitron-V1) achieved Level 4, with Mini-Gemini scoring 1.56, and no model reached Level 5 (total synergy). AI practitioners can use General-Level and General-Bench to rigorously assess and compare MLLM synergistic abilities, providing a roadmap for developing more robust generalists that can better integrate and transfer knowledge across diverse multimodal inputs and tasks, a critical step towards AGI. |
| Perception, Reason, Think, and Plan: A Survey on Large Multimodal
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.04921) or [HuggingFace](https://huggingface.co/papers/2505.04921))| imryanxu, xyidealist, TerenceL-TL, foggyforest, YunxinLi | This survey details the evolution of Large Multimodal Reasoning Models (LMRMs) across four developmental stages, identifies current limitations, and proposes a future direction towards Native LMRMs (N-LMRMs) capable of integrated omni-modal perception, agentic reasoning, and generative capabilities. The paper aims to provide a comprehensive, structured review of multimodal reasoning research, analyze the entire roadmap from early modular designs to state-of-the-art LMRMs, and project future developments for next-generation systems. The research employs a survey methodology, synthesizing over 540 publications to delineate a four-stage developmental roadmap (Perception-Driven Modular, Language-Centric Short, Language-Centric Long, and prospective Native LMRMs), supported by analysis of existing models, benchmarks, and experimental insights from models like OpenAI's O3 and O4-mini. Current LMRMs, while advancing, show significant limitations in omni-modal generalization and agentic behavior; for instance, on the OmniMMI benchmark, even commercial models like Gemini-1.5-Pro and GPT-4o achieve less than 20% average accuracy, and performance drops further on tasks requiring unified understanding across multiple modalities. AI practitioners should focus on developing N-LMRMs with unified architectures for heterogeneous modalities, interleaved multimodal reasoning, and continuous learning from interaction, as current language-centric LMRMs are insufficient for complex, real-world omni-modal and agentic tasks. |
| Flow-GRPO: Training Flow Matching Models via Online RL (Read more on [arXiv](https://arxiv.org/abs/2505.05470) or [HuggingFace](https://huggingface.co/papers/2505.05470))| dizhang, Xintao, CheeryLJH, Lp256, liuhuohuo | Flow-GRPO introduces online reinforcement learning (RL) to flow matching models by converting deterministic Ordinary Differential Equations (ODEs) to equivalent Stochastic Differential Equations (SDEs) for stochastic exploration and employing a Denoising Reduction strategy for efficient training. The main objective is to effectively integrate online RL, specifically Group Relative Policy Optimization (GRPO), with flow matching generative models to enhance their capabilities in complex text-to-image (T2I) tasks, such as compositional understanding and text rendering, while maintaining image quality and sampling efficiency. The key methodology involves two strategies: (1) an ODE-to-SDE conversion that transforms the model's deterministic generative process into a stochastic one, matching the original model's marginal distribution at all timesteps to enable statistical sampling for RL exploration, and (2) a Denoising Reduction strategy that reduces the number of denoising steps during RL training (e.g., 10 steps) compared to inference (e.g., 40 steps) to improve sampling efficiency. Flow-GRPO demonstrated significant improvements across multiple T2I tasks; notably, for complex compositions, the RL-tuned SD3.5-Medium model increased GenEval accuracy from 63% to 95%, while visual text rendering accuracy improved from 59% to 92%, with little to no reward hacking observed. The principal implication for AI practitioners is that online RL can be effectively applied to state-of-the-art flow matching models to enhance specific generation capabilities and align with human preferences by introducing stochasticity via SDE conversion and accelerating training through denoising reduction, with Kullback-Leibler (KL) constraints proving vital for preventing performance degradation in general image quality. |
| Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.02847) or [HuggingFace](https://huggingface.co/papers/2505.02847))| Peisong Wang, Qingxuan Jiang, Bang Zhang, zptu, vvibt | This paper introduces Sentient Agent as a Judge (SAGE), an automated framework using an LLM-powered agent to evaluate higher-order social cognition in large language models by simulating human-like emotional responses and inner thoughts. The primary objective is to develop a robust method for assessing LLMs' abilities to understand and respond to human emotions and intentions in multi-turn dialogues, moving beyond mere textual competence. SAGE employs a "Sentient Agent," instantiated with a persona, background, goals, and hidden intentions, which interacts with the LLM being tested; this agent uses multi-hop reasoning to simulate emotional changes (quantified as an emotion score trajectory) and generate contextually appropriate responses. The Sentient emotion score from SAGE shows strong correlation with human-centric psychological metrics like the Barrett-Lennard Relationship Inventory (BLRI) (Pearson r = 0.818) and utterance-level empathy (Pearson r = 0.788), and its leaderboard reveals GPT-40-Latest achieved the top Sentient score of 79.9. For AI practitioners, SAGE offers a principled and scalable tool to benchmark progress towards genuinely empathetic LLMs, with findings like GPT-40-Latest achieving a top Sentient score (79.9) with high token efficiency (3.3K tokens), indicating an advance in socially adept AI development. |
| Scalable Chain of Thoughts via Elastic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.05315) or [HuggingFace](https://huggingface.co/papers/2505.05315))| cxiong, JunnanLi, doyensahoo, hendrydong, yuhuixu | The paper introduces Elastic Reasoning, a framework for large reasoning models to produce scalable chain-of-thought outputs under strict inference budgets by separating reasoning into 'thinking' and 'solution' phases with independent budgets and training via budget-constrained rollouts. Its main objective is to enable robust and efficient reasoning from these models when faced with limited computational resources at inference time. The core methodology combines separate budgeting for thinking and solution generation at inference with a GRPO-based training strategy that simulates budget exhaustion, teaching the model to adapt to incomplete reasoning. Key results demonstrate that an E1-Math-1.5B model, trained with significantly fewer steps (200 vs. 700-820 for baselines), achieves 35.0% accuracy on AIME2024 with a 2K token budget, outperforming baselines, and reduces token usage by 32.1% in unconstrained settings compared to the original model while maintaining comparable performance. For AI practitioners, Elastic Reasoning offers a practical approach to deploy advanced reasoning models in resource-constrained environments by providing fine-grained control over inference costs without substantial performance loss or extensive retraining overhead. |
| FG-CLIP: Fine-Grained Visual and Textual Alignment (Read more on [arXiv](https://arxiv.org/abs/2505.05071) or [HuggingFace](https://huggingface.co/papers/2505.05071))| DaweiLiang, jinchenglijc, fanjing, binwang, xiechunyu | FG-CLIP is a model that significantly enhances fine-grained visual and textual understanding in multimodal systems. The primary objective was to address the limitations of existing CLIP-like models in comprehending detailed visual attributes and relationships, which often struggle due to coarse-grained input and a lack of region-specific alignment. FG-CLIP's methodology involves three key innovations: generating 1.6 billion long caption-image pairs for global semantic detail, constructing a dataset of 12 million images with 40 million region-specific bounding boxes aligned with detailed captions, and incorporating 10 million hard fine-grained negative samples, all trained using a two-stage process with an extended text encoder capacity. Extensive experiments demonstrate FG-CLIP's superiority; for instance, FG-CLIP (ViT-L/14) achieved 48.4% accuracy on the fine-grained understanding benchmark FG-OVD (hard subset), substantially outperforming the original CLIP's 15.4%. The principal implication for AI practitioners is that leveraging large-scale, meticulously curated datasets with detailed long captions, region-level annotations, and challenging negative samples is crucial for advancing the nuanced understanding and discriminative power of multimodal models, particularly for tasks requiring fine-grained distinctions. |
| 3D Scene Generation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2505.05474) or [HuggingFace](https://huggingface.co/papers/2505.05474))| Fangzhou Hong, liuziwei7, FrozenBurning, hzxie, wenbc21 | This survey systematically reviews and categorizes state-of-the-art 3D scene generation techniques, analyzing their foundations, trade-offs, datasets, and applications. The paper's objective is to provide a comprehensive overview of 3D scene generation, organizing existing approaches and identifying current challenges and future research directions at the intersection of generative AI, 3D vision, and embodied intelligence. The authors surveyed and classified existing methods into four main paradigms—procedural generation, neural 3D-based generation, image-based generation, and video-based generation—analyzing their technical foundations, trade-offs, and representative results, along with datasets and evaluation protocols. The survey highlights a significant growth in the field, particularly noting that in 2024, neural 3D-based generation and video-based generation methods saw 93 and 61 publications respectively (Figure 1, with 2025 data being partial up to April 30th). For AI practitioners, this work offers a structured guide to current 3D scene generation methods, their comparative strengths (summarized in Table 1), common datasets (Table 3), and evaluation protocols, facilitating informed decision-making for developing applications in areas such as immersive media, robotics, and autonomous driving. |
| ICon: In-Context Contribution for Automatic Data Selection (Read more on [arXiv](https://arxiv.org/abs/2505.05327) or [HuggingFace](https://huggingface.co/papers/2505.05327))| Zhifang Sui, soliz1998, yaolily, Rsy24, yyxsghx | The paper introduces ICON, a gradient-free method leveraging in-context learning (ICL) to automatically select high-contribution data for LLM instruction tuning, enhancing performance while reducing computational costs. The primary objective is to develop an efficient automated data selection method for instruction tuning that measures individual sample contribution without costly gradient computations or manually designed heuristics. ICON quantifies sample contribution by assessing performance shifts (via perplexity changes) on a diverse assessment set when a candidate sample is included in an ICL prompt, then uses these "ICON scores" to train a lightweight LoRA-based selection model. On LLaMA3.1-8B, training on 15% of ICON-selected Alpaca data outperformed full dataset training by 5.42 percentage points on average across 12 benchmarks and surpassed the best prior selection methods by 2.06 percentage points. AI practitioners can use ICON for more efficient and effective instruction tuning dataset curation, as it demonstrates that smaller, carefully selected subsets comprising diverse and appropriately difficult samples can yield superior model performance with significantly reduced computational overhead. |
| LiftFeat: 3D Geometry-Aware Local Feature Matching (Read more on [arXiv](https://arxiv.org/abs/2505.03422) or [HuggingFace](https://huggingface.co/papers/2505.03422))| Jinchi Zhu, Yuxuan Xiong, Zhou Zhao, Wenpeng Lai, pengliu123 | i) The paper introduces LiftFeat, a lightweight network integrating 3D geometric features to enhance 2D local feature matching robustness. ii) The main objective is to improve the discriminative ability of 2D feature descriptors under extreme conditions by incorporating 3D geometric information. iii) The methodology involves extracting 3D geometric features supervised by pseudo surface normal labels derived from monocular depth estimation and fusing these with 2D descriptors using a 3D Geometry-aware Feature Lifting module. iv) Experimental results show LiftFeat outperforms other lightweight methods on relative pose estimation, homography estimation, and visual localization, and runtime tests confirm that the method can achieve inference latency of 7.4 ms on edge devices. v) LiftFeat offers AI practitioners a computationally efficient method for enhancing feature matching in challenging scenarios by leveraging readily available 3D geometric context, which is helpful to integrate into robotic applications.  |
| X-Reasoner: Towards Generalizable Reasoning Across Modalities and
  Domains (Read more on [arXiv](https://arxiv.org/abs/2505.03981) or [HuggingFace](https://huggingface.co/papers/2505.03981))| RustyArchimedes, sidkiblawi, hiaoxui, shengz, qianchu | This paper introduces X-REASONER, a vision-language model that achieves strong generalizable reasoning across modalities and domains through post-training solely on general-domain text. The research investigates whether reasoning capabilities can be effectively generalized across different input modalities and specialized domains using only general-domain text-based post-training. X-REASONER employs a two-stage post-training process: supervised fine-tuning (SFT) on general-domain text with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards (RLVR) using mathematical text. Despite text-only training, X-REASONER surpasses prior 7B models trained with multimodal data on benchmarks like MMMU (Val) (56.4% vs. 55.0% SOTA) and MMMU-Pro (43.0% vs. 40.7% SOTA), while its medical-specialized variant, X-REASONER-MED, achieves new SOTA on medical tasks. The principal implication for AI practitioners is that carefully designed text-only post-training can be a highly data-efficient strategy to imbue models with robust, transferable reasoning skills, potentially reducing reliance on expensive multimodal or in-domain datasets. |
| Generating Physically Stable and Buildable LEGO Designs from Text (Read more on [arXiv](https://arxiv.org/abs/2505.05469) or [HuggingFace](https://huggingface.co/papers/2505.05469))| junyanz, devakramanan, RLCMU, kangled, AvaLovelace | i) The paper introduces LEGOGPT, an autoregressive model for generating physically stable and buildable LEGO designs from text prompts. ii) The primary objective is to generate LEGO brick models from text while ensuring physical stability and buildability. iii) The methodology involves constructing a dataset of LEGO designs with associated captions and training an autoregressive large language model to predict the next brick via next-token prediction, incorporating physics-aware constraints during training and inference. iv) The method achieves 98.8% stability on generated LEGO structures and outperforms other baselines in mean brick stability and CLIP score. v) LEGOGPT provides AI practitioners with a framework integrating language models and physics constraints for generating realizable 3D structures directly from text, enhancing design automation and robotic assembly applications.  |
| Crosslingual Reasoning through Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2505.05408) or [HuggingFace](https://huggingface.co/papers/2505.05408))| JuliaKreutzerCohere, gentaiscool, Muennighoff, MJonibek, yongzx | This research demonstrates that scaling test-time inference compute for English-centric reasoning language models (RLMs) substantially improves their multilingual mathematical reasoning, though this benefit is domain-specific and less effective for low-resource languages. The study investigates the extent to which English-centric RLMs, finetuned with long chain-of-thoughts, can generalize reasoning capabilities across diverse languages and domains by scaling inference-time compute. The authors evaluated s1 models (Qwen2.5-Instruct finetuned on 1k English STEM samples) across various sizes on multilingual benchmarks (e.g., MGSM, Global-MMLU), analyzing the effects of increased thinking tokens, language forcing strategies, and emergent language-mixing patterns like "quote-and-think." Crosslingual test-time scaling significantly improves multilingual math reasoning for models ≥3B parameters (e.g., a 14B s1 model showed a +Δ9.4% average accuracy gain on MGSM with more thinking tokens), often outperforming larger baseline models; however, models show poor out-of-domain generalization from STEM to cultural commonsense reasoning. Practitioners should consider test-time compute scaling for English-centric RLMs (≥3B) to enhance multilingual reasoning in high-resource languages for STEM tasks, but recognize its limitations for low-resource languages and out-of-domain applications, where specialized multilingual training data is still crucial. |
| PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes (Read more on [arXiv](https://arxiv.org/abs/2505.05288) or [HuggingFace](https://huggingface.co/papers/2505.05288))| abdo-eldesokey, zuluquebec, Aileron, Filippo8, Samir55 | This paper introduces PlaceIt3D, a novel task, benchmark, and dataset for language-guided 3D object placement in real scenes, along with a baseline method called PlaceWizard. The main objective is to develop a system that can find a physically plausible and semantically correct 3D position and orientation for an asset in a scene based on a natural language description, addressing the ambiguity of multiple valid solutions and complex 3D spatial reasoning. The proposed PlaceWizard method utilizes a point encoder for scene features, uniform spatial pooling, a pre-trained Point-BERT for asset encoding, and a Large Language Model (LLM) that processes these features and the text prompt to predict placement location, anchor objects (auxiliary), and rotation masks via specialized decoder heads. Primary results show PlaceWizard achieved a global constraint accuracy of 52.6% and a complete placement success rate of 29.4% on the new benchmark, significantly outperforming an adapted Reason3D baseline which scored 40.6% global constraint accuracy and 18.1% complete placement success. The principal implication for AI practitioners is that PlaceIt3D provides a challenging new benchmark and dataset for evaluating 3D LLMs, fostering the development of AI agents with enhanced capabilities for understanding and interacting with 3D environments based on natural language, crucial for robotics and AR/VR applications. |
| BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language
  Models in Chinese (Read more on [arXiv](https://arxiv.org/abs/2504.19314) or [HuggingFace](https://huggingface.co/papers/2504.19314))| Bruce Leon, HawkFaust, yeeeqichen99, MindYing, PALIN2018 | This paper introduces BrowseComp-ZH, a benchmark for evaluating the web browsing ability of Large Language Models (LLMs) in the Chinese language environment. The primary objective is to assess LLM agents on the Chinese web, considering its unique linguistic, infrastructural, and censorship-related complexities often overlooked by English-centric benchmarks. The methodology involves 289 reverse-engineered multi-hop questions spanning 11 diverse domains, subjected to a two-stage quality control protocol to ensure high difficulty and answer uniqueness, which were used to benchmark over 20 state-of-the-art LLMs and agentic search systems. Key results demonstrate that most models perform poorly, with many achieving accuracy rates below 10% and even the best-performing system, OpenAI's DeepResearch, reaching only 42.9% accuracy. For AI practitioners, this highlights a critical need to enhance LLMs' capabilities in effective retrieval, sophisticated reasoning, and information reconciliation to master complex web browsing tasks, particularly in non-English information ecosystems. |
| Chain-of-Thought Tokens are Computer Program Variables (Read more on [arXiv](https://arxiv.org/abs/2505.04955) or [HuggingFace](https://huggingface.co/papers/2505.04955))| Zhifang Sui, peiyiwang89, soliz1998 | i) This paper investigates the role of chain-of-thought (CoT) tokens in large language models (LLMs), proposing that they function similarly to variables in computer programs. ii) The research objective is to empirically study the function of CoT tokens, and whether they store intermediate values used in subsequent computations. iii) The methodology involves fine-tuning Qwen-2.5-1.5B on multi-digit multiplication and dynamic programming tasks, intervening on CoT tokens, and merging them into latent tokens to evaluate performance. iv) Results show that removing non-result tokens from CoT causes little performance drop, and the performance decreases by 9% when latent tokens store larger numbers in 4*5 DP problems indicating a computational complexity limit. v) The implication for AI practitioners is that LLMs treat CoT tokens similarly to program variables, so alternative forms of CoT should be explored to design more concise and efficient reasoning processes.  |
