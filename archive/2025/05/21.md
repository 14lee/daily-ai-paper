

## Papers for 2025-05-21

| Title | Authors | Summary |
|-------|---------|---------|
| Emerging Properties in Unified Multimodal Pretraining (Read more on [arXiv](https://arxiv.org/abs/2505.14683) or [HuggingFace](https://huggingface.co/papers/2505.14683))| Ziang, codecaution, whyu, gouc, Andy1621 | i) The paper introduces BAGEL, an open-source multimodal foundation model for understanding and generation. ii) The main objective is to create a unified model that natively supports both multimodal understanding and generation through pretraining on diverse interleaved data. iii) The methodology involves pretraining a decoder-only model on trillions of tokens curated from interleaved text, image, video, and web data using a Mixture-of-Transformer-Experts architecture. iv) BAGEL outperforms open-source VLMs on multimodal understanding benchmarks and achieves text-to-image quality competitive with state-of-the-art generators. v) AI practitioners can utilize BAGEL as a foundational model for developing advanced multimodal applications, leveraging its capabilities in tasks such as free-form image manipulation, future frame prediction, and 3D manipulation.  |
| SageAttention3: Microscaling FP4 Attention for Inference and An
  Exploration of 8-Bit Training (Read more on [arXiv](https://arxiv.org/abs/2505.11594) or [HuggingFace](https://huggingface.co/papers/2505.11594))| surfingtomchen, whx1003, haofeng666, Guyan, jt-zhang | i) This paper introduces SageAttention3, an FP4 attention mechanism for inference acceleration, and explores 8-bit attention (SageBwd) for training. ii) The primary objective is to enhance the efficiency of attention mechanisms through low-bit quantization for both inference and training tasks. iii) The methodology involves implementing FP4 microscaling quantization for inference and designing a trainable 8-bit attention mechanism for forward and backward propagation in training. iv) SageAttention3 achieves 1038 TOPS on RTX5090, a 5x speedup over FlashAttention for inference; SageBwd achieves lossless performance in fine-tuning tasks but slower convergence in pretraining. v) The FP4 attention and the 8-bit training exploration offer AI practitioners new approaches to accelerate inference and fine-tuning, respectively, for large models, although the suitability for pre-training tasks needs further investigation.  |
| VisualQuality-R1: Reasoning-Induced Image Quality Assessment via
  Reinforcement Learning to Rank (Read more on [arXiv](https://arxiv.org/abs/2505.14460) or [HuggingFace](https://huggingface.co/papers/2505.14460))| Kede Ma, Lei Zhang, Jie Liang, Jian Zou, TianheWu | VisualQuality-R1 introduces a reasoning-induced no-reference image quality assessment model trained via reinforcement learning to rank. The paper aims to improve image quality assessment by leveraging reasoning capabilities and addressing the relative nature of visual quality. Group relative policy optimization is used to generate multiple quality scores, and comparative probabilities are calculated using the Thurstone model. Experiments show VisualQuality-R1 outperforms existing models, achieving an average SRCC of 0.791 and PLCC of 0.831 across KADID-10K and SPAQ datasets in multi-dataset training. These results indicate an enhanced ability to generalize across distortion scenarios and provide contextually rich quality descriptions. The code for the project is available at https://github.com/TianheWu/VisualQuality-R1.  |
| Visual Agentic Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14246) or [HuggingFace](https://huggingface.co/papers/2505.14246))| sweetFruit, steins1096, zyshan, yuhangzang, ziyuliu | Visual-ARFT is presented as a method for improving reasoning in Large Vision-Language Models (LVLMs) through reinforcement fine-tuning with external tools. The research aims to enhance LVLMs' capabilities to use tools like web browsers for searching and code execution for image manipulation. Visual-ARFT uses a reward-driven training strategy and the Group Relative Policy Optimization (GRPO) algorithm. Experiments show Visual-ARFT outperforms baselines, achieving +18.6% F1 / +13.0% EM on the MAT-Coding benchmark and surpasses GPT-40 on this task, furthermore, it demonstrates generalization with +29.3% F1 / +25.9% EM on multi-hop QA benchmarks. Visual-ARFT offers AI practitioners a method for building more robust and generalizable multimodal agents.  |
| The Aloe Family Recipe for Open and Specialized Healthcare LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.04388) or [HuggingFace](https://huggingface.co/papers/2505.04388))| annariasdu, pabberpe, danihinjos, adriantormos, JordiBayarri-bsc | This paper introduces Aloe Beta, an open-source family of healthcare Large Language Models (LLMs). The research explores optimization of data preprocessing and training to create competitive medical LLMs, including model safety and efficacy enhancements. Key methods involve custom datasets, Direct Preference Optimization (DPO), and Retrieval-Augmented Generation (RAG). Results show competitive performance on healthcare benchmarks, with enhanced safety against jailbreaking attacks; the Qwen2.5-Aloe-Beta-72B model achieves top performance among open models on MCQA tasks. Aloe Beta offers a top-performing, ethically aligned open-source option for AI practitioners in healthcare, with the models and datasets made available under a permissive license.  |
| Optimizing Anytime Reasoning via Budget Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.13438) or [HuggingFace](https://huggingface.co/papers/2505.13438))| Wee Sun Lee, duchao, P2333, lkevinzc, QPHutu | i) This paper introduces AnytimeReasoner, a framework to optimize anytime reasoning performance in large language models (LLMs) by maximizing rewards under varying token budget constraints. ii) The primary research objective is to improve token efficiency and reasoning flexibility of LLMs under different resource constraints. iii) The key methodology involves truncating the thinking process at sampled token budgets, introducing verifiable dense rewards and employing Budget Relative Policy Optimization (BRPO) to improve advantage estimation. iv) Empirical results on mathematical reasoning tasks demonstrate AnytimeReasoner consistently outperforms GRPO across all thinking budgets and enhances both training and token efficiency; for AIME2024 there is an accuracy of 32.7% compared to MRT's reported 30.3%. v) The principal implication for AI practitioners is a more efficient and flexible approach for deploying LLMs in resource-constrained environments, where performance must be maintained even with limited computational budgets.  |
| Latent Flow Transformer (Read more on [arXiv](https://arxiv.org/abs/2505.14513) or [HuggingFace](https://huggingface.co/papers/2505.14513))| Pei-Chen Ho, dsshiu, menghsichen, FengTing, yenchen | i) The paper introduces the Latent Flow Transformer (LFT) for efficient language modeling, replacing transformer blocks with learned transport operators trained via flow matching. ii) The research objective is to reduce the parameter and compute cost of large language models (LLMs) by compressing layers using flow-based methods. iii) The methodology involves training a single transformer-like layer to learn a velocity field that maps latent states across multiple transformer layers, guided by a novel "Recoupling Ratio" metric for layer selection, with the proposed Flow Walking (FW) algorithm for trajectory learning. iv) The experiments on Pythia-410M demonstrate that LFT trained with flow matching can compress 6 of 24 layers and achieve a KL divergence of LM logits at 0.407, outperforming skipping 2 layers (KL divergence of 0.529). v) LFT provides AI practitioners with a new structural compression technique that leverages flow-based learning for efficient LLM design, potentially reducing model size while retaining performance.  |
| Neurosymbolic Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2505.13138) or [HuggingFace](https://huggingface.co/papers/2505.13138))| Antonio Vergari, ducdauge, pminervini, HEmile | i) This paper introduces neurosymbolic diffusion models (NESYDMs) to address limitations of independence assumptions in neurosymbolic predictors. ii) The research objective is to develop a neurosymbolic predictor that models dependencies between extracted symbols to improve uncertainty quantification and out-of-distribution generalization. iii) The methodology involves a discrete diffusion process that reuses the independence assumption from NeSy predictors at each diffusion step, enabling scalable learning while modeling symbol dependencies, which is trained with a derived continuous-time loss function. iv) Results on visual path planning demonstrate that NESYDMs achieve a state-of-the-art accuracy of 97.40% on a 30x30 grid, surpassing existing NeSy predictors, and demonstrate strong calibration on RSBench tasks. v) AI practitioners can leverage NESYDMs to build more reliable and generalizable AI systems by modeling dependencies between symbols in neurosymbolic reasoning tasks, particularly in safety-critical applications.  |
| Visionary-R1: Mitigating Shortcuts in Visual Reasoning with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.14677) or [HuggingFace](https://huggingface.co/papers/2505.14677))| Yixuan Li, Peng Gao, kaiyangzhou, yuhangzang, Jiaer-Xia | Visionary-R1 introduces a reinforcement learning framework to improve visual reasoning in VLMs by mitigating shortcut learning. The research aims to train VLMs, using only question-answer pairs and reinforcement learning, to perform reasoning on image data without explicit chain-of-thought supervision. The methodology involves training the model to generate a detailed image caption before reasoning and answering (caption-reason-answer). Experiments demonstrate that Visionary-R1 achieves improved performance, outperforming models like GPT-40 on MathVista by 7.9%. The principal implication is that enforcing visual understanding through captioning prior to reasoning enhances the generalizability of VLMs, offering a method for AI practitioners to develop more robust visual reasoning systems.  |
| General-Reasoner: Advancing LLM Reasoning Across All Domains (Read more on [arXiv](https://arxiv.org/abs/2505.14652) or [HuggingFace](https://huggingface.co/papers/2505.14652))| wenhu, zhangysk, DongfuJiang, SivilTaram, MrLight | i) The paper introduces GENERAL-REASONER, a new training paradigm to enhance LLM reasoning across diverse domains beyond mathematics and coding. ii) The main objective is to improve LLM reasoning capabilities in domains with diverse answer representations and limited data. iii) The methodology involves constructing a large-scale dataset of verifiable questions from web crawling and developing a generative model-based answer verifier. iv) Evaluation across 12 benchmarks shows GENERAL-REASONER outperforms baselines, improving MMLU-Pro and SuperGPQA performance by approximately 10%, while preserving mathematical reasoning capabilities. v) The primary implication for AI practitioners is a robust and generalizable LLM reasoning framework that extends beyond traditional mathematical and coding domains, improving model accuracy across a broader range of real-world reasoning tasks.  |
| Reasoning Models Better Express Their Confidence (Read more on [arXiv](https://arxiv.org/abs/2505.14489) or [HuggingFace](https://huggingface.co/papers/2505.14489))| YongilKim, Sunkyoung, soheeyang, seungone, DKYoon | i) Reasoning models exhibit superior confidence calibration compared to non-reasoning models due to slow thinking behaviors. ii) This work investigates whether reasoning models communicate their confidence accurately, specifically if slow thinking behaviors enhance confidence calibration. iii) The study benchmarks six reasoning models against non-reasoning counterparts across six datasets, measuring Expected Calibration Error (ECE), Brier Score, and AUROC. iv) Reasoning models achieved strictly better confidence calibration than non-reasoning models in 33 out of 36 settings; R1-Distill-Qwen exhibits near-perfect calibration above 60% confidence on TriviaQA. v) AI practitioners should consider reasoning models for tasks requiring reliable confidence estimation, as slow thinking enhances the alignment between predicted confidence and actual accuracy, thus potentially increasing trust and reliability of AI systems.  |
| Exploring Federated Pruning for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.13547) or [HuggingFace](https://huggingface.co/papers/2505.13547))| Liangqiong-QU, limingcv, MENGTINGLIU, jcccy, gpx333 | i) The paper introduces FedPrLLM, a federated learning framework for privacy-preserving pruning of large language models (LLMs). ii) The main objective is to address the challenge of pruning LLMs in privacy-sensitive domains without requiring access to public calibration samples. iii) The methodology involves clients calculating a pruning mask matrix based on local calibration data and sharing it with the server, which then aggregates these matrices to prune the global model. iv) Experiments demonstrate that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework, achieving comparable performance to iterative pruning while reducing communication costs. v) The findings suggest that layer comparison is a simple yet effective method for parameter comparison in federated pruning scenarios, indicating a practical approach for deploying compressed LLMs in privacy-sensitive applications.  |
| Reasoning Path Compression: Compressing Generation Trajectories for
  Efficient LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.13866) or [HuggingFace](https://huggingface.co/papers/2505.13866))| Jae-Joon Kim, YulhwaKim, dongwonjo, jiwonsong | Reasoning Path Compression (RPC) accelerates inference in reasoning-focused Large Language Models (LLMs) by compressing KV caches. The paper addresses the problem of increased memory usage and reduced throughput in LLMs due to long reasoning paths. RPC periodically compresses the KV cache, retaining high-importance scores, calculated using a selector window of recent queries. Experiments with QwQ-32B show up to 1.60× improvement in generation throughput with a 1.2% accuracy drop on the AIME 2024 benchmark. This method's implication is that exploiting semantic sparsity in reasoning traces can improve LLM deployment efficiency, offering a training-free method that is straightforward to integrate into existing pipelines.  |
| NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search (Read more on [arXiv](https://arxiv.org/abs/2505.14680) or [HuggingFace](https://huggingface.co/papers/2505.14680))| Wenjie Wang, chuats, jrwen, pl8787, KID-22 | i) This paper proposes NExT-Search, a new paradigm aimed at reintegrating fine-grained user feedback into generative AI search. ii) The main research objective is to address the limitations of current generative AI search systems, which lack effective feedback loops for refining individual components due to the reduced granularity of user feedback. iii) The methodology involves integrating two complementary modes: User Debug Mode for explicit user intervention and Shadow User Mode, which employs a personalized user agent to simulate user preferences and generate AI-assisted feedback. iv) The primary result is the conceptualization of a feedback store mechanism where users can share and potentially monetize their debugging efforts, though no quantitative results are reported in this perspective paper. v) AI practitioners can leverage NExT-Search to build feedback-rich AI search systems that continuously evolve alongside human feedback, emphasizing online adaptation and offline updates to refine query decomposition, retrieval, and generation models.  |
| Training-Free Watermarking for Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2505.14673) or [HuggingFace](https://huggingface.co/papers/2505.14673))| Shuai Yang, kaiyangzhou, Apostle723, yutchina02 | i) IndexMark, a training-free watermarking framework, is proposed for autoregressive image generation models. ii) The objective is to embed invisible and robust watermarks into images generated by autoregressive models without compromising image quality. iii) The methodology uses a match-then-replace strategy, selecting watermark tokens based on token similarity and employing an Index Encoder for verification with a cropping-robust validation scheme. iv) Experiments show IndexMark achieves state-of-the-art performance in image quality and verification accuracy (1.000 on watermark verification under clean conditions) while demonstrating robustness against perturbations like cropping and noise. v) The training-free nature and demonstrated robustness of IndexMark provide AI practitioners with a practical method for ensuring image traceability in autoregressive generative models without incurring additional training costs.  |
| VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.14640) or [HuggingFace](https://huggingface.co/papers/2505.14640))| Ping Nie, Yiming Jia, ZhuofengLi, wren93, tonymwt | i) VIDEOEVAL-PRO is introduced as a more robust long video understanding (LVU) benchmark. ii) The research aims to address the inflated performance and strong priors of existing LVU benchmarks and provide a realistic evaluation. iii) The methodology involves reformulating multiple-choice questions from existing benchmarks into open-ended questions and employing filtering methods based on video duration, answer type, answerability, and difficulty. iv) Evaluation of 21 video LMMs reveals a performance drop exceeding 25% on open-ended questions compared to multiple-choice questions, with models achieving only ~10% accuracy on VIDEOEVAL-PRO with a single input frame. v) VIDEOEVAL-PRO offers a more reliable measure of long video understanding progress, providing a more faithful assessment of LMMs' ability to integrate and reason over longer video contexts for AI practitioners.  |
| CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the
  Limits of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.13559) or [HuggingFace](https://huggingface.co/papers/2505.13559))| Eng Siong Chng, Lim Zhi Hao, Tanmay Surana, SkAndMl | i) CS-Sum is introduced as the first benchmark dataset for evaluating code-switching (CS) dialogue summarization in LLMs across Mandarin-English, Tamil-English, and Malay-English language pairs. ii) The research objective is to assess the comprehensibility of code-switching in LLMs through the task of CS dialogue to English summarization. iii) The methodology involves evaluating ten LLMs using few-shot learning, translate-summarize, and fine-tuning (LoRA, QLORA on synthetic data) approaches. iv) Results indicate that although automated metrics are high, LLMs make subtle errors altering the meaning of dialogues; error rates vary across CS pairs and models. v) This underscores the need for specialized training on code-switched data to improve LLM's ability to interpret multilingual prompts, thereby suggesting current models lack intrinsic CS comprehension, and that finetuning can amplify errors under distribution shift.  |
| Think Only When You Need with Large Hybrid-Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.14631) or [HuggingFace](https://huggingface.co/papers/2505.14631))| Zewen Chi, Qingxiu Dong, Shaohan Huang, YUSHUIWX, lingjie23 | i) The paper introduces Large Hybrid-Reasoning Models (LHRMs) that adaptively determine whether to engage in extended thinking processes based on query complexity. ii) The research aims to mitigate the overthinking problem in Large Reasoning Models (LRMs) by adaptively selecting between thinking and no-thinking modes. iii) The methodology involves a two-stage training pipeline: Hybrid Fine-Tuning (HFT) followed by Hybrid Group Policy Optimization (HGPO), and a metric called Hybrid Accuracy is used for evaluation. iv) Experiments show that LHRMs outperform existing LRMs and LLMs, demonstrating adaptive hybrid thinking on queries of varying difficulty; LHRMs achieve average improvements of 9.2% and 7.1% compared to HFT-DPO at the 1.5B and 7B scales, respectively. v) LHRMs provide AI practitioners with a more efficient reasoning model that reduces computational overhead on simple tasks while maintaining strong reasoning ability on complex queries, leading to better resource utilization and user experience.  |
| Fine-tuning Quantized Neural Networks with Zeroth-order Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.13430) or [HuggingFace](https://huggingface.co/papers/2505.13430))| Minxian Li, Jiayi Zhou, kaiyangzhou, chenyulin, sifengshang | i) The paper introduces Quantized Zeroth-order Optimization (QZO) for memory-efficient fine-tuning of quantized neural networks. ii) The research aims to minimize memory usage on model weights, gradients, and optimizer states during fine-tuning. iii) QZO approximates gradients by perturbing the continuous quantization scale and employs directional derivative clipping to stabilize training. iv) QZO reduces total memory cost by over 18× for 4-bit LLMs, enabling fine-tuning of Llama-2-13B and Stable Diffusion 3.5 Large on a single 24GB GPU. v) QZO provides AI practitioners with a method to fine-tune large models with significantly reduced memory requirements, potentially democratizing access to adapting such models on resource-constrained hardware.  |
| SSR: Enhancing Depth Perception in Vision-Language Models via
  Rationale-Guided Spatial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.12448) or [HuggingFace](https://huggingface.co/papers/2505.12448))| Han Zhao, Pengxiang Ding, Xiaomin Yu, Ming Ma, yliu-cs | This paper introduces SSR, a Spatial Sense and Reasoning method to improve spatial understanding in Vision-Language Models (VLMs) by converting raw depth data into structured textual rationales. The research aims to enhance spatial reasoning in VLMs by integrating depth information more effectively. The method involves transforming depth data into textual rationales, knowledge distillation to create compact latent embeddings, and a novel SSR-COT dataset for training and evaluation. Experiments on SSRBENCH showed SSR substantially improves depth utilization and enhances spatial reasoning, with SSR achieving a 13.6% improvement in average question answering accuracy compared to baseline models on certain spatial reasoning tasks. These results imply AI practitioners can enhance VLMs by incorporating structured depth information via textual rationales, improving spatial reasoning capabilities without extensive retraining.  |
| Reward Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2505.14674) or [HuggingFace](https://huggingface.co/papers/2505.14674))| Qingxiu Dong, Zewen Chi, Jiaxin Guo, YUSHUIWX, unilm | i) The paper introduces Reward Reasoning Models (RRMs), which perform explicit reasoning before generating rewards for language model outputs. ii) The main objective is to enhance reward model performance by effectively utilizing test-time compute for complex queries. iii) RRMs are trained using a reinforcement learning framework to foster self-evolved reward reasoning capabilities without explicit reasoning traces as training data. iv) Experiments show RRMs achieve superior performance on reward modeling benchmarks; RRM-32B attains an accuracy of 98.6% in the reasoning category of RewardBench. v) RRMs offer AI practitioners a method to improve reward modeling through deliberate reasoning and adaptive allocation of test-time compute, enhancing performance in tasks requiring nuanced analysis.  |
| Not All Correct Answers Are Equal: Why Your Distillation Source Matters (Read more on [arXiv](https://arxiv.org/abs/2505.14464) or [HuggingFace](https://huggingface.co/papers/2505.14464))| Sitong Zhao, Shuaiting Chen, Haotian Wang, Yunjie Ji, Emperorizzis | i) The paper investigates the impact of different teacher models on the quality of distilled reasoning datasets for language models. ii) The main objective is to determine how the source model used for distillation affects the reasoning performance of student models trained on the resulting datasets. iii) The methodology involves distilling data from three teacher models (AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1) on a corpus of 1.89 million queries, followed by training student models on each distilled dataset. iv) Results show that student models trained on AM-Thinking-v1 distilled data achieve superior performance, reaching 84.3 on AIME2024, and also demonstrate adaptive output behavior based on task complexity. v) The key implication is that the choice of the distillation source significantly influences downstream reasoning performance, and high-quality, verified reasoning traces from models like AM-Thinking-v1 are critical for creating effective reasoning-oriented language models; AM-Thinking-v1 is also demonstrated to have lower perplexity suggesting higher quality data.  |
| Hunyuan-Game: Industrial-grade Intelligent Game Creation Model (Read more on [arXiv](https://arxiv.org/abs/2505.14135) or [HuggingFace](https://huggingface.co/papers/2505.14135))| vcvcvn, tangjs, YellowAddice, zhengsj, lslrh | i) Hunyuan-Game is presented as a comprehensive AI-driven framework for procedural game asset generation, including image and video modalities. ii) The research aims to develop a suite of generative models capable of producing high-fidelity, controllable game content to enhance designer efficiency. iii) The methodology involves curating large-scale datasets of game and anime assets, fine-tuning diffusion transformer models, and implementing specialized prompt optimization and control mechanisms. iv) The system demonstrates state-of-the-art performance, with designer feedback indicating a 60% improvement in visual effects iteration efficiency using the introduced reference-based game visual effects generation approach. v) Hunyuan-Game offers AI/ML practitioners a practical framework and associated models for automating and enhancing content creation pipelines within the gaming industry, providing a foundation for further research and development in domain-specific generative AI.  |
| Warm Up Before You Train: Unlocking General Reasoning in
  Resource-Constrained Settings (Read more on [arXiv](https://arxiv.org/abs/2505.13718) or [HuggingFace](https://huggingface.co/papers/2505.13718))| Keith Ross, xanubhav81, AadimNepal, guactastesgood, safal312 | Designing reasoning-capable LLMs with limited training data can be improved using a two-stage approach of warmup followed by task adaptation. The research investigates if models trained with general reasoning strategies can rapidly adapt to new domains with minimal supervision. The methodology involves pre-training a model on Knights & Knaves logic puzzles to distill general reasoning skills, followed by RLVR fine-tuning on a limited set of domain-specific examples. Experiments show that the warmup phase leads to a +10.2% increase on MATH and +15.3% increase on HumanEval+ for the Qwen2.5-3B model, and that the warmed-up model outperforms the base model when both are RLVR trained on the same small datasets. AI practitioners can leverage the warmup technique to improve sample efficiency and maintain cross-domain generalizability when training robust reasoning LLMs in data-scarce environments. |
| Lessons from Defending Gemini Against Indirect Prompt Injections (Read more on [arXiv](https://arxiv.org/abs/2505.14534) or [HuggingFace](https://huggingface.co/papers/2505.14534))| cchoquette, julsh, tux, iliashum, chongyangs | i) This paper evaluates and improves the robustness of Gemini models against indirect prompt injection attacks in tool-use scenarios. ii) The main objective is to assess Gemini's adversarial robustness and identify key lessons for making the model more resilient to manipulation via untrusted data. iii) The methodology involves an adversarial evaluation framework that deploys adaptive attack techniques against Gemini, along with adversarial fine-tuning. iv) Gemini 2.5 achieved an average of approximately 47% reduction in attack success rate (ASR) across three attack techniques, and the warning defense achieved a 10.8% ASR defending Gemini 2.0 against the adaptive TAP attack. v) The principal implication is that adaptive evaluation and adversarial training are crucial for enhancing model security, while external defenses can complement model-level improvements. |
| Towards eliciting latent knowledge from LLMs with mechanistic
  interpretability (Read more on [arXiv](https://arxiv.org/abs/2505.14352) or [HuggingFace](https://huggingface.co/papers/2505.14352))| Emil Ryd, NeelNanda, srdm, bcywinski | i) This paper explores methods for eliciting hidden knowledge from language models. ii) The main research question is how to uncover a secret word internalised by a language model without explicit verbalisation. iii) The methodology involves training a Taboo model and then applying black-box and mechanistic interpretability techniques like Logit Lens and Sparse Autoencoders. iv) The primary result demonstrates that interpretability-based approaches can elicit the secret word, with "Another Model" black-box elicitation achieving 95% Pass@10. v) This suggests mechanistic interpretability is a promising direction for extracting hidden knowledge, but the model organism needs to be more complex.  |
| Truth Neurons (Read more on [arXiv](https://arxiv.org/abs/2505.12182) or [HuggingFace](https://huggingface.co/papers/2505.12182))| ZiningZhu, jordansuchow, ShirleyY, YupengCao, Acatsama | i) The paper identifies and analyzes "truth neurons" in language models that encode truthfulness in a subject-agnostic manner. ii) The research aims to identify neuron-level mechanisms encoding truthfulness within language models. iii) The methodology involves using integrated gradients to measure neuron attribution scores for truthful vs. untruthful responses, followed by systematic filtering to identify truth neurons. iv) Experiments across six language models reveal that suppressing identified truth neurons leads to statistically significant accuracy reductions on TruthfulQA (e.g., average accuracy of small-scale models decreases to 54.25%, representing a degradation of 10.49%) and generalizes to other benchmarks. v) The identification and analysis of truth neurons offer AI practitioners potential directions for improving the trustworthiness and reliability of language models by highlighting areas for targeted intervention and alignment.  |
| Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training (Read more on [arXiv](https://arxiv.org/abs/2505.14681) or [HuggingFace](https://huggingface.co/papers/2505.14681))| Jiahao Xu, Zhiwei He, Yue Wang, Xingyu Chen, Mengru Wang | i) The paper introduces Reinforcing Cognitive Experts (RICE), a novel inference-time method to enhance reasoning in Mixture-of-Experts (MoE) models without additional training. ii) The main research objective is to improve the cognitive efficiency of Large Reasoning Models (LRMs) by modulating experts correlated with reasoning. iii) The methodology involves identifying specialized "cognitive experts" using normalized Pointwise Mutual Information (nPMI) and selectively amplifying their activation during inference. iv) The approach improves reasoning accuracy on DeepSeek-R1, increasing AIME24 accuracy from 73.3% to 83.3% by reinforcing only the top two cognitive experts with a multiplier of 64. v) RICE offers a lightweight and interpretable method for AI practitioners to enhance reasoning in MoE-based LRMs, improving efficiency and accuracy without retraining or complex heuristics.  |
| Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair (Read more on [arXiv](https://arxiv.org/abs/2505.13103) or [HuggingFace](https://huggingface.co/papers/2505.13103))| Mathias Payer, Aiden Hall, Tianqi Fan, Han Zheng, iliashum | i) The paper introduces WILLIAMT, a cost-effective crash-site program repair tool. ii) The research aims to reduce the cost and improve the effectiveness of automated program repair, particularly for memory corruption vulnerabilities, using crash-site repair and template-guided patch generation. iii) The methodology involves a regex-based context retrieval and a template-guided patch generation to minimize LLM token usage. iv) Evaluation shows that WILLIAMT, when combined with CodeRover-S, reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% on ARVO, retaining 86.7% of CodeRover-S performance while saving 99.7% Token cost. v) The results imply that AI practitioners can leverage low-cost, template-guided APR for memory corruption vulnerabilities, significantly reducing resource consumption while maintaining repair effectiveness in open-source software.  |
| Phare: A Safety Probe for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.11365) or [HuggingFace](https://huggingface.co/papers/2505.11365))| Matteo Dora, inoki-giskard, bmalezieux, pierlj | Phare introduces a multilingual diagnostic framework to evaluate the safety of large language models (LLMs). The primary objective is to probe and evaluate LLM behavior across hallucination and reliability, social biases, and harmful content generation. The study evaluated 17 state-of-the-art LLMs using Phare, revealing systematic vulnerabilities, such as sycophancy, prompt sensitivity, and stereotype reproduction. The research found that confidence tone in user messages can significantly decrease debunking accuracy by up to 15%. Phare provides actionable insights for AI practitioners by highlighting specific failure modes to build more robust, aligned, and trustworthy language systems. The study does not contain information about the architecture of Phare.  |
| MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8 (Read more on [arXiv](https://arxiv.org/abs/2505.09569) or [HuggingFace](https://huggingface.co/papers/2505.09569))| Lin Chen, Qiang Zhou, omidvarb, sliuxl, linboliu | i) MigrationBench, a new benchmark, facilitates the evaluation of LLMs for Java code migration from version 8 to 17/21. ii) The research aims to provide a comprehensive benchmark for repository-level code migration to address the limitations of existing code generation and issue-resolution focused benchmarks. iii) The methodology involves curating a dataset of open-source Java repositories, developing an automated evaluation framework, and proposing a novel feedback mechanism named SD-Feedback. iv) Results show that SD-Feedback, when implemented with Claude-3.5-Sonnet-v2, achieves a 62.33% success rate (pass@1) for minimal migration on the selected subset of repositories. v) AI practitioners can use MigrationBench and SD-Feedback to improve LLM-driven code migration tools for enhancing software maintainability and facilitating Java version upgrades.  |
| Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic
  Reasoning Limits (Read more on [arXiv](https://arxiv.org/abs/2505.14178) or [HuggingFace](https://huggingface.co/papers/2505.14178))| Yiwei Xu, Jiaqi Wei, Juntai Cao, Charlesyooo, Wyattz23 | i) Tokenization schemes in LLMs can significantly constrain symbolic and arithmetic reasoning abilities. ii) The research investigates how tokenization schemes, specifically byte-pair encoding (BPE), affect the ability of LLMs to perform symbolic computation and arithmetic reasoning. iii) The methodology involves a theoretical analysis of token awareness and empirical evaluation across arithmetic and symbolic tasks with variations in tokenization and Chain-of-Thought prompting. iv) The study demonstrates that token structure dramatically affects reasoning performance, showing up to 80% performance degradation due to suboptimal tokenization and demonstrating GPT-40-mini can outperform o1 when tokenization is atomically aligned. v) AI practitioners should consider tokenization strategies as a critical factor in designing LLMs for symbolic and arithmetic reasoning tasks to unlock full computational potential, as model performance is deeply conditioned on token-level representations.  |
| CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via
  Competition (Read more on [arXiv](https://arxiv.org/abs/2505.13380) or [HuggingFace](https://huggingface.co/papers/2505.13380))| Van Nguyen, Quang Pham, Huy Nguyen, nhatho, DavidNguyen | CompeteSMoE introduces a novel routing mechanism for Sparse Mixture of Experts (SMoE) training based on a competition principle. This research addresses the challenge of suboptimal routing in SMOE by exploring whether experts that perform computation directly contribute to the routing process. The methodology involves distributing tokens to experts based on the highest neural response, with theoretical guarantees of better sample efficiency compared to softmax routing.  Experiments using a 5.1B parameter backbone demonstrates CompeteSMoE improved zero-shot performance across nine visual instruction tuning tasks.  This work offers AI/ML engineers an effective strategy to enhance large language model training by improving upon routing efficiency.  |
| Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative
  Verifier (Read more on [arXiv](https://arxiv.org/abs/2505.11966) or [HuggingFace](https://huggingface.co/papers/2505.11966))| Kezhi Li, Zhijian Xu, Zeju Li, XiangyuWen, Jianyuan1 | i) The paper introduces FlexiVe, a flexible generative verifier, and the Solve-Detect-Verify pipeline for efficient LLM reasoning. ii) The research aims to improve the trade-off between accuracy and computational efficiency in LLM reasoning by dynamically allocating verification resources. iii) The methodology involves a two-stage verification process: a fast, resource-efficient mode for quick error diagnosis and a slower, computationally-intensive mode for deeper analysis, managed by a flexible verification budget. iv) Experiments on the AIME2024 benchmark showed that Solve-Detect-Verify achieves higher accuracy while requiring approximately 4x fewer solutions compared to baseline approaches; also FlexiVe (specifically with the Flex@8 configuration) attains a higher F1 score while generating approximately 3x fewer tokens than the baseline on the Math benchmark. v) The primary implication for AI practitioners is a scalable and effective approach for enhancing LLM reasoning at test time, providing a means to balance accuracy and computational cost.  |
| To Bias or Not to Bias: Detecting bias in News with bias-detector (Read more on [arXiv](https://arxiv.org/abs/2505.13010) or [HuggingFace](https://huggingface.co/papers/2505.13010))| grohg, amosharafa, himel7 | i) This paper presents an improved RoBERTa-based model for sentence-level media bias detection. ii) The research aims to enhance the accuracy and statistical significance of bias detection in news articles compared to existing models. iii) The methodology involves fine-tuning a RoBERTa-base model on the BABE dataset and comparing its performance against a DA-ROBERTa baseline using McNemar's test and 5x2 cross-validation. iv) The fine-tuned RoBERTa model achieved a macro F1 score of 0.9257 on the BABE dataset, demonstrating statistically significant improvements (p < 2.45x10-9 in McNemar's test) over the DA-ROBERTa baseline. v) This research offers AI practitioners a more robust and statistically validated bias detection model, potentially reducing false positives/negatives in downstream tasks that rely on unbiased news analysis, while also establishing a framework for future comprehensive bias analysis.  |
| Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for
  Real-world Knowledge Injection (Read more on [arXiv](https://arxiv.org/abs/2505.12306) or [HuggingFace](https://huggingface.co/papers/2505.12306))| Shangbin Feng, Wenhao Yu, Yuwei Zhang, shangjingbo, KomeijiForce | i) This paper introduces WIKIDYK, a novel benchmark for evaluating knowledge injection in LLMs using real-world Wikipedia "Did You Know..." facts. ii) The main research question is whether LLMs can effectively memorize and internalize new knowledge after pre-training, comparing Causal Language Models (CLMs) against Bidirectional Language Models (BiLMs). iii) The methodology involves continued pre-training of various LLM architectures (CLMs and BiLMs) with WIKIDYK facts, followed by a multi-dimensional evaluation suite spanning question answering tasks. iv) The primary result indicates that BiLMs demonstrate significantly stronger knowledge memorization capabilities compared to CLMs, exhibiting a 23% higher accuracy in reliability; a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories further improves reliability accuracy by up to 29.1%. v) The principal implication for AI practitioners is that BiLMs may be more effective than CLMs for applications requiring robust knowledge integration, suggesting a potential shift or hybrid approach in model architecture design for knowledge-intensive tasks. |
| Masking in Multi-hop QA: An Analysis of How Language Models Perform with
  Context Permutation (Read more on [arXiv](https://arxiv.org/abs/2505.11754) or [HuggingFace](https://huggingface.co/papers/2505.11754))| Jeff Z. Pan, Mirella Lapata, pvougiou, hwy9855 | Language models (LMs) performance on multi-hop question answering (MHQA) is analyzed by varying the order of retrieved documents in the input context. The study investigates how encoder-decoder (Flan-T5) and decoder-only (Qwen, Llama) architectures respond to document permutations. The primary methodology involves evaluating LM accuracy on the MuSiQue dataset under different document orderings, distances, and completeness configurations. It was found that fine-tuned LMs favor forward-placed documents and bi-directional attention can improve performance. An attention weight analysis showed that LMs typically assign higher attention weights to relevant documents when answering correctly; specifically, accuracy of Qwen 7B was increased from 28.6% to 33.7% by sampling answers with different input document permutations and retaining answers for the inputs for which the LM assigned the largest peak attention score. This research suggests that optimizing document ordering and incorporating bidirectional attention can enhance LMs for knowledge-intensive tasks and improving the RAG paradigm by focusing on ranking based metrics.  |
| Incorporating brain-inspired mechanisms for multimodal learning in
  artificial intelligence (Read more on [arXiv](https://arxiv.org/abs/2505.10176) or [HuggingFace](https://huggingface.co/papers/2505.10176))| Xin Yang, Qingqun Kong, Yang Li, Dongcheng Zhao, Xiang He | i) This paper introduces an inverse effectiveness driven multimodal fusion (IEMF) strategy inspired by biological multimodal integration. ii) The main objective is to improve multimodal learning in artificial intelligence by incorporating the inverse effectiveness principle. iii) The methodology involves incorporating IEMF into neural network architectures, adapting the fusion module's weights based on the strength of unimodal inputs and multimodal outputs. iv) Experiments on audio-visual tasks demonstrate IEMF achieves up to 50% reduction in computational cost compared to baseline methods, while maintaining or improving performance. v) IEMF offers AI practitioners a biologically-inspired mechanism for enhancing multimodal fusion in neural networks, improving both performance and computational efficiency; it is unclear if they only measure compute cost by FLOPs.  |
| Understanding Gen Alpha Digital Language: Evaluation of LLM Safety
  Systems for Content Moderation (Read more on [arXiv](https://arxiv.org/abs/2505.10588) or [HuggingFace](https://huggingface.co/papers/2505.10588))| Fausto Giunchiglia, Manisha Mehta | i) This paper evaluates the efficacy of LLM-based content moderation systems in understanding and mitigating risks within Gen Alpha's digital communication. ii) The research investigates the ability of current AI systems to comprehend Gen Alpha's evolving linguistic patterns, including slang and context-dependent meanings, and detect harmful content. iii) The methodology involved creating a novel dataset of 100 Gen Alpha expressions and evaluating leading LLMs (GPT-4, Claude, Gemini, Llama 3) alongside human moderators and Gen Alpha users, across dimensions of basic understanding, context recognition, and safety implication detection. iv) The study found that Gen Alpha users demonstrated 92% accuracy in identifying potential harm, significantly exceeding the performance of both AI systems and human moderators, and that LLMs showed limitations in detecting masked risks and evolving language (32-42% accuracy). v) The principal implication is that AI practitioners must develop content moderation systems incorporating dynamic, context-aware capabilities and systematic bias audits, supplemented with human oversight, to effectively protect young users from online risks, especially given documented reluctance of Gen Alpha users to seek adult help, and platform-specific meaning variations.  |
