

## Papers for 2025-05-26

| Title | Authors | Summary |
|-------|---------|---------|
| TabSTAR: A Foundation Tabular Model With Semantically Target-Aware
  Representations (Read more on [arXiv](https://arxiv.org/abs/2505.18125) or [HuggingFace](https://huggingface.co/papers/2505.18125))| Roi Reichart, Alan Arazi, EilamSha | i) TabSTAR introduces a foundation tabular model with semantically target-aware representations for transfer learning on tabular data with textual features. ii) The research aims to enhance tabular learning performance, particularly in datasets with free-text, by incorporating target-specific semantic information. iii) TabSTAR utilizes a pre-trained text encoder, unfrozen and optimized with target tokens, to learn task-specific embeddings, and pretraining exhibits scaling laws. iv) TabSTAR achieves state-of-the-art performance on medium- and large-sized datasets, with a normalized score of 0.874 on classification benchmarks. v) The framework enables AI practitioners to utilize pre-trained models for tabular data, particularly with text, and efficiently adapt them for new tasks with low-resource settings by enabling transfer learning on tabular data with textual features.  |
| QwenLong-L1: Towards Long-Context Large Reasoning Models with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17667) or [HuggingFace](https://huggingface.co/papers/2505.17667))| Chenliang Li, Yingcheng Shi, Shengyi Liao, Weizhou Shen, Wanfq | QwenLong-L1 introduces a reinforcement learning framework for adapting large reasoning models to long-context scenarios. The research focuses on effectively extending LRM capabilities for processing long-context inputs via reinforcement learning. A progressive context scaling approach, with supervised fine-tuning and curriculum-guided phased RL, is employed to address training instability and inefficiency. Experiments on seven long-context document question-answering benchmarks show that QwenLong-L1-32B achieves an average gain of 5.1 points over the baseline R1-Distill-Qwen-32B. This work provides AI practitioners with a methodology for developing robust LRMs capable of reasoning across information-intensive environments.  |
| Reasoning Model is Stubborn: Diagnosing Instruction Overriding in
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.17225) or [HuggingFace](https://huggingface.co/papers/2505.17225))| Eunho Yang, Hyun Ryu, Chanjae Park, yjyjyj98, jadohu | i) The paper introduces ReasoningTrap, a diagnostic dataset for assessing instruction overriding, termed "reasoning rigidity," in large language models. ii) The main objective is to investigate and categorize the systematic failure of reasoning models to adhere to explicit instructions, instead defaulting to ingrained reasoning patterns. iii) The methodology involves creating modified versions of existing mathematical benchmarks (AIME, MATH500) and logic puzzles to require deviation from familiar reasoning strategies and subsequently categorizing contamination patterns. iv) The primary result identifies three distinct contamination modes: Interpretation Overload, Input Distrust, and Partial Instruction Attention and reports p-pass@1 scores for various models on the created datasets. v) The principal implication for AI practitioners is the identification and public release of a diagnostic set that will facilitate future research into mitigating reasoning rigidity in language models to improve the faithful execution of user instructions.  |
| One RL to See Them All: Visual Triple Unified Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.18129) or [HuggingFace](https://huggingface.co/papers/2505.18129))| Pengfei Li, Shaoxiang Chen, Linge Du, Yan Ma, Ryan1122 | i) The paper introduces V-Triune, a unified reinforcement learning system for vision-language models. ii) The research aims to enable VLMs to jointly learn visual reasoning and perception tasks within a single RL training pipeline. iii) V-Triune employs Sample-Level Data Formatting, Verifier-Level Reward Computation using a Dynamic IoU reward, and Source-Level Metric Monitoring. iv) The resulting model, Orsta, achieves gains on MEGA-Bench Core, with improvements ranging from +2.1% to +14.1% across 7B and 32B model variants. v) This unified RL approach for VLMs allows AI practitioners to train models for both visual reasoning and perception tasks, improving effectiveness and scalability with a single training paradigm.  |
| Quartet: Native FP4 Training Can Be Optimal for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.14669) or [HuggingFace](https://huggingface.co/papers/2505.14669))| Jiale Chen, Oliver Sieberling, Soroush Tabesh, Andrei Panferov, Roberto L. Castro | i) This paper introduces Quartet, an end-to-end FP4 training method for LLMs, achieving state-of-the-art accuracy through hardware-supported low-precision computation. ii) The main research objective is to enable accurate, fully FP4-based training of large language models by addressing accuracy degradation challenges. iii) The study employs a novel low-precision scaling law, customized CUDA kernels for NVIDIA Blackwell GPUs, and extensive evaluations on Llama-type models. iv) Quartet achieves almost 2x speedup relative to FP8 for linear layer computations on a Blackwell RTX 5090 GPU and state-of-the-art accuracy for FP4 precision on billion-scale models. v) AI practitioners can leverage Quartet to significantly improve the efficiency of LLM training, offering a competitive alternative to standard-precision and FP8 training via fully FP4-based methods.  |
| Distilling LLM Agent into Small Models with Retrieval and Code Tools (Read more on [arXiv](https://arxiv.org/abs/2505.17612) or [HuggingFace](https://huggingface.co/papers/2505.17612))| Sung Ju Hwang, Jaewoong Cho, Seanie Lee, Jongwon Jeong, Minki Kang | i) This paper introduces Agent Distillation, a framework to transfer task-solving behavior from large language model (LLM) agents to smaller language models (sLMs) using retrieval and code tools. ii) The research aims to enable sLMs to perform complex reasoning tasks by distilling the reasoning and tool-use capabilities of LLM-based agents. iii) The methodology includes a first-thought prefix to improve teacher-generated trajectories and self-consistent action generation to enhance test-time robustness of sLMs. iv) Experiments on factual and mathematical reasoning tasks demonstrate that sLMs (0.5B, 1.5B, 3B parameters) achieve performance competitive with larger models (1.5B, 3B, 7B) fine-tuned with chain-of-thought distillation. v) Agent Distillation provides AI practitioners with a method to create practical, tool-using small agents by enabling code execution and information retrieval in sLMs, improving their performance in complex reasoning tasks.  |
| PhyX: Does Your Model Have the "Wits" for Physical Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2505.15929) or [HuggingFace](https://huggingface.co/papers/2505.15929))| Yunta Hsieh, Qi Han, Hui Shen, John-ai-bee, taki555 | i) The paper introduces PHYX, a new benchmark for evaluating physical reasoning in AI models using visual scenarios. ii) The research aims to assess the integrated ability of AI models to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints in physical problem-solving. iii) The methodology involves a dataset of 3K multimodal questions spanning 6 reasoning types across 25 sub-domains within 6 core physics domains, evaluated using a strict three-step evaluation protocol. iv) The evaluation showed that even state-of-the-art models like GPT-40, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy, respectively, significantly lower than human experts. v) PHYX highlights the need for AI practitioners to develop models with improved integration of disciplinary knowledge, symbolic operations, and real-world constraints for high-level physical reasoning.  |
| QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.18092) or [HuggingFace](https://huggingface.co/papers/2505.18092))| Shaopeng Lai, Shengyi Liao, Chenliang Li, Weizhou Shen, Wanfq | QWENLONG-CPRS introduces a context compression framework for long-context large language models (LLMs). The research addresses computational overhead during prefill stages and the "lost in the middle" phenomenon in LLMs processing long sequences. A dynamic context optimization mechanism is used, enabling multi-granularity context compression guided by natural language instructions. Evaluations demonstrate a 21.59x context compression rate alongside 19.15-point average performance gains across various LLMs and benchmarks, with the system deployed with Qwen2.5-32B-Instruct surpassing leading proprietary LLMs. AI practitioners can leverage QWENLONG-CPRS for improved efficiency and performance in LLMs handling extended context inputs, potentially establishing new state-of-the-art performance.  |
| VeriThinker: Learning to Verify Makes Reasoning Model Efficient (Read more on [arXiv](https://arxiv.org/abs/2505.17941) or [HuggingFace](https://huggingface.co/papers/2505.17941))| Xinchao Wang, Ruonan Yu, Gongfan Fang, Xinyin Ma, Zigeng | i) This paper introduces VeriThinker, a novel approach to improve the efficiency of large reasoning models by fine-tuning them on a CoT verification task. ii) The research question addresses whether LRMs can be effectively compressed by learning to verify reasoning steps rather than relying on synthetic target reasoning chains. iii) The methodology involves Supervised Verification Fine-Tuning (SVFT), where an LRM is trained to classify CoT solutions as correct or incorrect using an auxiliary verification dataset. iv) Results show that VeriThinker reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%) and also generalizes to solution-wise speculative reasoning. v) AI practitioners can leverage VeriThinker's SVFT for compressing and accelerating LRMs, improving their deployment efficiency without sacrificing reasoning accuracy, and possibly improve the accuracy.  |
| Model Already Knows the Best Noise: Bayesian Active Noise Selection via
  Attention in Video Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2505.17561) or [HuggingFace](https://huggingface.co/papers/2505.17561))| Sanghyun Kim, Kwanyoung Kim | i) The paper introduces ANSE, a model-aware framework for selecting high-quality initial noise seeds in video diffusion models. ii) The research aims to improve video quality and temporal coherence in text-to-video generation by leveraging internal model signals. iii) A Bayesian Active Noise Selection via Attention (BANSA) score, quantifying attention-based uncertainty across stochastic attention samples, is used to evaluate noise seeds. iv) Experiments on CogVideoX-2B show ANSE improves VBench total score from 81.03 to 81.66 with only an 8% increase in inference time. v) AI practitioners can use ANSE to improve the quality and coherence of videos generated by diffusion models without retraining or external noise priors.  |
| MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated
  Experimental Feedback (Read more on [arXiv](https://arxiv.org/abs/2505.17873) or [HuggingFace](https://huggingface.co/papers/2505.17873))| Lidong Bing, Jue Wang, di-zhang-fdu, ZonglinY, wanhaoliu | i) The paper introduces experiment-guided hypothesis ranking, a novel approach for scientific discovery. ii) The main objective is to develop a method for prioritizing candidate hypotheses based on prior experimental results, addressing the limitation of relying solely on LLM's internal reasoning. iii) The research employs a simulator grounded in three domain-informed assumptions to model hypothesis performance, along with functional clustering of hypotheses. iv) Experiments demonstrate that the proposed method (CSX-Rank) reduces the number of trials required to identify ground-truth hypotheses by more than 50% on the TOMATO-chem dataset compared to pre-experiment baselines. v) AI practitioners can leverage this approach for more efficient hypothesis prioritization, particularly in resource-constrained domains where empirical validation is expensive, with implications for optimizing automated scientific discovery workflows.  |
| AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.16211) or [HuggingFace](https://huggingface.co/papers/2505.16211))| Jirui Han, Yile Liu, Can Shen, Kai Li, jiaxiaojunQAQ | i) AudioTrust is introduced as the first multifaceted benchmark to evaluate the trustworthiness of Audio Large Language Models (ALLMs). ii) The research aims to comprehensively assess key trustworthiness dimensions of ALLMs, including fairness, hallucination, safety, privacy, robustness, and authentication. iii) The methodology involves a dataset of over 4,420 audio/text samples, 18 experimental setups, and 9 audio-specific evaluation metrics, utilizing a large-scale automated pipeline for model scoring. iv) Experimental results revealed systematic biases in ALLMs and uneven protection across different types of sensitive information; for example, Defense Success Rate(DSR) ranged from 76.2 - 100 for various closed/open source models on jailbreak attacks. v) AudioTrust provides AI practitioners with insights into the limitations and security vulnerabilities of current ALLMs, informing the development of more secure and reliable audio models.  |
| Scaling Image and Video Generation via Test-Time Evolutionary Search (Read more on [arXiv](https://arxiv.org/abs/2505.17618) or [HuggingFace](https://huggingface.co/papers/2505.17618))| Di Zhang, Pengfei Wan, Xintao Wang, Jiajun Liang, haoranhe | Scaling Image and Video Generation via Test-Time Evolutionary Search introduces EvoSearch, a novel and generalist test-time scaling framework for image and video generation. The paper addresses test-time scaling for diffusion and flow models by reformulating it as an evolutionary search problem. EvoSearch incorporates selection and mutation mechanisms tailored for stochastic differential equation denoising, iteratively improving sample quality while preserving population diversity. Experimental results show EvoSearch enables Stable Diffusion 2.1 to exceed GPT40 performance and Wan 1.3B to outperform Wan 14B, despite having 10x fewer parameters. This provides AI practitioners with a method to significantly enhance generative model sample quality through strategic computation allocation during inference, without additional training.  |
| FullFront: Benchmarking MLLMs Across the Full Front-End Engineering
  Workflow (Read more on [arXiv](https://arxiv.org/abs/2505.17399) or [HuggingFace](https://huggingface.co/papers/2505.17399))| Yu Cheng, Linjie Li, Huichen Will Wang, Haoyu Sun, Kuvvi | FullFront is introduced as a benchmark for evaluating Multimodal Large Language Models (MLLMs) across the front-end engineering workflow. This research benchmarks MLLMs across three tasks: Webpage Design, Webpage Perception QA, and Webpage Code Generation. The methodology employs a novel two-stage process to transform real-world webpages into clean, standardized HTML. Empirical testing of MLLMs revealed limitations in page perception and code generation, specifically with image handling and layout, showing that the best-performing model, Claude 3.7 Sonnet, achieves an average accuracy below 55% in Webpage Perception QA tasks compared to human accuracy exceeding 95%. This indicates that current MLLM capabilities need enhancement for front-end development to bridge the performance gap with human experts, in fine-grained visual perception particularly. The benchmark and code are available for use by AI researchers to advance MLLM capabilities in front-end engineering.  |
| Teaching with Lies: Curriculum DPO on Synthetic Negatives for
  Hallucination Detection (Read more on [arXiv](https://arxiv.org/abs/2505.17558) or [HuggingFace](https://huggingface.co/papers/2505.17558))| Ying Ding, Liu Leqi, ashwinnv, SP2001 | i) This paper introduces HaluCheck, a hallucination detection LLM aligned with a curriculum-based DPO framework using high-quality synthetic negatives. ii) The research objective is to improve the accuracy of hallucination detection in large language models. iii) The methodology involves using carefully curated, difficulty-ranked hallucinated samples as negative examples in a Direct Preference Optimization (DPO) framework with curriculum learning. iv) HaluCheck 3B achieves up to a 24% relative gain in detection metrics (accuracy, precision, recall, and F1-score) on benchmarks like MedHallu and HaluEval, also demonstrating robustness in zero-shot settings. v) The principal implication for AI practitioners is that using curriculum DPO with high-quality hallucinated negatives provides a robust approach for aligning LLMs to better detect hallucinations, improving trustworthiness of generated content.  |
| Thought-Augmented Policy Optimization: Bridging External Guidance and
  Internal Capabilities (Read more on [arXiv](https://arxiv.org/abs/2505.15692) or [HuggingFace](https://huggingface.co/papers/2505.15692))| Zhengqi Wen, Shuai Zhang, Mingkuan Feng, ChonghuaLiao, Jinyang23 | i) This paper introduces Thought-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework for enhancing the reasoning capabilities of language models. ii) The research aims to bridge the gap between external guidance and a language model's internal reasoning capabilities for improved performance on reasoning tasks. iii) TAPO extends the Group Relative Policy Optimization (GRPO) method by incorporating a "thought library" of high-level thought patterns abstracted from prior samples, adaptively applied during training. iv) Experiments demonstrate that TAPO significantly outperforms GRPO, achieving a 99% performance increase on AIME and a 41% increase on AMC benchmarks and improved output explainability. v) TAPO offers AI practitioners a method for developing more robust and generalizable reasoning systems by integrating external abstract problem-solving guidance into reinforcement learning training, potentially leading to enhanced reasoning performance.  |
| Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration (Read more on [arXiv](https://arxiv.org/abs/2505.16479) or [HuggingFace](https://huggingface.co/papers/2505.16479))| Bin Xiao, Xiuli Bi, Yang Wei, Yunqiu, YuetongLiu | i) This paper introduces a novel framework, ClearNight, for multi-weather nighttime image restoration and a corresponding dataset, AllWeatherNight. ii) The main objective is to effectively restore nighttime images degraded by the complex interaction of multiple adverse weather conditions and non-uniform illumination. iii) The methodology involves a unified architecture integrating Retinex-based dual prior guidance and a weather-aware dynamic specificity-commonality collaboration strategy. iv) Results show ClearNight achieves state-of-the-art performance, demonstrating a PSNR of 32.5937 on the AllWeatherNight synthetic testing subset when restoring Raindrop images, suggesting a significant advancement in handling mixed degradations. v) AI practitioners can leverage ClearNight as a robust solution for improving the performance of computer vision systems in challenging nighttime and adverse weather scenarios, particularly where multiple degradation types coexist, advancing applications such as autonomous driving and surveillance.  |
| Teaching Large Language Models to Maintain Contextual Faithfulness via
  Synthetic Tasks and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16483) or [HuggingFace](https://huggingface.co/papers/2505.16483))| Zhitong Wang, Yuzhuo Bai, Cheng Gao, Shuzheng Si, BleachNick | i) This paper introduces CANOE, a framework to improve the contextual faithfulness of Large Language Models (LLMs) using synthetic data and reinforcement learning. ii) The research aims to enhance LLM faithfulness in both short and long-form generation tasks without human annotations, addressing the challenge of knowledge conflicts and limited scalability of manually annotated data. iii) The methodology involves synthesizing short-form question-answering (QA) data with four diverse tasks and employing Dual-GRPO, a rule-based reinforcement learning method with tailored rewards. iv) Experimental results demonstrate that CANOE improves LLM faithfulness, achieving a 22.6% overall score improvement for Llama3-Instruct-8B and surpassing state-of-the-art LLMs like GPT-40 in overall score. v) CANOE offers AI practitioners a method to significantly improve the contextual faithfulness of LLMs with synthetic data and rule-based RL, reducing hallucinations without reliance on human annotations or simply scaling model parameters. |
| Time-R1: Towards Comprehensive Temporal Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.13508) or [HuggingFace](https://huggingface.co/papers/2505.13508))| Jiaxuan You, Haoru Li, Haofei Yu, Peixuan Han, m-serious | i) Time-R1 is introduced as a framework for enhancing temporal reasoning capabilities in Large Language Models (LLMs). ii) The research objective is to equip a 3B-parameter LLM with comprehensive temporal abilities, encompassing understanding, prediction, and creative generation related to time. iii) A three-stage reinforcement learning (RL) curriculum with a dynamic rule-based reward system is used for fine-tuning the LLM (Qwen2.5-3B-Instruct). iv) Experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the 671B DeepSeek-R1, on future event prediction and creative scenario generation benchmarks. v) The principal implication for AI practitioners is that carefully engineered, progressive RL fine-tuning enables smaller, efficient models to achieve superior temporal performance, offering a scalable path towards time-aware AI, however some parts of the paper are unclear or seem to lack information.  |
| Speechless: Speech Instruction Training Without Speech for Low Resource
  Languages (Read more on [arXiv](https://arxiv.org/abs/2505.17417) or [HuggingFace](https://huggingface.co/papers/2505.17417))| Shreyas Gopal, Tuan Le Duc Anh, Huy Hoang Ha, Dinh Bach Vu, alandao | Speechless presents a novel method for training speech-instruction models for low-resource languages without requiring text-to-speech (TTS) systems. The research objective is to enable speech understanding in LLMs by mapping text instructions to quantized Whisper encoder representations, bypassing traditional speech synthesis. The methodology involves training a quantizer on ASR data, then training a decoder-only language model (Speechless) to generate semantic tokens from text, followed by fine-tuning an LLM using these synthetic semantic tokens. Experiments on the Vietnamese language showed Speechless achieved competitive ASR performance; models using the Speechless framework achieve 2.69% CER on CommonVoice Vietnamese with beam search inference. This offers AI practitioners a computationally efficient way to create voice assistants in languages where high-quality TTS resources are unavailable.  |
| Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse
  Attention (Read more on [arXiv](https://arxiv.org/abs/2505.17412) or [HuggingFace](https://huggingface.co/papers/2505.17412))| Yikang Yang, Yifei Zeng, Feihu Zhang, Youtian Lin, Shuang Wu | Direct3D-S2 introduces a scalable 3D generation framework using spatial sparse attention (SSA) to improve efficiency and quality in volumetric 3D shape generation. The research addresses the challenge of high computational and memory costs in generating high-resolution 3D shapes using signed distance functions. It employs a spatial sparse attention mechanism within a diffusion transformer to effectively process large token sets in sparse volumes. The method achieves a 3.9x speedup in the forward pass and a 9.6x speedup in the backward pass at 1024³ resolution compared to FlashAttention-2. Direct3D-S2 enables training at 1024³ resolution using only 8 GPUs, significantly reducing the resource requirements and enhancing accessibility for gigascale 3D generation for AI practitioners.  |
| RBench-V: A Primary Assessment for Visual Reasoning Models with
  Multi-modal Outputs (Read more on [arXiv](https://arxiv.org/abs/2505.16770) or [HuggingFace](https://huggingface.co/papers/2505.16770))| Qianrui Yang, uyzhang, Mo-ZheHan, CXY07, MenghaoGuo | i) RBench-V introduces a new benchmark for evaluating visual reasoning capabilities of multi-modal models through multi-modal outputs. ii) The primary research objective is to assess the ability of models to generate appropriate visual outputs during visual reasoning tasks, such as image manipulation and auxiliary line construction. iii) The methodology involves a hand-picked set of 803 questions covering math, physics, counting, and games, requiring image modifications as part of the solution. iv) The best performing model, o3, achieved only 25.8% accuracy on RBench-V, compared to 82.3% for human experts, demonstrating a significant performance gap. v) AI practitioners need to develop new techniques, potentially including multi-modal chain-of-thought or agent-based reasoning, to improve visual reasoning with multi-modal output capabilities in foundation models.  |
| s3: You Don't Need That Much Data to Train a Search Agent via RL (Read more on [arXiv](https://arxiv.org/abs/2505.14146) or [HuggingFace](https://huggingface.co/papers/2505.14146))| Zifeng Wang, Jinfeng Xiao, Jiacheng Lin, Xueqiang Xu, Pengcheng Jiang | s3 introduces a lightweight, model-agnostic framework for training search agents via reinforcement learning. The paper addresses the question of how to efficiently train a search agent to improve generation quality without modifying the generator LLM. s3 utilizes a novel Gain Beyond RAG (GBR) reward to train a searcher decoupled from the generator. Experiments show s3 requires only 2.4k training samples to outperform baselines trained on over 70× more data, achieving stronger downstream performance across six general QA and five medical QA benchmarks. s3 offers a more data-efficient approach to training retrieval components in RAG systems, potentially benefiting practitioners working with limited data.  |
| Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement
  Fine-Tuning of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.17826) or [HuggingFace](https://huggingface.co/papers/2505.17826))| Daoyuan Chen, Yuchang Sun, Yushuo Chen, Yanxi Chen, Xuchen Pan | i) Trinity-RFT is presented as a general-purpose framework for reinforcement fine-tuning (RFT) of large language models (LLMs). ii) The research aims to provide a flexible and scalable framework unifying synchronous/asynchronous, on-policy/off-policy, and online/offline RFT modes. iii) The framework employs a decoupled design with an RFT-core comprising an explorer, trainer, and buffer, and supports agent-environment interaction with data pipelines optimized for RFT. iv) The framework can be adapted for diverse applications, offering a platform for exploring advanced reinforcement learning paradigms, but specific quantitative results are not provided. v) Trinity-RFT provides AI practitioners a unified platform for experimenting with various RFT methodologies and exploring advanced RL paradigms for LLMs.  |
| Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16270) or [HuggingFace](https://huggingface.co/papers/2505.16270))| Ruizhong Qiu, Yunzhe Qi, Zihao Li, Yikun Ban, jiaruz2 | i) This paper introduces Transformer Copilot, a novel Pilot-Copilot framework for LLM fine-tuning that leverages an internal "Mistake Log" to enhance inference performance. ii) The primary objective is to improve LLM inference performance by retaining and leveraging the model's own learning signals during standard fine-tuning. iii) The methodology involves creating a Mistake Log to track model errors and training a Copilot model to rectify the Pilot model's logits through a joint training and fused inference paradigm. iv) Experiments across 12 benchmarks showed that Transformer Copilot consistently improves performance by up to 34.5%. v) The principal implication for AI practitioners is a novel approach for enhancing LLM performance with marginal computational overhead by exploiting internal learning signals, offering a potential improvement on standard supervised fine-tuning techniques.  |
| Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark
  Study (Read more on [arXiv](https://arxiv.org/abs/2505.15389) or [HuggingFace](https://huggingface.co/papers/2505.15389))| Hwanjo Yu, Jihae Jeong, Joonwon Jang, oneonlee | i) This paper introduces MEMESAFETYBENCH, a meme-based benchmark for evaluating VLM safety. ii) The study investigates the safety of VLMs when processing real-world meme images shared by ordinary users. iii) The methodology involves pairing real meme images with both harmful and benign instructions, utilizing a comprehensive safety taxonomy and LLM-based instruction generation to assess VLM responses across single and multi-turn interactions. iv) Results show VLMs are more vulnerable to meme-based harmful prompts than synthetic or typographic images, with memes significantly increasing harmful responses and decreasing refusals, and multi-turn interactions providing only partial mitigation. v) The findings highlight the necessity for ecologically valid VLM evaluations and stronger safety mechanisms due to the identified vulnerability of current VLMs to real-world meme-based prompts.  |
| Large Language Models Implicitly Learn to See and Hear Just By Reading (Read more on [arXiv](https://arxiv.org/abs/2505.17091) or [HuggingFace](https://huggingface.co/papers/2505.17091))| Mert Pilanci, Prateek Verma | i) This paper explores using text-LLM weights for audio and image classification by repurposing text-only trained models for non-text modalities. ii) The research investigates whether text-LLMs can inherently develop the ability to understand images and audio without explicit training on these modalities. iii) The methodology involves replacing the ViT/Audio-Transformer encoder with a fine-tuned text-LLM (GPT-2) and training a linear projection to map image/audio patches to the LLM's embedding space, employing PEFT techniques such as LORA. iv) The results show that fine-tuning text-LLMs for audio classification on FSD-50K achieves an accuracy of 44.1% with a GPT-1.5B backbone, and the method is applicable to image classification on CIFAR-10 and Fashion-MNIST, yielding competitive accuracy. v) AI practitioners can potentially reuse pre-trained text-LLMs for multimodal tasks by fine-tuning a small number of parameters instead of training new models from scratch, offering an efficient approach to representation learning for audio and image processing.  |
| Synthetic Data RL: Task Definition Is All You Need (Read more on [arXiv](https://arxiv.org/abs/2505.17063) or [HuggingFace](https://huggingface.co/papers/2505.17063))| Zekai Zhang, Zi-Ang Wang, Chuanwei Huang, Yiduo Guo, zguo0525 | i) The paper introduces Synthetic Data RL, a framework for adapting foundation models to specialized tasks using only synthetic data generated from task definitions. ii) The main objective is to reduce reliance on human-labeled data in reinforcement learning for adapting foundation models by using synthetically generated data based on task definitions. iii) The methodology involves generating question-answer pairs from task definitions and retrieved documents, adapting question difficulty based on model solvability, and selecting questions for RL training using the model's average pass rate. iv) The method achieves a 29.2% absolute improvement over the base model on GSM8K and also achieves 8.7% on MATH, 13.1% on GPQA, 8.9% on MedQA, 17.7% on CQA(law), and 13.7% on CFA (finance). v) This framework allows AI practitioners to adapt models efficiently without extensive human annotation, enabling more scalable and efficient RL-based model adaptation.  |
| RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation
  via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17540) or [HuggingFace](https://huggingface.co/papers/2505.17540))| Jianjin Zhang, Fangkai Yang, Pu Zhao, Lu Wang, Mingrui Wu | RePrompt enhances text-to-image generation by integrating explicit reasoning into the prompt refinement process. The research aims to improve the fidelity of T2I models in capturing user intentions from concise prompts. The methodology employs reinforcement learning to train a language model that generates structured, self-reflective prompts optimized for image-level outcomes based on human preference, semantic alignment, and visual composition. Experiments on GenEval demonstrate that RePrompt surpasses Qwen2.5 3B-enhanced baselines by +77.1% in the position category, indicating superior spatial grounding. RePrompt offers AI practitioners a method for boosting spatial layout fidelity and compositional generalization in T2I applications without relying on large language models or expensive optimization.  |
| On the Design of KL-Regularized Policy Gradient Algorithms for LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.17508) or [HuggingFace](https://huggingface.co/papers/2505.17508))| Quanquan Gu, Yang Yuan, Huizhuo Yuan, Lewis-Lau, yifAI | i) The paper proposes Regularized Policy Gradient (RPG), a framework for deriving and analyzing KL-regularized policy gradient methods in online reinforcement learning. ii) The main objective is to systematically explore how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online RL. iii) The methodology involves deriving policy gradients and surrogate loss functions for objectives regularized by forward and reverse KL divergences with normalized and unnormalized policy distributions and comparing them to GRPO, REINFORCE++, and DAPO. iv) Experiments show improved training stability and performance compared to baselines; for example, RPG-FKL achieves the best overall score (Best: 0.8836) on AMC23. v) The RPG framework and its variants can be adopted by AI practitioners for enhanced control over training dynamics and improved mathematical reasoning capabilities in LLMs through systematic KL-regularized policy gradient optimization.  |
| Diffusion Classifiers Understand Compositionality, but Conditions Apply (Read more on [arXiv](https://arxiv.org/abs/2505.17955) or [HuggingFace](https://huggingface.co/papers/2505.17955))| Anna Rohrbach, Seong Joon Oh, Yujin Jeong, Gigglingface | i) This paper investigates the compositional understanding capabilities of diffusion classifiers across diverse tasks and models. ii) The research aims to comprehensively evaluate and understand the discriminative abilities of diffusion classifiers in compositional scenarios, considering factors like dataset domains and timestep weighting. iii) The study employs an extensive evaluation of diffusion classifiers (SD 1.5, 2.0, and 3-m) on 10 compositional datasets, introducing a new diagnostic benchmark (SELF-BENCH) comprising diffusion-generated images, and explores timestep weighting strategies. iv) Results show diffusion models underperform CLIP on counting tasks and exhibit sensitivity to domain gaps, where SD3-m's discriminative accuracy reaches only 39%, with timestep reweighting improving performance in large domain gap scenarios; a correlation between visual similarity (CLIP distance) and optimal timestep weighting is also observed. v) AI practitioners should consider domain adaptation techniques and timestep weighting strategies when deploying diffusion classifiers for compositional tasks, particularly for models like SD3-m which demonstrates enhanced sensitivity.  |
| Interactive Post-Training for Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2505.17016) or [HuggingFace](https://huggingface.co/papers/2505.17016))| Philipp Krähenbühl, Yue Zhao, Kairan Dou, tanshh97 | RIPT-VLA introduces a reinforcement learning-based post-training paradigm to enhance vision-language-action (VLA) models. The research aims to improve VLA model adaptability to new tasks and environments using minimal supervision by interactive learning. The methodology involves a novel reinforcement learning framework with dynamic rollout sampling and leave-one-out advantage estimation, optimizing for sparse binary success rewards. Experiments show RIPT-VLA improves the 7B OpenVLA-OFT model to a 97.5% success rate and enhances the QueST model by 10.9% absolute success rate across various LIBERO suites. RIPT-VLA offers AI practitioners a computationally efficient and data-efficient method for post-training VLA models, enabling significant performance gains, particularly in low-data regimes.  |
| Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA (Read more on [arXiv](https://arxiv.org/abs/2505.16293) or [HuggingFace](https://huggingface.co/papers/2505.16293))| Sai Rajeswar, Shiva Krishna Reddy Malay, Khyati Mahajan, Masoud Hashemi, rmahesh | i) This paper introduces NotesWriting, a method to enhance iterative Retrieval-Augmented Generation (RAG) by generating concise notes from retrieved documents at each step. ii) The research aims to improve the effective context length of LLMs in iterative RAG, addressing issues of context overload, computational cost, distraction, and readability. iii) The proposed method involves using a smaller language model to extract relevant notes from retrieved documents based on the sub-question at each step, replacing raw documents with shorter, focused notes. iv) Experiments across three iterative RAG baselines, four multi-hop QA datasets, and two LLMs showed that NotesWriting yields an average improvement of 15.6 percentage points overall with minimal increase in output tokens. v) NotesWriting allows practitioners to improve the planning and reasoning capabilities of LLMs by increasing the volume of ingested text while using it alongside iterative RAG frameworks for multi-hop question answering tasks.  |
| NOVER: Incentive Training for Language Models via Verifier-Free
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16022) or [HuggingFace](https://huggingface.co/papers/2505.16022))| Yali Du, Chen Qian, Xinyu Wang, Siya Qi, Wei Liu | NOVER proposes a verifier-free reinforcement learning framework for incentive training of language models. The research investigates how to enable incentive training across text-to-text tasks without external verifiers or costly reward models. NOVER utilizes a model's own reasoning process perplexity as a reward proxy, calculated from standard supervised fine-tuning data, for lightweight RL training. Experiments show NOVER outperforms models distilled from large reasoning models like DeepSeek R1 671B by 7.7%. NOVER enables incentive-driven reinforcement learning across a range of tasks and facilitates approaches such as inverse incentive training. This work allows practitioners to apply incentive training to text generation tasks lacking readily available external verifiers.  |
| Keep Security! Benchmarking Security Policy Preservation in Large
  Language Model Contexts Against Indirect Attacks in Question Answering (Read more on [arXiv](https://arxiv.org/abs/2505.15805) or [HuggingFace](https://huggingface.co/papers/2505.15805))| Hwanhee Lee, Yonghyun Jun, Yumin Kim, HwanChang0106 | i) This paper introduces CoPriva, a benchmark for evaluating the ability of large language models (LLMs) to preserve contextual security policies against direct and indirect attacks in question answering. ii) The primary objective is to assess the vulnerability of LLMs in adhering to user-defined security policies within context, especially concerning information non-disclosure, under adversarial conditions. iii) The methodology involves creating a dataset of 4,184 instances derived from realistic contexts with explicit security policies and designing direct and indirect attack queries. iv) Evaluation of 10 LLMs revealed a significant vulnerability; many models leaked sensitive information, with indirect attacks exacerbating the issue, increasing leakage by over 40 percentage points on average. v) AI practitioners should be aware that current LLMs exhibit a critical gap in safety alignment for sensitive applications, necessitating more robust methods to guarantee contextual security, especially when models with higher faithfulness scores tend to leak more information because of a trade-off between helpfulness and policy compliance.  |
| TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in
  Real-World Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.12891) or [HuggingFace](https://huggingface.co/papers/2505.12891))| Tianyi Zhuang, Wen Luo, Wei Li, Shaohang Wei, songff | i) The paper introduces TIME, a multi-level benchmark for evaluating temporal reasoning in Large Language Models (LLMs) across real-world scenarios. ii) The main objective is to address the limitations of existing benchmarks by capturing real-world challenges such as intensive temporal information, fast-changing event dynamics, and complex social interactions. iii) The methodology involves constructing three sub-datasets, TIME-WIKI, TIME-NEWS, and TIME-DIAL, encompassing 38,522 QA pairs across 11 fine-grained sub-tasks, and evaluating various reasoning and non-reasoning LLMs. iv) Experimental results show that models exhibit suboptimal performance on the Timeline task, with small-scale vanilla models achieving accuracy below 10% on both TIME-WIKI and TIME-DIAL datasets. v) LLM practitioners should consider TIME-LITE, a human-annotated subset designed to foster research and standardized evaluation of temporal reasoning, which includes 938 carefully curated instances.  |
| Not All Models Suit Expert Offloading: On Local Routing Consistency of
  Mixture-of-Expert Models (Read more on [arXiv](https://arxiv.org/abs/2505.16056) or [HuggingFace](https://huggingface.co/papers/2505.16056))| Duyu Tang, Yitong Li, Miren Tian, Siyuan Wang, ljcleo | i) This paper introduces two metrics, SRP and SCH, to evaluate the local routing consistency in Mixture-of-Expert (MoE) language models for efficient expert offloading. ii) The research investigates how local routing consistency varies across different MoE model architectures and expert specializations, with the goal of guiding memory-efficient MoE deployment. iii) The methodology involves analyzing 20 MoE LLMs by quantifying local routing consistency using Segment Routing Best Performance (SRP) and Segment Cache Best Hit Rate (SCH) metrics across different segment lengths and cache sizes. iv) The primary results show that MoE models applying MoE on every layer and without shared experts exhibit the highest local routing consistency, and a cache size of approximately 2x the number of active experts achieves optimal balance, furthermore SRP strongly correlated to domain specialization of experts. v) AI practitioners can use these metrics to design and deploy memory-efficient MoE models, balancing cache effectiveness and efficiency during inference, and prioritize models exhibiting high local routing consistency for easier expert offloading strategies.  |
| Revisiting Residual Connections: Orthogonal Updates for Stable and
  Efficient Deep Networks (Read more on [arXiv](https://arxiv.org/abs/2505.11881) or [HuggingFace](https://huggingface.co/papers/2505.11881))| Younjae Yu, Suhwan Choi, Siyeol Kim, Woohyun Cho, Giyeong Oh | i) This paper introduces Orthogonal Residual Update, a novel technique for enhancing deep network training. ii) The research objective is to improve generalization accuracy and training stability in deep neural networks by modifying residual connections. iii) The methodology involves decomposing the module's output into components parallel and orthogonal to the input stream, adding only the orthogonal component during the update. iv) Experiments across diverse architectures and datasets, including ViT-B on ImageNet-1k, demonstrated a +4.3%p top-1 accuracy gain. v) Orthogonal Residual Update can improve deep network design, offering enhanced performance and efficiency, which can lead to practical improvements in stability of AI models.  |
