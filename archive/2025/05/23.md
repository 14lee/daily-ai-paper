

## Papers for 2025-05-23

| Title | Authors | Summary |
|-------|---------|---------|
| NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop
  System from Hypothesis to Verification (Read more on [arXiv](https://arxiv.org/abs/2505.16938) or [HuggingFace](https://huggingface.co/papers/2505.16938))| Jiakang Yuan, Xiangchao Yan, Shiyang Feng, Bo Zhang, NovelSeek Team | i) The paper presents NOVELSEEK, a closed-loop multi-agent framework for autonomous scientific research (ASR) across various scientific domains. ii) The research aims to facilitate innovative research by automating the entire research cycle from idea generation to experimental validation. iii) The methodology involves self-evolving idea generation with human interaction, idea-to-methodology construction, and multi-round automated experiment execution with a unified multi-agent system. iv) In reaction yield prediction, NOVELSEEK improved performance from 27.6% to 35.4% in 12 hours and increased enhancer activity prediction accuracy from 0.52 to 0.79 in 4 hours. In 2D semantic segmentation, precision rose from 78.8% to 81.0% in 30 hours. v) The NOVELSEEK framework enables AI practitioners to automate and accelerate scientific research tasks, reducing reliance on manual effort and enabling faster innovation cycles across diverse scientific fields including domains with complex codes. |
| Scaling Reasoning, Losing Control: Evaluating Instruction Following in
  Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.14810) or [HuggingFace](https://huggingface.co/papers/2505.14810))| Yu Cheng, Xiaoye Qu, Jiawei Gu, yaful, TingchenFu | i) The paper introduces MathIF, a new benchmark for evaluating instruction following in Large Reasoning Models (LRMs). ii) It investigates the tension between scaling reasoning capabilities and maintaining controllability in LRMs. iii) The methodology involves evaluating 23 LRMs on MathIF, which contains 420 math reasoning problems combined with 15 programmatically verifiable instruction constraints. iv) Results show that the best-performing model, Qwen3-14B, achieves only 50.71% accuracy on strict instruction following, and increasing CoT length degrades instruction adherence. v) This suggests AI practitioners face a trade-off between improving reasoning depth and ensuring adherence to user-specified constraints during LRM development.  |
| Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16410) or [HuggingFace](https://huggingface.co/papers/2505.16410))| Hongjin Qian, Jiajie Jin, Xiaoxi Li, Yifei Chen, Guanting Dong | Tool-Star is an RL-based framework for LLMs to autonomously invoke multiple external tools during reasoning. The paper addresses the challenge of multi-tool collaborative reasoning in LLMs. It uses a tool-integrated reasoning data synthesis pipeline combining tool-integrated prompting with hint-based sampling, followed by a two-stage training framework. Experiments on 10 reasoning benchmarks show Tool-Star achieves over 40% average accuracy across datasets. The principal implication is an effective and efficient multi-tool collaboration method for enhancing LLM reasoning, providing AI practitioners with a means to improve LLM performance on complex tasks requiring external tool usage.  |
| KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models (Read more on [arXiv](https://arxiv.org/abs/2505.16707) or [HuggingFace](https://huggingface.co/papers/2505.16707))| Xianfang Zeng, Xinyu Ye, Xinting Hu, Zonghui Li, Yongliang Wu | i) The paper introduces KRIS-Bench, a benchmark for evaluating knowledge-based reasoning in instruction-based image editing models. ii) The research aims to assess the capacity of image editing models to perform tasks requiring factual, conceptual, and procedural knowledge. iii) The methodology involves creating a diagnostic benchmark with 22 tasks spanning 7 reasoning dimensions and a novel Knowledge Plausibility metric. iv) Experiments on 10 models revealed gaps in reasoning performance, with GPT-4o achieving the highest overall score but exhibiting limitations in accurately interpreting chemical reactions. v) KRIS-Bench provides AI practitioners with a fine-grained evaluation framework for developing knowledge-centric image editing systems.  |
| Pixel Reasoner: Incentivizing Pixel-Space Reasoning with
  Curiosity-Driven Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15966) or [HuggingFace](https://huggingface.co/papers/2505.15966))| Fangzhen Lin, Weimin Ren, Haozhe Wang, Alex Su, wenhu | i) This paper introduces Pixel-Reasoner, a novel framework enabling Vision-Language Models (VLMs) to reason directly in the pixel space using visual operations. ii) The primary objective is to equip VLMs with visual reasoning operations like zoom-in and select-frame, allowing them to interact with and infer from visual data more effectively. iii) The methodology involves a two-phase training approach: instruction tuning on synthesized reasoning traces followed by reinforcement learning with a curiosity-driven reward scheme. iv) The 7B Pixel-Reasoner model achieves 84% accuracy on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, surpassing existing open-source models. v) The principal implication is that incentivizing pixel-space reasoning significantly improves VLM performance on visually intensive tasks, offering AI practitioners a method for enhancing visual understanding and reasoning capabilities.  |
| QuickVideo: Real-Time Long Video Understanding with System Algorithm
  Co-Design (Read more on [arXiv](https://arxiv.org/abs/2505.16175) or [HuggingFace](https://huggingface.co/papers/2505.16175))| Wenhu Chen, Tianyu Pang, Chao Du, Dongfu Jiang, Benjamin Schneider | i) QuickVideo accelerates long video understanding for VideoLLMs through system-algorithm co-design. ii) The research objective is to reduce the computational overhead of long video processing to enable real-time applications. iii) The methodology includes a parallelized CPU-based video decoder (QuickCodec), a memory-efficient prefilling method using KV-cache pruning (QuickPrefill), and an overlapping execution scheme. iv) QuickVideo reduces the inference time of a 30-minute video input by more than 3x, from 69.7 seconds to 20.0 seconds. v) QuickVideo provides AI practitioners with an optimized framework that can significantly accelerate long video understanding, enabling more efficient VideoLLM applications even on limited hardware.  |
| GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation
  with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17022) or [HuggingFace](https://huggingface.co/papers/2505.17022))| Linjiang Huang, Kun Wang, Yuqing Wang, Rongyao Fang, Chengqi Duan | i) GoT-R1 is a reinforcement learning framework to improve semantic-spatial reasoning for visual generation in MLLMs. ii) The research aims to enhance the ability of MLLMs to handle complex compositional prompts in visual generation through improved reasoning. iii) The methodology employs a dual-stage multi-dimensional reward framework with MLLMs to evaluate both the reasoning process and the final image output, optimized via Group Relative Policy Optimization (GRPO). iv) Experimental results on T2I-CompBench show significant improvements, particularly in compositional tasks; GoT-R1-7B achieved a score of 0.94 in two-object generation in GenEval benchmark, up from 0.69 of GoT; v) The framework's capacity to autonomously discover effective reasoning strategies via RL enables AI practitioners to generate more accurate and contextually aware visual content, enhancing compositional image synthesis.  |
| LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16933) or [HuggingFace](https://huggingface.co/papers/2505.16933))| Jun Zhou, Jun Hu, Xiaolu Zhang, Shen Nie, Zebin You | LLaDA-V is introduced as a purely diffusion-based Multimodal Large Language Model (MLLM) with visual instruction tuning. The research investigates how to effectively extend large language diffusion models for multimodal understanding, focusing on visual instruction tuning. LLaDA-V incorporates a vision encoder and MLP connector to project visual features into the language embedding space and is trained on multi-turn multimodal dialogues. LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs and demonstrates stronger data scalability on several benchmarks when compared to LLaMA3-V.  LLaDA-V's superior scalability when compared to LLaMA3-V suggests that large language diffusion models show promise in multimodal contexts.  |
| Risk-Averse Reinforcement Learning with Itakura-Saito Loss (Read more on [arXiv](https://arxiv.org/abs/2505.16925) or [HuggingFace](https://huggingface.co/papers/2505.16925))| Alexander Korotin, Evgeny Burnaev, Anita Toleutaeva, Olivier Croissant, i-udovichenko | i) This paper introduces a novel and numerically stable loss function based on Itakura-Saito divergence for risk-averse reinforcement learning with exponential utility. ii) The main objective is to develop a loss function that addresses the numerical instability issues of existing exponential-utility RL approaches while preserving theoretical guarantees. iii) The methodology involves deriving Bellman equations for exponential utility and employing reinforcement learning algorithms using the proposed Itakura-Saito loss function for learning state-value and action-value functions. iv) The experiments demonstrate that the proposed Itakura-Saito loss outperforms alternatives in portfolio optimization, deep hedging tasks, and robust combinatorial optimization problems, exhibiting more stable convergence. v) The Itakura-Saito loss offers AI practitioners a numerically stable and theoretically sound alternative to exponential MSE for training risk-averse RL agents, particularly in high-stakes applications requiring reliable convergence.  |
| Scaling Diffusion Transformers Efficiently via μP (Read more on [arXiv](https://arxiv.org/abs/2505.15270) or [HuggingFace](https://huggingface.co/papers/2505.15270))| Zhi Tian, Wei Huang, Rongzhen Wang, Xinyu Zhang, ChenyuZheng | i) This paper generalizes Maximal Update Parametrization (µP) to diffusion Transformers for efficient scaling. ii) The main objective is to determine if the µP properties observed in vanilla Transformers extend to diffusion Transformers, enabling stable hyperparameter transfer. iii) The methodology involves proving the µP formulation for diffusion Transformers aligns with vanilla Transformers and validating this through large-scale image and text-to-image generation experiments. iv) The primary result shows that DiT-XL-2-µP with a transferred learning rate achieves 2.9x faster convergence compared to the original DiT-XL-2; further scaling experiments on PixArt-a and MMDiT models also demonstrate improved performance. v) These results suggest that AI practitioners can leverage µP to efficiently scale diffusion Transformers, reducing hyperparameter tuning costs while maintaining or improving model performance in large-scale generation tasks.  |
| Let LLMs Break Free from Overthinking via Self-Braking Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14604) or [HuggingFace](https://huggingface.co/papers/2505.14604))| Wenqi Zhang, Haolei Xu, Yongliang Shen, Yuchen Yan, Haoran Zhao | i) The paper introduces Self-Braking Tuning (SBT), a novel framework enabling Large Reasoning Models (LRMs) to autonomously regulate reasoning length and mitigate overthinking. ii) The research aims to enable LRMs to autonomously recognize excessive reasoning and terminate their thinking process appropriately without external interventions. iii) The methodology involves constructing overthinking identification metrics, developing data construction strategies (SBT-E and SBT-D) for adaptive reasoning lengths, and introducing a braking prompt mechanism. iv) Experiments show that SBT reduces token consumption by up to 60% on mathematical benchmarks like AIME and GSM8K, while maintaining comparable accuracy to unconstrained models. v) For AI practitioners, SBT offers a method to significantly reduce computational overhead in LRMs by enabling self-regulation of reasoning depth, directly impacting the cost-effectiveness and deployment feasibility of these models.  |
| Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14684) or [HuggingFace](https://huggingface.co/papers/2505.14684))| Guiyang Hou, Wenqi Zhang, Yongliang Shen, Yuchen Yan, Haolei Xu | i) This paper introduces CoT-Bridge, a method to automatically identify and bridge Thought Leaps in Chain-of-Thought (CoT) reasoning to improve model learning and generalization. ii) The research aims to address the negative impact of Thought Leaps (omitted intermediate reasoning steps) on the performance of Large Language Models (LLMs) in mathematical tasks. iii) The authors constructed ScaleQM+, a specialized training dataset, and trained the CoT-Bridge model to detect leaps and generate missing intermediate reasoning steps. iv) Experiments on mathematical reasoning benchmarks show that models fine-tuned on bridged datasets outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. v) CoT-Bridge serves as a plug-and-play module compatible with existing optimization techniques to improve data quality for downstream tasks such as knowledge distillation and reinforcement learning, enhancing the effectiveness of LLMs in mathematical reasoning and improving generalization to other reasoning tasks, such as OOD logical reasoning tasks (↑2.99%).  |
| Backdoor Cleaning without External Guidance in MLLM Fine-tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16916) or [HuggingFace](https://huggingface.co/papers/2505.16916))| Xun Xiao, Jinhe Bi, Jian Liang, Wenke Huang, Xuankun Rong | i) This paper introduces Believe Your Eyes (BYE), a data filtering framework for mitigating backdoor attacks in multimodal large language models (MLLMs) during fine-tuning. ii) The research aims to address the security risks introduced by malicious fine-tuning in MLLMs, specifically the injection of backdoor triggers, without relying on external guidance. iii) BYE leverages cross-modal attention entropy as a self-supervised signal, extracting attention maps, computing entropy scores, profiling sensitive layers using bimodal separation, and employing unsupervised clustering to filter suspicious samples. iv) Experiments demonstrate BYE achieves near-zero attack success rates (e.g., reducing ASR to 7.18% on RSVQA with InternVL) while maintaining clean-task performance. v) AI practitioners can utilize BYE as a robust, generalizable, and self-contained solution for filtering poisoned data and enhancing the security of MLLMs in fine-tuning-as-a-service settings without clean supervision or model modifications.  |
| Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel
  Decoding (Read more on [arXiv](https://arxiv.org/abs/2505.16990) or [HuggingFace](https://huggingface.co/papers/2505.16990))| Xinchao Wang, Xinyin Ma, Runpeng Yu | Dimple is a Discrete Diffusion Multimodal Large Language Model (DMLLM) designed for parallel decoding. The research addresses training instability, suboptimal performance, and length bias observed in purely discrete diffusion approaches for DMLLMs. The methodology combines an initial autoregressive pre-training phase with a subsequent diffusion-based masked language modeling phase, incorporating confident decoding to improve inference efficiency. The Dimple-7B model achieves a 3.9% performance increase over LLaVA-NEXT on MLLM benchmarks using a similar training data, indicating comparable performance of DMLLM to autoregressive models under similar training data scales. For AI practitioners, this demonstrates the feasibility of DMLLMs and provides techniques for enhancing inference efficiency and controllability in multimodal generation tasks, offering a new paradigm beyond autoregressive generation.  |
| VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game
  Quality Assurance (Read more on [arXiv](https://arxiv.org/abs/2505.15952) or [HuggingFace](https://huggingface.co/papers/2505.15952))| Nabajeet Barman, Saman Zadtootaghaj, Abhijay Ghildyal, corpaul, taesiri | i) The paper introduces VideoGameQA-Bench, a new benchmark for evaluating Vision-Language Models (VLMs) in video game Quality Assurance (QA) tasks. ii) The main objective is to provide a comprehensive benchmark to assess VLM performance in real-world game QA scenarios, including visual unit testing, glitch detection, and bug reporting. iii) The methodology involves curating a dataset of 4,786 questions and images/videos from over 800 games and 9 synthetic scenes, followed by evaluating the performance of 16 VLMs on the defined tasks. iv) The primary result indicates that frontier VLMs achieve up to 82.8% accuracy on image-based glitch detection and 78.1% on video-based glitch detection, but struggle with tasks requiring fine-grained detail analysis and common-sense reasoning. v) The principal implication for AI practitioners is the identification of specific limitations in current VLMs for automating video game QA, highlighting the need for improved spatial reasoning and detail extraction capabilities.  |
| Training-Free Efficient Video Generation via Dynamic Token Carving (Read more on [arXiv](https://arxiv.org/abs/2505.16864) or [HuggingFace](https://huggingface.co/papers/2505.16864))| Bohao Peng, Shaoteng Liu, Bin Xia, Jinbo Xing, Yuechen Zhang | i) This paper presents Jenga, a training-free inference pipeline to improve the efficiency of video generation using Diffusion Transformer (DiT) models. ii) The main objective is to reduce the computational cost associated with DiT models for video generation without requiring model retraining. iii) The methodology combines dynamic attention carving, using 3D space-filling curves to select relevant token interactions, with a progressive resolution generation strategy. iv) Jenga achieves up to 8.83x speedup on HunyuanT2V with only a 0.01% performance drop on VBench. v) Jenga's plug-and-play nature enables practical, high-quality video generation on modern hardware by significantly reducing inference time, making it relevant for AI practitioners seeking to deploy video generation models efficiently.  |
| Understanding Generative AI Capabilities in Everyday Image Editing Tasks (Read more on [arXiv](https://arxiv.org/abs/2505.16181) or [HuggingFace](https://huggingface.co/papers/2505.16181))| Franck Dernoncourt, Viet Dac Lai, loganbolton, Franck-Dernoncourt, taesiri | This paper analyzes generative AI's effectiveness in real-world image editing. It addresses the question of what types of image editing requests can be successfully handled by current AI editors compared to human editors. The study involves analyzing 83k real-world requests from the /r/PhotoshopRequest Reddit community with their corresponding 305k human-made edits, evaluating them against edits from 49 AI editors and ratings from vision-language models (VLMs). The primary result indicates that AI editors can fulfill approximately 33% of real-world image-editing requests, based on human ratings, with VLMs showing biased judgements. The principal implication is that AI practitioners should focus on improving AI editors' ability to handle precise editing tasks and preserve subject identity, as well as addressing biases in VLM judgment metrics.  |
| SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward (Read more on [arXiv](https://arxiv.org/abs/2505.17018) or [HuggingFace](https://huggingface.co/papers/2505.17018))| Xiangyu Yue, Dongzhan Zhou, Haoming Lyu, Kaituo Feng, Kaixuan Fan | i) The paper introduces SophiaVL-R1, a multimodal large language model trained with Trust-GRPO, incorporating model-generated thinking rewards alongside rule-based outcome rewards. ii) The main objective is to enhance MLLMs' reasoning and generalization capabilities by providing supervision over the thinking process. iii) The methodology involves training a thinking reward model, implementing Trust-GRPO to weigh the thinking reward's trustworthiness, and using an annealing training strategy. iv) Experimental results show SophiaVL-R1-7B achieves 71.3% accuracy on MathVista and outperforms LLaVA-OneVision-72B on multiple benchmarks, and demonstrate consistently strong performance across general ability benchmarks. v) AI practitioners can utilize the Trust-GRPO algorithm to improve the reliability of reward signals in reinforcement learning for MLLMs, leading to better reasoning and generalization.  |
| SpatialScore: Towards Unified Evaluation for Multimodal Spatial
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.17012) or [HuggingFace](https://huggingface.co/papers/2505.17012))| Yanfeng Wang, Ya Zhang, Yaohui Chen, Xiao Huang, Haoning Wu | i) The paper introduces SpatialScore, a comprehensive benchmark for evaluating spatial understanding in multimodal large language models (MLLMs). ii) The main objective is to assess the capabilities of existing MLLMs in 3D spatial perception and understanding. iii) The methodology involves creating a new benchmark, SpatialScore, integrating a novel dataset VGBench with data from 11 existing datasets, and developing SpatialAgent, a multi-agent system equipped with specialized tools for spatial reasoning. iv) The SpatialScore benchmark includes 28K samples with a challenging subset (SpatialScore-Hard) of 1.4K samples, and evaluations reveal that while SpatialAgent improves performance, current MLLMs still lag behind human performance. v) The comprehensive and diverse nature of SpatialScore provides AI practitioners with a rigorous testbed and insights for future MLLM development, highlighting the need for fundamental architectural innovations in spatial reasoning.  |
| LaViDa: A Large Diffusion Language Model for Multimodal Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.16839) or [HuggingFace](https://huggingface.co/papers/2505.16839))| Yusuke Kato, Akash Gokul, Hritik Bansal, Konstantinos Kallidromitis, Shufan Li | LaViDa introduces a diffusion-based VLM for multimodal understanding, offering an alternative to autoregressive models. The research focuses on developing diffusion models (DMs) for vision-language tasks using complementary masking, Prefix-DLM inference, and timestep shifting techniques. Experiments show LaViDa achieves competitive performance on multimodal benchmarks like MMMU while providing advantages such as speed-quality tradeoff; specifically, LaViDa surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr on COCO captioning with a 1.92x speedup. This work offers AI practitioners a competitive, controllable, and efficient VLM alternative to autoregressive models, especially for tasks requiring bidirectional reasoning or flexible speed-quality trade-offs. The paper seems to lack information on the exact model architecture and datasets used to train the model.  |
| TinyV: Reducing False Negatives in Verification Improves RL for LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.14625) or [HuggingFace](https://huggingface.co/papers/2505.14625))| Luyao Niu, Bhaskar Ramasubramanian, Fengqing Jiang, Yuetai Li, Zhangchen Xu | TinyV reduces false negatives in verification to improve RL for LLM reasoning. The paper investigates the impact of false negatives (FNs) in reward signals provided by verifiers during RL training of LLMs for reasoning tasks. It mitigates FNs by proposing TINYV, a lightweight LLM-based verifier that augments rule-based methods. Empirical analysis on the Big-Math-RL-Verified dataset reveals over 38% of model-generated responses suffer from false negatives, impairing RL training. Integrating TINYV boosts pass rates by up to 10% across math-reasoning benchmarks and accelerates convergence relative to baselines. Addressing verifier false negatives is critical for improving RL-based fine-tuning of LLMs, allowing for more robust policy optimization.  |
| Training-Free Reasoning and Reflection in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2505.16151) or [HuggingFace](https://huggingface.co/papers/2505.16151))| Zhenzhong Chen, Hongchen Wei | i) The paper introduces FRANK, a training-free method for endowing Multimodal Large Language Models (MLLMs) with reasoning and reflection capabilities. ii) The main objective is to enhance the reasoning abilities of existing MLLMs without requiring additional training data or gradient updates. iii) FRANK leverages a hierarchical weight merging approach that combines a vision-pretrained MLLM with a reasoning-specialized LLM, guided by layer-wise functional specialization and Taylor-derived closed-form fusion. iv) FRANK-38B achieves an accuracy of 69.2 on the MMMU benchmark, outperforming InternVL2.5-38B by +5.3 and surpassing GPT-40. v) FRANK provides AI practitioners with a cost-effective strategy to imbue off-the-shelf MLLMs with advanced reasoning capabilities, eliminating the need for resource-intensive retraining or scarce, high-quality multimodal reasoning datasets.  |
| GRIT: Teaching MLLMs to Think with Images (Read more on [arXiv](https://arxiv.org/abs/2505.15879) or [HuggingFace](https://huggingface.co/papers/2505.15879))| Ching-Chen Kuo, Kaizhi Zheng, Diji Yang, Xuehai He, Yue Fan | i) The paper introduces Grounded Reasoning with Images and Text (GRIT), a method for training Multimodal Large Language Models (MLLMs) to generate reasoning chains grounded in visual data using bounding box coordinates. ii) The research aims to enable MLLMs to perform visual reasoning with explicit integration of visual information via grounded reasoning chains. iii) GRIT uses a reinforcement learning approach, GRPO-GR, employing rewards focused on answer accuracy and the format of grounded reasoning outputs, eliminating the need for reasoning chain annotations or bounding box labels. iv) Experiments show that GRIT-trained models, using only 20 image-question-answer triplets from VSR and TallyQA, achieve a GPT-as-judge answer accuracy of 72.9% on VSR and 47.8% on TallyQA. v) GRIT offers AI practitioners a data-efficient method for training MLLMs to generate coherent, visually-grounded reasoning chains, unifying grounding and reasoning abilities without extensive data annotation.  |
| AGENTIF: Benchmarking Instruction Following of Large Language Models in
  Agentic Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.16944) or [HuggingFace](https://huggingface.co/papers/2505.16944))| Youfeng Liu, Amy Xin, Xiaozhi Wang, Hao Peng, Yunjia Qi | AGENTIF is introduced as a benchmark for evaluating instruction following in LLMs within agentic contexts. The research addresses whether LLMs can reliably follow lengthy instructions with complex constraints common in real-world agentic applications. The study uses 707 human-annotated instructions across 50 real-world agentic tasks annotated with constraints and evaluation metrics including code-based, LLM-based, and hybrid methods. Results show current models perform poorly, especially with complex constraint structures and tool specifications; the best-performing model follows fewer than 30% of instructions perfectly. AGENTIF highlights the need for improved LLMs in adhering to complex instructions for AI practitioners developing LLM-based agents, particularly concerning conditional and tool constraints.  |
| Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.16854) or [HuggingFace](https://huggingface.co/papers/2505.16854))| Mike Zheng Shou, James Cheng, Kevin Qinghong Lin, Jiaqi Wang | i) This paper introduces TON, a reinforcement learning framework for vision-language models that enables selective reasoning to improve efficiency. ii) The research aims to enable VLMs to decide when reasoning is necessary, reducing unnecessary computation. iii) TON employs a two-stage training strategy: supervised fine-tuning (SFT) with "thought dropout" and group relative policy optimization (GRPO) to maximize task-aware outcome rewards. iv) Experiments show that TON reduces completion length by up to 90% compared to vanilla GRPO without sacrificing performance, and in some cases improving it, along with up to a 17% accuracy improvement on GeoQA. v) TON allows AI practitioners to significantly reduce computational costs in VLMs by adaptively allocating reasoning based on task complexity.  |
| AceReason-Nemotron: Advancing Math and Code Reasoning through
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16400) or [HuggingFace](https://huggingface.co/papers/2505.16400))| Chankyu Lee, Zihan Liu, Yang Chen, wping, zhuoliny | i) The paper introduces AceReason-Nemotron, a reinforcement learning approach to enhance math and code reasoning in language models. ii) The research investigates how large-scale RL can improve reasoning capabilities of small- and mid-sized SFT models beyond distillation-based methods. iii) The methodology involves separate math-only and code-only RL training stages, along with robust data curation and curriculum learning with increasing response lengths. iv) AceReason-Nemotron achieves +14.6% / +17.2% improvement on AIME 2025 math benchmark for the 7B / 14B models and +6.8% / +5.8% on LiveCodeBench for 7B / 14B models through math-only RL. v) AI practitioners can leverage this approach to improve reasoning performance in smaller models by employing separate domain-specific RL training stages, particularly math-only RL for cross-domain improvements.  |
| VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced
  Multimodal Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2505.16192) or [HuggingFace](https://huggingface.co/papers/2505.16192))| Haiyang Xu, Han Yang, Wei Ye, Yongrui Heng, Chaoya Jiang | i) The paper introduces VLM-R³, a framework enhancing multimodal reasoning in Visual Language Models (MLLMs) through region recognition, reasoning, and refinement. ii) The main objective is to equip MLLMs with the ability to dynamically focus on and revisit visual regions to improve the grounding of textual reasoning in visual evidence. iii) The methodology includes a Region-Conditioned Reinforcement Policy Optimization (R-GRPO) training paradigm and a curated Visuo-Lingual Interleaved Rationale (VLIR) corpus for step-level supervision on region selection and textual justification. iv) The primary result shows VLM-R³ achieves state-of-the-art performance on MathVista, ScienceQA, and other benchmarks, with a 2.2% improvement on MathVista and a 14.33% improvement on ScienceQA. v) The principal implication for AI practitioners is a new benchmark for fine-grained, visually-grounded inference, especially for tasks requiring subtle spatial reasoning or fine-grained visual cue extraction.  |
| OViP: Online Vision-Language Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15963) or [HuggingFace](https://huggingface.co/papers/2505.15963))| Cheng Zeng, Jianxiang Wang, Zejun Li, Siyuan Wang, Shujun Liu | OViP: Online Vision-Language Preference Learning (OViP) dynamically constructs contrastive training data for large vision-language models (LVLMs) by using the model's own hallucinated outputs to mitigate misalignment with visual inputs. The research aims to improve LVLM's faithfulness to visual content by adaptively aligning textual and visual preferences. OViP dynamically constructs contrastive training data using real-time sampling of LVLM outputs and synthesizes negative images using a diffusion model based on semantic differences between response pairs. Experiments show OViP achieves a Hallucination Reduction Index (HRI) of 9.58 on the LLaVA-1.5-7B model, demonstrating reduced hallucinations while preserving multi-modal capabilities. This failure-driven training approach allows AI practitioners to adaptively align both textual and visual preferences, reducing hallucinations in LVLMs more effectively compared to methods relying on static datasets.  |
| Reinforcement Learning Finetunes Small Subnetworks in Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.11711) or [HuggingFace](https://huggingface.co/papers/2505.11711))| Hao Peng, Dilek Hakkani-Tur, Lifan Yuan, sagnikM | i) Reinforcement learning (RL) in Large Language Models (LLMs) induces parameter update sparsity, affecting only a small subnetwork. ii) This paper investigates the extent and implications of RL-induced parameter update sparsity during LLM finetuning, and if a subnetwork alone can reproduce the full-finetuned model. iii) Publicly released LLM checkpoints finetuned with various RL algorithms were analyzed, measuring update sparsity by comparing parameters before and after RL or SFT, and a subnetwork-only finetuning approach was evaluated. iv) Across different RL algorithms and LLMs, RL finetuning updates only 5%-30% of the parameters, while the rest remain effectively unchanged; Finetuning this subnetwork alone can match or surpass full-model finetuning performance, suggesting the remaining parameters play little role. v) AI practitioners can potentially reduce computational costs in RL-based LLM finetuning by focusing optimization on small, consistently active subnetworks without significant performance degradation, thereby allowing for more efficient resource allocation.  |
| Let Androids Dream of Electric Sheep: A Human-like Image Implication
  Understanding and Reasoning Framework (Read more on [arXiv](https://arxiv.org/abs/2505.17019) or [HuggingFace](https://huggingface.co/papers/2505.17019))| Yazhe Niu, Chenhao Zhang | i) This paper introduces Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. ii) The main objective is to address the limitations of existing multimodal large language models (MLLMs) in understanding the contextual implications of images. iii) LAD employs a three-stage framework: Perception, Search, and Reasoning, which converts visual information into textual representations, integrates cross-domain knowledge, and generates context-aligned implications via explicit reasoning. iv) Experiments show that LAD achieves state-of-the-art (SOTA) performance on an English image implication benchmark and demonstrates a 68.2% relative improvement on the English Multiple-Choice Question task compared to the GPT-40-mini model. v) LAD provides AI practitioners with a new methodology for enhancing the contextual understanding of images by AI systems through a framework that simulates human-like cognitive processes, potentially improving vision-language reasoning capabilities.  |
| SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16186) or [HuggingFace](https://huggingface.co/papers/2505.16186))| Aosong Feng, Jayanth Srinivasa, Gaowen Liu, Xuandong Zhao, Kaiwen Zhou | i) SafeKey enhances safety reasoning in Large Reasoning Models (LRMs) against harmful queries and jailbreak attacks. ii) The paper investigates how to improve safety generalization in LRMs, specifically addressing the limitations of supervised fine-tuned models against unseen malicious prompts. iii) The method proposes a "SafeKey" framework with two objectives: a Dual-Path Safety Head to enhance safety signals and Query-Mask Modeling to improve attention on query understanding. iv) Experiments show SafeKey lowers the average harmfulness rate by 9.6% across safety benchmarks, while maintaining general abilities. v) SafeKey provides AI practitioners with a method to reshape internal attention patterns and improve hidden representation quality for more robust safety alignment in LRMs.  |
| Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot
  Manipulation Datasets (Read more on [arXiv](https://arxiv.org/abs/2505.15517) or [HuggingFace](https://huggingface.co/papers/2505.15517))| Ken Goldberg, Zehan Ma, Shuangyu Xie, keplerccc | i) Robo2VLM is introduced as a framework for generating a Visual Question Answering (VQA) dataset from real-world robot manipulation trajectories to evaluate and enhance VLMs. ii) The research aims to improve VLMs' spatial and interaction reasoning capabilities through a dataset derived from robotic manipulation. iii) The methodology involves segmenting robot trajectories into manipulation phases using proprioceptive and kinematic data to generate VQA pairs with spatial, goal-conditioned, and interaction-based questions. iv) The paper presents Robo2VLM-1, a dataset with 684,710 questions, and shows that fine-tuning LLaVA on it improves spatial and interaction capabilities, with a maximum 50% accuracy gain in state reasoning and task understanding. v) The Robo2VLM-1 dataset provides AI practitioners with a benchmark to evaluate and fine-tune VLMs for enhanced spatial reasoning in robotic manipulation tasks.  |
| Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.17015) or [HuggingFace](https://huggingface.co/papers/2505.17015))| Xiaodong Wang, Xingyu Chen, Hao Tang, Weiyao Wang, Runsen Xu | Multi-SpatialMLLM introduces a novel framework for enhancing spatial understanding in MLLMs across multiple frames. The research aims to equip MLLMs with robust multi-frame spatial reasoning capabilities. It employs the MultiSPA dataset, a collection of 27 million samples, and a benchmark to test a spectrum of spatial tasks. Multi-SpatialMLLM achieves a 36% average performance gain over baselines and proprietary systems on spatial reasoning tasks. The model's improved multi-frame spatial reasoning offers AI practitioners an effective tool for advancing robotics and autonomous systems by providing enhanced spatial awareness. The paper lacks information regarding the specific architecture details of Multi-SpatialMLLM.  |
| Steering Large Language Models for Machine Translation Personalization (Read more on [arXiv](https://arxiv.org/abs/2505.16612) or [HuggingFace](https://huggingface.co/papers/2505.16612))| Malvina Nissim, Elisabetta Fersini, Arianna Bisazza, Daniel Scalena, gsarti | i) This paper explores methods for personalizing large language model (LLM)-based machine translation in low-resource literary settings using prompting and steering techniques. ii) The research aims to develop strategies to steer LLM generations towards a personalized style in machine translation, particularly in the challenging literary domain where stylistic requirements are less explicit. iii) The methodology involves comparing prompt-based approaches with steering techniques that intervene on model internals, utilizing contrastive frameworks with sparse autoencoders (SAEs) to extract salient personalization properties. iv) Results demonstrate that contrastive SAE steering achieves strong personalization while preserving translation quality, achieving between 77% and 99% accuracy in discerning translation styles, and that the learned SAE latents are meaningfully connected to stylistic patterns. v) The principal implication for AI practitioners is the potential of contrastive SAE steering as a data-efficient method to personalize machine translation outputs in low-resource scenarios without compromising translation quality, which can inform development of personalized MT systems, especially in cases of limited style examples.  |
| When Do LLMs Admit Their Mistakes? Understanding the Role of Model
  Belief in Retraction (Read more on [arXiv](https://arxiv.org/abs/2505.16170) or [HuggingFace](https://huggingface.co/papers/2505.16170))| Robin Jia, ayyyq | i) This paper studies when and why large language models (LLMs) retract incorrect answers, defining retraction as acknowledging previous errors. ii) The main research question is to understand the factors influencing LLMs' decision to retract incorrect answers, specifically examining the role of model belief. iii) The methodology involves constructing model-specific continuation datasets with constraint satisfaction and reversal curse questions, probing LLMs' internal representations to infer beliefs, and steering model activations to manipulate beliefs. iv) Results show LLMs infrequently retract, retraction is linked to internal belief, and supervised fine-tuning improves retraction performance, achieving up to 84.53% recall on the WIKIDATA dataset after fine-tuning. v) The principal implication for AI practitioners is that aligning LLMs' internal beliefs with ground truth can significantly enhance the reliability and reduce misinformation risks in LLM applications.  |
| Date Fragments: A Hidden Bottleneck of Tokenization for Temporal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16088) or [HuggingFace](https://huggingface.co/papers/2505.16088))| Wei Zhao, Maxime Peyrard, Gagan Bhatia | i) This paper investigates how date tokenization impacts temporal reasoning in large language models (LLMs). ii) The study aims to quantify the relationship between date fragmentation during tokenization and the accuracy of temporal reasoning tasks. iii) The authors introduced DATEAUGBENCH, a dataset of 6500 examples, and a metric called date fragmentation ratio, using layer-wise probing and causal attention-hop analyses to evaluate LLMs' ability to handle fragmented dates. iv) Experiments reveal up to a 10-point accuracy drop on uncommon dates due to excessive fragmentation. v) The findings suggest that AI practitioners should consider date-aware vocabularies and adaptive tokenizers to maintain date component integrity, improving the temporal reasoning performance of LLMs in time-sensitive applications.  |
| How Do Large Vision-Language Models See Text in Image? Unveiling the
  Distinctive Role of OCR Heads (Read more on [arXiv](https://arxiv.org/abs/2505.15865) or [HuggingFace](https://huggingface.co/papers/2505.15865))| Hwanhee Lee, Sunghyun Ryu, Hwan Chang, Ingeol Baek | i) This paper investigates the mechanisms by which Large Vision Language Models (LVLMs) process and extract textual information from images, focusing on the role of Optical Character Recognition (OCR) heads. ii) The research aims to identify and characterize the specific attention heads within LVLMs responsible for recognizing and extracting text from images, differentiating them from existing retrieval heads. iii) The methodology involves introducing a scoring-based method to identify OCR heads, analyzing their sparsity, distinctiveness, and activation patterns, and evaluating their behavior in downstream tasks using CoT prompting and attention masking. iv) Results indicate OCR heads are less sparse, qualitatively distinct from retrieval heads, and exhibit static activation patterns, with masking OCR heads causing a performance decline in VQA tasks and a redistribution of the sink token improving performance by up to 0.9% in DocVQA for InternVL-8B. v) The implication for AI practitioners is understanding and manipulating OCR heads within LVLMs can improve OCR-VQA performance, enhancing multimodal reasoning and reducing hallucination in applications involving embedded text.  |
| MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language (Read more on [arXiv](https://arxiv.org/abs/2505.14395) or [HuggingFace](https://huggingface.co/papers/2505.14395))| Jiho Jin, Eunsu Kim, Seogyeong Jeong, aliceoh, seyoungsong | i) MUG-Eval is introduced as a novel, language-agnostic framework for evaluating multilingual text generation in LLMs. ii) The research aims to provide a scalable and reliable method for assessing LLM generation capabilities, particularly in low-resource languages where traditional metrics are limited. iii) The methodology involves transforming existing benchmarks into conversational tasks requiring two LLM instances to communicate in the target language, with algorithmic evaluation of task success. iv) Experiments across 30 languages and 8 LLMs demonstrate strong correlations with established benchmarks (r > 0.75) and indicate effective discriminative power across models and languages. v) MUG-Eval offers AI practitioners a resource-efficient approach for standardized multilingual generation evaluations, facilitating model comparisons across a diverse range of languages without requiring language-specific NLP tools or LLMs-as-judges.  |
| SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution (Read more on [arXiv](https://arxiv.org/abs/2505.16048) or [HuggingFace](https://huggingface.co/papers/2505.16048))| philippds | i) The paper introduces SPhyR, a new dataset and benchmark for evaluating spatial-physical reasoning in Large Language Models (LLMs) using topology optimization tasks. ii) The primary objective is to assess LLMs' ability to reason about optimal material distribution under structural constraints such as boundary conditions, applied forces, and supports. iii) The methodology involves presenting LLMs with 2D topology optimization problems, varying in difficulty from masked region completion to full material distribution prediction, grounded solely in force and support conditions. iv) Experiments with several LLMs (GPT-4.1, Claude 3.7 Sonnet, Gemini 2.5 Pro, and DeepSeek-R1) showed limited ability to reason about global structure; for example, Gemini 2.5 Pro achieved an average exact match of 26.75% on hard tasks. v) The principal implication for AI practitioners is the identification of a significant gap in current LLMs' ability to integrate spatial layout with physical constraints, suggesting the need for architectures or training strategies incorporating explicit physical priors for engineering and design applications.  |
