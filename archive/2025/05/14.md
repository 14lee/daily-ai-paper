

## Papers for 2025-05-14

| Title | Authors | Summary |
|-------|---------|---------|
| MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable
  Speaker Encoder (Read more on [arXiv](https://arxiv.org/abs/2505.07916) or [HuggingFace](https://huggingface.co/papers/2505.07916))| Congchao Guo, Bowen Zhang, ymzhang0519, mqyang1s, JunjieYan | MiniMax-Speech is an autoregressive Transformer-based TTS model with a learnable speaker encoder. The research aims to achieve high-quality speech synthesis with zero-shot voice cloning capabilities. The methodology involves jointly training a speaker encoder with the AR model and using a Flow-VAE to improve audio quality and speaker similarity. The model achieves state-of-the-art results on objective voice cloning metrics (Word Error Rate) on Seed-TTS-eval test set, specifically WER of 0.83 in zero-shot setting. AI practitioners can leverage this model architecture and training strategy for improved voice cloning performance in TTS systems, especially in scenarios with limited speaker data.  |
| A Multi-Dimensional Constraint Framework for Evaluating and Improving
  Instruction Following in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.07591) or [HuggingFace](https://huggingface.co/papers/2505.07591))| xjhuang, sean-xl-y, wuyilong, avonfwj, Junjie-Ye | i) The paper introduces a multi-dimensional constraint framework to evaluate and improve instruction following in LLMs. ii) The main research objective is to address limitations of existing benchmarks that rely on templated prompts and lack real-world usage diversity. iii) The methodology involves developing an automated instruction generation pipeline incorporating constraint expansion, conflict detection, and instruction rewriting to produce code-verifiable test samples. iv) The primary result is a dataset of 1,200 diverse instruction-following cases that, when used in reinforcement learning (GRPO), resulted in performance gains, with average performance dropping from 77.67% at Level I to 32.96% at Level IV. v) The principal implication for AI practitioners is a method for generating targeted training data to enhance constraint recognition and adherence in LLMs, particularly through modifications in attention module parameters.  |
| Measuring General Intelligence with Generated Games (Read more on [arXiv](https://arxiv.org/abs/2505.07215) or [HuggingFace](https://huggingface.co/papers/2505.07215))| William Chen, David Huang, nickatomlin, danjklein, vivekverma | i) The paper introduces gg-bench, a synthetically generated benchmark for evaluating general reasoning in language models via novel games. ii) The research aims to assess language models' ability to generalize and act in unseen environments through gameplay. iii) The methodology involves using a language model to generate game descriptions and Gym implementations, training RL agents via self-play, and evaluating language models' win rates against these agents. iv) State-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as 01, 03-mini and DeepSeek-R1 achieve average winrates of 31-36%. v) The findings suggest that current language models struggle with strategic reasoning and adaptability in novel game environments, indicating a need for improved generalization capabilities for AI practitioners developing reasoning systems.  |
| SkillFormer: Unified Multi-View Video Understanding for Proficiency
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2505.08665) or [HuggingFace](https://huggingface.co/papers/2505.08665))| ucaclio, EdBianchi | SkillFormer is presented as a parameter-efficient architecture for multi-view proficiency estimation from videos. The research aims to develop a unified model for skill assessment using egocentric and exocentric videos. The methodology involves a TimeSformer backbone with a CrossViewFusion module using multi-head cross-attention and LoRA fine-tuning. The model achieves 47.5% accuracy on the EgoExo4D dataset in the Ego+Exos setting, using 4.5x fewer parameters than prior baselines. SkillFormer offers AI practitioners a computationally efficient architecture for multi-view skill assessment, potentially improving the performance of applications in sports, rehabilitation, and training.  |
| NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged
  Information Guidance (Read more on [arXiv](https://arxiv.org/abs/2505.08712) or [HuggingFace](https://huggingface.co/papers/2505.08712))| Yujian Zhang, Jiaqi Peng, Jiangmiao, fulifuli666, WadeCai | NavDP introduces a navigation diffusion policy trained solely in simulation for zero-shot transfer to real-world robots. The research aims to develop an end-to-end framework for robot navigation that generalizes across different robot embodiments and unstructured environments using only simulation data. NavDP combines diffusion-based trajectory generation with a critic function for trajectory selection, conditioned on local observation tokens encoded by a policy transformer, and trained using privileged information. NavDP achieves a 30% success rate improvement by incorporating Gaussian Splatting based real-to-sim fine-tuning data, while maintaining generalization. AI practitioners can leverage NavDP's approach to develop scalable and generalizable robot navigation policies trained in simulation, reducing the reliance on expensive real-world data collection.  |
| ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness
  Prediction via Human-AI Collaborative Annotation (Read more on [arXiv](https://arxiv.org/abs/2505.07416) or [HuggingFace](https://huggingface.co/papers/2505.07416))| Kiet Van Nguyen, Dat Minh Nguyen, sonlam1102, trucnguyen28 | ViMRHP introduces a new Vietnamese dataset for multimodal review helpfulness prediction, leveraging human-AI collaborative annotation. The research aims to address the lack of linguistic diversity in existing review helpfulness datasets by creating a large-scale Vietnamese dataset. The methodology involved a two-step annotation process using LLMs followed by human verification and refinement. The experiments demonstrate that human-verified annotations achieve higher quality, showing a Cohen's Kappa score of 31.34% for ground truth Helpfulness Score, indicating "Fair Agreement" compared to AI annotations. The principal implication for AI practitioners is the demonstrated need for human verification to ensure data quality in complex annotation tasks despite the advantages of LLMs in reducing costs and annotation time.  |
