

## Papers for 2025-05-22

| Title | Authors | Summary |
|-------|---------|---------|
| Web-Shepherd: Advancing PRMs for Reinforcing Web Agents (Read more on [arXiv](https://arxiv.org/abs/2505.15277) or [HuggingFace](https://huggingface.co/papers/2505.15277))| Seungone Kim, Junhee Cho, donghalim, KimSHine, hyungjoochae | i) WEB-SHEPHERD is introduced as a process reward model (PRM) for web navigation to assess trajectories in a step-level. ii) The primary objective is to develop a PRM for web navigation that addresses the limitations of using MLLMs as reward models, particularly concerning cost and speed. iii) The methodology involves constructing WEBPRM COLLECTION, a dataset of 40K step-level preference pairs with checklists and introducing WEBREWARDBENCH for evaluating PRMs. iv) Experimental results show that WEB-SHEPHERD achieves approximately 30 points better accuracy than using GPT-40 on WEBREWARDBENCH. v) The key implication is a more cost-effective web navigation trajectory verification strategy for AI practitioners, allowing for 10x lower cost compared to GPT-40-mini when WEB-SHEPHERD is the verifier with GPT-40-mini policy on WebArena-lite with 10.9 points better performance.  |
| Scaling Law for Quantization-Aware Training (Read more on [arXiv](https://arxiv.org/abs/2505.14302) or [HuggingFace](https://huggingface.co/papers/2505.14302))| Zeyue Xue, Yutao Zeng, Jing Liu, Chaoyi Zhang, ChenMnZ | Quantization-aware training (QAT) scaling laws are explored, focusing on W4A4 quantization. The research addresses the question of how quantization error in QAT scales with model size, training data, and quantization granularity. A unified scaling law is proposed and validated through 268 QAT experiments using Llama3-style models. The results show quantization error decreases with model size but increases with training tokens and coarser quantization granularity; utilizing 8-bit for the FC2 layer input improves W4A4 QAT, reducing quantization error by 42.9% at coarser granularities. These findings suggest AI practitioners should consider both weight and activation quantization error, especially for FC2 layers, to enhance QAT performance in ultra-low bit-width scenarios.  |
| UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.14231) or [HuggingFace](https://huggingface.co/papers/2505.14231))| Jing Tang, Yong Liu, Mingxing Li, Sule Bai, xiaochonglinghu | i) UniVG-R1 introduces a reasoning-guided MLLM for universal visual grounding, enhancing reasoning capabilities through reinforcement learning. ii) The research aims to improve visual grounding performance, especially for complex instructions across multiple images, by enhancing reasoning capabilities. iii) The methodology involves constructing a high-quality CoT grounding dataset, supervised fine-tuning, and rule-based GRPO reinforcement learning with difficulty-aware weight adjustment. iv) The UniVG-R1 achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement and demonstrates a 23.4% average improvement in zero-shot performance across four image and video reasoning grounding benchmarks. v) AI practitioners can leverage UniVG-R1's framework to enhance the reasoning and generalization capabilities of MLLMs for visual grounding tasks, particularly in scenarios requiring complex instruction understanding and multi-image reasoning.  |
| MMaDA: Multimodal Large Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15809) or [HuggingFace](https://huggingface.co/papers/2505.15809))| Ke Shen, Bowen Li, Ling Yang, comin, tyfeld | i) The paper introduces MMaDA, a multimodal diffusion foundation model. ii) The objective is to design a unified multimodal diffusion architecture that achieves superior performance in textual reasoning, multimodal understanding, and text-to-image generation. iii) The methodology involves a unified diffusion architecture, mixed long chain-of-thought (CoT) fine-tuning, and a unified policy-gradient-based reinforcement learning algorithm (UniGRPO). iv) MMaDA-8B surpasses LLaMA-3-7B and Qwen2-7B in textual reasoning; it excels over SDXL and Janus in text-to-image generation. v) The unified architecture and post-training strategies in MMaDA provide AI practitioners with a comprehensive framework for future research in unifying diffusion architectures.  |
| Diffusion vs. Autoregressive Language Models: A Text Embedding
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.15045) or [HuggingFace](https://huggingface.co/papers/2505.15045))| Anh Tuan Luu, Arman Cohan, LYGeng, yilunzhao, siyue | i) This paper introduces DIFFEMBED, a diffusion language model-based approach for text embeddings, contrasting it with autoregressive language model (LLM) embeddings. ii) The research investigates whether diffusion language models, with their inherent bidirectional architecture, are better suited for text embedding tasks compared to LLMs that use unidirectional attention. iii) The methodology involves training a diffusion LM (DREAM-7B) and LLMs on a public dataset (Public E5) and a newly created reasoning-intensive dataset (REASONAUG) using contrastive learning. iv) Results show that DIFFEMBED outperforms LLM-based models by 20% on long-document retrieval and 8% on reasoning-intensive retrieval; DIFFEMBED achieves 100% accuracy on passkey retrieval and 86.8% on needle-in-a-haystack tasks in the LONGEMBED benchmark v) AI practitioners can leverage diffusion language models like DIFFEMBED to improve text embedding performance, particularly in applications requiring robust handling of long and complex contexts such as document retrieval and reasoning tasks by implementing bidirectional attention in embedding models.  |
| Efficient Agent Training for Computer Use (Read more on [arXiv](https://arxiv.org/abs/2505.13909) or [HuggingFace](https://huggingface.co/papers/2505.13909))| Pengfei Liu, zizi-0123, henryhe0123 | i) The paper introduces PC Agent-E, a framework for efficient training of computer use agents using a small dataset augmented with synthesized actions. ii) The research aims to develop an agent training approach that reduces the reliance on large-scale human demonstrations for computer use tasks. iii) The methodology involves augmenting a small set of 312 human-annotated trajectories by synthesizing diverse action decisions using Claude 3.7 Sonnet, followed by supervised fine-tuning. iv) PC Agent-E achieves a 141% relative performance improvement over the Qwen2.5-VL-72B baseline on the WindowsAgentArena-V2 benchmark. v) The research implies that strong computer use capabilities can be achieved with limited high-quality trajectory data, offering a more efficient approach to training computer use agents for AI practitioners.  |
| Learn to Reason Efficiently with Adaptive Length-based Reward Shaping (Read more on [arXiv](https://arxiv.org/abs/2505.15612) or [HuggingFace](https://huggingface.co/papers/2505.15612))| Yuzhen Huang, Yiyun Deng, Ruochen Zhou, yuntian-deng, PeterV09 | i) The paper introduces LASER-D, an RL method for improving reasoning efficiency in large reasoning models (LRMs). ii) The research investigates how to promote reasoning efficiency in LRMs by dynamically adjusting the length-based reward shaping according to problem difficulty. iii) The methodology involves RL training with a length-based step reward, adaptive target length adjustment, and difficulty-aware reward shaping. iv) Experiments on DeepSeek-R1-Distill models demonstrated a +6.1 accuracy improvement on AIME2024 while reducing token usage by 63%. v) The principal implication is an enhanced method for AI practitioners to improve LRM reasoning performance with greater response length efficiency by using dynamic and difficulty-aware length-based reward shaping.  |
| When to Continue Thinking: Adaptive Thinking Mode Switching for
  Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15400) or [HuggingFace](https://huggingface.co/papers/2505.15400))| Haodong Zhao, Yaawennn, Machine981, Amanda2023, DadaCloud01 | i) This paper introduces Adaptive Self-Recovery Reasoning (ASRR), a framework for dynamically adjusting reasoning length in Large Reasoning Models (LRMs). ii) The research investigates how to reduce computational overhead in LRMs by suppressing unnecessary reasoning while enabling implicit self-recovery. iii) ASRR employs accuracy-aware length reward regulation, conditionally applying length penalties based on group-level accuracy to balance efficiency and correctness. iv) Experiments show ASRR reduces reasoning budget by up to 32.5% (1.5B model) and 25.7% (7B model) with minimal accuracy loss (1.2% and 0.6% pass@1, respectively) and improves harmless rates by +21.7% on safety benchmarks. v) ASRR provides AI practitioners a method to improve LRM efficiency and safety by adaptively allocating reasoning effort based on problem difficulty, reducing computational cost without significantly impacting performance.  |
| Vid2World: Crafting Video Diffusion Models to Interactive World Models (Read more on [arXiv](https://arxiv.org/abs/2505.14357) or [HuggingFace](https://huggingface.co/papers/2505.14357))| Mingsheng Long, Shangchen Miao, Qixing Zhou, manchery, knightnemo | Vid2World introduces a method to transform pre-trained video diffusion models into interactive world models. The research aims to bridge the gap between video diffusion models and interactive world models by enabling causal generation and action conditioning. The methodology involves causalization of a pre-trained video diffusion model through architectural modifications and a causal action guidance mechanism. Experiments show that Vid2World achieves state-of-the-art performance in video prediction tasks and demonstrated 81.8% relative performance improvement in game simulation. AI practitioners can leverage Vid2World to repurpose highly capable video diffusion models for interactive world modeling, addressing challenges of coarse generation quality and excessive data requirements.  |
| IA-T2I: Internet-Augmented Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2505.15779) or [HuggingFace](https://huggingface.co/papers/2505.15779))| Yifan Chang, Mingliang Zhai, Yukang Feng, Jianwen Sun, Chuanhao Li | i) The paper introduces an Internet-Augmented Text-to-Image generation (IA-T2I) framework to improve T2I models' performance when generating images from text prompts containing uncertain knowledge. ii) The research aims to enhance T2I models by integrating reference images retrieved from the Internet to address scenarios where knowledge implied in text prompts is uncertain, ambiguous, or recently updated. iii) The IA-T2I framework incorporates an active retrieval module, a hierarchical image selection module, and a self-reflection mechanism to retrieve and refine reference images, augmenting the T2I generation process. iv) Experiments using the introduced Img-Ref-T2I dataset demonstrated that the IA-T2I framework outperforms GPT-40 by approximately 30% in human evaluations. v) IA-T2I framework offers AI practitioners a methodology to improve T2I model accuracy by dynamically incorporating external visual information, particularly beneficial when dealing with evolving or ambiguous concepts not adequately represented in the model's training data.  |
| Deliberation on Priors: Trustworthy Reasoning of Large Language Models
  on Knowledge Graphs (Read more on [arXiv](https://arxiv.org/abs/2505.15210) or [HuggingFace](https://huggingface.co/papers/2505.15210))| Jun Liu, Rui Xing, Zhitao Gao, Jie Ma, stillqu | Deliberation on Priors (DP) is introduced as a framework to improve the trustworthiness of LLM reasoning over knowledge graphs. The paper addresses the challenge of LLMs generating hallucinations due to insufficient knowledge. The methodology involves a progressive knowledge distillation strategy integrating structural priors and a reasoning-introspection strategy incorporating constraint priors. Experiments on WebQuestionsSP, ComplexWebQuestions, and MetaQA datasets show DP achieves state-of-the-art results, including a 13% Hit@1 improvement on ComplexWebQuestions; the paper demonstrates that integrating prior knowledge and constraints enhances the reliability of LLM-generated responses, implying AI practitioners should prioritize incorporating external knowledge and constraint-based verification to improve the trustworthiness of LLM-based systems.  |
| lmgame-Bench: How Good are LLMs at Playing Games? (Read more on [arXiv](https://arxiv.org/abs/2505.15146) or [HuggingFace](https://huggingface.co/papers/2505.15146))| Eric P. Xing, Haoyang Yu, Mingjia Huo, Yuxuan13, Snyhlxde | i) The paper introduces lmgame-Bench, a benchmark for evaluating LLMs in video game playing. ii) The main research question addresses whether video game environments can effectively evaluate the perception, memory, and planning capabilities of LLMs, and how to mitigate common challenges. iii) The methodology involves creating a Gym-style API for platformer, puzzle, and narrative games, combined with lightweight perception and memory scaffolds, contamination mitigation techniques, and standardized prompt optimization. iv) Results show that using Imgame-Bench can yield an 86.7% game run success rate which is beyond using harness or without any support in distinguishing model performance, while the standardized prompt optimization reduces performance variance across different empirically optimized initializations by 33.8% to 63.5%. v) Imgame-Bench provides AI practitioners with a more reliable and informative evaluation environment, highlighting the importance of gaming harnesses, contamination control, and prompt tuning for LLM agent performance in interactive settings. The paper also indicates that RL training in-game transfers well to planning and agentic tasks. It's unclear whether all 13 models trained with RL and assessed.  |
| Constructing a 3D Town from a Single Image (Read more on [arXiv](https://arxiv.org/abs/2505.15765) or [HuggingFace](https://huggingface.co/papers/2505.15765))| Xin Eric Wang, Jie Yang, Jing Gu, Ruijian Zhang, Kaizhi Zheng | i) This paper introduces 3DTown, a training-free framework for generating coherent 3D scenes from a single top-down image. ii) The research aims to synthesize realistic and geometrically consistent 3D scenes from a single image without requiring 3D training data or fine-tuning. iii) The method employs a region-based generation strategy and spatial-aware 3D inpainting using pretrained object generators and masked rectified flow. iv) Experiments demonstrate that 3DTown outperforms state-of-the-art baselines, achieving a GPT-40-based texture win rate of 92.3% versus 7.7% for Hunyuan3D-2. v) The primary implication for AI practitioners is the demonstration of a modular, training-free approach to 3D scene synthesis that overcomes resolution bottlenecks and geometry inconsistencies, offering a potentially scalable method for generating structured 3D environments from minimal input.  |
| dKV-Cache: The Cache for Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15781) or [HuggingFace](https://huggingface.co/papers/2505.15781))| Xinchao Wang, Gongfan Fang, Runpeng Yu, Xinyin Ma | i) The paper introduces dKV-Cache, a delayed key-value caching mechanism to accelerate inference in Diffusion Language Models (DLMs). ii) The research aims to address the inference inefficiency of DLMs by adapting the KV-cache technique used in autoregressive models. iii) The methodology involves a delayed caching strategy for key and value states, implemented in two variants: dKV-Cache-Decode and dKV-Cache-Greedy. iv) Experiments on LLaDA and Dream-Base-7B models demonstrate 2-10x inference speedup with minimal performance degradation using dKV-Cache. v) dKV-Cache provides AI practitioners a training-free method to accelerate DLM inference, potentially narrowing the performance gap between DLMs and autoregressive models.  |
| How Should We Enhance the Safety of Large Reasoning Models: An Empirical
  Study (Read more on [arXiv](https://arxiv.org/abs/2505.15404) or [HuggingFace](https://huggingface.co/papers/2505.15404))| Qi Zhu, Victor Shea-Jay Huang, Xian Qi Loye, Zhexin Zhang, yangjunxiao2021 | i) This paper empirically investigates methods for enhancing the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT). ii) The main research question explores how to improve the safety performance of LRMs without compromising reasoning capabilities. iii) The methodology involves analyzing failure patterns in distilled safe responses, modifying prompting strategies, and comparing different reasoning processes (short, template-based, and long-form CoT). iv) Results show directly distilling safe responses fails to significantly enhance safety, identifying lack of safety awareness, overthinking, and inconsistency as key failure patterns; explicitly addressing these reduces the Attack Success Rate (ASR) of PAIR from 77.0% to 7.0%. v) The findings suggest that simpler reasoning processes can be as effective as longer chains, easing the learning process for models and including benign reasoning data can balance safety and over-refusal; however, there is limited information on the impact of this method on larger models.  |
| Learning to Reason via Mixture-of-Thought for Logical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15817) or [HuggingFace](https://huggingface.co/papers/2505.15817))| Heng Huang, R. Thomas McCoy, Simeng Han, Lichang Chen, TongZheng1999 | i) This paper introduces Mixture-of-Thought (MoT), a framework enabling LLMs to reason across modalities like natural language, code, and truth-table reasoning for logical tasks. ii) The research aims to improve LLMs' logical reasoning capabilities by training them to utilize multiple reasoning modalities synergistically. iii) MoT employs a two-phase approach: self-evolving MoT training using filtered, self-generated rationales across modalities, and MoT inference which leverages the synergy of the modalities. iv) Experiments on FOLIO and ProofWriter show MoT outperforms single-modality baselines, achieving up to +11.7pp average accuracy gain, and demonstrate that a 9B MoT model matches GPT-4 + Logic-LM performance on FOLIO. v) The MoT framework provides AI practitioners with a method for improving logical reasoning in LLMs by combining multiple reasoning modalities which can be achieved through a two-phase approach of MoT training and MoT inference.  |
| Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data
  Could Be Secretly Stolen! (Read more on [arXiv](https://arxiv.org/abs/2505.15656) or [HuggingFace](https://huggingface.co/papers/2505.15656))| Hongning Wang, Shiyao Cui, Yuhao Sun, Zhexin Zhang, yangjunxiao2021 | Fine-tuning open-source LLMs can create a risk of downstream data extraction via backdoor training. The research investigates the potential for creators of open-source LLMs to extract fine-tuning data from downstream users. It uses supervised fine-tuning (SFT) and reinforcement learning to inject backdoors that trigger query reproduction. Experiments across four models show up to 76.3% of fine-tuning data can be extracted in practical settings, increasing to 94.9% in more ideal conditions. This poses a data breaching risk for AI practitioners who fine-tune open-source LLMs with proprietary data, requiring enhanced security measures.  |
| RLVR-World: Training World Models with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.13934) or [HuggingFace](https://huggingface.co/papers/2505.13934))| Mingsheng Long, Ningya Feng, Shaofeng Yin, manchery | i) RLVR-World is introduced as a framework to optimize world models via reinforcement learning with verifiable rewards (RLVR). ii) The research aims to improve world models by directly optimizing task-specific metrics rather than surrogate objectives like maximum likelihood estimation (MLE). iii) The method involves tokenizing states and actions as sequences and using verifiable rewards based on decoded predictions for RLVR. iv) Experiments show RLVR improves LLMs, achieving +30.7% accuracy on text-based game state prediction and improves video world models with a +9.2% relative LPIPS improvement on robot manipulation, even with limited RLVR gradient steps. v) RLVR offers AI practitioners a post-training technique to refine generative models by directly optimizing for specific task-aligned metrics, enhancing utility beyond pre-training. |
| Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous
  Concept Space (Read more on [arXiv](https://arxiv.org/abs/2505.15778) or [HuggingFace](https://huggingface.co/papers/2505.15778))| Chenyang Zhao, Ao Shen, Weixiang Yan, Xuehai He, Zhen Zhang | i) The paper introduces Soft Thinking, a training-free method that improves LLM reasoning by operating in a continuous concept space of probability-weighted token embeddings. ii) The research aims to unlock the reasoning potential of LLMs by enabling soft, abstract concept manipulation beyond discrete language tokens. iii) The methodology involves replacing discrete token selection in Chain-of-Thought prompting with probabilistic soft aggregation over the entire vocabulary, forming a continuous concept space. iv) Soft Thinking improves pass@1 accuracy by up to 2.48 points on mathematical and coding benchmarks while reducing token usage by up to 22.4% compared to standard Chain-of-Thought. v) AI practitioners can leverage Soft Thinking as a drop-in replacement for Chain-of-Thought prompting to improve both accuracy and efficiency of LLMs without additional training.  |
| ConvSearch-R1: Enhancing Query Reformulation for Conversational Search
  with Reasoning via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15776) or [HuggingFace](https://huggingface.co/papers/2505.15776))| Xipeng Qiu, Kai Song, Ruijun Feng, Siyin Wang, BeastyZ | i) This paper introduces ConvSearch-R1, a novel self-driven framework for conversational query reformulation (CQR) using reinforcement learning, eliminating the need for external rewrite supervision. ii) The research objective is to align query reformulation models with downstream retrievers effectively without human-annotated rewrites. iii) The methodology employs a two-stage approach: Self-Driven Policy Warm-Up (SD-PWU) through retrieval-guided self-distillation, and Retrieval-Guided Reinforcement Learning (RL) with a rank-incentive reward shaping mechanism. iv) Experiments on TopiOCQA demonstrate ConvSearch-R1 achieves over 10% average improvement across metrics compared to previous state-of-the-art results with 3B parameter models and no external supervision. v) ConvSearch-R1 provides AI practitioners with a self-supervised CQR framework, reducing annotation costs and enhancing retrieval performance by aligning query reformulation with retriever ranking signals.  |
| BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs (Read more on [arXiv](https://arxiv.org/abs/2505.13529) or [HuggingFace](https://huggingface.co/papers/2505.13529))| Chujie Zheng, Xiaoce Wang, Haoran Liu, Jinzhe Tu, yangjunxiao2021 | i) The paper introduces BARREL, a framework to improve the factual reliability of Large Reasoning Models (LRMs) by promoting boundary-aware reasoning. ii) The main research question is how to mitigate pathological reasoning patterns leading to overconfident and incorrect answers in LRMs. iii) The methodology involves BARREL-training, comprising knowledge labeling, reasoning trace construction using Supervised Fine-Tuning (SFT), and Group Relative Policy Optimization (GRPO). iv) Experiments showed that BARREL-training increased the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while maintaining a competitive accuracy of 40.7%. v) For AI practitioners, BARREL offers a technique for enhancing the factual reliability of LRMs by promoting uncertainty-aware refusal, which can be integrated into existing training pipelines to develop more trustworthy systems.  |
| This Time is Different: An Observability Perspective on Time Series
  Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2505.14766) or [HuggingFace](https://huggingface.co/papers/2505.14766))| Chris Lettieri, Salahidine Lemaachi, Youssef Doubli, Emaad Khwaja, Ben Cohen | i) The paper introduces TOTO, a 151-million-parameter time series forecasting foundation model, and BOOM, a large-scale observability benchmark dataset. ii) The main objective is to develop a time series foundation model optimized for observability metrics and to provide a benchmark for evaluating such models. iii) TOTO employs a decoder-only architecture with causal normalization, patch embedding, proportional factorized attention, and a Student-T mixture model head, pre-trained on observability data, public datasets, and synthetic data. iv) Evaluations show TOTO achieves state-of-the-art performance on both BOOM and general-purpose benchmarks, with a 12% improvement in CRPS on BOOM compared to other methods. v) The principal implication for AI practitioners is the availability of an open-source foundation model and a benchmark tailored to observability data, enhancing zero-shot forecasting capabilities for monitoring and anomaly detection in distributed systems.  |
| Text Generation Beyond Discrete Token Sampling (Read more on [arXiv](https://arxiv.org/abs/2505.14827) or [HuggingFace](https://huggingface.co/papers/2505.14827))| Jianfeng Gao, Jingbo Shang, Chandan Singh, Liyuan Liu, Yufan Zhuang | i) The paper introduces Mixture of Inputs (MOI), a training-free method to enhance autoregressive language models by preserving token distribution information. ii) The main objective is to improve text quality and reasoning capabilities in LLMs by modifying the input to incorporate the distribution of predicted tokens, rather than solely the sampled token. iii) The methodology involves a Bayesian estimation approach that blends the generated discrete token with the previously discarded token distribution using posterior expectation. iv) MOI achieves consistent performance improvements across tasks, demonstrated by a +2.36% average absolute gain for Nemotron-Super-49B, with the largest improvement on GPQA-Diamond (+4.1%). v) MOI offers AI practitioners a way to improve reasoning tasks without retraining, by combining discrete choices with probabilistic contexts to enhance accuracy without sacrificing decoding efficiency.  |
| AutoMat: Enabling Automated Crystal Structure Reconstruction from
  Microscopy via Agentic Tool Use (Read more on [arXiv](https://arxiv.org/abs/2505.12650) or [HuggingFace](https://huggingface.co/papers/2505.12650))| Jiangjie Qiu, Xiao Chen, Yizhe Chen, IvanTang, yaotianvector | AutoMat is an agent-assisted pipeline for automated crystal structure reconstruction from STEM images. The research aims to automate the transformation of STEM images into simulation-ready crystal structures and predict their physical properties. It employs a pattern-adaptive denoising network (MOE-DIVAESR), physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation via MatterSim, and orchestrates these tools with a text-only LLM. AutoMat achieves a projected lattice RMSD of 0.11 ± 0.03 Å and energy MAE below 350 meV/atom, outperforming vision-language models and domain-specific tools. AutoMat provides a framework enabling AI practitioners to generate reliable atomistic structures from microscopy data for ML model training and validation.  |
| VARD: Efficient and Dense Fine-Tuning for Diffusion Models with
  Value-based RL (Read more on [arXiv](https://arxiv.org/abs/2505.15791) or [HuggingFace](https://huggingface.co/papers/2505.15791))| Bangyan Liao, Siteng Huang, Yufei Huang, Zifeng Zhuang, Fengyuan Dai | i) The paper introduces VARD, a value-based reinforcement learning approach for efficient and stable fine-tuning of diffusion models, particularly with non-differentiable rewards. ii) The main objective is to improve the training efficiency and stability of diffusion models when fine-tuning them for specific desirable properties, especially in scenarios with non-differentiable rewards. iii) VARD learns a process reward model (PRM) akin to a value function in RL, to provide dense, differentiable supervision signals throughout the diffusion trajectory, supplemented by KL regularization to maintain proximity to the pre-trained model. iv) Experiments demonstrate that VARD achieves better trajectory guidance, leading to faster convergence and improved sample quality, extending RL applicability to complex non-differentiable reward functions; VARD w/o KL and VARD exhibit steeper growth trajectories than baselines with respect to reward. v) VARD provides AI practitioners with a method for stable and efficient fine-tuning of diffusion models using potentially non-differentiable rewards, offering enhanced sample quality and trajectory control.  |
| RL Tango: Reinforcing Generator and Verifier Together for Language
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15034) or [HuggingFace](https://huggingface.co/papers/2505.15034))| Duane S. Boning, Zhang-Wei Hong, Maohao Shen, Zhengqi Gao, sunshinekevin | i) The paper introduces TANGO, a reinforcement learning (RL) framework for jointly training an LLM generator and a generative LLM verifier in an interleaved manner to improve language reasoning. ii) The primary objective is to overcome limitations of fixed or discriminatively trained verifiers in existing RL post-training methods for LLMs, which are susceptible to reward hacking and poor generalization. iii) TANGO uses RL to concurrently train both an LLM generator and a process-level generative LLM verifier based solely on outcome-level verification correctness rewards without explicit process-level annotations. iv) Experiments demonstrate TANGO achieves state-of-the-art results among 7B/8B-scale models, with the generator attaining best-in-class performance across five competition-level math benchmarks and the verifier leading on the ProcessBench dataset; TANGO with GRPO doubles the accuracy on the most challenging benchmark, AIME 2025, relative to vanilla GRPO. v) The principal implication for AI practitioners is that co-evolving generator and verifier components in RL frameworks can lead to improved reasoning capabilities and generalization, offering a more robust alternative to relying on fixed or SFT-trained verifiers in LLM post-training.  |
| Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM (Read more on [arXiv](https://arxiv.org/abs/2505.15816) or [HuggingFace](https://huggingface.co/papers/2505.15816))| Ziwei Liu, Lewei Lu, Penghao Wu | i) This paper introduces ProxyV, a method to reduce computation in Large Multimodal Models (LMMs) by using proxy vision tokens. ii) The research investigates how to reduce computation associated with vision tokens in decoder-only LMMs without sacrificing performance. iii) The method involves downsampling original vision tokens to create proxy tokens, processing these proxy tokens through self-attention and feed-forward networks, and then using them to guide updates of the original vision tokens. iv) ProxyV reduces prefilling FLOPs and time by 43% and 40%, respectively while achieving 101% performance on fine-grained benchmarks when applied to Vicuna1.5-7B. v) AI practitioners can use ProxyV to enhance the efficiency of LMMs in scenarios with long visual sequences, as it effectively mitigates the computational burden of vision tokens without compromising performance, and the paper also proposes a non-spatial variant of ProxyV which can be seamlessly integrated with token reduction methods to further enhance efficiency.  |
| Evaluate Bias without Manual Test Sets: A Concept Representation
  Perspective for LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.15524) or [HuggingFace](https://huggingface.co/papers/2505.15524))| Zirui Song, Chenxi Wang, Wei Liu, Kaiyang Wan, Lang Gao | i) This paper introduces BIASLENS, a test-set-free framework for evaluating biases in Large Language Models (LLMs) by analyzing concept representations. ii) The research aims to overcome the limitations of existing bias evaluation methods by shifting the focus from behavioral differences to conceptual representations within LLMs. iii) BIASLENS combines Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs) to extract and compare interpretable concept representations, quantifying bias via representational similarity between target and reference concepts. iv) Experiments demonstrate BIASLENS achieves a Spearman correlation r > 0.85 with traditional bias evaluation metrics and uncovers biases not easily detectable by existing methods. v) BIASLENS provides AI practitioners with a scalable and efficient methodology for bias discovery in LLMs, facilitating improvements in fairness and transparency without requiring manual test set creation.  |
| PiFlow: Principle-aware Scientific Discovery with Multi-Agent
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2505.15047) or [HuggingFace](https://huggingface.co/papers/2505.15047))| Hongyu Chen, Tao Lin, Yingming Pu | i) PiFlow is presented as a framework for structured uncertainty reduction in automated scientific discovery using LLM-based multi-agent systems. ii) The paper addresses the research question of how to improve scientific discovery efficiency and solution quality by incorporating scientific principles into the hypothesis generation process. iii) The method employs an information-theoretical approach with Min-Max optimization to select high-potential scientific principles for guiding hypothesis generation, validation, and refinement. iv) Evaluations across three scientific domains show PiFlow achieves a 73.55% increase in the AUC of property values versus exploration steps and a 94.06% improvement in solution quality compared to a vanilla agent system. v) PiFlow offers AI practitioners a plug-and-play method for enhancing scientific discovery MAS, potentially leading to more efficient exploration of complex search spaces.  |
| Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large
  Audio-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15406) or [HuggingFace](https://huggingface.co/papers/2505.15406))| Lang Gao, Mingzhe Li, Mingxuan Cui, Qian Jiang, Zirui Song | i) This paper introduces AJailBench, a benchmark for evaluating jailbreak vulnerabilities in Large Audio-Language Models (LAMs). ii) The main objective is to provide a systematic, quantitative evaluation of LAM safety against adversarial audio prompts. iii) The methodology involves constructing a dataset of adversarial audio prompts converted from textual jailbreak attacks and creating an Audio Perturbation Toolkit (APT) to generate dynamic adversarial variants, using Bayesian optimization under semantic consistency constraints. iv) The results indicate that state-of-the-art LAMs exhibit vulnerabilities to both static and optimized adversarial audio inputs, with even small, semantically preserved perturbations significantly reducing safety performance, and no single model demonstrating robust performance across all attacks. v) AI practitioners should be aware that current LAMs are vulnerable to audio jailbreak attacks that can bypass safety mechanisms, requiring more robust and semantically aware defense strategies and that signal-level manipulations can be a key attack vector.  |
| WebNovelBench: Placing LLM Novelists on the Web Novel Distribution (Read more on [arXiv](https://arxiv.org/abs/2505.14818) or [HuggingFace](https://huggingface.co/papers/2505.14818))| Haidong Wang, Jun Zheng, Leon Lin | i) WebNovelBench is introduced as a new benchmark for evaluating long-form story generation by Large Language Models (LLMs). ii) The research aims to establish a comprehensive and objective methodology for assessing and ranking LLMs' storytelling capabilities relative to human-authored works. iii) The methodology involves a synopsis-to-story generation task using a dataset of over 4,000 Chinese web novels and an LLM-as-Judge approach evaluating eight narrative dimensions. iv) Experiments involving 24 LLMs demonstrate effective differentiation between human-written and LLM-generated content, with top models achieving norm scores up to 5.21, indicating strong alignment with high-quality human writing. v) WebNovelBench provides AI practitioners with a scalable, replicable, and data-driven framework for assessing and advancing LLM-driven narrative generation, enabling standardized comparisons and insights for future model development.  |
| Prior Prompt Engineering for Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14157) or [HuggingFace](https://huggingface.co/papers/2505.14157))| Sarana Nutanong, Potsawee Manakul, kunato, pittawat | i) This paper explores the influence of different prior prompt engineering (pPE) approaches in reinforcement fine-tuning (RFT) of language models. ii) The research investigates whether different pPE strategies can guide language models to internalize distinct behaviors after RFT. iii) Five representative inference-time prompt engineering (iPE) strategies were translated into corresponding pPE approaches and used to train Qwen2.5-7B models with math-only data, followed by quantitative and qualitative evaluations. iv) Results show that all pPE-trained models surpassed their iPE-prompted baselines, with the null-example pPE approach achieving the largest average performance gain and highest improvement on GPQA-Diamond; training dynamics were largely similar across pPE variants. v) The findings demonstrate that pPE is a powerful yet understudied axis for RFT, allowing AI practitioners a way to incentivize diverse behaviors without changing algorithms, reward shaping, or data curation.  |
| Language Specific Knowledge: Do Models Know Better in X than in English? (Read more on [arXiv](https://arxiv.org/abs/2505.14990) or [HuggingFace](https://huggingface.co/papers/2505.14990))| Dilek Hakkani-Tür, Nimet Beyza Bozdag, Ishika Agarwal | i) This paper introduces and investigates Language Specific Knowledge (LSK) in language models, exploring whether models exhibit better performance in certain languages for specific topics. ii) The research aims to determine if multilingual language models possess varying degrees of expertise across languages for different knowledge domains, and if this can be leveraged to improve reasoning. iii) The methodology involves a two-stage framework called LSKEXTRACTOR: mapping LSK by conducting chain-of-thought (CoT) reasoning in 13 languages on culture-specific datasets, and LSK-informed reasoning using the identified expert languages during inference. iv) The experiments show an average relative improvement of 10% in accuracy by using LSKEXTRACTOR across various models and datasets. v) The principal implication is that AI practitioners can enhance the performance of language models by strategically incorporating code-switching to leverage language-specific knowledge identified through the LSKEXTRACTOR framework for improved accuracy and cultural alignment.  |
| MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation
  of LLM Hallucinations (Read more on [arXiv](https://arxiv.org/abs/2505.14101) or [HuggingFace](https://huggingface.co/papers/2505.14101))| Johannes Bjerva, Katja Hose, Russa Biswas, ernlavr | i) MultiHal, a new multilingual benchmark for evaluating LLM hallucinations, is introduced. ii) The research aims to provide a knowledge graph-grounded, multilingual testbed for generative text evaluation to address limitations in current factuality benchmarks. iii) The methodology involves mining 140k KG-paths from open-domain KGs, pruning them to 25.9k high-quality paths, and translating question-answer pairs with KG paths into five languages. iv) Baseline evaluation shows a 0.12 to 0.36 point increase in semantic similarity scores using KG-RAG over vanilla QA in multiple languages and models. v) MultiHal facilitates future research into graph-based hallucination mitigation and fact-checking tasks for improving LLM faithfulness and KG integration for AI developers.  |
| HumaniBench: A Human-Centric Framework for Large Multimodal Models
  Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.11454) or [HuggingFace](https://huggingface.co/papers/2505.11454))| Mukund S. Chettiar, Ashmal Vayani, Vahid Reza Khazaie, Aravind Narayanan, shainaraza | HumaniBench introduces a new benchmark for evaluating Large Multimodal Models (LMMs) on human-centered criteria. The research aims to provide a holistic assessment of LMMs regarding fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness. The methodology involves curating a dataset of 32K real-world image-question pairs, annotated using a GPT-4o-assisted pipeline and verified by domain experts, across seven diverse tasks. Benchmarking 15 LMMs revealed that proprietary models generally perform better, but gaps remain in robustness and visual grounding; Qwen2.5-7B achieved 84.87% in Understanding on particular tasks. The benchmark facilitates diagnosing alignment gaps and steering LMMs toward accurate and socially responsible behavior, offering AI practitioners a rigorous test-bed for optimizing LMMs for human values.  |
