

## Papers for 2025-05-06

| Title | Authors | Summary |
|-------|---------|---------|
| Voila: Voice-Language Foundation Models for Real-Time Autonomous
  Interaction and Voice Role-Play (Read more on [arXiv](https://arxiv.org/abs/2505.02707) or [HuggingFace](https://huggingface.co/papers/2505.02707))| Yu Shu, Yemin Shi, zhitinghu, Jaward, guangyil | The paper introduces Voila, a family of open-sourced, end-to-end voice-language foundation models designed for real-time, autonomous, and emotionally expressive human-AI interaction, supporting tasks like dialogue, ASR, and TTS. Its primary objective is to enable voice AI agents to interact autonomously and proactively by moving beyond reactive pipeline systems towards full-duplex, low-latency conversations preserving rich vocal nuances. Voila utilizes a hierarchical multi-scale Transformer architecture with an LLM backbone and a hierarchical audio generator, a novel voice tokenizer (Voila-Tokenizer) that distills semantic and acoustic information into layered RVQ tokens, and a structured text-audio interleaved alignment strategy for multi-task training. Voila achieves a response latency of 195 milliseconds and an accuracy of 30.56 on its custom Voila Benchmark, significantly outperforming prior models, and a 2.7% Word Error Rate for ASR on LibriSpeech test-clean (when trained with LibriSpeech data). This provides AI practitioners with an open-source foundation for developing next-generation autonomous voice AI systems with improved naturalness, responsiveness, and customizability, offering a unified model that effectively integrates LLM reasoning with nuanced voice processing. |
| RM-R1: Reward Modeling as Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.02387) or [HuggingFace](https://huggingface.co/papers/2505.02387))| Ziqi Wang, zhangdenghui123, Merlin-Hongru, gaotang, XtremSup | The paper introduces RM-R1, a family of Reasoning Reward Models (REASRMS) that formulate reward modeling as an explicit reasoning task to enhance LLM alignment. The research aims to improve the interpretability and performance of reward models for LLMs by integrating deep, interpretable reasoning capabilities into the reward generation and judgment process. RM-R1 is trained using a two-stage pipeline involving: 1) distillation of high-quality reasoning chains, often employing a Chain-of-Rubrics (CoR) framework, from stronger teacher models, and 2) subsequent reinforcement learning with verifiable rewards (RLVR) using Group Relative Policy Optimization (GRPO). RM-R1 models demonstrate state-of-the-art or near state-of-the-art performance, outperforming significantly larger open-weight and proprietary models by up to 13.8% on benchmarks like RewardBench, where RM-R1-QWEN-INSTRUCT-32B achieved 92.9% overall accuracy. AI practitioners can develop more robust, accurate, and interpretable LLM alignment systems by shifting from opaque scalar rewards to generative reward models that explicitly reason and justify their judgments, particularly through structured reasoning distillation and targeted RL. |
| Grokking in the Wild: Data Augmentation for Real-World Multi-Hop
  Reasoning with Transformers (Read more on [arXiv](https://arxiv.org/abs/2504.20752) or [HuggingFace](https://huggingface.co/papers/2504.20752))| Gjergji Kasneci, Roman Abramov, fsteinbauer | This paper demonstrates that augmenting real-world knowledge graphs with synthetic data to increase the ratio of inferred to atomic facts ($\phi_r$) enables Transformers to "grok" multi-hop reasoning, achieving high performance on factual question answering. The main research objective was to determine if Transformers can achieve "grokking" (transitioning from memorization to generalization) on real-world multi-hop factual reasoning tasks by synthetically increasing the ratio of inferred facts to atomic facts ($\phi_r$) in the training data above a critical threshold. The key methodology involved augmenting the 2WikiMultiHopQA dataset by generating synthetic atomic and multi-hop (inferred) facts to elevate the relation-specific ratio $\phi_r$ (e.g., to an achieved ratio of 8 for comparison tasks). A GPT-2 small model was then trained from scratch on this augmented data for an extended period (e.g., ~300k steps). The primary result showed that the grokked GPT-2 small model achieved 96% Out-of-Distribution (OOD) accuracy on the 2WikiMultiHopQA structured comparison task, significantly outperforming larger models like GPT-4o (reported at 87% for the same task average in Figure 1, or 0.87 for comparison in Table 3) and o1-mini (reported at 89% in Figure 1, or 0.88 for comparison in Table 3) which did not undergo the same data augmentation. The principal implication for AI practitioners is that targeted data augmentation to ensure a high density of multi-step inference examples relative to atomic facts can unlock robust multi-hop reasoning capabilities even in smaller Transformer models, offering a pathway to more efficient and potentially more interpretable factual reasoning systems without solely relying on model scale. (The paper's arXiv date is listed as "29 Apr 2025", which is unusual.) |
| FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.02735) or [HuggingFace](https://huggingface.co/papers/2505.02735))| ZhengYuan, yifanzhang114, Liam-Liu, prt66, zhouliang | This paper introduces FormalMATH, a large-scale Lean4 benchmark with 5,560 formally verified problems to evaluate the formal mathematical reasoning of large language models. The primary objective is to address limitations in the scope and scale of existing formal mathematics benchmarks and to rigorously assess current LLM-based theorem provers. FormalMATH was created via a human-in-the-loop autoformalization pipeline integrating specialized LLMs for statement generation, multi-LLM semantic verification, and negation-based disproof, achieving a 72.09% pass rate for candidate statements undergoing final manual expert verification. Evaluations on FormalMATH showed that even the strongest LLM-based theorem provers have significant limitations, achieving only a 16.46% success rate (Pass@32), and revealed that natural-language solution guidance can negatively impact formal proof success in chain-of-thought scenarios. FormalMATH offers a robust benchmark for advancing LLM-based formal theorem proving, highlighting needs for improved cross-domain generalization, deeper deductive capabilities beyond simple automation, and better integration of formal and informal reasoning. |
| ReplaceMe: Network Simplification via Layer Pruning and Linear
  Transformations (Read more on [arXiv](https://arxiv.org/abs/2505.02819) or [HuggingFace](https://huggingface.co/papers/2505.02819))| szagoruyko121, stamatisl, madrugado, ammarali32, dimitriish | ReplaceMe is a generalized training-free depth pruning method that replaces contiguous transformer blocks with an estimated linear transformation, maintaining high performance. The main objective is to simplify transformer networks by pruning layers and approximating their functionality with a single linear operation, estimated using a small calibration dataset, without requiring retraining. The key methodology involves identifying prunable blocks based on inter-layer activation distances (cosine distance preferred) and then computing an optimal linear transformation (LT) to replace these blocks, which is subsequently merged into a remaining layer. Primary results show that ReplaceMe can prune up to 25% of a Llama 2 7B model while retaining 92.5% of its original performance on open benchmarks using the cosine distance objective for LT estimation, significantly outperforming UIDL in compression time and environmental impact. For AI practitioners, ReplaceMe offers a computationally efficient, training-free approach to compress large language models, reducing latency and resource demands with minimal performance loss, thus facilitating more accessible deployment. |
| Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization
  in Rejection Sampling and RL (Read more on [arXiv](https://arxiv.org/abs/2505.02391) or [HuggingFace](https://huggingface.co/papers/2505.02391))| nanjiang, WeiXiong, hendrydong, HanningZhang, FlippyDora | This paper introduces GVM-RAFT, a dynamic sample allocation strategy that optimizes Chain-of-Thought (CoT) reasoner training by minimizing stochastic gradient variance. The primary objective is to improve the efficiency of CoT training, which often suffers from inefficient stochastic gradient estimation due to static sampling strategies, by dynamically allocating computational resources based on prompt-specific characteristics. GVM-RAFT proposes a prompt-specific Dynamic Sample Allocation Strategy that monitors prompt acceptance rates and stochastic gradient norms to minimize gradient variance under a computational budget, derived within an Expectation-Maximization framework. Experiments on mathematical reasoning tasks show that GVM-RAFT achieves a 2-4Ã— speedup in convergence and considerable accuracy improvements over vanilla RAFT, for instance, GVM-RAFT++ improved the 5-benchmark average accuracy from 36.42% to 39.64% on Qwen2.5-Math-1.5B. AI practitioners can utilize this method to more efficiently fine-tune CoT models through rejection sampling or reinforcement learning, by adaptively allocating inference budgets to different prompts, thus accelerating training and enhancing final model accuracy. |
| Practical Efficiency of Muon for Pretraining (Read more on [arXiv](https://arxiv.org/abs/2505.02222) or [HuggingFace](https://huggingface.co/papers/2505.02222))| cadarsh-essential, monk-essential, karlstratos, ampolloreno, ishaan-essential | This research demonstrates Muon, a second-order optimizer, expands the compute-time Pareto frontier over AdamW for pretraining and introduces an efficient muP-based telescoping hyperparameter tuning method. Its main objective is to investigate Muon's practical efficiency compared to AdamW in large-scale language model pretraining, particularly the compute-time tradeoff and hyperparameter selection. Key methodology involved comparing optimizers via iso-loss frontiers on a compute-time plane using models up to 4 billion parameters, and developing a "telescoping" algorithm for maximal update parameterization (muP) that accounts for error sources. Primary results indicate Muon requires 10-15% fewer tokens than AdamW to achieve an identical loss, and the telescoping algorithm enables allocating over 20% of the total compute budget to the final model training run while ensuring near-optimal hyperparameters. For AI practitioners, this implies Muon offers a more data-efficient pretraining alternative to AdamW, especially at large batch sizes, and the telescoping muP approach facilitates cost-effective hyperparameter tuning, reducing overall training time and computational resources. |
| A Survey on Inference Engines for Large Language Models: Perspectives on
  Optimization and Efficiency (Read more on [arXiv](https://arxiv.org/abs/2505.01658) or [HuggingFace](https://huggingface.co/papers/2505.01658))| Sungryeol Jeon, leejaymin, Devcow, oos2, inputsh | This paper presents a comprehensive survey of 25 LLM inference engines, evaluating their optimization techniques, hardware support, and ecosystem maturity to guide efficient deployment. The primary objective is to systematically compare these open-source and commercial engines, identifying their design goals, supported features, and suitability for throughput- or latency-sensitive LLM services. The methodology involves analyzing each engine's architecture, supported optimization categories (e.g., parallelism, compression, caching per Table 7), hardware compatibility (Table 4), and non-technical indicators like GitHub activity and documentation quality (Table 3). Key findings show significant diversity: engines like Ollama gained high user preference (209.6 average daily GitHub star growth) for ease of use, while solutions like vLLM and TensorRT-LLM offer extensive, specialized optimizations for demanding server-side inference, supporting techniques such as PagedAttention and various parallelisms. The principal implication for AI practitioners is a structured guide for selecting optimal inference engines based on specific performance requirements, hardware constraints, and the trade-offs between ease-of-use, feature support, and ecosystem maturity, facilitating more efficient and cost-effective LLM service deployment. |
| R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.02835) or [HuggingFace](https://huggingface.co/papers/2505.02835))| KevinTowne, KaiyuValley, bhsc24, XingyuLu, yifanzhang114 | This paper introduces R1-Reward, a multimodal reward model (MRM) trained via a novel StableReinforce algorithm to enhance reward modeling through stable reinforcement learning. The primary objective is to explore and improve the application of reinforcement learning (RL) for multimodal reward modeling by addressing the instability issues of existing RL algorithms in this context. The key methodology involves reformulating reward modeling as a rule-based RL task and developing StableReinforce, which incorporates refined training loss (Pre-CLIP), advantage estimation (Advantage Filter), and a novel consistency reward mechanism using an MLLM referee. R1-Reward achieves state-of-the-art performance, demonstrating a 14.3% improvement on the Multimodal Reward Bench compared to previous SOTA models. For AI practitioners, this work provides a robust method (StableReinforce) and a high-performing model (R1-Reward) for developing more accurate MRMs, crucial for improving MLLM alignment, data filtering, and evaluation. |
| Think on your Feet: Adaptive Thinking via Reinforcement Learning for
  Social Agents (Read more on [arXiv](https://arxiv.org/abs/2505.02156) or [HuggingFace](https://huggingface.co/papers/2505.02156))| Xinghua Zhang, Haobo Wang, bingliwu, Yongbin-Li, iiiiwis | This paper introduces Adaptive Mode Learning (AML) with an Adaptive Mode Policy Optimization (AMPO) algorithm to enable social agents to dynamically adjust reasoning depth in social interactions. The main objective is to develop language agents that can dynamically adjust their reasoning depth based on real-time context in social simulations, unlike current approaches that use fixed reasoning depths or lack reasoning capabilities. The key methodology involves defining four thinking modes (intuitive reaction to deep contemplation) and using the AMPO algorithm, which incorporates multi-granular thinking mode design, context-aware mode switching, and token-efficient reasoning, trained via behavioral cloning and reinforcement learning. Primary results show AML achieves 15.6% higher task performance than state-of-the-art methods, and notably, outperforms GRPO by 7.0% in performance with 32.8% shorter reasoning chains. For AI practitioners, AMPO provides a framework to develop more human-like, adaptive, and token-efficient social agents capable of context-sensitive reasoning in complex social environments. |
| SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from
  Sparse and Noisy Demonstrations (Read more on [arXiv](https://arxiv.org/abs/2505.02094) or [HuggingFace](https://huggingface.co/papers/2505.02094))| Hok Wai Tsui, Yinhuai Wang, cqf, Crimnos, IngridYU | SkillMimic-V2 introduces a framework for learning robust and generalizable robot interaction skills from sparse and noisy demonstrations by augmenting data and employing adaptive training. The main research objective is to overcome demonstration noise and coverage limitations in Reinforcement Learning from Interaction Demonstration (RLID), enabling robots to learn complex skills from limited and imperfect human demonstrations. The key methodology involves two data augmentation techniquesâ€”Stitched Trajectory Graph (STG) and State Transition Field (STF)â€”an Adaptive Trajectory Sampling (ATS) strategy for curriculum generation, and a History Encoder (HE) for memory-dependent skills. The method enhances generalization performance by over 35%; for instance, on the BallPlay-M benchmark, it achieved an average Îµ-Neighborhood Success Rate (ÎµNSR) of 49.3% compared to 18.3% for the baseline SkillMimic (SM). The principal implication for AI practitioners is that this approach allows for the training of AI agents for complex physical interaction tasks using sparse and noisy demonstrations, significantly improving skill robustness and generalization beyond the provided data. |
| Agentic Reasoning and Tool Integration for LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.01441) or [HuggingFace](https://huggingface.co/papers/2505.01441))| akshaynambi, akshaynambi, Raghav2002, joykirat | The paper introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework using outcome-based reinforcement learning (RL) to enable LLMs to autonomously reason and integrate external tools for complex problem-solving. The objective is to enable LLMs to autonomously decide when, how, and which tools to invoke within multi-step reasoning chains, learning robust strategies via RL without step-level supervision. ARTIST trains LLMs using Group Relative Policy Optimization (GRPO), interleaving text-based reasoning with tool invocations and outputs, guided by a composite reward function (correctness, format, tool success) and loss masking for tool outputs. ARTIST achieved up to a 22% absolute improvement in mathematical reasoning (e.g., Qwen2.5-14B-ARTIST scored 0.55 Pass@1 on AMC) and more than doubled accuracy on some multi-turn function calling tasks compared to base models. Integrating agentic reasoning with dynamic tool use via outcome-based RL, as in ARTIST, offers a robust path to enhance LLMs for complex tasks requiring external interaction, without needing detailed step-by-step supervision data. |
| SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based
  Image Editing (Read more on [arXiv](https://arxiv.org/abs/2505.02370) or [HuggingFace](https://huggingface.co/papers/2505.02370))| Xin Gu, Zilence006, lionwen, xiaoying0505, limingcv | SuperEdit introduces a data-oriented method to improve instruction-based image editing by rectifying editing instructions using diffusion priors and facilitating supervision with contrastive learning. The primary objective is to address noisy supervision in instruction-based image editing by developing more effective editing instructions that better align with original-edited image pairs, thereby improving model performance without requiring architectural changes or extensive pre-training. The methodology involves: i) Rectifying editing instructions by guiding a Vision-Language Model (GPT-4O) with diffusion generation priors, which link inference timesteps to specific image attribute changes (global layout, local objects, style/details); and ii) Constructing contrastive supervision signals by generating positive (rectified) and negative (incorrect, subtly altered) instructions from the VLM and training the editing model using a triplet loss. SuperEdit demonstrated a 9.19% performance improvement over the prior state-of-the-art SmartEdit on the Real-Edit benchmark (achieving an Overall Score of 3.91), while utilizing 30x less training data (40K samples) and a 13x smaller model (1.1B parameters). For AI practitioners, this research highlights that significant performance gains in instruction-based image editing can be achieved by focusing on the quality and precision of supervision signals rather than solely on model architecture complexity or extensive pre-training, suggesting a more data-centric and efficient path for model improvement. |
| Low-Precision Training of Large Language Models: Methods, Challenges,
  and Opportunities (Read more on [arXiv](https://arxiv.org/abs/2505.01043) or [HuggingFace](https://huggingface.co/papers/2505.01043))| Li Shen, Guoxia, csdvT, GGJY, Zhiwei840 | This survey comprehensively reviews low-precision training techniques for Large Language Models (LLMs), categorizing approaches by numerical formats to address research fragmentation. The primary objective is to systematically organize existing methodsâ€”fixed-point/integer-based, floating-point-based, and customized formatsâ€”and discuss quantization-aware training (QAT) and system support. The paper reveals an increasing adoption of integer and low-precision floating-point methods, citing an example where FP8-LM achieved a 75% training speedup compared to BF16 for a 175B parameter model. For AI practitioners, this survey offers a structured understanding of how to implement more resource-efficient LLM training pipelines by selecting appropriate low-precision techniques and leveraging evolving hardware support. |
| Ming-Lite-Uni: Advancements in Unified Architecture for Natural
  Multimodal Interaction (Read more on [arXiv](https://arxiv.org/abs/2505.02471) or [HuggingFace](https://huggingface.co/papers/2505.02471))| bear-xxy, jianxinsun, chenjingdong, zhengdd0422, BiaoGong | Ming-Lite-Uni is an open-source multimodal framework unifying vision and language through a novel visual generator, multi-scale learnable tokens, and a native autoregressive model for tasks like text-to-image generation and instruction-based editing. The paper aims to demonstrate a unified autoregressive multimodal model built upon multi-scale learnable tokens with fine-tuned diffusion models and to accelerate community engagement by open-sourcing its implementation, improving upon the integrated MetaQueries and M2-omni frameworks. The framework leverages a fixed Multimodal Large Language Model (MLLM) (Llama3-based M2-omni) and fine-tunes an external diffusion model using newly designed multi-scale learnable query tokens, a multi-scale representation alignment strategy (minimizing Mean Squared Error between DiT backbone intermediate states and final semantic representations), and a FlowMatching loss. Ming-Lite-Uni achieved an overall accuracy of 0.62 on the GenEval benchmark for text-to-image generation, notably scoring 0.99 on single-object generation, and demonstrated strong multimodal understanding with an 80.7 MMB score and 72.3 MM-Vet score. This work provides AI practitioners with an open-source, unified architecture that effectively integrates understanding and generation capabilities, offering a practical foundation for developing advanced multimodal AI systems with robust interactive and generative performance. |
| TEMPURA: Temporal Event Masked Prediction and Understanding for
  Reasoning in Action (Read more on [arXiv](https://arxiv.org/abs/2505.01583) or [HuggingFace](https://huggingface.co/papers/2505.01583))| vibhav-vineet, yilche, wchai, hsiangwei0903, andaba | TEMPURA is a two-stage training framework employing masked event prediction and dense captioning, supported by the new VER dataset, to significantly improve temporal event understanding and causal reasoning in video action. The research aims to enhance video Large Multi-modal Models' (LMMs) capability to understand causal event relationships and achieve fine-grained temporal grounding in videos by enabling them to infer missing events and segment videos into detailed, temporally-aligned event descriptions. TEMPURA utilizes a two-stage training pipeline: first, it applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations; second, it learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions, using the newly curated VER dataset (500K videos, 18K hours). TEMPURA significantly outperforms strong baseline models without task-specific fine-tuning, achieving a mean Intersection over Union (mIoU) of 39.2 on the Charades-STA benchmark (a 6.3 point improvement over the baseline) and a HIT@1 score of 51.7 on the QVHighlights dataset (a 6.9 point improvement). This work provides AI engineers and data scientists with a structured two-stage training approach and a large-scale dataset (VER) for developing video LMMs with enhanced abilities to reason about event causality and perform fine-grained temporal segmentation, crucial for applications like highlight detection and detailed video analysis. |
| LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive
  Streaming Speech Synthesis (Read more on [arXiv](https://arxiv.org/abs/2505.02625) or [HuggingFace](https://huggingface.co/papers/2505.02625))| Yang Feng, Yan Zhou, zhangshaolei, guoshoutao, poeroz | LLaMA-Omni 2 is a series of modular Speech Language Models (0.5B-14B parameters) achieving real-time, high-quality spoken chatbot interaction through an autoregressive streaming speech synthesis pipeline. The main objective is to develop an end-to-end spoken language model capable of real-time, intelligent, and natural speech interaction, addressing limitations of cascaded systems and the extensive data requirements of native SpeechLMs, while retaining strong underlying text capabilities. The system integrates a Whisper speech encoder and adapter with a Qwen2.5 LLM, followed by a streaming speech generation module; this module comprises an autoregressive text-to-speech language model (MTTS), initialized from Qwen2.5-0.5B and utilizing a "Read-R-Write-W" strategy to generate speech tokens, which are then converted to mel spectrograms by a causal flow matching model and HiFi-GAN vocoder, with the entire system fine-tuned on 200K synthesized multi-turn speech dialogues. LLaMA-Omni 2 demonstrates strong performance, with the 7B parameter model achieving 31.3% accuracy on the Web Questions speech-to-speech benchmark, significantly outperforming GLM-4-Voice (15.9%) and exhibiting a latency of 582.91ms for the first speech chunk (using R=3, W=10). The principal implication for AI practitioners is that this modular approach, leveraging pre-trained LLMs with specialized speech components and an efficient streaming architecture, enables the development of high-performance, real-time spoken dialogue systems using substantially less speech-specific training data (200K samples) compared to large native SpeechLMs, offering a more data-efficient pathway. |
| MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset
  via Attention Routing (Read more on [arXiv](https://arxiv.org/abs/2505.02823) or [HuggingFace](https://huggingface.co/papers/2505.02823))| Chong Mou, Pengze Zhang, heqian, yanze, Zinan123212 | MUSAR introduces a framework for multi-subject image customization using only single-subject training data via attention routing. The primary objective is to overcome the challenges of acquiring diverse multi-subject training data and mitigating attribute entanglement between subjects in text-to-image generation. MUSAR employs de-biased diptych learning, which constructs multi-subject training pairs from single-subject images and corrects systemic biases using static attention routing and dual-branch LoRA, alongside a dynamic attention routing mechanism that adaptively maps image regions to their corresponding conditional subjects to prevent entanglement. Quantitatively, on DreamBench multi-subject customization, MUSAR achieved a DINO score of 0.704 and a CLIP-I score of 0.720, outperforming methods trained on actual multi-subject datasets. This work provides AI practitioners a data-efficient pathway to develop robust multi-subject customization models without relying on difficult-to-obtain multi-subject datasets, by leveraging synthesized training data and refined attention mechanisms. |
| Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural
  Radiance Fields (Read more on [arXiv](https://arxiv.org/abs/2505.02005) or [HuggingFace](https://huggingface.co/papers/2505.02005))| Dan Xu, Xue Xiao, Ping Yin, Zhenxing Mi | This paper introduces Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) framework for efficiently learning decomposition and heterogeneous representations of large-scale Neural Radiance Fields. The main objective is to develop a highly scalable NeRF method that addresses learnable scene decomposition, models scene heterogeneity, and improves modeling efficiency for complex, large-scale scenes in an end-to-end manner. The key methodology involves a hash-based gating network that learns to decompose scenes and allocate 3D points to a set of distinct, heterogeneous hash experts, each designed with different hash grid resolution ranges, all co-optimized within a Sparsely Gated Mixture of Experts (MoE) NeRF framework. Primary results demonstrate state-of-the-art rendering accuracy and significant efficiency improvements; for instance, Switch-NeRF++ achieves an 8x acceleration in training and a 16x acceleration in rendering (e.g., rendering a 1152x864 image in 6.65s versus 110s for Switch-NeRF) compared to the best-performing competitor Switch-NeRF, and outperforms INGP on the UrbanBIS dataset (PSNR 20.76 vs 19.58). The principal implication for AI practitioners is the provision of a more practical and efficient solution for applying NeRFs to real-world, large-scale 3D scene modeling, enabling higher quality and faster reconstruction with reduced computational resources, particularly for scenes with diverse content. |
| Unlearning Sensitive Information in Multimodal LLMs: Benchmark and
  Attack-Defense Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.01456) or [HuggingFace](https://huggingface.co/papers/2505.01456))| Jie Peng, Peter Hase, mohitbansal, a2889184, vaidehi99 | This paper introduces UNLOK-VQA, a benchmark, and an attack-defense framework for evaluating targeted unlearning of sensitive information in Multimodal Large Language Models (MLLMs). The main objective is to systematically evaluate the effectiveness of unlearning methods in MLLMs, particularly for deleting specific multimodal knowledge while preserving model utility. The methodology involves generating the UNLOK-VQA dataset with varied proximity samples for efficacy, generalization, and specificity testing, and an attack-defense framework comprising seven attack types (e.g., a novel Probability Delta2 whitebox attack) against six LoRA-based unlearning defense objectives. Primary results show that multimodal extraction attacks (45.5% success rate against a baseline defense) are more effective than image-only (32%) or text-only (39%) attacks, though the Head Projection (HP) defense significantly reduces multimodal blackbox attack success to 15.7%. For AI practitioners, this research underscores the heightened risk of sensitive information leakage in MLLMs via multimodal inputs and provides a benchmark (UNLOK-VQA) and evidence that specific defense strategies (like HP) are critical for mitigating these vulnerabilities during MLLM development and deployment. |
