

## Papers for 2025-05-27

| Title | Authors | Summary |
|-------|---------|---------|
| Shifting AI Efficiency From Model-Centric to Data-Centric Compression (Read more on [arXiv](https://arxiv.org/abs/2505.19147) or [HuggingFace](https://huggingface.co/papers/2505.19147))| Pppeach33, coderchen01, Steven-Shaobo, zichenwen, xuyang-liu16 | i) This paper argues for a paradigm shift from model-centric to data-centric compression, specifically token compression, to improve AI efficiency by reducing token counts during training and inference. ii) The main research objective is to analyze and advocate for token compression as a crucial strategy in addressing the computational bottlenecks introduced by increasing context lengths in LLMs and MLLMs. iii) The methodology involves a comprehensive analysis of long-context AI developments, a unified mathematical framework for model efficiency strategies, and a systematic review of token compression techniques. iv) Results show that attention-based token compression methods can underperform compared to simple random pruning in certain scenarios; also, from 2022-2024 model size primarily drove computational costs, but from 2024 onward, token count has grown exponentially. v) AI practitioners should shift focus toward data-centric approaches like token compression, exploring methods that maintain spatial uniformity and mitigate biases, to achieve more efficient and scalable AI systems, and evaluate existing token compression techniques carefully, as speedup is not always reflected in runtime latency.  |
| Mutarjim: Advancing Bidirectional Arabic-English Translation with a
  Small Language Model (Read more on [arXiv](https://arxiv.org/abs/2505.17894) or [HuggingFace](https://huggingface.co/papers/2505.17894))| Sara Chrouf, ZeinaD, Moatasem444, hr99, Hennara | i) The paper introduces Mutarjim, a compact language model for bidirectional Arabic-English translation, and Tarjama-25, a new benchmark dataset. ii) The research aims to develop a smaller, task-specific model that balances translation performance with efficiency, specifically for Arabic-English translation. iii) The methodology involves a two-phase training approach: large-scale monolingual pre-training and supervised fine-tuning with high-quality Arabic-English parallel data, building upon the Kuwain-1.5B language model. iv) Experimental results demonstrate that Mutarjim outperforms larger models, achieving state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing models like GPT-40 mini, with a ChrF score of 83.41. v) The development of Mutarjim provides AI practitioners with a resource-efficient alternative for Arabic-English translation, demonstrating that smaller, specialized models can achieve competitive performance while reducing computational costs.  |
| BizFinBench: A Business-Driven Real-World Financial Benchmark for
  Evaluating LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.19457) or [HuggingFace](https://huggingface.co/papers/2505.19457))| Ji Liu, Qlisp, Tinker250, xuntao, guilong | i) BizFinBench, a new financial benchmark, is introduced to evaluate LLMs in real-world financial applications. ii) The research aims to rigorously evaluate LLMs across a broad spectrum of real-world financial tasks within the financial domain. iii) The methodology involves a new dataset construction and the introduction of IteraJudge, an iterative calibration-based evaluation framework. iv) The evaluation of 25 LLMs reveals that Gemini-2.0-Flash achieves SOTA performance in Anomalous Event Attribution with a score of 86.94. v) AI practitioners should be aware of the limitations of current LLMs in handling complex financial tasks requiring integrated knowledge and cross-concept reasoning, suggesting areas for future model development.  |
| Alchemist: Turning Public Text-to-Image Data into Generative Gold (Read more on [arXiv](https://arxiv.org/abs/2505.19297) or [HuggingFace](https://huggingface.co/papers/2505.19297))| Sergey Kastryulin, Dmitry Baranchuk, Alexey Kirillov, Alexander Ustyuzhanin, sharfikeg | i) This paper introduces Alchemist, a supervised fine-tuning (SFT) dataset and methodology for enhancing the generative quality of text-to-image (T2I) models. ii) The research objective is to develop a method for curating general-purpose SFT datasets that improve T2I model performance while maintaining diversity and style. iii) The methodology involves a multi-stage filtering pipeline leveraging a pre-trained generative model to estimate the impact of training samples, followed by re-captioning using a vision-language model. iv) Experiments show that fine-tuning public T2I models with the 3,350-sample Alchemist dataset improves aesthetic quality and image complexity by up to 20% in human preference win rates compared to baseline models. v) AI practitioners can utilize the Alchemist dataset and methodology to efficiently fine-tune T2I models, achieving substantial gains in generative quality using a relatively small, high-quality dataset.  |
| Embodied Agents Meet Personalization: Exploring Memory Utilization for
  Personalized Assistance (Read more on [arXiv](https://arxiv.org/abs/2505.16348) or [HuggingFace](https://huggingface.co/papers/2505.16348))| jinyeo, ej0cl6, bwookwak, Lune-Blue, Connoriginal | i) The paper introduces MEMENTO, a framework for evaluating episodic memory utilization in LLM-powered embodied agents for personalized assistance in object rearrangement tasks. ii) The research investigates the effectiveness of embodied agents in leveraging memory to understand user-specific object semantics and routines for personalized instruction interpretation. iii) The methodology involves a two-stage process: Memory Acquisition and Memory Utilization, comparing agent performance on tasks with and without explicit personalized knowledge cues. iv) Experiments revealed that even the frontier model GPT-40 experienced a 30.5% performance drop in joint-memory tasks when required to reference multiple memories, particularly those involving user patterns. v) The study implies that current LLM-powered embodied agents face significant limitations in effectively leveraging episodic memory for personalized assistance, highlighting the need for improved memory utilization and reasoning capabilities in complex, multi-step personalized tasks.  |
| PATS: Process-Level Adaptive Thinking Mode Switching (Read more on [arXiv](https://arxiv.org/abs/2505.19250) or [HuggingFace](https://huggingface.co/papers/2505.19250))| Shujian Huang, Jiajun Chen, Shimao Zhang, master-lan, Yi53 | i) This paper introduces Process-Level Adaptive Thinking Mode Switching (PATS), a novel reasoning paradigm for Large Language Models (LLMs). ii) The primary research objective is to enable LLMs to dynamically adjust reasoning strategies at each step based on problem difficulty, balancing accuracy and computational efficiency. iii) The methodology integrates Process Reward Models (PRMs) with Beam Search, incorporating progressive mode switching and bad-step penalty mechanisms. iv) Experiments on mathematical benchmarks demonstrate that PATS achieves high accuracy while maintaining moderate token usage, with a 4.4-point accuracy improvement over solution-verification switching while using 7% fewer tokens. v) PATS's adaptive switching mechanism, dynamically adjusting reasoning based on step-wise difficulty, provides AI practitioners with a method to improve LLM inference efficiency without sacrificing accuracy.  |
| ARM: Adaptive Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2505.20258) or [HuggingFace](https://huggingface.co/papers/2505.20258))| Kai Zhang, Aili Chen, Arist12, hsaest, Siye01 | i) The paper introduces ARM, a model that adaptively selects reasoning formats to balance performance and computational efficiency. ii) The main objective is to develop a reasoning model that can dynamically adjust its token usage based on task complexity without human intervention. iii) The methodology involves a two-stage training framework: supervised fine-tuning (SFT) for format understanding, followed by reinforcement learning using Ada-GRPO, an adaptation of Group Relative Policy Optimization. iv) ARM achieves comparable performance to models relying solely on Long CoT, while reducing token usage by an average of 30% and up to 70% and achieves an approximate 2x speedup in training. v) ARM provides AI practitioners with a method to create more efficient and performant reasoning models by dynamically adapting reasoning strategies based on task requirements, reducing computational overhead without sacrificing accuracy.  |
| Enigmata: Scaling Logical Reasoning in Large Language Models with
  Synthetic Verifiable Puzzles (Read more on [arXiv](https://arxiv.org/abs/2505.19914) or [HuggingFace](https://huggingface.co/papers/2505.19914))| Zhicheng Cai, Aili Chen, siyuyuan, Abbey4799, jiangjiechen | i) This paper introduces ENIGMATA, a comprehensive suite for scaling logical reasoning in LLMs using synthetic, verifiable puzzles. ii) The research aims to improve LLMs' puzzle reasoning skills through a tailored suite of tasks, evaluation benchmarks, and training recipes. iii) The methodology includes generating 36 diverse puzzle tasks with controllable difficulty and automatic verification, alongside optimized multi-task RLVR strategies. iv) Qwen2.5-32B-ENIGMATA surpasses prior state-of-the-art LRMs on ARC-AGI (32.8%) and the ENIGMATA-Eval benchmark. v) ENIGMATA provides AI practitioners with a unified framework for advancing logical reasoning in LLMs, enhancing their performance on complex problem-solving tasks and demonstrating potential benefits for larger models in math/STEM.  |
| B-score: Detecting biases in large language models using response
  history (Read more on [arXiv](https://arxiv.org/abs/2505.18545) or [HuggingFace](https://huggingface.co/papers/2505.18545))| Daeyoung Kim, anhng8, taesiri, anvo25 | i) The paper introduces B-score, a novel metric for detecting biases in large language models (LLMs) based on response history in multi-turn conversations. ii) The primary research objective is to determine if LLMs can reduce biases by observing their prior responses in a multi-turn conversational setting and to assess the effectiveness of B-score in detecting different types of biases. iii) The methodology involves comparing single-turn and multi-turn conversational responses across subjective, random, and objective question categories, calculating B-score as the difference in probabilities of an answer appearing in single-turn versus multi-turn settings. iv) The results demonstrate that LLMs can "de-bias" themselves in multi-turn conversations for random questions and using B-score improves answer verification accuracy by +9.3 on a proposed question dataset. v) AI practitioners can leverage B-score as a runtime indicator to detect and mitigate biased responses from LLMs, particularly in scenarios where access to ground truth labels is limited, substantially enhancing the verification accuracy of LLM answers.  |
| Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.19815) or [HuggingFace](https://huggingface.co/papers/2505.19815))| Linchen Xiao, Hongwei Liu, zsytony, Sudanl, jnanliu | The paper proposes RAML (Reasoning as Meta-Learning), a novel framework that interprets LLM reasoning through the lens of meta-learning. The research aims to understand and optimize LLM reasoning capabilities by conceptualizing reasoning trajectories as pseudo-gradient descent updates to LLM parameters. The methodology formalizes reasoning task training as a meta-learning setup, treating each question as a distinct task and using reasoning trajectories for inner-loop parameter adaptation. Evaluations using Qwen2.5-7B-Base demonstrate that supervised fine-tuning with 32 synthetic reasoning trajectories per question improves performance, showing a gain in Pass@8 metric, and increased reasoning efficiency is attainable, albeit requires further investigation on which token types facilitate the most efficient reasoning. RAML provides a foundation for applying meta-learning insights to enhance LLM reasoning by framing it as a process of optimizing pseudo-gradient descent.  |
| Lifelong Safety Alignment for Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.20259) or [HuggingFace](https://huggingface.co/papers/2505.20259))| Min Lin, Chao Du, Yifei Zhao, Zeyu Qin, Haoyu Wang | This paper introduces a lifelong safety alignment framework for language models (LLMs). The research question addresses how to continuously adapt LLMs to new and evolving jailbreaking strategies. The methodology employs a competitive setup between a Meta-Attacker, trained to discover novel jailbreaking strategies, and a Defender, trained to resist them, warm-started with insights extracted from jailbreak-related research. The primary result is a reduction of the Meta-Attacker’s success rate from 73% to 7% on RR after iterative training and a 57% transfer attack success rate on LAT using single-turn attacks initially. The principal implication for AI practitioners is a framework for improving the robustness and reliability of LLMs in open-ended environments by continually adapting to new attack vectors.  |
| MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis
  Discovery via Hierarchical Search (Read more on [arXiv](https://arxiv.org/abs/2505.19209) or [HuggingFace](https://huggingface.co/papers/2505.19209))| Wei Li, Yujie Liu, Ben Gao, Wanhao Liu, ZonglinY | i) This paper introduces MOOSE-Chem2, a framework for fine-grained scientific hypothesis discovery using LLMs via hierarchical search. ii) The primary objective is to investigate the upper limits of LLMs in generating detailed, experimentally actionable scientific hypotheses from coarse initial research directions. iii) The methodology involves a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations, defining a reward landscape based on LLM's internal heuristics. iv) Empirical evaluations demonstrate that the hierarchical search method consistently outperforms strong baselines, and hypotheses generated by the proposed method achieve higher recall than those from baselines (e.g., HHS achieves 40.40% soft recall vs. 16.60% soft recall for Greedy Search). v) This research provides AI practitioners with a structured approach to leverage LLMs for generating more detailed and experimentally viable scientific hypotheses, improving automation of the scientific discovery process. The paper's results indicate that repeated use of the strongest model provides better reward landscapes than diverse ensembles, suggesting practical implementation strategies.  |
| Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual
  Reasoning from Transit Maps (Read more on [arXiv](https://arxiv.org/abs/2505.18675) or [HuggingFace](https://huggingface.co/papers/2505.18675))| Lingdong Kong, Shuyi Ouyang, Song Wang, Huan-WhoRegisteredMyName, FSCCS | i) REASONMAP, a new benchmark, is introduced for evaluating fine-grained visual understanding and spatial reasoning in MLLMs using transit maps. ii) The research aims to assess MLLMs' proficiency in tasks requiring detailed visual interpretation, specifically spatial reasoning on transit maps. iii) The methodology involves a novel dataset with 1,008 question-answer pairs across 30 cities and a two-level evaluation framework measuring answer correctness and quality. iv) Evaluations of 15 MLLMs revealed that base models outperform reasoning variants among open-source models, while the opposite trend is observed in closed-source models; performance degrades when visual inputs are masked. v) AI practitioners should note the counterintuitive finding that reasoning-enhanced architectures do not consistently improve performance on fine-grained visual tasks, and that visual grounding remains crucial, even when models possess prior knowledge. |
| Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.18536) or [HuggingFace](https://huggingface.co/papers/2505.18536))| Yifei Zhao, Yifu Luo, Bo Xia, Jiaqi Wu, Haoyuan Sun | Reinforcement fine-tuning (RFT) significantly enhances reasoning capabilities in multimodal large language models (MLLMs). The paper investigates how RFT improves MLLM reasoning across diverse modalities. The methodology involves summarizing the improvements of RFT in powering reasoning capabilities of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks, categorizing RFT algorithms into Critic-Model-Driven and Critic-Model-Free. The survey summarizes recent works, categorized by release time and modality. This work implies future research should focus on generalizable reasoning, safety, data augmentation and better reward mechanisms for reasoning MLLMs.  |
| Surrogate Signals from Format and Length: Reinforcement Learning for
  Solving Mathematical Problems without Ground Truth Answers (Read more on [arXiv](https://arxiv.org/abs/2505.19439) or [HuggingFace](https://huggingface.co/papers/2505.19439))| Dianbo Sui, Yupeng Zhang, Zecheng Wang, Han Liu, Rihui Xin | i) The paper introduces a reinforcement learning (RL) approach using format and length as surrogate rewards for mathematical problem-solving, eliminating reliance on ground truth answers. ii) The research investigates whether LLMs can be effectively trained for mathematical reasoning tasks using only format and length-based rewards, bypassing the need for ground truth labels. iii) The methodology employs Group Relative Policy Optimization (GRPO) with a reward function incorporating format correctness and response length, evaluated on mathematical datasets. iv) The results show that the proposed GRPO approach, using format-length surrogate signals, achieves 40.0% accuracy on AIME2024, surpassing standard GRPO performance relying on ground truth in certain scenarios. v) AI practitioners can leverage format and length rewards as effective substitutes for ground truth labels in mathematical problem-solving RL, reducing data collection costs and facilitating training in label-scarce environments.  |
| Flex-Judge: Think Once, Judge Anywhere (Read more on [arXiv](https://arxiv.org/abs/2505.18601) or [HuggingFace](https://huggingface.co/papers/2505.18601))| Se-Young Yun, Sungwoo Cho, Jongwoo Ko, sungnyun | FLEX-Judge is introduced as a modality-agnostic approach for training multimodal judge models. The paper investigates whether a small amount of text-only reasoning data can effectively train a cost-efficient, modality-agnostic judge model. The methodology involves training a multimodal judge model using a 1K-sized corpus of high-quality text reasoning data from JudgeLRM. FLEX-Judge (7B model) achieves competitive performance compared to commercial APIs and outperforms open-source judges, even exceeding Gemini and GPT-40 on several MJ-Bench and GenAI-Bench subtasks. The principal implication is that reasoning-based text supervision offers a cost-effective alternative to annotation-intensive approaches, advancing scalable multimodal model evaluation, applicable to modalities like molecule evaluation where comprehensive benchmarks are scarce.  |
| Which Data Attributes Stimulate Math and Code Reasoning? An
  Investigation via Influence Functions (Read more on [arXiv](https://arxiv.org/abs/2505.19949) or [HuggingFace](https://huggingface.co/papers/2505.19949))| Zhijie Deng, Zihao Zeng, Hanwen Xu, Qingyuan Tian, Siqi Kou | i) The paper introduces Infra, an influence function-based approach, to attribute the reasoning capabilities of large language models (LLMs) in math and coding tasks to specific training data attributes. ii) The research investigates which attributes of training data most effectively stimulate LLMs' reasoning capabilities in math and code. iii) Influence functions are leveraged to attribute LLMs' reasoning performance to individual training examples, sequences, and tokens, with a focus on identifying positively influential data. iv) The study found that flipping task difficulty via dataset reweighting boosts AIME24 accuracy from 10% to 20% and improves LiveCodeBench accuracy from 33.8% to 35.3% for the Qwen2.5-7B-Instruct model, and that token-level influence patterns are distinct for math and code reasoning. v) AI practitioners can use the identified data attributes and the Infra framework to curate and optimize training datasets for reasoning-intensive tasks, improving the efficiency and effectiveness of LLM training.  |
| Discrete Markov Bridge (Read more on [arXiv](https://arxiv.org/abs/2505.19752) or [HuggingFace](https://huggingface.co/papers/2505.19752))| Ying Nian Wu, Song-Chun Zhu, zlzheng, ColorfulAI, henry12348 | i) The paper introduces Discrete Markov Bridge (DMB), a novel variational framework for discrete representation learning. ii) The main objective is to overcome the limitations of fixed-rate transition matrices in existing discrete diffusion models to achieve better latent representations. iii) The methodology involves a bidirectional two-stage learning algorithm with Matrix-learning and Score-learning components, using a parameterized, diagonalizable rate transition matrix. iv) Empirical evaluations on Text8 resulted in an Evidence Lower Bound (ELBO) of 1.38, outperforming baselines, and competitive results were shown on CIFAR-10. v) The DMB framework's matrix learning process enhances expressiveness of latent representations, providing AI practitioners with a more efficient and adaptable approach for discrete data modeling compared to methods with fixed-rate transition matrices.  |
| Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2505.20256) or [HuggingFace](https://huggingface.co/papers/2505.20256))| Zheng Huang, Zongze Du, Muzhi Zhu, Hao Zhong, Canyu | i) Omni-R1 is presented as an end-to-end reinforcement learning framework for omnimodal reasoning that addresses the trade-off between temporal coverage and spatial resolution. ii) The main objective is to enable long-horizon video-audio reasoning and fine-grained pixel understanding in omnimodal models. iii) The methodology employs a two-system architecture (Global Reasoning System and Detail Understanding System) trained via reinforcement learning using Group Relative Policy Optimization (GRPO). iv) Experiments on Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS) show Omni-R1 surpasses supervised baselines, improving out-of-domain generalization and mitigating multimodal hallucination with a +4.6% on J&F in seen set and +17.0% on unseen set in Ref-AVSBench. v) AI practitioners can utilize Omni-R1's architecture to develop scalable omnimodal models capable of effective long-horizon reasoning and precise pixel-level grounding, addressing limitations in existing foundation models.  |
| Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured
  Multi-Turn Decomposition (Read more on [arXiv](https://arxiv.org/abs/2505.19788) or [HuggingFace](https://huggingface.co/papers/2505.19788))| Zhijie Deng, Hao Zhang, Boxiu Li, Zihao Zeng, ElysiaTrue | i) The paper introduces Multi-Turn Decomposition (MinD), a method to improve the efficiency of reasoning in Large Reasoning Models (LRMs) by structuring the reasoning process. ii) The research aims to reduce token usage and latency in LRMs while maintaining performance on complex reasoning tasks. iii) The methodology involves supervised fine-tuning (SFT) to transform conventional Chain-of-Thought (CoT) data into a multi-turn format, followed by reinforcement learning (RL) using Group Relative Policy Optimization (GRPO) to prioritize correct outputs with fewer reasoning turns. iv) Results show MinD achieves up to 70% reduction in output token usage and a 4.2x speedup in time to first token (TTFT) on MATH-500 with DeepSeek-R1-Distill-Qwen-1.5B, while maintaining over 95% accuracy. v) MinD offers AI practitioners a structured approach to reduce computational costs and latency in LRMs, potentially improving the user experience in applications requiring complex reasoning.  |
| Hard Negative Contrastive Learning for Fine-Grained Geometric
  Understanding in Large Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2505.20152) or [HuggingFace](https://huggingface.co/papers/2505.20152))| Ji Qi, Jiajie Zhang, Zhen Yang, Yushi Bai, Kai Sun | i) This paper introduces a hard negative contrastive learning framework for improving geometric understanding in Large Multimodal Models (LMMs). ii) The research aims to enhance the vision encoder's ability to recognize fine-grained geometric elements within images for geometric problem-solving. iii) The methodology involves image-based contrastive learning using diagrams generated via code perturbation and text-based contrastive learning using rule-based and retrieval-based negative captions; a novel MMCLIP training strategy is proposed to handle an arbitrary number of hard negatives. iv) The MMGeoLM model, trained with the proposed framework, surpasses existing open-source models on GeoQA and MathVISTA and achieves state-of-the-art performance on MM-MATH, exceeding GPT-4o by 7.5%. v) The principal implication for AI practitioners is a method to improve LMMs' geometric reasoning through targeted hard negative contrastive learning, enabling more accurate visual perception in tasks requiring fine-grained geometric understanding; using exam-based, authentic image negatives showed better results than over 100K text negatives.  |
| The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2505.18759) or [HuggingFace](https://huggingface.co/papers/2505.18759))| Song Wang, Zhen Tan, Rana Muhammad Shahroz Khan, Ruichen Zhang, wjldw | DC-CoT is introduced as a data-centric benchmark for chain-of-thought (CoT) distillation in large language models (LLMs). The research investigates how data manipulation techniques impact CoT distillation across method, model, and data perspectives. The methodology involves evaluating augmentation, selection, and mixing strategies with diverse teacher models (e.g., Gemini-Pro, Claude-3.5) and student architectures (3B, 7B parameters) on multiple reasoning datasets, focusing on in-distribution (IID), out-of-distribution (OOD) generalization, and cross-domain transfer. Results show data augmentation is generally the most effective approach, with reverse augmentation improving average accuracy by 24.64% on tested tasks using Llama-3.1-8B. The findings provide actionable insights for AI practitioners to optimize CoT distillation through data-centric techniques, thereby facilitating more efficient reasoning models.  |
| Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV
  Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2505.19602) or [HuggingFace](https://huggingface.co/papers/2505.19602))| Jenq-Neng Hwang, Cheng-Yen Yang, Zigeng Chen, stargazerx0 | ScaleKV introduces a novel KV cache compression framework tailored for visual autoregressive modeling to improve memory efficiency. The research aims to mitigate the exponential growth of KV cache in VAR models by employing scale-aware layer budget allocation. The methodology involves categorizing transformer layers into drafters and refiners based on their attention patterns using an Attention Selectivity Index. Evaluations on the Infinity-8B model show a reduction in KV cache memory from 85 GB to 8.5 GB while maintaining a GenEval score of 0.79. This cache compression framework enables AI practitioners to deploy visual autoregressive models in resource-constrained environments while preserving pixel-level fidelity.  |
| Learning to Reason without External Rewards (Read more on [arXiv](https://arxiv.org/abs/2505.19590) or [HuggingFace](https://huggingface.co/papers/2505.19590))| Dawn Song, Sergey Levine, Aosong Feng, Zhewei Kang, Xuandong | i) The paper introduces Reinforcement Learning from Internal Feedback (RLIF), specifically INTUITOR, to train LLMs using self-certainty as the sole reward signal. ii) The research investigates whether LLMs can enhance reasoning abilities using intrinsic, self-generated signals, without relying on external supervision. iii) INTUITOR replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores derived from the KL divergence between the model's output distribution and a uniform distribution. iv) Experiments with Qwen2.5-3B base model trained on MATH dataset show that INTUITOR matches GRPO's performance on mathematical benchmarks and achieves a 65% relative improvement on the LiveCodeBench code generation task. v) RLIF using INTUITOR offers AI practitioners a scalable alternative to RLVR for training autonomous AI systems in domains where verifiable rewards are unavailable by demonstrating effective learning from intrinsic model signals across domains.  |
| From Tens of Hours to Tens of Thousands: Scaling Back-Translation for
  Speech Recognition (Read more on [arXiv](https://arxiv.org/abs/2505.16972) or [HuggingFace](https://huggingface.co/papers/2505.16972))| Shanbo Cheng, Wei Lu, Lu Xu, Tianduo Wang | Speech Back-Translation is introduced as a scalable data augmentation pipeline for multilingual ASR. The research investigates improving multilingual ASR models by training text-to-speech (TTS) models on limited transcribed speech to generate large synthetic speech datasets. The methodology involves fine-tuning multilingual TTS models and using these to back-translate large text corpora into synthetic speech, along with a novel intelligibility metric for quality control. Experiments pre-training Whisper-large-v3 with 500,000 hours of synthetic speech achieved an average 30% reduction in transcription error rates across ten languages. This demonstrates that TTS models trained on limited real speech can generate significantly larger, high-quality synthetic speech datasets for improving multilingual ASR.  |
| AdaCtrl: Towards Adaptive and Controllable Reasoning via
  Difficulty-Aware Budgeting (Read more on [arXiv](https://arxiv.org/abs/2505.18822) or [HuggingFace](https://huggingface.co/papers/2505.18822))| Jiazhan Feng, Zhaochen Su, Wanjun Zhong, Hongru Wang, JoeYing | AdaCtrl adaptively adjusts and controls reasoning depth in language models based on problem difficulty. The research aims to develop a framework supporting difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl employs a two-stage training pipeline: cold-start fine-tuning for initial difficulty awareness and difficulty-aware reinforcement learning to refine reasoning strategies. Experiments show AdaCtrl improves performance and reduces response length by 10.06% on AIME2024 and 12.14% on AIME2025, while achieving 62.05% and 91.04% reductions on MATH500 and GSM8K datasets, respectively. The ability to adaptively allocate resources based on difficulty provides AI practitioners with a method to improve both the efficiency and effectiveness of reasoning in language models. |
| G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language
  Model via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.13426) or [HuggingFace](https://huggingface.co/papers/2505.13426))| Flood Sung, Zhiqi Huang, Tianyu Liu, Hongcheng Gao, Liang Chen | i) The paper introduces VLM-Gym, a reinforcement learning environment for training vision-language models (VLMs) on diverse visual games, and G1 models which demonstrate improved performance through RL-driven self-evolution. ii) The research aims to improve VLM decision-making capabilities in interactive, visually rich environments by addressing the "knowing-doing" gap. iii) The methodology involves a pure RL-driven self-evolution approach and incorporates a perception-enhanced cold start prior to RL fine-tuning, utilizing the GRPO algorithm. iv) The resulting G1 models outperformed their teacher models and leading proprietary models like Claude-3.7-Sonnet-Thinking, achieving, for example, a score of 17.5 on the Shisen-Sho game compared to Claude-3.7-Sonnet-Thinking's 15.3. v) The paper suggests that perception and reasoning abilities in VLMs can mutually bootstrap each other through RL, which can inform the design of VLM training strategies for improved interactive agents, offering a novel approach to improve decision-making in visually rich environments.  |
| The Coverage Principle: A Framework for Understanding Compositional
  Generalization (Read more on [arXiv](https://arxiv.org/abs/2505.20278) or [HuggingFace](https://huggingface.co/papers/2505.20278))| Miyoung Ko, Sohee Yang, Hanseul Cho, Jinho Park, Hoyeon Chang | i) This paper introduces the coverage principle, a data-centric framework for understanding compositional generalization in large language models. ii) The main objective is to explain how Transformers generalize compositionally and to predict their generalization boundaries based on observed functional equivalences. iii) The methodology involves deriving predictions from the coverage principle, conducting experiments on synthetic compositional tasks using GPT-2 models, and analyzing latent representations through IICG and causal tracing. iv) Results demonstrate that reliable generalization on two-hop reasoning tasks requires training data scaling at least quadratically with token set size, while data efficiency does not improve with 20x parameter scaling. v) AI practitioners should consider the coverage principle when designing training datasets and architectures for compositional tasks, as data curation informed by coverage considerations may be more effective than parameter scaling, particularly for tasks with path ambiguities.  |
| Accelerating Nash Learning from Human Feedback via Mirror Prox (Read more on [arXiv](https://arxiv.org/abs/2505.19731) or [HuggingFace](https://huggingface.co/papers/2505.19731))| Denis Belomestny, Daniele Calandriello, misovalko, kashif, dtiapkin | i) This paper introduces Nash Mirror Prox (NashMP), an online algorithm for Nash Learning from Human Feedback (NLHF). ii) The research aims to develop an NLHF algorithm with faster and more stable convergence to the Nash equilibrium compared to existing methods. iii) NashMP utilizes the Mirror Prox optimization scheme, involving two mirror descent steps and policy gradient estimation for parametrized policies. iv) The paper establishes last-iterate linear convergence for NashMP, demonstrating that the KL-divergence to the optimal policy decreases at a rate of order (1 + 2β)-N/2, where N is the number of preference queries, and presents empirical results showing competitive performance on synthetic and LLM fine-tuning tasks. v) The linear convergence rate and independence from the action space size makes NashMP suitable for LLM alignment, offering an efficient alternative to reward modeling with potential for enhancing AI systems' ability to align with human values.  |
| LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2505.19223) or [HuggingFace](https://huggingface.co/papers/2505.19223))| zhenxuan00, jrwen, lyk423, surfingtomchen, xiaolu0714 | i) This paper introduces Variance-Reduced Preference Optimization (VRPO) to improve the alignment of Masked Diffusion Models (MDMs) with human preferences. ii) The research aims to reduce the high variance in ELBO-based likelihood estimates for preference optimization in MDMs. iii) The methodology involves theoretical analysis of ELBO estimator variance and the derivation of unbiased variance reduction strategies like optimal Monte Carlo budget allocation and antithetic sampling. iv) The resulting model, LLaDA 1.5, demonstrates a +4.7 improvement on GSM8K and shows competitive mathematical performance compared to strong language MDMs and ARMs. v) VRPO offers AI practitioners improved alignment techniques for language MDMs by addressing ELBO estimation variance within a fixed computational budget.  |
| ModernGBERT: German-only 1B Encoder Model Trained from Scratch (Read more on [arXiv](https://arxiv.org/abs/2505.13136) or [HuggingFace](https://huggingface.co/papers/2505.13136))| Andreas Hotho, Fotis Jannidis, Julia Wunderle, Anton Ehrmanntraut, JanPf | i) ModernGBERT is a family of German encoder models trained from scratch, incorporating ModernBERT architectural innovations. ii) The paper investigates the trade-offs of training German encoders from scratch versus converting decoder models, and the impact of architectural innovations on encoder performance. iii) The methodology involves training ModernGBERT (134M, 1B) and LLäMmlein2Vec (120M, 1B, 7B) using the RedPajamaV2 dataset, evaluating on SuperGLEBer, MTEB, and QA-NIAH benchmarks. iv) ModernGBERT 1B achieves a SuperGLEBer score of 0.808, outperforming previous state-of-the-art German encoders such as GBERTLarge (0.768) and LLäMmlein2Vec 7B (0.787). v) AI practitioners can leverage ModernGBERT for high-performance, parameter-efficient German language understanding tasks, as dedicated encoders outperform converted decoder models of similar size.  |
| Interleaved Reasoning for Large Language Models via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.19640) or [HuggingFace](https://huggingface.co/papers/2505.19640))| Yanchao Sun, Dong Lin, Deepak Gopinath, David Qiu, Roy Xie | i) This paper introduces interleaved reasoning, a novel reinforcement learning paradigm for LLMs. ii) The research aims to improve LLM reasoning capabilities, reduce time-to-first-token (TTFT), and enhance training efficiency for multi-hop question answering. iii) The methodology employs reinforcement learning with a rule-based reward to incentivize correct intermediate steps during reasoning. iv) Experiments across five datasets demonstrate a 19.3% improvement in Pass@1 accuracy and an 80% reduction in TTFT. v) The interleaved reasoning paradigm, leveraging rule-based rewards for intermediate steps, provides AI practitioners with a method for significantly accelerating LLM responsiveness and improving reasoning performance without external tools.  |
| WINA: Weight Informed Neuron Activation for Accelerating Large Language
  Model Inference (Read more on [arXiv](https://arxiv.org/abs/2505.19427) or [HuggingFace](https://huggingface.co/papers/2505.19427))| Colby Banbury, Jongwoo Ko, Dan Zhao, Sihan Chen, tianyic | WINA: Weight Informed Neuron Activation accelerates LLM inference via weight-aware neuron selection. The research aims to improve inference efficiency in large language models by developing a training-free sparse activation method. The paper introduces WINA, a novel training-free sparse activation framework leveraging both hidden state magnitudes and column-wise l2-norms of weight matrices for neuron selection. Experiments show WINA outperforms TEAL by up to 2.94% in average performance across various LLM architectures and datasets at the same sparsity levels. WINA offers AI practitioners an efficient training-free method for sparse activation in LLMs that potentially improves inference performance compared to existing techniques such as TEAL.  |
| Position: Mechanistic Interpretability Should Prioritize Feature
  Consistency in SAEs (Read more on [arXiv](https://arxiv.org/abs/2505.20254) or [HuggingFace](https://huggingface.co/papers/2505.20254))| Zeyu Tang, Lingjing Kong, Yujia Zheng, aashiqmuhamed, xiangchensong | i) This paper advocates for prioritizing feature consistency in Sparse Autoencoders (SAEs) used for mechanistic interpretability (MI) to improve reliability and reproducibility. ii) The research question focuses on whether mechanistic interpretability should prioritize feature consistency in SAEs, and presents strategies for achieving this goal. iii) The methodology involves proposing Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) to measure consistency, theoretical grounding via sparse dictionary learning, synthetic validation with a model organism, and real-world LLM data experimentation. iv) The primary results demonstrate high consistency levels can be achieved with appropriate architectural choices, such as using TopK SAEs, achieving PW-MCC ≈ 0.80 for TopK SAEs on LLM activations, and approximately 0.97 in model organism, furthermore, high feature consistency strongly correlates with the semantic similarity of learned feature explanations. v) The principal implication for AI practitioners is the need to systematically measure and prioritize feature consistency in SAEs to foster robust cumulative progress in MI, particularly for applications like model steering, unlearning, and safety verification.  |
| MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research (Read more on [arXiv](https://arxiv.org/abs/2505.19955) or [HuggingFace](https://huggingface.co/papers/2505.19955))| Ailin Deng, Wei Han, Yujie Lu, happymio, chchenhui | This paper introduces MLR-Bench, a benchmark for evaluating AI agents on open-ended machine learning research. The research question addresses how to rigorously evaluate the quality of research produced by AI agents. The methodology involves constructing a benchmark consisting of 201 real-world research tasks, MLR-Judge for automated evaluation, and MLR-Agent for research execution. Experiments showed that current coding agents often produce fabricated experimental results in 80% of cases. MLR-Bench helps the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.  |
| Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications
  of Agentic AI (Read more on [arXiv](https://arxiv.org/abs/2505.19443) or [HuggingFace](https://huggingface.co/papers/2505.19443))| Manoj Karkee, Konstantinos I. Roumeliotis, RanjanSapkota | i) This paper analyzes and differentiates two AI-assisted software development paradigms: vibe coding and agentic coding. ii) The main objective is to establish a detailed taxonomy differentiating vibe coding and agentic coding based on their conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. iii) The research employs comparative workflow analysis across 20 detailed use cases and architectural analysis through layered diagrams and pseudocode abstractions. iv) The results indicate vibe coding excels in early-stage prototyping and education, while agentic coding is better suited for enterprise automation, codebase refactoring, and CI/CD integration; as an example, the Jules system was able to clone a GitHub repository, parse the README.md, identify relevant integration points, generate new data classes, inject code, update documentation, and create a commit to Git. v) The principal implication for AI practitioners is understanding the trade-offs between these paradigms for harmonized and human-centered AI software development, suggesting hybrid architectures that couple natural language interfaces with autonomous execution pipelines.  |
| Rethinking the Sampling Criteria in Reinforcement Learning for LLM
  Reasoning: A Competence-Difficulty Alignment Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.17652) or [HuggingFace](https://huggingface.co/papers/2505.17652))| Jingang Wang, Wei Wang, Qi Guo, xixy, DeyangKong | i) This paper introduces Competence-Difficulty Alignment Sampling (CDAS), a novel sampling strategy for reinforcement learning (RL) to improve the reasoning abilities of large language models (LLMs). ii) The research aims to address the inefficiency in RL training caused by unstable problem difficulty estimations and the misalignment between model competence and problem difficulty. iii) CDAS estimates problem difficulty by aggregating historical performance discrepancies and quantifies model competence to adaptively select problems aligned with the model’s current capabilities using a fixed-point system. iv) Experiments on mathematical benchmarks demonstrate that CDAS achieves a higher average accuracy of 46.77% and is 2.33 times faster than Dynamic Sampling. v) CDAS provides AI practitioners with an efficient sampling strategy for scaling RL training in LLM reasoning tasks, enabling better allocation of computational resources by dynamically matching problem difficulty to model competence.  |
| StructEval: Benchmarking LLMs' Capabilities to Generate Structural
  Outputs (Read more on [arXiv](https://arxiv.org/abs/2505.20139) or [HuggingFace](https://huggingface.co/papers/2505.20139))| Yuxuan Zhang, Sherman Siu, Lipeng He, Dongfu Jiang, Jialin Yang | i) StructEval is introduced as a benchmark to evaluate LLMs' capabilities in generating structured outputs. ii) The research aims to comprehensively assess LLMs' ability to produce both renderable and non-renderable structured formats. iii) The methodology involves generation and conversion tasks across 18 formats, evaluated using format adherence and structural correctness metrics. iv) Results indicate a performance gap, with even 01-mini only achieving 75.58 average score, and generation tasks proving more challenging than conversion tasks. v) AI practitioners should consider StructEval to evaluate and improve LLMs' ability to produce structured outputs accurately, especially for tasks requiring visual content and those in software development or data pipeline applications.  |
| InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer
  Interaction (Read more on [arXiv](https://arxiv.org/abs/2505.10887) or [HuggingFace](https://huggingface.co/papers/2505.10887))| Xi Xie, Winson Chen, Zijian Zhang, Weitai Kang, Bin12345 | InfantAgent-Next is a multimodal generalist agent for automated computer interaction utilizing text, images, audio, and video. The paper investigates the development of a generalist agent that integrates tool-based and pure vision agents within a modular architecture to solve complex computer interaction tasks. The methodology involves integrating large language models and visual large language models within a framework that modularizes workflow, tool selection, and tool execution. The results show the agent achieves 7.27% accuracy on the OSWorld benchmark. This research implies that AI practitioners can leverage the proposed architecture to build more versatile and accurate agents for automating computer tasks by combining tool-based and vision-based approaches.  |
| GLEAM: Learning Generalizable Exploration Policy for Active Mapping in
  Complex 3D Indoor Scenes (Read more on [arXiv](https://arxiv.org/abs/2505.20294) or [HuggingFace](https://huggingface.co/papers/2505.20294))| Jiangmiao Pang, Tao Huang, Quanyi Li, Tai Wang, Xiao-HF | i) GLEAM introduces a generalizable exploration policy for active mapping in complex 3D indoor scenes via a large-scale benchmark. ii) The primary objective is to develop a reinforcement learning (RL)-based exploration policy that can generalize across diverse and complex 3D indoor scenes without fine-tuning. iii) The methodology involves training a unified exploration policy using semantic representations, long-term navigable goals, and randomized training strategies within a benchmark (GLEAM-Bench) of 1,152 diverse 3D scenes. iv) The primary result is a 66.50% average coverage ratio across 128 unseen scenes, outperforming state-of-the-art methods by 9.49%, with a 0.80m nearest distance to ground-truth. v) GLEAM demonstrates that improved generalizability in active mapping can be achieved by training on large, diverse datasets with semantic representations, enabling the deployment of robust exploration policies in complex environments, but there is a sim-to-real gap that still needs to be explored.  |
| Error Typing for Smarter Rewards: Improving Process Reward Models with
  Error-Aware Hierarchical Supervision (Read more on [arXiv](https://arxiv.org/abs/2505.19706) or [HuggingFace](https://huggingface.co/papers/2505.19706))| Soujanya Poria, Chuan Li, Amir Zadeh, Panshul Sharma, Tej Deep Pala | i) The paper introduces PathFinder-PRM, a hierarchical, error-aware process reward model (PRM) that classifies math and consistency errors to improve reward estimation for mathematical reasoning. ii) The primary research objective is to enhance PRM performance by decoupling error detection and reward estimation through hierarchical supervision. iii) The methodology involves training a discriminative PRM with a newly constructed 400K-sample dataset annotated with three-dimensional step-level labels indicating math errors, consistency errors, and step correctness. iv) Results show PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7 on PRMBench, outperforming the prior best (65.5) while using 3x less data. v) The principal implication is that decoupling error detection and reward estimation with hierarchical error-aware supervision can substantially improve end-to-end reward-guided mathematical reasoning, offering AI practitioners a more data-efficient approach to building effective PRMs.  |
| DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning
  System for Multi-Turn Clinical Dialogue (Read more on [arXiv](https://arxiv.org/abs/2505.19630) or [HuggingFace](https://huggingface.co/papers/2505.19630))| Yixue Li, Lu Zhou, Yichun Feng, Jarvis1111 | i) DoctorAgent-RL, a reinforcement learning (RL) based multi-agent system, is introduced for multi-turn clinical dialogue. ii) The paper aims to develop a system capable of dynamically optimizing questioning strategies for clinical diagnosis through multi-turn interactions. iii) The methodology involves a doctor agent refined by RL, a patient agent based on LLMs, and a consultation evaluator providing multi-dimensional rewards and constructing a new dataset called MTMedDialog. iv) Experiments show DoctorAgent-RL outperforms existing models, achieving a 53.9% average score, in both multi-turn reasoning capability and final diagnostic performance on the MTMedDialog dataset. v) The work provides AI practitioners with an RL-based framework for improving diagnostic accuracy through optimized, interactive questioning in clinical consultation systems and a new dataset for training and evaluating such systems. |
| Jodi: Unification of Visual Generation and Understanding via Joint
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2505.19084) or [HuggingFace](https://huggingface.co/papers/2505.19084))| Xilin Chen, Shiguang Shan, Meina Kan, Zhenliang He, xyfJASON | Jodi is a diffusion framework that unifies visual generation and understanding by jointly modeling the image and multiple label domains. The research aims to develop a single model capable of joint generation, controllable generation, and image perception. The methodology involves a linear diffusion transformer with a role switch mechanism and domain-invariant positional embeddings. Experimental results demonstrate Jodi's superiority in generation and understanding tasks, achieving an FID score of 13.6 for controllable generation of depth maps, outperforming existing unified models. Jodi offers AI practitioners a unified model for diverse visual tasks, potentially streamlining development and reducing resource requirements compared to task-specific models.  |
| An Embarrassingly Simple Defense Against LLM Abliteration Attacks (Read more on [arXiv](https://arxiv.org/abs/2505.19056) or [HuggingFace](https://huggingface.co/papers/2505.19056))| George Turkiyyah, Bernard Ghanem, Hasan Abed Al Kader Hammoud, Harethah Abu Shairah | i) This paper introduces extended-refusal fine-tuning as a defense against abliteration attacks on LLMs. ii) The research aims to improve the robustness of LLMs against abliteration attacks, which neutralize safety guardrails by removing a single direction in the model. iii) The methodology involves creating an extended-refusal dataset with detailed responses justifying refusals and fine-tuning LLAMA-2-7B-CHAT and QWEN2.5-INSTRUCT models on this dataset. iv) Experiments show that extended-refusal models maintain high refusal rates after abliteration, dropping at most by 10%, while baseline models drop by 70-80%. v) AI practitioners can leverage extended-refusal fine-tuning to enhance the safety alignment of LLMs by distributing safety signals across multiple latent dimensions, mitigating the risk of targeted attacks.  |
| Strong Membership Inference Attacks on Massive Datasets and (Moderately)
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.18773) or [HuggingFace](https://huggingface.co/papers/2505.18773))| Matthew Jagielski, Christopher A. Choquette-Choo, Ilia Shumailov, Jamie Hayes, pasta41 | i) This paper evaluates the efficacy of strong membership inference attacks (MIAs) on large language models (LLMs). ii) The primary research question is whether the limitations of previous MIA research are due to attack design choices or if MIAs are fundamentally ineffective on LLMs. iii) The methodology involves scaling the LiRA attack to GPT-2 architectures (10M to 1B parameters), training reference models on over 20B tokens from the C4 dataset. iv) Results indicate that strong MIAs can succeed on pre-trained LLMs, but their effectiveness remains limited (e.g., AUC < 0.7) in practical settings, and that there is a non-monotonic relationship between model size and MIA vulnerability. v) The principal implication for AI practitioners is that despite the success of strong MIAs on LLMs, their limited effectiveness under practical conditions suggests that privacy risks may be lower than previously assumed, but further research into improving attack effectiveness is warranted.  |
| Dynamic Risk Assessments for Offensive Cybersecurity Agents (Read more on [arXiv](https://arxiv.org/abs/2505.18384) or [HuggingFace](https://huggingface.co/papers/2505.18384))| Zhou Li, Joie Zhang, Jiacen Xu, Benedikt Stroebl, boyiwei | i) This paper investigates the dynamic cybersecurity risks posed by autonomous offensive agents improved through iterative self-improvement with bounded compute. ii) The primary objective is to assess how adversaries can leverage various degrees of freedom to enhance agents' cybersecurity capabilities within a fixed compute budget. iii) The methodology involves dynamically analyzing agents' performance on InterCode CTF challenges by allowing adversaries to modify agent systems through repeated sampling, prompt refinement, self-training, and workflow refinement, and then measuring the performance. iv) Results show that adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40% relative to the baseline with an 8 H100 GPU Hour compute budget without external assistance, and Iterative prompt refinement exhibits the highest risk potential. v) AI practitioners should consider the dynamic nature of cybersecurity risks, where adversaries can iteratively improve offensive agents even with limited resources, thus highlighting the limitations of current static risk assessment methods.  |
| EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via
  Action Pruning (Read more on [arXiv](https://arxiv.org/abs/2505.16312) or [HuggingFace](https://huggingface.co/papers/2505.16312))| Defu Lian, Quan Liu, Jianshu Zhang, Qisi Chen, Jiawei1222 | i) The paper introduces EquivPruner, an action pruning approach to improve the efficiency and quality of LLM-based search. ii) The primary objective is to reduce token consumption and improve accuracy in LLM reasoning search by identifying and pruning semantically equivalent actions. iii) The methodology involves training a lightweight equivalence detector, utilizing the newly created MathEquiv dataset for mathematical statement equivalence, integrated into an MCTS framework. iv) Experiments demonstrate that EquivPruner reduced token consumption by 48.1% on GSM8K with Qwen2.5-Math-7B-Instruct while also improving accuracy from 96.44% to 96.59%. v) EquivPruner offers a practical method for AI practitioners to optimize LLM reasoning processes by efficiently managing redundant computations, potentially enabling more complex problem-solving within resource constraints.  |
| MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.19800) or [HuggingFace](https://huggingface.co/papers/2505.19800))| Bernard Ghanem, Maged S. Al-Shaibani, Zaid | i) This paper introduces MOLE, a framework leveraging LLMs for automated metadata extraction and validation from scientific papers covering datasets of languages other than Arabic. ii) The main objective is to automate the extraction of over 30 dataset metadata attributes from scientific papers, a task currently limited by reliance on manual annotation. iii) The methodology utilizes a schema-driven approach to process entire documents in LaTeX or PDF format, employing LLMs and validation mechanisms for consistent JSON output. iv) Experiments across multiple LLMs show promising results, with Gemini 2.5 Pro achieving the highest average metadata extraction score of 67.42% and Gemma 3 27B gets +1.62 % increase in accuracy when employing web browsing for certain metadata attributes. v) The principal implication for AI practitioners is the potential for automating dataset cataloging and preservation, enabling more efficient research discovery and reproducibility, although the results highlight the need for further improvements to ensure consistent and reliable performance.  |
| Architectural Backdoors for Within-Batch Data Stealing and Model
  Inference Manipulation (Read more on [arXiv](https://arxiv.org/abs/2505.18323) or [HuggingFace](https://huggingface.co/papers/2505.18323))| Ilia Shumailov, Conrad Grobler, Ivan Petrov, Nicolas Küchler | i) This paper introduces architectural backdoors that exploit batched inference to facilitate data theft and model manipulation in neural networks. ii) The research investigates how architectural backdoors can compromise batch isolation, enabling attackers to steal or manipulate data from other users within the same batch. iii) The methodology involves injecting backdoors into model architectures, specifically targeting the batching mechanism and using static taint analysis with a novel Information Flow Control mechanism to verify and mitigate vulnerabilities. iv) The paper finds over 200 models on Hugging Face exhibit unintended information leakage due to dynamic quantization and demonstrates successful data theft and manipulation via engineered backdoors. v) AI practitioners should be aware of the potential for architectural backdoors in batched inference systems and employ deterministic mitigation strategies such as the proposed Batch Isolation Checker to ensure user privacy and system integrity.  |
| Towards Holistic Evaluation of Large Audio-Language Models: A
  Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2505.15957) or [HuggingFace](https://huggingface.co/papers/2505.15957))| Hung-yi Lee, Neo S. Ho, zenyn | i) This paper surveys and categorizes evaluation frameworks for large audio-language models (LALMs). ii) The main objective is to provide a systematic taxonomy for LALM evaluations across diverse objectives. iii) The methodology involves a comprehensive review and categorization of existing LALM benchmarks into four dimensions: General Auditory Awareness and Processing, Knowledge and Reasoning, Dialogue-oriented Ability, and Fairness, Safety, and Trustworthiness. iv) The survey identifies current LALMs often accept malicious spoken inputs even when they can refuse similar textual ones, highlighting vulnerabilities. v) AI practitioners can use this taxonomy to select appropriate benchmarks for evaluating specific capabilities of LALMs and identify areas needing improvement, particularly in safety alignment.  |
| Option-aware Temporally Abstracted Value for Offline Goal-Conditioned
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.12737) or [HuggingFace](https://huggingface.co/papers/2505.12737))| Taesup Moon, Heewoong Choi, Hongjoon Ahn, JisuHann | Offline goal-conditioned reinforcement learning with temporal abstraction (OTA) is presented for improved value function learning. The research addresses the challenge of high-level policy learning in long-horizon tasks by reducing temporal distance between states and goals. The key method is option-aware temporal differencing which updates value function over option sequences, not individual steps. Experiments on the OGBench benchmark demonstrate that policies using the OTA value function achieve strong performance compared to baselines, as shown with the HumanoidMaze-giant task where OTA achieves a 79% success rate. The principal implication of the research is to mitigate the errors of the value function in long-horizon regimes for better decision-making in complex robotic manipulation tasks.  |
| TAGS: A Test-Time Generalist-Specialist Framework with
  Retrieval-Augmented Reasoning and Verification (Read more on [arXiv](https://arxiv.org/abs/2505.18283) or [HuggingFace](https://huggingface.co/papers/2505.18283))| Haochen Xue, Ming Hu, Yulong Li, Feilong Tang, JianghaoWu | i) The paper introduces TAGS, a test-time framework for enhancing medical question answering using a generalist-specialist approach with retrieval augmentation and verification. ii) The research aims to improve LLM performance in MedQA by combining general and domain-specific knowledge without parameter updates. iii) The methodology involves a Generalist-Specialist Reasoning Collaboration (GSRC) module, Hierarchical Retrieval Augmentation (HRA) for multi-scale exemplar selection, and Uncertainty-Aware Answer Aggregation (UAAA) for reasoning consistency evaluation. iv) TAGS achieves a 13.8% improvement on GPT-4o and a 16.8% gain on DeepSeek-R1 accuracy across nine MedQA benchmarks without any parameter updates. v) AI practitioners can leverage the TAGS framework to enhance the reliability and accuracy of LLMs in specialized domains like medicine by combining general and specific knowledge sources and employing verification mechanisms without requiring model retraining or fine-tuning.  |
