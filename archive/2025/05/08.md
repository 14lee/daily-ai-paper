

## Papers for 2025-05-08

| Title | Authors | Summary |
|-------|---------|---------|
| Unified Multimodal Understanding and Generation Models: Advances,
  Challenges, and Opportunities (Read more on [arXiv](https://arxiv.org/abs/2505.02567) or [HuggingFace](https://huggingface.co/papers/2505.02567))| Minghao Fu, Jintao Guo, Xinjie Zhang, Flourish, Suikong | This paper surveys recent advancements in unified multimodal models integrating vision-language understanding and generation. The objective is to provide a comprehensive overview of current efforts to unify disparate architectural paradigms for multimodal understanding (often autoregressive) and generation (often diffusion-based). The paper reviews and categorizes existing unified models based on their core architecture (diffusion-based, autoregressive-based, or hybrid) and image tokenization strategies (e.g., pixel, semantic, learnable query), also compiling relevant datasets and benchmarks. The survey highlights a rapid growth, identifying over 40 distinct unified models emerging between 2023 and early 2025 (Fig. 1), with varied approaches such as Emu2 (LLaMA backbone, EVA-CLIP encoder, SDXL decoder) and Janus-Pro (DeepSeek-LLM backbone, SigLIP + VQGAN encoders). AI practitioners receive a structured guide to the diverse architectures (e.g., autoregressive MLLMs using semantic encoders like CLIP paired with diffusion decoders), key datasets (e.g., LAION 5.9B image-text pairs), and benchmarks, aiding the development and evaluation of sophisticated unified multimodal systems. |
| ZeroSearch: Incentivize the Search Capability of LLMs without Searching (Read more on [arXiv](https://arxiv.org/abs/2505.04588) or [HuggingFace](https://huggingface.co/papers/2505.04588))| Yingyan Hou, Xuanbo Fan, Zile Qiao, Hao Sun, SpaceProduct | ZEROSEARCH is a reinforcement learning framework that enhances LLM search capabilities by fine-tuning an LLM to simulate a search engine, thus avoiding real search engine interactions and associated API costs. Its objective is to improve LLMs' search and reasoning without the high costs and document quality unpredictability of live search engine interactions. The core methodology involves supervised fine-tuning of a "simulation LLM" to generate controlled-quality documents (relevant or noisy) for queries, coupled with a curriculum learning strategy that progressively increases retrieval difficulty during RL training, and a loss masking mechanism for retrieved tokens. ZEROSEARCH consistently outperforms real search engine-based methods, with a 14B parameter simulation LLM achieving an average Exact Match score of 33.97 across several question answering datasets, surpassing Google Search which scored 32.47, while demonstrating stable learning and generalizability. This offers AI practitioners a cost-effective and stable approach to develop LLMs with strong search and reasoning skills by simulating search environments, reducing reliance on expensive APIs and improving control over training data quality. |
| PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with
  Auto-Regressive Transformer (Read more on [arXiv](https://arxiv.org/abs/2505.04622) or [HuggingFace](https://huggingface.co/papers/2505.04622))| Yiqin Zhu, Yanning Zhou, Jingwen Ye, loktarxiao, hyz317 | PrimitiveAnything introduces a novel framework for generating 3D primitive assemblies by learning from human-crafted abstractions using an auto-regressive transformer. The main objective is to enable the generation of high-quality primitive assemblies that align with human perception and maintain geometric fidelity across diverse 3D shape categories, by reformulating shape abstraction as a sequence generation task. The methodology involves an ambiguity-free parameterization scheme for multiple primitive types, a shape-conditioned decoder-only transformer for auto-regressive primitive generation, and a cascaded primitive decoder to model attribute dependencies, trained on a large-scale dataset of human-crafted abstractions. Primary results demonstrate superior performance, achieving a Voxel-IoU of 0.484 on the HumanPrim test set, significantly outperforming optimization-based methods like EMS (0.259) and MP (0.201). The principal implication for AI practitioners is a method to create semantically structured and editable 3D content that is lightweight and aligns with human cognitive processes, useful for applications requiring efficient and interpretable 3D representations, such as user-generated content in games or computer-aided design. |
| HunyuanCustom: A Multimodal-Driven Architecture for Customized Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2505.04512) or [HuggingFace](https://huggingface.co/papers/2505.04512))| Yuan Zhou, Sen Liang, Zhengguang Zhou, Zhentao Yu, Teng Hu | HunyuanCustom is a novel multimodal-driven architecture for customized video generation that prioritizes subject consistency across image, audio, video, and text inputs. The primary objective is to enable flexible, user-defined video generation featuring specific subjects with robust identity preservation and multi-modal controllability. The framework, built on HunyuanVideo, incorporates a LLaVA-based text-image fusion module, an image ID enhancement module using temporal concatenation, and distinct injection mechanisms including an AudioNet for audio and a patchify-based feature-alignment network for video conditioning. HunyuanCustom significantly outperforms existing methods, achieving a Face-Sim score of 0.627 for ID consistency, surpassing competitors in single- and multi-subject scenarios. This work offers AI practitioners a robust approach for developing highly controllable, identity-preserving video generation systems, with direct applications in areas requiring precise subject customization like virtual human creation and fine-grained video editing. |
| Beyond Recognition: Evaluating Visual Perspective Taking in Vision
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.03821) or [HuggingFace](https://huggingface.co/papers/2505.03821))| Maciej Wołczyk, Michał Nauman, Piotr Miłoś, Alicja Ziarko, Gracjan | This research evaluates Vision Language Models' (VLMs) visual perspective taking (VPT) capabilities, revealing strong scene understanding but significant deficiencies in spatial reasoning and perspective-taking. The primary objective is to investigate the ability of state-of-the-art VLMs to perform visual perspective taking by assessing three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. The study employed 144 unique visual tasks featuring a humanoid minifigure and an object in systematically varied spatial configurations and viewpoints, with each task accompanied by 7 open-ended diagnostic questions; model responses were evaluated using a precision-based correctness metric. While VLMs excelled in scene understanding (e.g., GPT-4o achieved 100.0% correctness), their performance significantly declined for spatial reasoning (e.g., GPT-4o at 72.9% for minifigure orientation) and further deteriorated for visual perspective taking (e.g., GPT-4o at 59.0% for determining object location from the minifigure's viewpoint). The principal implication for AI practitioners is that current VLMs lack robust internal geometric and perspective-dependent spatial reasoning, indicating a need for future VLM development to integrate explicit geometric representations and tailored training protocols beyond surface-level object recognition for reliable application in complex, interactive domains. |
| Benchmarking LLMs' Swarm intelligence (Read more on [arXiv](https://arxiv.org/abs/2505.04364) or [HuggingFace](https://huggingface.co/papers/2505.04364))| Hao Sun, Ji-Rong Wen, Mowen Huang, 6cf | This paper introduces SwarmBench, a benchmark for evaluating the emergent swarm intelligence of Large Language Models (LLMs) operating as decentralized agents under strict local perception and communication constraints. The research aims to systematically assess whether LLMs can exhibit effective coordination and collective intelligence akin to natural swarms when faced with limited local information, by evaluating their performance on five multi-agent tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid world. The methodology involves LLM-driven agents operating with a k × k local view (e.g., 5x5 in main experiments) and optional local messaging, evaluated in a zero-shot setting using metrics for task success and emergent group dynamics. Evaluations of thirteen LLMs revealed significant performance variability, with emergent physical group dynamics, such as behavioral variability (`std_action_entropy` correlating with score at r = 0.300), explaining approximately 24.5% of task score variance, while explicit communication characteristics showed a weaker influence. For AI practitioners, this implies that when designing LLM-based multi-agent systems under severe decentralization, focusing on enhancing emergent physical coordination strategies may yield more significant performance gains than solely refining explicit communication protocols, as current LLMs struggle with robust planning under such constraints. |
| Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal
  Problem-Solving (Read more on [arXiv](https://arxiv.org/abs/2505.04528) or [HuggingFace](https://huggingface.co/papers/2505.04528))| Qinxiang Cao, Xingzhi Qi, Renqiu Xia, Xinhao Zheng, purewhite42 | This paper presents a principled formulation of problem-solving as a deterministic MDP, introduces FPS and D-FPS frameworks for process-verified solving in FTP environments, and new benchmarks with the RPE evaluation metric. The research aims to establish a rigorous and verifiable approach to formal problem-solving beyond traditional theorem proving, enabling AI agents to produce process-level auditable solutions. Key methodologies include defining problem-solving as a deterministic Markov Decision Process, implementing the Formal Problem-Solving (FPS) and Deductive FPS (D-FPS) frameworks in Lean 4, constructing three novel benchmarks (FormalMath500, MiniF2F-Solving, PutnamBench-Solving), and proposing Restricted Propositional Equivalence (RPE) for answer correctness evaluation. Primary results show that SOTA FTP models using FPS solved at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving according to RPE, while D-FPS, though achieving lower solving rates, yielded nearly zero incorrectly submitted answers. For AI practitioners, these frameworks and benchmarks provide essential tools for developing and evaluating AI systems capable of verifiable, step-by-step formal reasoning, critical for applications requiring high trustworthiness and auditable solution processes. |
| OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue
  Resolution (Read more on [arXiv](https://arxiv.org/abs/2505.04606) or [HuggingFace](https://huggingface.co/papers/2505.04606))| Jiachi Chen, Yanlin Wang, Runhan Jiang, Lianghong Guo, itaowe | The paper introduces OmniGIRL, a novel multilingual, multimodal, and multi-domain benchmark for GitHub issue resolution. The primary objective is to create a comprehensive benchmark to evaluate the capabilities of Large Language Models (LLMs) in resolving diverse, real-world GitHub issues, addressing limitations of existing benchmarks regarding language, domain, and input modality. OmniGIRL was constructed by collecting 959 task instances from 15 popular repositories across four programming languages (Python, JavaScript, TypeScript, Java) and eight domains, including issues with textual, image, and website link information, followed by execution-based verification. Evaluations show current LLMs have limited performance on OmniGIRL; notably, the best-performing model, GPT-40 with the Agentless-X method, resolved only 8.6% of the total issues, and for issues requiring image understanding, Claude-3.5-Sonnet resolved only 10.5% using an oracle retrieval method with image-augmented text. AI practitioners should be aware that current LLMs significantly struggle with complex, multilingual, and multimodal software engineering tasks like GitHub issue resolution, indicating a substantial need for improved model capabilities and methods to handle cross-file and multimodal contexts effectively. |
| OpenHelix: A Short Survey, Empirical Analysis, and Open-Source
  Dual-System VLA Model for Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2505.03912) or [HuggingFace](https://huggingface.co/papers/2505.03912))| Xinyang Tong, Shuanghao Bai, Wenxuan Song, Pengxiang Ding, Can Cui | This paper presents OpenHelix, an open-source dual-system Vision-Language-Action (VLA) model for robotic manipulation, alongside a survey and empirical analysis of dual-system VLA design choices. Its main objective is to systematically evaluate core design elements of dual-system VLA architectures, such as MLLM training, policy training, and integration strategies, and to propose an effective, low-cost open-source model based on these findings. The study employs empirical evaluations on the CALVIN benchmark, varying MLLM training (frozen, fine-tuning, prompt-tuning), policy training (from scratch, fine-tuning pre-trained), and integration strategies (projector pre-alignment, auxiliary tasks), leading to the OpenHelix model which uses prompt-tuned LLaVA-7B and a pre-trained 3D Diffuser Actor policy with an auxiliary multimodal reasoning task. Key results demonstrate that MLLM prompt tuning with an auxiliary task significantly improves performance, with the proposed configuration achieving a 4.01 average task completion length on CALVIN (Table 7) and 46.9% 5-task completion success on CALVIN ABC-D with 60-step asynchronous inference (Table 8). For AI practitioners, this implies that prompt-tuning large MLLMs with auxiliary tasks for enhanced visual reasoning, coupled with careful pre-alignment of dual-system components, is a highly effective strategy for robotic VLA development, and that asynchronous inference between systems often has minimal impact on overall performance. |
| OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents (Read more on [arXiv](https://arxiv.org/abs/2505.03570) or [HuggingFace](https://huggingface.co/papers/2505.03570))| Sinéad Ryan, Arturo Márquez Flores, Patrick Barker, Daniel Jeffries, mariya-davydova | The paper introduces OSUniverse, a benchmark for evaluating multimodal GUI-navigation AI agents on complex desktop tasks with automated validation. The main objective is to provide a robust, extensible benchmark with increasing task complexity to measure the capabilities of GUI-navigation AI agents and to assess current state-of-the-art (SOTA) performance. The methodology involves defining tasks in YAML, running them in Dockerized desktop environments (AgentDesk) using a SurfKit-compatible runtime, and employing automated validation with Gemini models for scoring, supplemented by a human review interface. Primary results show that SOTA agents (at publication) achieve less than 50% accuracy, with the top agent (`computer-use-preview-2025-03-11`) scoring 47.80%; the automated validation mechanism exhibits an average error rate below 2% (1.64% with Gemini 2.0 Flash). The principal implication for AI practitioners is that OSUniverse provides a challenging and calibrated benchmark with automated, non-deterministic validation to assess GUI-navigation agents, highlighting that even top proprietary models require custom agentic code and specialized training for optimal performance, with open-weight models lagging. |
| Knowledge Augmented Complex Problem Solving with Large Language Models:
  A Survey (Read more on [arXiv](https://arxiv.org/abs/2505.03418) or [HuggingFace](https://huggingface.co/papers/2505.03418))| Yuqi Zhu, Yuchen Tian, Junwei Su, Lun Du, Da Zheng | This survey examines the capabilities and limitations of Large Language Models (LLMs) in complex problem-solving, focusing on multi-step reasoning, knowledge augmentation, and result verification across various domains. The paper aims to provide a comprehensive overview of current LLM techniques for tackling complex problems, highlight challenges such as data scarcity and computational costs, and discuss future research directions. The survey analyzes methodologies including Chain-of-Thought (CoT) reasoning for multi-step problem decomposition, knowledge augmentation via retrieval-augmented generation (RAG) and knowledge graphs, and various result verification techniques such as LLM-based verifiers and tool-assisted validation. Key findings highlighted include the inference scaling law, where solution coverage can grow nearly log-linearly with the number of sampled reasoning paths [10], and that training dedicated verifier models significantly improves solve rates on tasks like GSM8K math problems compared to only fine-tuning the generator LLM [20]. For AI practitioners, this implies that LLM problem-solving can be substantially enhanced by systematically integrating structured reasoning processes, incorporating external knowledge sources, and employing robust verification loops, while also needing to address the high computational demands of extensive search and reasoning. |
| R&B: Domain Regrouping and Data Mixture Balancing for Efficient
  Foundation Model Training (Read more on [arXiv](https://arxiv.org/abs/2505.00358) or [HuggingFace](https://huggingface.co/papers/2505.00358))| Ziyi Chu, Avi Trost, John Cooper, Tzu-Heng Huang, Albert Ge | R&B is a two-stage framework that improves foundation model training efficiency by first re-partitioning data based on semantic similarity (Regroup) and then dynamically optimizing data mixture proportions using domain gradients (Balance). The paper addresses how to overcome the limitations of predetermined data domains and the computational inefficiency of existing data mixing methods in foundation model training. R&B employs semantic clustering (e.g., k-means on embeddings) for data regrouping and leverages a Gram matrix of domain gradients, updated during training, to dynamically reweight skill mixtures via a regularized softmax optimization. Empirically, R&B matches or exceeds state-of-the-art data mixing performance while significantly reducing computational overhead, requiring as little as 0.01% additional compute; for instance, on SUP-NATINST, R&B achieved a loss of 2.381 with 0.009% overhead. AI practitioners can significantly reduce computational costs and potentially improve performance in foundation model training by adopting semantic data regrouping and gradient-based dynamic mixture balancing, avoiding expensive per-skill evaluations. |
| Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly
  Detection (Read more on [arXiv](https://arxiv.org/abs/2505.02393) or [HuggingFace](https://huggingface.co/papers/2505.02393))| Mohsen Imani, Paper9795, Eavn | This paper introduces IEF-VAD, a framework that synthesizes event representations from RGB videos and fuses them with image features using an uncertainty-aware process, aiming to enhance video anomaly detection by integrating temporal cues from synthetic event data with spatial RGB information. The key methodology involves extracting image and synthetic event features via CLIP, modeling sensor noise with a Student's-t likelihood, and deriving inverse-variance weights through Laplace approximation for fusion. Furthermore, IEF-VAD employs Kalman-style sequential updates and an iterative refinement network to denoise the fused latent state before classification using a composite loss function including KL divergence and modality alignment terms. IEF-VAD achieved state-of-the-art results, such as an AUC of 88.67% on UCF-Crime and 92.90% on MSAD (Student's-t model), with masking experiments confirming adaptive uncertainty weighting. For AI practitioners, this work shows that fusing synthetic event data with RGB data via principled uncertainty estimation (e.g., Student's-t noise model, inverse-variance weighting) can significantly improve video anomaly detection by capturing motion cues without dedicated event sensors, offering a practical enhancement for video understanding systems. |
| Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI
  Knowledge Co-Creation (Read more on [arXiv](https://arxiv.org/abs/2505.03105) or [HuggingFace](https://huggingface.co/papers/2505.03105))| linxule | This paper introduces Cognitio Emergens (CE), a comprehensive theoretical framework for understanding and guiding the co-evolutionary nature of human-AI partnerships in scientific knowledge co-creation. The primary objective is to propose the CE framework to address limitations in existing models by capturing the dynamic, emergent, and co-evolutionary processes through which scientific understanding is co-created. The methodology is primarily theoretical, synthesizing theories like autopoiesis and social systems theory to define CE through three core components: Agency Configurations, Epistemic Dimensions, and Partnership Dynamics. The primary result is the CE framework itself, detailing three Agency Configurations, six Epistemic Dimensions (e.g., Divergent Intelligence, Synthesis Intelligence) forming "capability signatures" (Section 3.2.4) for diagnostic purposes, and six Partnership Dynamics; the paper, being a framework proposal, does not present empirical quantitative findings. For AI practitioners, CE offers tools to design AI systems as evolving epistemic partners, focusing on dynamic agency and specific collaborative capabilities rather than solely on narrow performance metrics. |
