

## Papers for 2025-05-16

| Title | Authors | Summary |
|-------|---------|---------|
| Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.10554) or [HuggingFace](https://huggingface.co/papers/2505.10554))| cxiong, amritasaha87, yuhuixu, hendrydong, zhiyuanhucs | Large reasoning models are aligned with deduction, induction, and abduction meta-abilities to enhance reasoning capabilities. The research aims to improve the scalability and reliability of large reasoning models (LRMs) by explicitly aligning them with meta-abilities rather than relying on emergent behaviors. The methodology involves a three-stage pipeline: individual alignment with meta-abilities using automatically generated tasks, parameter-space merging, and domain-specific reinforcement learning. The proposed method boosts performance by over 10% compared to instruction-tuned baselines and achieves an additional 2% average gain through domain-specific RL. Explicit meta-ability alignment offers a scalable foundation for reasoning in large models. The paper seems to lack comparative result or reference on how many parameters were used in the baseline model.  |
| System Prompt Optimization with Meta-Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09666) or [HuggingFace](https://huggingface.co/papers/2505.09666))| Sung Ju Hwang, jinheon, YuminChoi | i) This paper introduces a meta-learning framework, MetaSPO, for optimizing task-agnostic system prompts to improve Large Language Model (LLM) performance across diverse user prompts and unseen tasks. ii) The research objective is to develop a bilevel system prompt optimization method that designs system prompts robust to varying user prompts and transferable to new tasks. iii) MetaSPO uses a meta-learning approach to optimize system prompts over multiple datasets, iteratively updating user prompts to enhance synergy. iv) Experiments across 14 unseen datasets in 5 domains demonstrated that MetaSPO produces generalizable system prompts and facilitates rapid adaptation, achieving performance improvements and reducing optimization steps. v) MetaSPO offers AI practitioners a method to enhance LLM generalization and adaptation by automating the design of robust system prompts.  |
| EnerVerse-AC: Envisioning Embodied Environments with Action Condition (Read more on [arXiv](https://arxiv.org/abs/2505.09723) or [HuggingFace](https://huggingface.co/papers/2505.09723))| hsli-cuhk, pathcn, thuhsy, Shengcong, YuxinJiang | ENERVERSE-AC is an action-conditional world model for generating future visual observations in robotic manipulation. The research aims to create a realistic and controllable robotic inference environment by conditioning video generation on predicted agent actions. The methodology introduces a multi-level action-conditioning mechanism, ray map encoding for dynamic multi-view image generation, and failure trajectory expansion. Experiments showed that using the generated data for data augmentation improved policy training success rate from 0.28 to 0.36. This work reduces the cost of robotic manipulation testing by providing an alternative to physical robots or complex simulations for AI practitioners involved in robotics and imitation learning.  |
| The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think (Read more on [arXiv](https://arxiv.org/abs/2505.10185) or [HuggingFace](https://huggingface.co/papers/2505.10185))| hbin0701, dreamgonfly, Minju2136, seungone, Seongyun | i) The paper introduces the CoT ENCYCLOPEDIA, a framework for analyzing, predicting, and controlling reasoning strategies in large language models (LLMs) using chain-of-thought (CoT) prompting. ii) The primary objective is to provide a bottom-up methodology to identify and interpret diverse reasoning strategies employed by LLMs, circumventing limitations of predefined strategy types. iii) The method extracts reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. iv) The framework achieves a 92–97% perceived reasonableness score in human evaluations and improves performance by 2.5–8.3% on various benchmarks by guiding models towards more effective reasoning strategies; it also reveals that training data format (free-form vs. multiple choice) has a greater impact than data domain on reasoning behaviors. v) The principal implication for AI practitioners is the provision of a diagnostic and practical tool for shaping reasoning behaviors in LLMs, particularly by selecting appropriate training data formats and enabling controllability through model merging.  |
| EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied
  World Models (Read more on [arXiv](https://arxiv.org/abs/2505.09694) or [HuggingFace](https://huggingface.co/papers/2505.09694))| sundrops, AutobotZero, pathcn, Shengcong, thuhsy | EWMBench is introduced as a benchmark for evaluating embodied world models (EWMs). The research addresses the challenge of evaluating EWMs beyond general perceptual metrics, focusing on physically grounded and action-consistent behaviors. The methodology involves a curated dataset with diverse scenes/motion patterns and a multi-dimensional evaluation toolkit assessing visual scene consistency, motion correctness, and semantic alignment. The evaluation metrics exposed that current video generation models have limitations when used for embodied tasks. The benchmark and evaluation tools are publicly available to guide future development in the field, although quantitative results are not explicitly stated in the provided abstract.  |
| End-to-End Vision Tokenizer Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.10562) or [HuggingFace](https://huggingface.co/papers/2505.10562))| RobertLuo1, Paranioar, YufengCui, ryanzhangfan, gilnore | i) The paper introduces ETT, an end-to-end approach for tuning vision tokenizers jointly with downstream autoregressive tasks. ii) The main research question is whether jointly optimizing vision tokenization and target autoregressive tasks improves performance compared to using frozen vision tokenizers. iii) The methodology involves leveraging the visual embeddings of the tokenizer codebook and optimizing the vision tokenizer with both reconstruction and caption objectives, without adjusting the LLM codebook or architecture. iv) The primary results demonstrate performance gains of 2-6% on multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving reconstruction capability. v) ETT provides AI practitioners a simple method for improving multimodal foundation models by end-to-end vision tokenizer tuning, enhancing performance in image generation and understanding tasks without significantly altering existing LLM architectures.  |
| MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine
  Learning Engineering (Read more on [arXiv](https://arxiv.org/abs/2505.07782) or [HuggingFace](https://huggingface.co/papers/2505.07782))| percyliang, Solute, yinghaoli-yh, yczhuang, Jerrycool | i) MLE-Dojo is introduced as a Gym-style framework for reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in machine learning engineering (MLE) workflows. ii) The research aims to provide an interactive environment for iterative experimentation, debugging, and refinement of LLM agent solutions in real-world MLE tasks. iii) The methodology involves building a fully executable environment upon 200+ real-world Kaggle challenges, facilitating comprehensive agent training via supervised fine-tuning and reinforcement learning. iv) Evaluations of eight frontier LLMs show iterative improvements, but also reveal limitations in autonomously generating long-horizon solutions and resolving complex errors, with HumanRank scores and Elo rankings presented as evaluation metrics. v) MLE-Dojo provides AI practitioners with a benchmark to tune model-based agents through diverse data sources, tools, and evaluation protocols to improve interoperability, scalability, and reproducibility.  |
| WorldPM: Scaling Human Preference Modeling (Read more on [arXiv](https://arxiv.org/abs/2505.10527) or [HuggingFace](https://huggingface.co/papers/2505.10527))| Zhenru Zhang, Le Yu, Keming Lu, Runji Lin, Binghai Wang | i) This paper introduces World Preference Modeling (WorldPM) for scaling human preference models. ii) The primary objective is to investigate scaling laws in preference modeling using large datasets and models. iii) The research methodology involves training language models (1.5B to 72B parameters) on a 15M-sample dataset of human preferences gathered from online forums. iv) Results indicate that adversarial metrics scale with increased data and model size, objective metrics show emergent behavior in larger models, and integrating WorldPM into RLHF pipelines improved evaluations by 4% to 8% in in-house evaluations. v) WorldPM offers AI practitioners a foundation for improving the generalization performance of preference fine-tuning across various datasets, especially with limited data.  |
| Achieving Tokenizer Flexibility in Language Models through Heuristic
  Adaptation and Supertoken Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09738) or [HuggingFace](https://huggingface.co/papers/2505.09738))| Vinayak Pahalwan, Shaurya Sharthak, adarshxs, adi-kmt | i) This paper introduces TokenAdapt, a framework for adapting language models to new tokenizers using a hybrid heuristic initialization and explores supertoken learning. ii) The main objective is to mitigate tokenizer lock-in issues in LLMs by facilitating efficient tokenizer transplantation without substantial retraining. iii) The methodology involves a hybrid heuristic combining local subword decomposition and global semantic similarity, alongside pre-tokenization learning of multi-word supertokens. iv) Empirical results demonstrate that TokenAdapt consistently yields lower perplexity ratios compared to ReTok and TransTokenizer baselines, achieving up to approximately a 2-fold improvement in aggregate perplexity scores. v) TokenAdapt offers AI practitioners a practical method for adapting LLMs to specialized domains or languages by minimizing retraining costs and improving tokenization efficiency.  |
| Style Customization of Text-to-Vector Generation with Image Diffusion
  Priors (Read more on [arXiv](https://arxiv.org/abs/2505.10558) or [HuggingFace](https://huggingface.co/papers/2505.10558))| Jing Liao, CHERRY-Z, intchous | i) This paper introduces a two-stage pipeline for style-customizable text-to-vector graphic (SVG) generation leveraging diffusion models. ii) The research aims to enable style customization in text-to-vector generation while preserving structural regularity in the resulting SVGs. iii) The methodology involves training a path-level T2V diffusion model followed by style distillation from customized image diffusion models. iv) The model achieved a Path FID of 37.51, indicating improved structural regularity, and user studies showed preference for the method's visual quality at 53.2%. v) AI practitioners can use this model to generate stylized vector graphics from text prompts, enabling efficient content creation with consistent visual aesthetics.  |
| J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.10320) or [HuggingFace](https://huggingface.co/papers/2505.10320))| Xian Li, Ping Yu, Tianlu Wang, Chenxi Whitehouse, swarna92 | i) The paper introduces J1, a reinforcement learning (RL) method for training LLM-as-a-Judge models to improve reasoning and judgment capabilities. ii) The research objective is to develop a method that incentivizes thinking and mitigates judgment bias in LLMs used for evaluation tasks. iii) The methodology involves converting both verifiable and non-verifiable prompts into judgment tasks with verifiable rewards and using Group Relative Policy Optimization (GRPO) for training. iv) Results show that the J1 method outperforms existing 8B and 70B models on several benchmarks, including PPE, with J1-Llama-70B achieving an overall accuracy of 69.6, and even surpasses R1 on certain non-verifiable tasks. v) The main implication for AI practitioners is the provision of an effective RL approach for creating generalist judge models capable of evaluating diverse LLM responses, which can be used for improving all stages of LLM development.  |
| PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing (Read more on [arXiv](https://arxiv.org/abs/2505.09990) or [HuggingFace](https://huggingface.co/papers/2505.09990))| Boyang Li, Haoquan Fang, Yi Ru Wang, Jiafei Duan, Long Cheng | PointArena is introduced as a comprehensive platform for evaluating multimodal pointing capabilities in AI systems. The research aims to provide a benchmark for assessing how well multimodal models can ground language within visual contexts using pointing. The methodology involves a curated dataset (Point-Bench) with approximately 1,000 pointing tasks, an interactive arena (Point-Battle) for pairwise model comparisons, and a real-world robotic manipulation system (Point-Act). The results showed Molmo-72B consistently outperformed other models and supervised training significantly enhances model performance; Point-Battle has gathered over 4,500 anonymized votes. The implication for AI practitioners is the critical role of precise pointing capabilities in bridging abstract reasoning with concrete actions, providing a benchmark to improve and evaluate multimodal models for robotics, assistive technology, and interactive AI systems.  |
| Depth Anything with Any Prior (Read more on [arXiv](https://arxiv.org/abs/2505.10565) or [HuggingFace](https://huggingface.co/papers/2505.10565))| Ziang Zhang, Jialei Wang, Lihe Yang, Siyu Chen, sleetwang6 | i) The paper introduces Prior Depth Anything, a framework for generating accurate and dense metric depth maps by integrating incomplete metric measurements with relative depth predictions. ii) The objective is to develop a depth estimation model that can effectively utilize diverse and potentially incomplete depth priors to produce detailed and accurate metric depth maps. iii) The methodology involves a coarse-to-fine pipeline with pixel-level metric alignment and a conditioned monocular depth estimation model. iv) The model achieves state-of-the-art zero-shot performance across depth completion, super-resolution, and inpainting tasks on 7 real-world datasets. v) Prior Depth Anything provides AI practitioners with a flexible approach to improve depth estimation in various applications by effectively utilizing available depth priors and allows test-time improvements with different model settings.  |
| OpenThinkIMG: Learning to Think with Images via Visual Tool
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.08617) or [HuggingFace](https://huggingface.co/papers/2505.08617))| Zhengyuan Yang, Yunzhuo Hao, Mingyang Song, Linjie Li, Zhaochen Su | i) The paper introduces OPENTHINKIMG, a framework for tool-augmented Large Vision-Language Models (LVLMs), and V-TOOLRL, a reinforcement learning method for adaptive tool use. ii) The primary objective is to enable LVLMs to learn dynamic policies for invoking external vision tools to improve visual reasoning. iii) The methodology involves supervised fine-tuning (SFT) for policy initialization and a reinforcement learning (RL) framework, V-TOOLRL, for adaptive tool usage based on feedback from tool interactions. iv) Results show a +28.83 accuracy point improvement over the SFT-initialized counterpart on chart reasoning tasks and surpasses GPT-4.1 by +8.68 accuracy points. v) AI practitioners can utilize OPENTHINKIMG and V-TOOLRL to develop more robust LVLMs capable of dynamic, tool-augmented visual reasoning for solving complex multimodal tasks.  |
| AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection (Read more on [arXiv](https://arxiv.org/abs/2505.09926) or [HuggingFace](https://huggingface.co/papers/2505.09926))| Weixi Zhang, Yuezhi Cai, Jiangtao Yan, Yue Zhu, Bin-Bin Gao | AdaptCLIP adapts CLIP for universal visual anomaly detection across unseen domains. The research aims to develop a zero-/few-shot anomaly detection model requiring no target domain fine-tuning. AdaptCLIP employs alternately learned visual and textual representations with contextual and aligned residual feature comparative learning. It achieves state-of-the-art performance on 12 anomaly detection benchmarks, including a 10%+ improvement in pixel AUPR on challenging datasets. This method offers a flexible and adaptable approach for anomaly detection in practical AI applications by significantly improving zero-shot performance.  |
| ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible
  Long-term Tracking (Read more on [arXiv](https://arxiv.org/abs/2505.08581) or [HuggingFace](https://huggingface.co/papers/2505.08581))| Guanyi Qin, Ziyue Wang, Xuxiao Luo, Mingqi Gao, HeverLaw | i) ReSurgSAM2 is introduced as a two-stage framework for surgical referring segmentation, integrating text-referred target detection with long-term tracking using Segment Anything Model 2 (SAM2). ii) The paper addresses the limitations of existing surgical video segmentation methods by improving efficiency and long-term tracking in complex surgical scenarios. iii) The method uses a cross-modal spatial-temporal Mamba (CSTMamba) for precise detection and credible initial frame selection (CIFS), followed by a diversity-driven long-term memory (DLM) mechanism for consistent object tracking. iv) The model achieves real-time performance at 61.2 FPS with substantial improvements in accuracy and efficiency compared to existing methods. v) This work provides AI practitioners with a more accurate and efficient method for surgical video analysis, enabling enhanced interactive surgical tools and real-time decision support systems.  |
| QuXAI: Explainers for Hybrid Quantum Machine Learning Models (Read more on [arXiv](https://arxiv.org/abs/2505.10167) or [HuggingFace](https://huggingface.co/papers/2505.10167))| Rafiul Islam, Md Jafor Sadek, Shehenaz Khaled, imostafizur, AlignAI | i) The paper introduces QuXAI, a framework with a Q-MEDLEY explainer, for interpreting feature importance in hybrid quantum-classical machine learning (HQML) models. ii) The research aims to provide robust global and local explainability for HQML architectures involving quantum feature encoding and classical learning. iii) The methodology involves constructing HQML models, using the Q-MEDLEY explainer that synthesizes Drop-Column Importance (DCI) and Permutation Importance (PI), and visualizing the results. iv) Results show Q-MEDLEY effectively identifies influential classical aspects in HQML models, distinguishing them from noise, and achieves high Recall@3 scores in classical ML settings. v) QuXAI offers AI practitioners a means to improve interpretability and reliability in HQML models by revealing feature contributions throughout the hybrid processing pipeline.  |
| Exploring the Deep Fusion of Large Language Models and Diffusion
  Transformers for Text-to-Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2505.10046) or [HuggingFace](https://huggingface.co/papers/2505.10046))| Saining Xie, Sayak Paul, Xichen Pan, Boyang Zheng, Bingda Tang | i) This paper explores deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for text-to-image synthesis. ii) The main objective is to conduct an empirical study on text-to-image generation, analyzing design choices, and providing a reproducible recipe for training. iii) The methodology involves controlled comparisons with shallow fusion baselines, where text representations are integrated into each DiT layer from a single LLM layer using late fusion. iv) Results show the deep fusion model achieves better image-text alignment (GenEval 0.51) compared to self-attention DiT (GenEval 0.42) and competitive inference latency; also removing timestep conditioning improves FID. v) The principal implication is a practical recipe for text-to-image synthesis using deep fusion that is competitive with alternative approaches, also LLM and DiT model designs can be effectively decoupled enabling application of separate scaling laws and design principles.  |
| Parallel Scaling Law for Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.10475) or [HuggingFace](https://huggingface.co/papers/2505.10475))| Dayiheng Liu, Jiaxi Yang, Zeyu Cui, Binyuan Hui, Mouxiang Chen | i) The paper introduces parallel scaling (PARSCALE), a novel method to scale language models by increasing parallel computation. ii) The research aims to improve language model scaling efficiency by increasing parallel computation instead of solely relying on parameter or inference-time scaling. iii) PARSCALE employs multiple learnable transformations on the input, processes them in parallel, and dynamically aggregates the outputs. iv) Experiments show that PARSCALE achieves performance similar to scaling parameters by O(log P), but with up to 22x less memory increase and 6x less latency increase compared to parameter scaling. v) PARSCALE offers AI practitioners a memory-efficient strategy for deploying more powerful language models in low-resource scenarios through increased parallel computation rather than extensive parameter scaling.  |
| MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09265) or [HuggingFace](https://huggingface.co/papers/2505.09265))| csgaobb | i) This paper introduces MetaUAS, a novel framework for universal anomaly segmentation utilizing a pure vision model and one-prompt meta-learning. ii) The main objective is to develop a universal anomaly segmentation model that can effectively segment any novel or unseen visual anomalies using only a single normal image prompt. iii) The methodology involves unifying anomaly segmentation into change segmentation, leveraging synthetic image pairs for training, and employing a soft feature alignment module to handle geometrical variations. iv) MetaUAS achieves state-of-the-art performance on three industrial anomaly benchmarks; specifically, it outperforms existing methods while using 10x fewer parameters and demonstrating a 100x speed improvement over WinCLIP+. v) MetaUAS offers AI practitioners a novel, efficient, and training-free approach to anomaly segmentation by providing an alternative to vision-language models, relying instead on a pure vision model and synthesized training data for improved generalization.  |
| Learning to Detect Multi-class Anomalies with Just One Normal Image
  Prompt (Read more on [arXiv](https://arxiv.org/abs/2505.09264) or [HuggingFace](https://huggingface.co/papers/2505.09264))| csgaobb | i) This paper presents OneNIP, a unified anomaly detection framework using a single normal image prompt to enhance reconstruction-based anomaly detection. ii) The research aims to improve the performance and generalization of unified anomaly detection models by guiding feature reconstruction with a normal image prompt. iii) The proposed methodology involves unsupervised reconstruction and restoration streams, combined with a supervised refiner that regresses reconstruction errors, using both real normal and synthesized anomalous images. iv) The OneNIP method outperforms previous methods on industry anomaly detection benchmarks, achieving a pixel-level anomaly segmentation performance of 63.7% on MVTec, a significant improvement over UniAD’s 44.7%. v) OneNIP provides AI practitioners with an effective approach to multi-class anomaly detection using a single normal image prompt, offering improved accuracy and faster convergence compared to existing reconstruction-based methods, particularly beneficial for industrial applications.  |
| Few-Shot Anomaly-Driven Generation for Anomaly Classification and
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2505.09263) or [HuggingFace](https://huggingface.co/papers/2505.09263))| Yunsheng Wu, Chengjie Wang, Jun Liu, Guan Gui, csgaobb | i) The paper introduces AnoGen, a few-shot anomaly-driven generation method to synthesize realistic and diverse anomaly samples for training anomaly detection models. ii) The research aims to address the problem of scarce anomaly samples by generating synthetic anomalies guided by a few real anomalies to improve anomaly classification and segmentation. iii) The methodology involves learning anomaly distribution from a few real anomalies, guiding a diffusion model using embeddings and bounding boxes to generate synthetic anomalies, and weakly-supervised training of anomaly detection models. iv) Experiments on MVTec dataset show that DRAEM and DesTSeg with AnoGen achieved a 5.8% and 1.5% improvement in AU-PR metric on segmentation, respectively. v) AI practitioners can leverage AnoGen to augment limited anomaly datasets with realistic synthetic data, leading to enhanced performance of anomaly detection models, particularly in segmentation tasks.  |
