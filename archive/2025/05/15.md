

## Papers for 2025-05-15

| Title | Authors | Summary |
|-------|---------|---------|
| DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception (Read more on [arXiv](https://arxiv.org/abs/2505.04410) or [HuggingFace](https://huggingface.co/papers/2505.04410))| Yichi Chen, Bin Kang, Yulin Li, Bin Chen, xiaomoguhzz | i) DeCLIP enhances CLIP for open-vocabulary dense prediction by decoupling and separately optimizing content and context features. ii) The research addresses CLIP's limitations in local feature representation for dense prediction tasks due to its image tokens' inability to effectively aggregate information from spatially/semantically related regions. iii) It employs a decoupled distillation design, fine-tuning "content" features with image crop representations and "context" features under guidance from vision foundation models. iv) Experiments show DeCLIP outperforms existing methods across tasks like object detection and semantic segmentation; DeCLIP improves F-ViT on OV-COCO by 3.5 mAP for novel classes. v) DeCLIP offers AI practitioners an unsupervised method to improve CLIP's local feature discriminability and spatial consistency, enhancing performance in open-vocabulary dense prediction tasks, facilitating integration in downstream applications.  |
| BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and Dataset (Read more on [arXiv](https://arxiv.org/abs/2505.09568) or [HuggingFace](https://huggingface.co/papers/2505.09568))| Zhiyang Xu, Jiuhai Chen, xurantju, zhoutianyi, xcpan | i) The paper presents BLIP3-0, a family of unified multimodal models for image understanding and generation. ii) The research aims to determine the optimal model architecture and training strategy for unified multimodal frameworks, particularly focusing on image generation. iii) The methodology involves a diffusion transformer for generating CLIP image features and a sequential pretraining strategy of first training on image understanding and then image generation. iv) BLIP3-0 achieves a GenEval score of 0.84, with the 8B model scoring 1682.6 on MME-P and 50.6 on MMMU; a human study indicates statistical confidence for improved visual quality and prompt alignment versus Janus Pro. v) The results suggest that CLIP image features coupled with flow matching and a sequential training strategy can enhance performance in unified multimodal tasks, indicating practical advantages for AI practitioners developing similar unified models.  |
| Insights into DeepSeek-V3: Scaling Challenges and Reflections on
  Hardware for AI Architectures (Read more on [arXiv](https://arxiv.org/abs/2505.09343) or [HuggingFace](https://huggingface.co/papers/2505.09343))| Huazuo Gao, Damai Dai, Chong Ruan, Chengqi Deng, Chenggang Zhao | DeepSeek-V3 achieves state-of-the-art performance through hardware-aware model co-design, optimizing for cost-efficient training and inference at scale. The paper investigates how hardware characteristics influence model architecture and identifies potential future hardware directions for AI. DeepSeek-V3 employs Multi-head Latent Attention (MLA), Mixture of Experts (MoE), FP8 mixed-precision training, and a Multi-Plane Network Topology to overcome hardware limitations. Using 2,048 NVIDIA H800 GPUs, DeepSeek-V3 achieves a KV cache size of 70.272 KB per token, significantly less than Qwen-2.5 72B's 327.680 KB and LLaMA-3.1 405B's 516.096 KB, demonstrating enhanced memory efficiency. The findings emphasize the importance of hardware and model co-design for scalable AI, providing a practical blueprint for innovating next-generation AI systems by precisely scaling low-precision computation units and emphasizing scale-up and scale-out convergence.  |
| Marigold: Affordable Adaptation of Diffusion-Based Image Generators for
  Image Analysis (Read more on [arXiv](https://arxiv.org/abs/2505.09358) or [HuggingFace](https://huggingface.co/papers/2505.09358))| Nando Metzger, Tianfu Wang, Kevin Qu, Bingxin Ke, konradschindler | i) Marigold presents a fine-tuning protocol and associated conditional diffusion models adapted from pretrained latent diffusion models for image analysis tasks. ii) The research objective is to leverage the knowledge embedded in generative image models for dense image analysis, including monocular depth estimation, surface normals prediction, and intrinsic image decomposition. iii) The methodology involves fine-tuning a Stable Diffusion model with synthetic data and a resource-efficient protocol, reusing the LDM's VAE to encode both input images and output modalities into the latent space. iv) Marigold demonstrates state-of-the-art zero-shot generalization, achieving high performance on depth estimation, surface normal prediction, and intrinsic image decomposition on datasets without observing a single image other than synthetic rooms. Marigold can produce depth estimates in under 100ms. v) This work offers AI practitioners a practical approach to repurpose readily available foundational generative models with minimal computational overhead to achieve high-performing image analysis capabilities in data-scarce settings, enabling rapid prototyping and deployment of robust vision systems.  |
| SweRank: Software Issue Localization with Code Ranking (Read more on [arXiv](https://arxiv.org/abs/2505.07849) or [HuggingFace](https://huggingface.co/papers/2505.07849))| Xuan Phi Nguyen, Ye Liu, JaeHyeok Doo, Tarun Suresh, Revanth Gangi Reddy | i) The paper introduces SWERANK, a retrieve-and-rerank framework for software issue localization, and a corresponding dataset, SWELOC. ii) The research aims to develop a more effective and efficient method for identifying relevant code locations for software issues described in natural language. iii) The methodology employs a bi-encoder for retrieval (SWERANKEMBED) and a listwise-trained LLM for reranking (SWERANKLLM), trained using a new dataset, SWELOC, curated from GitHub. iv) Experimental results on SWE-Bench-Lite show that SWERANK achieves state-of-the-art performance, with SWERANKEMBED-LARGE achieving 71.90% Acc@10 for function localization, surpassing existing agent-based approaches at a significantly lower cost. v) SWERANK provides AI practitioners with a cost-effective and accurate alternative to agent-based systems for software issue localization, enabling efficient integration into automated software engineering tools.  |
| VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large
  Video Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.08455) or [HuggingFace](https://huggingface.co/papers/2505.08455))| Ali Etemad, pritamqu | i) VCRBench, a new benchmark, is introduced to evaluate long-form causal reasoning in Large Video Language Models (LVLMs). ii) The research aims to assess LVLMs' ability to identify, reason about, and sequence events in procedural videos to achieve specific goals. iii) VCRBench uses procedurally-generated videos of shuffled everyday activities to test LVLMs' ability to correctly order causally-related steps. iv) Evaluations show that current LVLMs struggle, performing at or below random guess, with the best model falling nearly 40% short of human performance; however, using Recognition-Reasoning Decomposition (RRD) improves accuracy by up to 25.2%. v) RRD, a modular approach decomposing video reasoning into video recognition and causal reasoning, improves LVLM performance, indicating that explicitly separating these sub-tasks can enhance causal reasoning capabilities for AI practitioners developing video-based reasoning systems.  |
| Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM? (Read more on [arXiv](https://arxiv.org/abs/2505.09439) or [HuggingFace](https://huggingface.co/papers/2505.09439))| Hilde Kuehne, Samuel Thomas, Edson Araujo, Saurabhchand Bhati, h9LtLSb | Omni-R1, a streamlined GRPO fine-tuning of Qwen2.5-Omni, attains new State-of-the-Art performance on the MMAU benchmark for audio question answering. The research investigates whether audio fine-tuning is truly necessary for audio LLMs. GRPO fine-tuning was applied to Qwen2.5-Omni using both human-annotated and automatically generated audio question-answering datasets. Results show that Omni-R1 achieves the highest MMAU accuracies across sounds, music, speech, and overall average, with a peak Test-mini accuracy of 71.3% and Test-full accuracy of 71.2%. Text-only fine-tuning can significantly improve audio performance, suggesting improved text-based reasoning contributes substantially to the improved audio question answering ability in the model.  |
| DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person
  Recognition (Read more on [arXiv](https://arxiv.org/abs/2505.04793) or [HuggingFace](https://huggingface.co/papers/2505.04793))| Carolina Fernandes, Satish Mekewad, Pavan Kumar MP, Nzakiese Mbongo, Kailash A. Hambarde | i) This paper introduces DetReIDX, a new dataset for person re-identification (ReID) designed to stress-test algorithms under real-world UAV surveillance conditions. ii) The main objective is to provide a challenging dataset that realistically incorporates data variability factors often lacking in existing ReID datasets, such as viewpoint changes, scale variations, clothing changes, and occlusion. iii) The methodology involved collecting a multi-session aerial-ground dataset of 509 identities across seven university campuses, with UAV altitudes ranging from 5.8 to 120 meters, and annotating 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. iv) Experiments using SOTA methods on DetReIDX revealed a performance degradation of up to 80% in detection accuracy and over 70% in Rank-1 ReID compared to their performance on existing datasets. v) DetReIDX provides AI practitioners a new benchmark dataset to develop more robust and generalizable person ReID systems capable of handling the challenges inherent in real-world UAV deployments, including addressing the limitations of models relying on superficial appearance cues.  |
