

## Papers for 2025-05-07

| Title | Authors | Summary |
|-------|---------|---------|
| Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.03318) or [HuggingFace](https://huggingface.co/papers/2505.03318))| Qinglin Lu, Chunyu Wang, Zhimin Li, Yibin Wang, yuhangzang | This paper introduces UNIFIEDREWARD-THINK, a unified multimodal Chain-of-Thought (CoT) reward model enhanced by reinforcement fine-tuning to improve reward signal accuracy for visual understanding and generation tasks. The main objective is to enable reliable, multi-dimensional CoT reasoning in reward models by eliciting and incentivizing latent complex reasoning capabilities, despite the scarcity of explicit CoT supervision data. The key methodology employs a three-stage training pipeline: (1) cold-starting by distilling CoT reward format from GPT-4o, (2) refining through rejection sampling on large-scale unified preference data, and (3) leveraging Group Relative Policy Optimization (GRPO) for reinforcement fine-tuning using verifiable format and accuracy rewards. UNIFIEDREWARD-THINK achieved superior performance, for example, attaining a 72.3% macro accuracy on the VLRewardBench for image understanding, compared to 66.6% by the UnifiedReward baseline, and also demonstrated improved implicit reasoning capabilities when CoT was not explicitly output. For AI practitioners, this work offers a method to develop more accurate and interpretable multimodal reward models by incorporating CoT through reinforcement learning, which can significantly enhance the alignment of vision models with human preferences, even with limited explicit CoT data. |
| Absolute Zero: Reinforced Self-play Reasoning with Zero Data (Read more on [arXiv](https://arxiv.org/abs/2505.03335) or [HuggingFace](https://huggingface.co/papers/2505.03335))| Andrew Zhao, zlzheng, shenzhi-wang, Yang130, kevinwyr | This paper introduces Absolute Zero, an RLVR paradigm where a model self-improves reasoning by autonomously proposing and solving tasks using only a code executor for verifiable rewards, without any external data. The research aims to develop a system where a large language model can enhance its reasoning capabilities purely through self-play, eliminating reliance on human-curated data for task definition or solution verification. The core methodology involves the Absolute Zero Reasoner (AZR), a single model acting as both a task proposer (rewarded for task learnability) and a solver (rewarded for solution correctness) for self-generated coding tasks (deduction, abduction, induction), with a code executor providing feedback and Task-Relative REINFORCE++ for updates. AZR, trained entirely without external data, achieved state-of-the-art performance, surpassing previous zero-setting models that used curated data by an average of 1.8 absolute points on combined coding and math reasoning benchmarks. This paradigm offers AI practitioners a pathway to build more autonomous reasoning systems capable of self-generating training curricula and improving without continuous human data supervision, potentially overcoming data bottlenecks and enabling learning beyond human-provided tasks. |
| FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.03730) or [HuggingFace](https://huggingface.co/papers/2505.03730))| Yansong Tang, Ying Shan, Zhaoyang Zhang, Shiyi Zhang, JunhaoZhuang | FlexiAct proposes a novel framework for transferring actions from a reference video to an arbitrary target image, achieving flexible action control in heterogeneous scenarios with varying spatial structures while maintaining appearance consistency. The main objective is to overcome the limitations of existing action customization methods that require strict spatial alignment (layout, skeleton, viewpoint) between reference and target, by enabling action transfer across diverse subjects and domains. The methodology involves two key components: RefAdapter, a lightweight image-conditioned adapter for spatial adaptation and consistency preservation, and Frequency-aware Action Extraction (FAE), which dynamically adjusts attention to frequency-specific embeddings during the denoising process to precisely extract motion. Experiments show FlexiAct effectively transfers actions in diverse scenarios; in human evaluations, FlexiAct was preferred over a base model for motion consistency (79.5% vs. 20.5%) and appearance consistency (78.3% vs. 21.7%). For AI practitioners, FlexiAct offers a robust method for action-conditioned video generation where reference and target subjects differ significantly, broadening applications in animation and content creation by decoupling action from strict spatial constraints and utilizing dynamic, frequency-aware attention modulation. |
| RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale (Read more on [arXiv](https://arxiv.org/abs/2505.03005) or [HuggingFace](https://huggingface.co/papers/2505.03005))| Eugene Cheah, Janna Lu, Eric Alcaide, SmerkyG | The paper introduces RADLADS, a rapid and cost-effective protocol for converting pre-trained softmax attention transformers into performant linear attention decoder models, alongside two new RWKV-variant architectures, RAD-RWKV6 and RAD-RWKV7. The primary objective is to develop a highly efficient method to distill knowledge from large softmax attention transformers into linear attention models, requiring significantly less data (350-700M tokens, <0.005% of original pre-training data) and compute than full pre-training, while preserving near-original model quality and achieving state-of-the-art performance for linear attention models. RADLADS employs a three-step conversion: 1) Attention Weights Transfer from the teacher, 2) Attention Hidden State Alignment using L2 loss on 100M tokens to match teacher attention hidden states, and 3) Knowledge Distillation of teacher output logits using Kullback-Leibler divergence loss on 250M-700M tokens, followed by optional fine-tuning. A key result is that a converted 72B Qwen2.5 model (QRWKV6-72B-Instruct) achieved an MMLU score of 0.754, closely matching its teacher's 0.751, establishing new state-of-the-art downstream performance for a pure RNN language model of its size. For AI practitioners, RADLADS offers a practical pathway to create large-scale, inference-efficient linear attention models from existing powerful softmax transformers with significantly reduced costs, facilitating broader adoption of models with O(1) per-token inference complexity. |
| RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM
  Inference (Read more on [arXiv](https://arxiv.org/abs/2505.02922) or [HuggingFace](https://huggingface.co/papers/2505.02922))| Chengruidong Zhang, Jinkai Zhang, Yaoqi Chen, qianxizhang, baotonglu | RetroInfer presents a novel vector-storage system to accelerate long-context Large Language Model (LLM) inference by exploiting attention sparsity. The primary objective is to address GPU memory and bandwidth constraints that hinder efficient inference for LLMs with extended context lengths. Its core methodology involves the "wave index," an Attention-aWare Vector index for retrieving critical tokens using tripartite attention approximation, accuracy-bounded estimation, and segmented clustering, complemented by a "wave buffer" for coordinating KV cache placement and hardware operations. Experiments demonstrate up to 4.5x speedup over full attention within GPU memory limits and up to 10.5x over sparse attention baselines when extending KV cache to CPU memory, while maintaining full-attention-level accuracy. For AI practitioners, RetroInfer offers a system to significantly improve throughput and scalability for deploying LLMs with very long contexts without compromising model accuracy. |
| Decoding Open-Ended Information Seeking Goals from Eye Movements in
  Reading (Read more on [arXiv](https://arxiv.org/abs/2505.02872) or [HuggingFace](https://huggingface.co/papers/2505.02872))| Yevgeni Berzak, Yoav Meiri, Omer Shubi, Cfir Avraham Hadar | This research investigates decoding open-ended, text-specific information-seeking goals from readers' eye movements using multimodal LLMs. The primary objective is to determine if a reader's specific question for a text can be automatically decoded from their eye movements, assessed via goal classification and reconstruction tasks. The methodology involves discriminative (adapted Haller RNN, ROBERTEye-Fixations) and novel generative (DalEye-LLaVA, DalEye-Llama) multimodal LLMs combining text and eye-tracking features from the OneStop dataset. ROBERTEye-Fixations achieved the highest classification accuracy at 49.3% overall (chance 33.0%), and significantly, 57.3% (chance 49.9%) in distinguishing questions over identical text spans, demonstrating extraction of fine-grained goal information. This suggests AI practitioners can leverage eye-tracking with LLMs to infer user-specific information needs for personalized systems, though precise goal generation requires further advancement. |
| An Empirical Study of Qwen3 Quantization (Read more on [arXiv](https://arxiv.org/abs/2505.02214) or [HuggingFace](https://huggingface.co/papers/2505.02214))| Xudong Ma, Yue Feng, Yuye Li, HaoranChu, Xingyu-Zheng | This paper empirically evaluates the quantization robustness of the Qwen3 LLM series using five post-training quantization (PTQ) methods across bit-widths from 1 to 8 bits. The study's main objective is to systematically assess Qwen3's performance degradation under various quantization settings to identify opportunities and challenges in compressing these state-of-the-art models. The methodology involves applying five PTQ techniques (RTN, GPTQ, AWQ, SmoothQuant, BiLLM) to Qwen3 models, testing weight-only (1-8 bits) and weight-activation quantization, with performance measured on perplexity, 0-shot reasoning tasks, and 5-shot MMLU. Primary results indicate that while Qwen3 achieves near-lossless performance at 8-bit quantization, it shows noticeable degradation at 4-bits (e.g., Qwen3-8B's MMLU score drops from 74.7 in FP16 to 69.3 with 4-bit AWQ per-group quantization) and experiences more pronounced degradation at 3-bits or fewer, particularly compared to previous model generations. Principal implication for AI practitioners: When deploying Qwen3, practitioners can expect robust performance with 8-bit quantization, but must carefully evaluate the noticeable performance trade-offs at 4-bits and the significant degradation at 3-bits or below, indicating a need for advanced quantization strategies or careful capability assessments for ultra-low precision applications of these models. |
| Multi-Agent System for Comprehensive Soccer Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.03735) or [HuggingFace](https://huggingface.co/papers/2505.03735))| Yanfeng Wang, Ya Zhang, Zifeng Li, haoningwu, Homie0609 | This paper introduces SoccerAgent, a multi-agent system for holistic soccer understanding, accompanied by SoccerWiki, a multimodal knowledge base, and SoccerBench, a comprehensive benchmark. The main research objective is to develop a comprehensive framework for AI-driven soccer understanding that moves beyond isolated tasks to enable knowledge-driven reasoning. The key methodology involves constructing SoccerWiki with information on 9,471 players and 266 teams, creating SoccerBench with ~10K multimodal multi-choice QA pairs across 13 tasks, and developing SoccerAgent, a multi-agent system that decomposes questions and invokes 18 specialized tools. SoccerAgent achieved 85.0% accuracy on TextQA tasks and 60.9% on VideoQA tasks within SoccerBench, outperforming existing Multimodal Large Language Models. The principal implication for AI practitioners is the provision of a new benchmark (SoccerBench) and a multi-agent system architecture (SoccerAgent) that demonstrates effective task decomposition and tool utilization for complex, domain-specific multimodal understanding, offering a template for similar AI applications. |
| Geospatial Mechanistic Interpretability of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.03368) or [HuggingFace](https://huggingface.co/papers/2505.03368))| Kevin Roitero, Stefano Mizzaro, sdesabbata | This paper introduces a framework using spatial analysis and sparse autoencoders to interpret how Large Language Models internally represent geographical information. Its objective is to understand the internal mechanisms LLMs use to process and encode geospatial data. The study extracted activations from Mistral-7B-Instruct-v0.2 for placename prompts, analyzed them using spatial autocorrelation, then applied sparse autoencoders to decompose activations from layer 15 into features, which were also spatially analyzed. While 14.98% of raw neuron activations across multiple layers exhibited polysemantic spatial patterns, sparse autoencoder decomposition of layer 15 activations yielded only 0.2% (67 of 32,768) of features with significant spatial autocorrelation, indicating sparse, though sometimes more monosemantic, geospatial encoding and highlighting areas for further research in decomposition techniques. The principal implication for AI practitioners is that this framework offers a method to interpret LLMs' complex and sparsely distributed geographical representations, which is critical for developing more reliable and well-understood foundation models for geospatial applications by revealing how models internally handle such data. |
| InfoVids: Reimagining the Viewer Experience with Alternative
  Visualization-Presenter Relationships (Read more on [arXiv](https://arxiv.org/abs/2505.03164) or [HuggingFace](https://huggingface.co/papers/2505.03164))| Kevin Hsu, Ivy Chen, Tongyu Zhou, Ji Won Chung, Franck-Dernoncourt | This paper introduces "InfoVids," an augmented reality (AR) paradigm that integrates presenters and visualizations within a shared 3D space to enhance viewer experience compared to traditional 2D slide-based presentations. The primary objective is to investigate how these alternative spatial arrangements and interactions affect viewer engagement, perceived presenter immersion, and attention dynamics. Researchers developed four InfoVid case technology probes using ARKit and a custom Body Object Model (BOM), which were then compared against 2D baseline equivalents by 30 public participants through surveys and semi-structured interviews. Results showed InfoVids significantly shifted viewer attention towards the presenter (e.g., for AIRPLANEVIS, 16 out of 30 participants shifted focus to the presenter) and were generally perceived as more engaging and immersive. For AI practitioners developing data communication or presentation tools, this research indicates that co-locating presenters and AR visualizations can create more human-centric experiences, suggesting a valuable approach for designing AI-driven data storytelling systems that prioritize presenter engagement. |
| VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient
  Large Speech-Language Model (Read more on [arXiv](https://arxiv.org/abs/2505.03739) or [HuggingFace](https://huggingface.co/papers/2505.03739))| Lijiang Li, Heting Gao, Chaoyou Fu, Yunhang Shen, Zuwei Long | VITA-Audio is an end-to-end large speech model designed for fast interleaved cross-modal token generation to reduce high first-audio-token latency in streaming applications. The primary objective is to achieve real-time audio generation within end-to-end speech models, specifically enabling zero audio token delay after the initial LLM forward pass. This is accomplished using lightweight Multiple Cross-modal Token Prediction (MCTP) modules that efficiently generate multiple audio tokens directly from LLM hidden states within a single model forward pass, combined with a four-stage progressive training strategy. VITA-Audio demonstrates a 3-5x inference speedup at the 7B parameter scale and reduces the first audio token chunk generation time from 236ms (Vanilla mode) to 53ms (Boost mode). The principal implication for AI practitioners is that VITA-Audio offers an effective architecture for developing highly responsive, real-time conversational AI systems by enabling immediate audio output from the first forward pass. |
| Invoke Interfaces Only When Needed: Adaptive Invocation for Large
  Language Models in Question Answering (Read more on [arXiv](https://arxiv.org/abs/2505.02311) or [HuggingFace](https://huggingface.co/papers/2505.02311))| Biao Qin, Chunlai Zhou, Robot2050 | This paper proposes AttenHScore, an unsupervised metric for adaptive LLM invocation in Question Answering by detecting Small Language Model (SLM) hallucinations in real-time, complemented by an uncertainty-aware text re-ranking strategy. The main objective is to precisely determine when to invoke a large language model (LLM) if a small language model (SLM) is likely hallucinating, thereby optimizing the trade-off between performance and cost in collaborative LM systems. The key methodology involves "AttenHScore," which calculates the accumulation and propagation of hallucinations during SLM generation using token probabilities (Pmax(xi)) and attention scores (Atten(xi)), and an uncertainty-based re-ranking of retrieved documents by guiding SLMs to generate queries from text chunks. Primary results show AttenHScore outperforms baselines; for example, with Llama3-8B-Instruct on SQuAD, it achieved an AUCS of 0.8715 and ACCr of 0.8176. The re-ranking strategy improved the F1 score of Vicuna-7B-v1.5 by 3.37 on MultiFieldQA-zh. The principal implication for AI practitioners is the provision of a plug-and-play, unsupervised method to reduce computational costs and improve QA system efficiency by adaptively invoking expensive LLMs only when SLMs demonstrate signs of hallucination, without needing additional model training. |
| HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.21650) or [HuggingFace](https://huggingface.co/papers/2504.21650))| Yonghong Tian, Xinhua Cheng, Jiawen Guan, Haiyang Zhou, Drexubery | HoloTime is a novel framework that generates immersive panoramic 4D scenes from images or prompts by integrating specialized video diffusion for panoramic video creation and a robust 4D reconstruction pipeline. Its primary objective is to overcome the limitations of existing methods in producing truly immersive, dynamic 360-degree 4D scene-level assets for VR/AR applications. The methodology combines the "360World" dataset of fixed-camera panoramic videos, a "Panoramic Animator" (a two-stage motion-guided image-to-video diffusion model with hybrid fine-tuning and panoramic circular techniques), and "Panoramic Space-Time Reconstruction" (using space-time aligned depth estimation and 4D Gaussian Splatting). The framework demonstrates superior performance, with HoloTime achieving an 87.74% user preference for graphics quality in image-driven 4D scene generation compared to 3D-Cinemagraphy (1.94%), and significantly higher user ratings for text-driven panoramic video quality. For AI practitioners, HoloTime offers a method to create high-fidelity, spatially and temporally consistent panoramic 4D environments, enhancing immersive experiences, and provides the 360World dataset as a resource for developing similar panoramic video generation models. |
| Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in
  Smart Personal Assistant (Read more on [arXiv](https://arxiv.org/abs/2504.18373) or [HuggingFace](https://huggingface.co/papers/2504.18373))| Xiaoyu Shen, lorashen | This paper introduces Auto-SLURP, a benchmark dataset for evaluating LLM-based multi-agent frameworks for smart personal assistants. The main objective is to provide a standardized benchmark for comprehensive end-to-end evaluation of these frameworks, covering language understanding, task execution, and response generation. Auto-SLURP extends the original SLURP dataset by relabeling slots and integrating simulated servers and external services, with experiments conducted on frameworks like CamelAI, LangGraph, AutoGen, and AgentLite using GPT-4. Primary results show AgentLite achieved the highest accuracy at 0.46, and finetuning an intent agent (LLAMA-3 8B) on AutoGen improved its accuracy from 0.40 to 0.62, a 55% performance increase. The principal implication for AI practitioners is that Auto-SLURP offers a challenging testbed for developing and iterating on more reliable multi-agent personal assistant systems, revealing that current frameworks require significant improvement, especially in areas like intent processing. |
