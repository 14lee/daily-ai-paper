

## Papers for 2025-05-05

| Title | Authors | Summary |
|-------|---------|---------|
| PixelHacker: Image Inpainting with Structural and Semantic Consistency (Read more on [arXiv](https://arxiv.org/abs/2504.20438) or [HuggingFace](https://huggingface.co/papers/2504.20438))| xinggangw, steelozazala, wenyuliu, SmileTAT, Uyoung | PixelHacker introduces Latent Categories Guidance (LCG) within a diffusion model for structurally and semantically consistent image inpainting. The objective is to overcome limitations of existing inpainting methods that struggle with complex structures and semantics, leading to artifacts and logically incoherent results. The key methodology is Latent Categories Guidance (LCG), utilizing separate fixed-size embeddings for latent 'foreground' and 'background' features derived from diverse mask types (semantic, random), injected into a diffusion model's denoising steps via linear attention. PixelHacker demonstrated superior performance, achieving a state-of-the-art FID of 8.59 on the Places2 test set (512 resolution, 40-50% masks), outperforming models like SDXL. For practitioners, the LCG approach demonstrates an effective technique to enhance structural and semantic coherence in diffusion-based inpainting models by conditioning on coarse foreground/background distinctions, rather than complex textual prompts or fine-grained labels, potentially simplifying guidance while improving output quality for image editing applications. |
| Improving Editability in Image Generation with Layer-wise Memory (Read more on [arXiv](https://arxiv.org/abs/2505.01079) or [HuggingFace](https://huggingface.co/papers/2505.01079))| Jaesik Park, Jaeah Lee, carpedkm | This paper introduces a framework employing layer-wise memory to improve control and consistency in sequential, mask-guided image editing. The primary objective is to enable multiple edits while preserving background integrity and ensuring natural integration of new elements using only rough user masks, overcoming limitations of single-object editing methods. Key methodologies include a layer-wise memory storing previous edit latents and prompts, Background Consistency Guidance (BCG) for stable background preservation and efficient latent blending, and Multi-Query Disentanglement (MQD) in cross-attention for coherent object integration across layers. The proposed method demonstrates superior performance on a new Multi-Edit Benchmark, achieving a BLEU-4 score of 36.59 and a CLIP score of 64.29, outperforming existing editing and layout-to-image models in sequential tasks. For AI practitioners, this framework offers a robust technique for developing interactive editing systems capable of complex, multi-step modifications with minimal user effort while maintaining high fidelity and contextual coherence. |
| Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG
  Evaluation Prompts (Read more on [arXiv](https://arxiv.org/abs/2504.21117) or [HuggingFace](https://huggingface.co/papers/2504.21117))| Wenge Rong, Yiqi Liu, Chenghao Xiao, Hanhua Hong, yangwang825 | This paper introduces inversion learning to automatically generate highly effective, model-specific evaluation prompts for NLG systems using just a single evaluation sample. The objective is to overcome the limitations and prompt sensitivity issues inherent in manually crafted prompts used for LLM-based evaluation. The key methodology involves training an inversion model to learn the reverse mapping from an LLM evaluator's output (e.g., human score) back to the corresponding input instruction (evaluation prompt). Results show inversion prompts consistently outperform human-crafted and forward prompts across tasks and models; for LLaMA-3.1-8B-Instruct (Black-Box), inversion prompts achieved a 33% higher average Spearman correlation than forward prompts, demonstrating model-specificity is crucial. The principal implication for AI practitioners is that generating tailored evaluation prompts via inversion learning, instead of using generic ones, leads to more robust, efficient, and reliable LLM-based evaluation. |
| Llama-Nemotron: Efficient Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.00949) or [HuggingFace](https://huggingface.co/papers/2505.00949))| Ran El-Yaniv, Mohammad Dabbah, Izik Golan, Itay Levy, Akhiad Bercovich | The Llama-Nemotron paper introduces an open family of heterogeneous reasoning models (Nano-8B, Super-49B, Ultra-253B) optimized for efficiency and enterprise use. The main objective was to develop models delivering exceptional reasoning capabilities combined with high inference throughput and memory efficiency under a permissive open license. Key methodologies include neural architecture search (NAS) from Llama 3 models using the Puzzle framework, FFN Fusion, knowledge distillation, continued pretraining, supervised fine-tuning (SFT) on curated synthetic data, and large-scale reinforcement learning (RL). Primary results demonstrate the flagship LN-Ultra (253B) achieves state-of-the-art open model performance, outperforming DeepSeek-R1 on benchmarks like GPQA-Diamond (76.0%) while offering significantly higher inference throughput (e.g., 4x at 500/2000 ISL/OSL on 8xH100). For AI practitioners, this provides commercially permissive, high-performance reasoning models optimized for efficient deployment, featuring a novel dynamic reasoning toggle to switch between chat and reasoning modes. |
| CORG: Generating Answers from Complex, Interrelated Contexts (Read more on [arXiv](https://arxiv.org/abs/2505.00023) or [HuggingFace](https://huggingface.co/papers/2505.00023))| Trung Bui, aifactoryysh, Franck-Dernoncourt, hyunjilee | This paper introduces CORG, a framework for language models to generate answers from complex corpora by organizing interrelated contexts into processed groups. The objective is to improve language model answer generation accuracy, recall, and disambiguation when processing multiple contexts exhibiting distracting, ambiguous, counterfactual, or duplicated relationships. CORG employs a graph constructor to identify context interrelationships, a reranker to organize contexts into optimized groups based on relationship type, and an aggregator to generate cited answers per group. Results demonstrate CORG's effectiveness; on the AmbigDocs+ dataset with Llama2-7B, CORG achieved a Disambig-F1 score of 22.0, substantially outperforming grouping baselines like KMeans (3.6) and single-pass methods like base processing (17.0). AI practitioners can utilize CORG as an inference-time solution to enhance the robustness of retrieval-augmented generation systems facing inconsistent real-world documents, improving answer quality and entity disambiguation without requiring model retraining. |
| Real-World Gaps in AI Governance Research (Read more on [arXiv](https://arxiv.org/abs/2505.00174) or [HuggingFace](https://huggingface.co/papers/2505.00174))| Tim O'Reilly, sruly, isobelmoure, strauss-NYC | This paper analyzes AI safety and reliability research, revealing a corporate focus on pre-deployment and gaps in post-deployment risk analysis. The objective was to compare the research outputs and priorities of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, OpenAI) versus top AI universities regarding AI safety and reliability, particularly pre- versus post-deployment issues. Methodology involved analyzing 1,178 safety/reliability papers from 9,439 generative AI papers (Jan 2020-Mar 2025), applying fractional authorship adjustments, classifying papers using GPT-4o mini, and conducting keyword searches for specific risk domains. Primary results indicate corporate research increasingly concentrates on pre-deployment alignment and testing, while only 4% of corporate safety papers address high-risk deployment domains (e.g., misinformation, medical contexts, hallucinations, copyright); ethics and bias research is now predominantly academic. The principal implication for AI practitioners is that current corporate-led research may underemphasize critical risks emerging *after* deployment, necessitating caution as established best practices for real-world operational safety and reliability remain underdeveloped in public research. |
| TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2505.00562) or [HuggingFace](https://huggingface.co/papers/2505.00562))| Chuchu Fan, yuemithucsd | TeLoGraF introduces a graph-encoded flow matching framework for planning trajectories that satisfy general Signal Temporal Logic (STL) specifications. The objective is to learn a single conditional generative model capable of handling diverse STL specifications as input without requiring retraining for new formulas. It encodes STL specifications as syntax graphs processed by a Graph Neural Network (GNN) whose embedding conditions a flow-matching model to generate trajectories. Results show TeLoGraF outperforms baselines in STL satisfaction rates across five environments; notably, its "Fast" variant achieves up to 123.6X faster inference than gradient-based methods on the Franka Panda benchmark while maintaining high satisfaction. For AI practitioners, this provides a significantly faster inference method for planning under complex temporal and logical constraints in robotics and cyber-physical systems, though performance degrades on heavily out-of-distribution STLs. |
| X-Cross: Dynamic Integration of Language Models for Cross-Domain
  Sequential Recommendation (Read more on [arXiv](https://arxiv.org/abs/2504.20859) or [HuggingFace](https://huggingface.co/papers/2504.20859))| Haggai Roitman, liorrokach, Bshapira, yeshel, guyhadad01 | This paper presents X-Cross, a model for cross-domain sequential recommendation via dynamic, layer-wise integration of language models fine-tuned with LoRA. The primary objective is to enable effective sequential recommendation in new target domains by transferring knowledge from multiple source-domain models, requiring minimal target-domain data and avoiding full model retraining. X-Cross utilizes trainable integrators at each layer to dynamically compute weights for combining activations from frozen, LoRA-adapted source domain language models, refining representations progressively. Results show X-Cross achieves performance comparable to target-domain LoRA fine-tuning using only 25% of the adapter parameters, and requires significantly less fine-tuning data (e.g., 83.3% less for Electronics domain adaptation) to surpass baseline performance. For AI practitioners, X-Cross provides a parameter- and data-efficient method for adapting recommendation systems to new domains, reducing computational overhead and data requirements in data-constrained or rapidly evolving environments. |
