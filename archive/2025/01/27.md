

## Papers for 2025-01-27

| Title | Authors | Summary |
|-------|---------|---------|
| Humanity's Last Exam (Read more on [arXiv](https://arxiv.org/abs/2501.14249) or [HuggingFace](https://huggingface.co/papers/2501.14249))| Josephina Hu, Nathaniel Li, Ziwen Han, Alice Gatti, Long Phan | Humanity's Last Exam introduces a new multi-modal benchmark to evaluate large language model capabilities at the forefront of human knowledge.  The research objective was to create a challenging, closed-ended benchmark resistant to simple internet retrieval, exceeding the accuracy of state-of-the-art LLMs on existing benchmarks.  A multi-stage review process, involving LLM difficulty checks and expert review, was employed to curate 3,000 questions across various subjects.  Results showed that all state-of-the-art models achieved less than 10% accuracy, highlighting a significant gap between current LLM capabilities and human expert performance.  This benchmark's creation provides a critical tool for evaluating and guiding future LLM development, demonstrating the limitations of current models on complex academic questions.  |
| Redundancy Principles for MLLMs Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2501.13953) or [HuggingFace](https://huggingface.co/papers/2501.13953))| Chunyi Li, Xiangyu Zhao, Zicheng Zhang, KennyUTC, nebulae09 | This paper introduces a framework for evaluating and addressing redundancy in multi-modal large language model (MLLM) benchmarks. The main research question is how to quantify and mitigate redundancy across dimensions, instances, and benchmarks in MLLM evaluation. The key methodology involves calculating the correlation between MLLM performance rankings across different dimensions, instances, and benchmarks using metrics like SRCC, PLCC, and R2. The primary results show that a majority of existing MLLM benchmarks exhibit significant instance redundancy, with over 50% of instances being redundant in many cases, and that the widely used MathVista benchmark displays lower redundancy compared to other math-focused benchmarks. The principal implication for AI practitioners is that they should carefully evaluate and address redundancy in benchmarks to ensure efficient and accurate MLLM evaluation, particularly by checking dimension, instance, and cross-benchmark redundancy.  |
| Chain-of-Retrieval Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2501.14342) or [HuggingFace](https://huggingface.co/papers/2501.14342))| Zhicheng Dou, Xiaolong Huang, Nan Yang, Haonan Chen, Liang Wang | This paper introduces Chain-of-Retrieval Augmented Generation (CoRAG), a novel framework for training large language models (LLMs) to retrieve and reason over information step-by-step. The main research question is whether explicitly training LLMs to iteratively retrieve information can improve their performance on complex, multi-hop reasoning tasks compared to traditional single-step retrieval-augmented generation (RAG) methods. The key methodology involves using rejection sampling to automatically generate intermediate retrieval chains for training and employing various decoding strategies, including greedy decoding, best-of-N sampling, and tree search, to control test-time compute. The primary result is that CoRAG substantially outperforms strong baselines on multi-hop question-answering tasks, achieving more than a 10-point improvement in EM score on the MuSiQue dataset. The principal implication for AI practitioners is that CoRAG offers a more effective approach to retrieval-augmented generation, particularly for complex queries, by enabling dynamic query reformulation and iterative information retrieval.  |
| RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques (Read more on [arXiv](https://arxiv.org/abs/2501.14492) or [HuggingFace](https://huggingface.co/papers/2501.14492))| Ruoyu Sun, Tian Ding, Zhenyang Xiao, Ziniu Li, Zhengyang Tang | RealCritic is a new benchmark for evaluating the effectiveness of large language models' (LLMs) critiques by measuring their impact on solution refinement. The main research question is how to effectively measure the quality of critiques generated by LLMs. The key methodology is a closed-loop approach that evaluates the quality of corrections generated from the critiques, including self-critique, cross-critique, and iterative critique scenarios. The primary results show that the o1-mini model outperforms others in self-critique, with a +3.3% average improvement over direct solutions, while other models show varying or negative performance changes. The principal implication for AI practitioners is that evaluating critique effectiveness through solution improvement provides a more accurate measure of critique quality compared to existing open-loop methods, which is crucial for developing LLMs with robust self-reflection capabilities.  |
| Relightable Full-Body Gaussian Codec Avatars (Read more on [arXiv](https://arxiv.org/abs/2501.14726) or [HuggingFace](https://huggingface.co/papers/2501.14726))| Timur Bagautdinov, Igor Santesteban, Tomas Simon, Shaofei Wang, psyth | This paper introduces Relightable Full-Body Gaussian Codec Avatars, a novel approach for modeling and rendering relightable, animatable full-body human avatars with high-fidelity details. The main research question is how to accurately model the relightable appearance of articulated full-body avatars, including body, face, and hands, under various lighting conditions and poses. The key methodology combines 3D Gaussian Splatting with learnable, orientation-dependent zonal harmonics for diffuse radiance transfer, a shadow network to predict non-local shadowing, and deferred shading for specular radiance transfer. The primary results show that the proposed method outperforms existing physically-based rendering approaches, achieving a PSNR of 29.48 dB and an SSIM of 0.8046 on held-out test data, demonstrating superior rendering quality and generalization. For AI practitioners, the principal implication is that this method provides a more accurate and efficient way to create and animate relightable full-body avatars, which can be instrumental for applications in virtual reality, telepresence, and digital human creation.  |
