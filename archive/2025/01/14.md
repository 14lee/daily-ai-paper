

## Papers for 2025-01-14

| Title | Authors | Summary |
|-------|---------|---------|
| The Lessons of Developing Process Reward Models in Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2501.07301) or [HuggingFace](https://huggingface.co/papers/2501.07301))| RunjiLin, BeichenZhang, wuyangzhen, chujiezheng, Zhenru | This paper investigates the development of Process Reward Models (PRMs) for mathematical reasoning in large language models (LLMs). The main research question is how to effectively construct and evaluate PRMs to improve the process supervision in mathematical reasoning. The key methodology involves a consensus filtering mechanism that integrates Monte Carlo (MC) estimation with LLM-as-a-judge for data annotation and a combination of response-level and step-level metrics for evaluation. The primary results show that the consensus filtering mechanism improves PRM performance, with Qwen2.5-Math-PRM-7B achieving a 67.6% average accuracy on the Best-of-8 evaluation, outperforming other 7B PRMs. The principal implication for AI practitioners is that combining MC estimation with LLM-as-a-judge and using comprehensive evaluation strategies can lead to more robust and reliable PRMs for enhancing mathematical reasoning in LLMs.  |
| Tensor Product Attention Is All You Need (Read more on [arXiv](https://arxiv.org/abs/2501.06425) or [HuggingFace](https://huggingface.co/papers/2501.06425))| Huizhuo Yuan, Yifeng Liu, thughost, zhenqincn, yifAI | Tensor Product Attention (TPA) is a novel attention mechanism that improves memory efficiency during inference in language models. The main research question is how to reduce the memory overhead of key-value (KV) caches in language models while maintaining or improving performance. The key methodology is using tensor decompositions to represent queries, keys, and values compactly, integrating with Rotary Positional Embedding (RoPE). Primary results show that TPA reduces KV cache size by up to 10x or more during inference and achieves lower validation perplexity than baselines like Multi-Head Attention (MHA), as evidenced by TPA achieving an average of 51.41% in zero-shot mode versus MHA's 50.11% on medium-size models. The principal implication for AI practitioners is that TPA offers a more memory-efficient way to deploy large language models, enabling the processing of significantly longer sequences under fixed resource constraints.  |
| $\text{Transformer}^2$: Self-adaptive LLMs (Read more on [arXiv](https://arxiv.org/abs/2501.06252) or [HuggingFace](https://huggingface.co/papers/2501.06252))| tyj2022, edoarc, lfsm | Transformer², a self-adaptation framework for large language models (LLMs), enhances LLMs' performance on unseen tasks in real-time. The main research objective is to develop a framework that enables LLMs to adapt to diverse tasks dynamically without extensive fine-tuning. The key methodology involves a two-pass mechanism during inference, employing task-specific "expert" vectors trained using reinforcement learning, and a novel parameter-efficient fine-tuning method called Singular Value Fine-tuning (SVF). A primary result is that SVF fine-tuning of LLAMA3-8B-INSTRUCT boosted performance on the GSM8K task from a baseline score of 75.89 to 79.15. The principal implication for AI practitioners is that Transformer² provides a scalable and efficient solution for enhancing LLM adaptability and task-specific performance, particularly valuable for dynamic, self-organizing AI systems.  |
| VideoAuteur: Towards Long Narrative Video Generation (Read more on [arXiv](https://arxiv.org/abs/2501.06173) or [HuggingFace](https://huggingface.co/papers/2501.06173))| Jiepeng Cen, Liangke Gui, Lu Qi, Feng Cheng, lambertxiao | VideoAuteur introduces a new method for long-form narrative video generation in the cooking domain. The main research objective is to generate coherent and informative long-form videos that convey clear narratives. The key methodology involves curating a large-scale cooking video dataset (CookGen) and developing an interleaved auto-regressive model, "VideoAuteur," which sequentially generates actions, captions, and keyframes, conditioning a video generation model. The primary result is that the proposed method achieves substantial improvements in generating visually detailed and semantically aligned keyframes, with human evaluations showing an 82.0 rating for their caption quality compared to 79.3 for Qwen2-VL-72B. The principal implication for AI practitioners is that the VideoAuteur model and CookGen dataset can be used to enhance long-form narrative video generation, offering a framework for creating more coherent and contextually rich videos.  |
| WebWalker: Benchmarking LLMs in Web Traversal (Read more on [arXiv](https://arxiv.org/abs/2501.07572) or [HuggingFace](https://huggingface.co/papers/2501.07572))| zhoudeyu, Runnaning, ZekunXi, wzl0228, callanwu | WebWalkerQA is a new benchmark for evaluating large language models (LLMs) on web traversal tasks. The main research question is how well LLMs can navigate and extract information from websites to answer complex, multi-step queries. The key methodology is a multi-agent framework called WebWalker, which uses explorer and critic agents to simulate human-like web navigation, combined with a dataset of 680 queries across 1373 webpages. A primary result is that the best-performing model achieved only 37.50% accuracy on the WebWalkerQA benchmark. The principal implication for AI practitioners is that current LLMs struggle with deep web traversal tasks, and WebWalker can be integrated with retrieval-augmented generation (RAG) systems to enhance their ability to navigate and utilize information from websites.  |
| O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2501.06458) or [HuggingFace](https://huggingface.co/papers/2501.06458))| Gui Geng, Pengfei, alanyoung058, ZhenHuang, zongzi | The paper explores inference-time scaling in large language models (LLMs) for medical reasoning tasks, demonstrating improved performance through extended reasoning processes. The main research question is whether increasing inference time can enhance the performance of LLMs on medical reasoning benchmarks of varying complexity. The key methodology involves fine-tuning LLMs on synthesized datasets that demonstrate extended reasoning (LongStep and LongMonolog) and evaluating their performance on MedQA, Medbullets, and JAMA Clinical Challenges using metrics like accuracy and average output token length. The primary results show that increasing inference time leads to improved performance, with models trained on extended reasoning data achieving accuracy improvements of 6-11% using a training set of only 500 samples. For AI practitioners, the principal implication is that scaling inference time by incorporating structured thought processes can significantly enhance LLMs' ability to address complex medical reasoning tasks, even with limited training data.  |
| MinMo: A Multimodal Large Language Model for Seamless Voice Interaction (Read more on [arXiv](https://arxiv.org/abs/2501.06282) or [HuggingFace](https://huggingface.co/papers/2501.06282))| langgz, gaoruize, zhihaodu, Yingda, chenmengzhe | MinMo is an 8-billion-parameter multimodal large language model designed for seamless voice interactions. The main research objective is to develop a model that addresses limitations of prior aligned multimodal models, specifically in maintaining text-LLM capabilities while achieving state-of-the-art voice comprehension and generation. The key methodology involves multi-stage training on 1.4 million hours of diverse speech data, aligning speech-to-text, text-to-speech, speech-to-speech, and duplex interactions. The primary result is that MinMo achieves state-of-the-art performance across various benchmarks, including spoken dialogue and multilingual speech recognition, with a speech-to-text latency of approximately 100ms. The principal implication for AI practitioners is that MinMo provides a robust framework for developing voice interaction systems, demonstrating strong performance in full-duplex conversations and nuanced speech generation.  |
| SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training (Read more on [arXiv](https://arxiv.org/abs/2501.06842) or [HuggingFace](https://huggingface.co/papers/2501.06842))| Zhangyang Wang, Lu Liu, Gaojie Jin, Ziquan Zhu, Tianjin Huang | This paper introduces Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer to address gradient and loss spikes in large language model (LLM) training. The main research question is how to mitigate the negative impact of gradient spikes on LLM training stability and performance. The key methodology involves integrating momentum reset and spike-aware gradient clipping into the Adam optimizer, along with a sparse momentum technique for memory efficiency. Primary results show that SPAM outperforms Adam and its variants across various tasks; for example, SPAM achieved a perplexity of 30.46 on the C4 dataset with the LLaMA-60M model, compared to 34.09 for Adam. The principal implication for AI practitioners is that SPAM provides a more stable and resource-efficient optimizer for training LLMs, directly addressing a known issue that affects model performance and training cost.  |
| BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature (Read more on [arXiv](https://arxiv.org/abs/2501.07171) or [HuggingFace](https://huggingface.co/papers/2501.07171))| yeunglevy, yuhuizhang, jnirschl, minwoosun, lozanoe | Here is a concise summary of the research paper "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature":  The paper introduces BIOMEDICA, a framework for curating a large-scale biomedical image-caption dataset from open-access scientific literature and using it to train vision-language models. The main research objective is to address the scarcity of publicly available, diverse biomedical image-caption datasets for training generalist biomedical vision-language models. The key methodology involves an ETL pipeline to extract and serialize image-caption pairs from PubMed Central Open Access articles, followed by expert-guided annotation of image clusters and continual pre-training of CLIP-style models on the resulting dataset. The primary result is that the best model (BMCA-CLIP) achieved a 6.56% average improvement in zero-shot classification across 40 biomedical tasks compared to prior state-of-the-art models. The principal implication for AI practitioners is that BIOMEDICA provides a valuable resource for training and evaluating vision-language models for diverse biomedical applications, demonstrated by the strong zero-shot performance of BMCA-CLIP, even with 10x less compute.  |
| ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2501.06590) or [HuggingFace](https://huggingface.co/papers/2501.06590))| Wangchunshu, siruo2, super-dainiu, CamelH, RTT1 | ChemAgent: A novel framework improving chemical reasoning in large language models through a dynamic, self-updating library. Main research question or objective: To address the challenges of large language models (LLMs) in handling domain-specific formulas, executing accurate reasoning, and integrating code effectively in chemical reasoning tasks. Key methodology used: Development of a dynamic, self-updating library that decomposes chemical tasks into sub-tasks, compiles them into a structured collection, and retrieves/refines pertinent information for future queries, alongside three types of memory (planning, execution, knowledge) and a library-enhanced reasoning component. Primary results: ChemAgent achieved performance gains of up to 46% (using GPT-4) on four chemical reasoning datasets from SciBench, significantly outperforming existing methods. Principal implication for AI practitioners: AI practitioners can leverage ChemAgent's self-updating library and memory components to enhance LLMs' performance on complex, multi-step reasoning tasks, particularly in specialized domains like chemistry.  |
| UnCommon Objects in 3D (Read more on [arXiv](https://arxiv.org/abs/2501.07574) or [HuggingFace](https://huggingface.co/papers/2501.07574))| EarlGr, Jiali, zarzarj, JianyuanWang, wenchang05 | This paper introduces UnCommon Objects in 3D (uCO3D), a new object-centric 3D dataset for deep learning and generative AI. The main research objective is to address the scarcity of high-quality, diverse real-world 3D object datasets for training AI models. The key methodology involves collecting 360° videos of over 1,000 object categories, annotated with 3D camera poses, point clouds, captions, and 3D Gaussian Splat reconstructions, validated through extensive quality checks. The primary result is that uCO3D contains 170,000 scenes, and models trained on uCO3D outperform those trained on MVImgNet and CO3Dv2 in few-view 3D reconstruction and novel-view synthesis tasks. For AI practitioners, uCO3D provides a higher-quality dataset for training 3D deep learning models, directly improving the performance of models in tasks such as 3D object reconstruction and generation.  |
