

## Papers for 2025-01-13

| Title | Authors | Summary |
|-------|---------|---------|
| OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints (Read more on [arXiv](https://arxiv.org/abs/2501.03841) or [HuggingFace](https://huggingface.co/papers/2501.03841))| Wenlong Gao, Tianshu Wu, Ergogogogo, JiyaoZhang, pmj110119 | Here is a concise summary of the paper "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints":  i) The paper introduces OmniManip, a novel system for open-vocabulary robotic manipulation that uses object-centric interaction primitives as spatial constraints to bridge the gap between vision-language models (VLMs) and low-level precision. ii) The main research objective is to develop a more efficient and generalizable representation that bridges VLM high-level reasoning with precise, low-level robotic manipulation. iii) The key methodology involves using a dual closed-loop system: one loop for high-level planning through primitive resampling, interaction rendering, and VLM checking, and another for low-level execution via 6D pose tracking, along with representing object interactions within a canonical space to define actionable 3D spatial constraints. iv) Primary results show that OmniManip achieved a 68.3% success rate in closed-loop, zero-shot generalization across diverse robotic manipulation tasks, outperforming the best baseline (ReKep) which achieved 45.0%. v) The principal implication for AI practitioners is that OmniManip provides a framework for automating large-scale simulation data generation and developing robotic systems capable of robust, real-time control without requiring VLM fine-tuning.  |
| VideoRAG: Retrieval-Augmented Generation over Video Corpus (Read more on [arXiv](https://arxiv.org/abs/2501.05874) or [HuggingFace](https://huggingface.co/papers/2501.05874))| Sung Ju Hwang, jinheon, KangsanKim71, starsuzi | VideoRAG introduces a novel framework for retrieval-augmented generation using video corpora.  The research objective was to improve factual accuracy in large language models by dynamically retrieving and incorporating relevant video content into the generation process.  The methodology involved leveraging large video language models (LVLMs) to process both visual and textual information from videos for retrieval and generation.  Results showed VideoRAG-VT (using both visual and textual video features) achieved a ROUGE-L score of 0.252, significantly outperforming text-only baselines.  This demonstrates the efficacy of incorporating video data into RAG, suggesting that incorporating multimodal data, particularly video, enhances the accuracy and quality of generated responses.  |
| OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? (Read more on [arXiv](https://arxiv.org/abs/2501.05510) or [HuggingFace](https://huggingface.co/papers/2501.05510))| qiaozc, zyh, HelloJiang, Niujunbo2002, JoeLeelyf | OVO-Bench is a new benchmark for evaluating online video understanding capabilities of Video Large Language Models (Video-LLMs). The main research question is: How effective are current Video-LLMs at understanding video content in an online, real-world setting where questions are posed at specific timestamps? The key methodology involves creating a dataset (OVO-Bench) of 644 videos with 2,814 human-curated meta-annotations, and evaluating nine Video-LLMs using a pipeline that queries models along the video timeline under three scenarios (Backward Tracing, Real-Time Understanding, Forward Active Responding). The primary results show that even the best-performing model, Gemini 1.5 Pro, achieved only 65.25% overall accuracy, significantly lower than human performance, and forward active responding accuracy was 57.15%. The principal implication for AI practitioners is that current Video-LLMs still struggle with online video understanding tasks that require temporal awareness, highlighting a need for model development focusing on real-time processing and continuous adaptation to incoming video streams.  |
| LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2501.06186) or [HuggingFace](https://huggingface.co/papers/2501.06186))| Dinura Dissanayake, hishamcholakkal, ahmedheakl, Ritesh-hf, omkarthawakar | LlamaV-01 introduces a framework for advancing step-by-step visual reasoning in large language models (LLMs). The main research objective is to develop a comprehensive framework for evaluating and enhancing step-by-step visual reasoning in LLMs, addressing the limitations of current models that primarily focus on end-task accuracy. The key methodology includes the introduction of a new benchmark (VRC-Bench) for multi-step reasoning, a novel metric evaluating reasoning quality at the step level, and a new multimodal visual reasoning model (LlamaV-01) trained using a multi-step curriculum learning approach. The primary results show that LlamaV-01 achieves an average score of 67.3 across six benchmarks, with an absolute gain of 3.8% over the Llava-CoT model while being 5x faster during inference. The principal implication for AI practitioners is that using this framework, including the VRC-Bench and the LlamaV-01 model, can lead to more accurate, interpretable, and efficient visual reasoning systems.  |
| Enabling Scalable Oversight via Self-Evolving Critic (Read more on [arXiv](https://arxiv.org/abs/2501.05727) or [HuggingFace](https://huggingface.co/papers/2501.05727))| Losin94, Benyou, yeshoubaizi, ziniuli, tangzhy | This paper introduces SCRIT, a framework that enables the self-evolution of critique abilities in large language models (LLMs) for scalable oversight. The main research question is how to enhance the critique capabilities of LLMs without relying on external supervision from humans or stronger models. The key methodology used is a two-step process involving contrastive-based self-critic generation using reference solutions and a self-validation mechanism that ensures critique quality through correction outcomes, followed by self-training on the validated data. The primary results show that SCRIT, implemented with Qwen2.5-72B-Instruct, achieves up to a 10.3% improvement on critique-correction and error identification benchmarks, with the average F1 score on error identification tasks rising from 37.8% to 45.0%. The principal implication for AI practitioners is that SCRIT offers a method for improving LLMs' abilities to critique and correct mathematical reasoning problems without the need for costly human annotations or access to more powerful models, demonstrating a path towards more autonomous model refinement.  |
| ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning (Read more on [arXiv](https://arxiv.org/abs/2501.04698) or [HuggingFace](https://huggingface.co/papers/2501.04698))| Ruimao, Xintao, Qiulin, ziyangy, Yuzhou914 | ConceptMaster is introduced as a novel framework for multi-concept video customization using diffusion transformer models without requiring test-time tuning. The main research question is how to achieve high-fidelity multi-concept video customization while effectively decoupling identities and maintaining concept fidelity. The key methodology involves learning decoupled multi-concept embeddings via a Decouple Attention Module (DAM) and injecting them into diffusion models using a standalone Multi-Concept Injector (MC-Injector), alongside a data construction pipeline for creating high-quality multi-concept video-entity pairs. The primary result is that ConceptMaster achieved a score of 22.378 on identity decoupling, outperforming other compared methods on the MC-Bench benchmark. The principal implication for AI practitioners is that ConceptMaster provides an effective method for generating personalized and semantically accurate videos across multiple concepts without the need for additional test-time tuning, enhancing the practicality of video customization in real-world applications.  |
| Multi-subject Open-set Personalization in Video Generation (Read more on [arXiv](https://arxiv.org/abs/2501.06187) or [HuggingFace](https://huggingface.co/papers/2501.06187))| universome, studyfang, willi-menapace, aliaksandr-siarohin, tschen | Video Alchemist is introduced, a video generation model capable of multi-subject, open-set personalization for foreground objects and backgrounds without test-time optimization. The main research objective is to develop a video personalization model that can incorporate multiple subjects and open-set entities into generated videos without requiring fine-tuning for new concepts. The key methodology involves a new Diffusion Transformer module that fuses conditional reference images and corresponding subject-level text prompts with cross-attention layers, along with a data construction pipeline featuring extensive image augmentations. The primary result is that Video Alchemist outperforms existing personalization methods, achieving a 23.2% higher subject similarity than VideoBooth in quantitative evaluations. For AI practitioners, Video Alchemist offers a new approach to video generation with enhanced personalization capabilities, directly applicable to creating customized videos with specific subjects and contexts.  |
| ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding (Read more on [arXiv](https://arxiv.org/abs/2501.05452) or [HuggingFace](https://huggingface.co/papers/2501.05452))| danielpaulroth, jw2yang, zyang39, mqliu, Fiaa | ReFocus is a framework that equips multimodal Large Language Models (LLMs) with the ability to generate "visual thoughts" by performing visual editing on structured images such as tables and charts. The main research question is how to improve multimodal LLMs' selective attention and multi-hop visual reasoning capability on structured images. The key methodology involves prompting LLMs to generate Python code to call visual editing tools that modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas to enhance visual reasoning. The primary results show that ReFocus improves performance on table and chart understanding tasks, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks over GPT-4o without visual editing. For AI practitioners, ReFocus offers a simple yet effective framework to enhance multimodal LLMs' performance on structured image understanding by integrating visual reasoning as an intermediate step.  |
| Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains (Read more on [arXiv](https://arxiv.org/abs/2501.05707) or [HuggingFace](https://huggingface.co/papers/2501.05707))| Shuang Li, Joshua B. Tenenbaum, Antoniotorralbaborruel, yilundu, vsub851 | This paper introduces a multiagent finetuning approach for improving large language models (LLMs) through self-generated synthetic data. The main research question is whether finetuning a multiagent society of LLMs, rather than a single model, can enhance reasoning performance and preserve diversity over multiple rounds of self-improvement. The key methodology involves specializing independent LLMs as generation or critic agents via finetuning on data generated through multiagent debate, followed by iterative finetuning of these agents on their own generated data. The primary result is that across five rounds of finetuning using the Phi-3 model, the accuracy of multiagent finetuning improved from 58.8% to 66.0% on the MATH dataset. The principal implication is that AI practitioners can leverage multiagent finetuning to enhance LLM performance beyond the limitations of single-agent self-improvement, particularly on complex reasoning tasks.  |
| Infecting Generative AI With Viruses (Read more on [arXiv](https://arxiv.org/abs/2501.05542) or [HuggingFace](https://huggingface.co/papers/2501.05542))| fgmckee, dnoever | Here is a concise summary of the research paper:  i) This study examines the security of Vision-Language Models (VLMs) by embedding the EICAR test file in JPEG images and assessing the models' ability to handle and potentially execute it. ii) The main research objective is to evaluate whether VLMs can be used as a vector to transport, manipulate, and potentially execute a surrogate malware (EICAR) embedded within image files. iii) The key methodology involved appending the EICAR string to JPEG images, uploading them to various LLMs, and using Python scripts within the LLMs' environments to extract and manipulate the embedded string. iv) The primary results showed that the EICAR string could be consistently masked in image metadata, and successfully extracted using Python within the LLM environments; for example, 1 out of 55 virus detectors flagged the initial pixel file with the appended EICAR string. v) The principal implication for AI practitioners is the need to develop robust file inspection methods for VLMs to detect and prevent the manipulation of potentially malicious code embedded in image files.  |
