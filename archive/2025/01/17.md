

## Papers for 2025-01-17

| Title | Authors | Summary |
|-------|---------|---------|
| OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking (Read more on [arXiv](https://arxiv.org/abs/2501.09751) or [HuggingFace](https://huggingface.co/papers/2501.09751))| Ningyu, Runnaning, callanwu, JizhanFang, ZekunXi | OmniThink is a novel machine writing framework that emulates human-like iterative expansion and reflection to enhance the quality of generated long-form articles. The main research question is whether simulating the cognitive behavior of learners through continuous reflection and exploration can improve the knowledge density and quality of machine-generated articles. The key methodology involves an iterative process of expansion, using search engines to retrieve information and construct an information tree, and reflection, refining retrieved information and updating a conceptual pool to guide further expansion. Primary results show that OmniThink achieved a knowledge density of 22.31 when using GPT-4o as a backbone, surpassing the Co-STORM model's knowledge density of 19.53. The principal implication for AI practitioners is that incorporating iterative expansion and reflection processes in machine writing can enhance the information density and novelty of generated content without compromising coherence or depth.  |
| Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps (Read more on [arXiv](https://arxiv.org/abs/2501.09732) or [HuggingFace](https://huggingface.co/papers/2501.09732))| mingdazhang, ycsu, hexianghu, S8T, willllis | This paper explores inference-time scaling for diffusion models by optimizing the sampling process through noise search. The main research question is how to improve the generation performance of diffusion models by increasing computation during inference beyond simply increasing denoising steps. The key methodology involves formulating the search for optimal initial noise as a search problem, using verifiers to evaluate candidates and algorithms to refine noise candidates iteratively. The primary results show that increasing inference-time compute via search significantly improves sample quality, with a 3.6% relative improvement in the LLM Grader metric when using the Verifier Ensemble on the DrawBench dataset with 3840 NFEs allocated to search. The principal implication for AI practitioners is that allocating computational resources to noise search during inference can substantially enhance the performance of diffusion models across various tasks, offering a new avenue for scaling beyond training-time optimization.  |
| Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators (Read more on [arXiv](https://arxiv.org/abs/2501.09484) or [HuggingFace](https://huggingface.co/papers/2501.09484))| Quan Tu, hsaest, ShizhengLi, sdujq, zhaocheng | This paper investigates the relationship between inquiry and diagnosis in online medical consultations using AI patient simulators. The main research question is how the quality of inquiries generated by different doctor models impacts diagnostic accuracy in a simulated online medical consultation setting. The key methodology involved training a patient simulator on synthesized doctor-patient dialogues, then using it to evaluate the inquiry-diagnosis relationship by interacting with various doctor models and assessing subsequent diagnostic accuracy. A primary result was that inquiries generated by the Claude model had consistently lower diagnostic accuracy compared to other models such as GPT-40, with Claude achieving 43.9% accuracy after 5 inquiry rounds compared to GPT-40's 48.1% when diagnosed by the 01-preview model. The principal implication for AI practitioners is that the quality of inquiries significantly affects diagnostic accuracy, suggesting that developing models with robust inquiry capabilities is crucial for effective AI-driven medical diagnosis.  |
| SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces (Read more on [arXiv](https://arxiv.org/abs/2501.09756) or [HuggingFace](https://huggingface.co/papers/2501.09756))| Jingyuan Liu, Yannick Hold-Geoffroy, Sumit Chaturvedi, zhixinshu, mengweir | SynthLight is a diffusion model for portrait relighting that learns to re-render synthetic faces based on changes in environmental lighting conditions. The main research question is how to effectively model portrait relighting as a re-rendering problem using synthetic data and a diffusion model, while bridging the domain gap between synthetic and real images. The key methodology involves training a diffusion model on synthetic portrait pairs generated with a physically-based rendering engine, employing multi-task training with real human portraits, and using an inference-time diffusion sampling procedure based on classifier-free guidance. The primary results show that SynthLight achieves comparable or superior quantitative results to state-of-the-art methods on Light Stage data, with a LPIPS score of 0.165 on the Light Stage test set, and user studies indicate superior visual quality, lighting, and identity preservation. The principal implication for AI practitioners is that SynthLight demonstrates the feasibility of using synthetic data to train a diffusion model for high-quality portrait relighting, offering a viable alternative to methods relying on real-world labeled data, such as Light Stage data.  |
| FAST: Efficient Action Tokenization for Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2501.09747) or [HuggingFace](https://huggingface.co/papers/2501.09747))| oier-mees, dannydriess, brianichter, kylestach, KarlP | This paper introduces FAST, a new action tokenization method for training vision-language-action (VLA) models based on the discrete cosine transform (DCT). The main research objective is to develop an action tokenization scheme that enables efficient training of autoregressive VLA policies on high-frequency and highly dexterous robot action data. The key methodology involves applying DCT to action sequences, quantizing the resulting coefficients, and compressing them using byte-pair encoding (BPE). The primary results show that VLA models trained with FAST achieve comparable performance to state-of-the-art diffusion-based models while reducing training time by up to 5x. The principal implication is that AI practitioners can use FAST as an efficient and effective action tokenizer to train high-performing autoregressive VLA models for robotic control, especially for tasks requiring high-frequency actions.  |
| Learnings from Scaling Visual Tokenizers for Reconstruction and Generation (Read more on [arXiv](https://arxiv.org/abs/2501.09755) or [HuggingFace](https://huggingface.co/papers/2501.09755))| David Yan, Philippe Hansen-Estruch, endernewton, Tingbo, orrzohar | Here is a concise summary of the research paper:  i) The paper explores scaling properties of Transformer-based auto-encoders, termed ViTok, for visual tokenization in image and video reconstruction and generation tasks. ii) The main research objective is to investigate how design choices and scaling of auto-encoder components influence reconstruction and downstream generative performance. iii) The key methodology involves replacing convolutional backbones with a Vision Transformer (ViT) architecture enhanced with Llama, training on large-scale image and video datasets, and systematically scaling the bottleneck size, encoder, and decoder to analyze their impacts. iv) A primary result is that scaling the bottleneck size E to 8192 for ViTok S-B/16 achieves a rFID score of 0.8 on 256p image reconstruction, but increasing E beyond an optimal point degrades generative performance. v) For AI practitioners, the principal implication is that scaling the decoder while optimizing the bottleneck size E enhances reconstruction performance, but scaling the encoder does not consistently improve reconstruction or generation, which indicates the importance of focusing scaling efforts on the decoder and bottleneck.  |
| RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation (Read more on [arXiv](https://arxiv.org/abs/2501.08617) or [HuggingFace](https://huggingface.co/papers/2501.08617))| Jaime Fern√°ndez Fisac, Thomas L. Griffiths, Ryan Liu, Haimin Hu, kaiquliang | Generative AI systems can be aligned with human values by using Reinforcement Learning from Hindsight Simulation (RLHS), a novel method introduced to improve upon Reinforcement Learning from Human Feedback (RLHF). The main research question is whether decoupling human feedback from the prediction of downstream outcomes can mitigate misalignment in RLHF. The key methodology used is hindsight simulation, where evaluators are shown simulated downstream outcomes of an interaction before providing feedback on model behavior. The primary result is that RLHS consistently outperforms RLHF in human user studies, with models trained using RLHS achieving a higher true utility score (0.43) compared to RLHF models (-0.16). The principal implication for AI practitioners is that using hindsight simulation during training can significantly reduce model misalignment with human values, leading to more truthful and helpful AI assistants.  |
| Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.09686) or [HuggingFace](https://huggingface.co/papers/2501.09686))| Ouyangtj, zhazhahui7, berserkerko, zzfoutofspace, haohao11 | Large language models (LLMs) are being enhanced through reinforcement learning to improve their reasoning capabilities for complex tasks. The main research objective is to develop methods for training and deploying LLMs as "Large Reasoning Models" capable of advanced, human-like reasoning. Key methodologies include automated data construction via process reward models (PRMs), reinforcement learning from AI feedback (RLAIF), and test-time scaling with PRM-guided search. Primary results show that the "01" model series achieves 83.3% success in competitive programming through structured analytical approach and knowledge integration, demonstrating significant improvements in reasoning tasks. The principal implication for AI practitioners is that integrating "thought" sequences and scaling computation during both training and test times can substantially enhance LLMs' reasoning abilities, paving the way for more powerful reasoning AI systems.  |
| AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2501.09503) or [HuggingFace](https://huggingface.co/papers/2501.09503))| Junjie He, Liefeng, gengyifeng, ashui, tuoyuxiang | Here is a concise summary of the research paper "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation":  i) AnyStory is a unified framework for generating personalized images of single or multiple subjects from text prompts while preserving subject fidelity and alignment with descriptions. ii) The main research objective is to develop a method for high-fidelity personalized text-to-image generation that can handle both single and multiple subjects without blending or sacrificing details. iii) The key methodology involves an "encode-then-route" approach, using a simplified ReferenceNet combined with a CLIP vision encoder for subject encoding and a decoupled instance-aware subject router for guiding subject condition injection during the denoising process. iv) The primary results show that AnyStory effectively preserves subject details, aligns with text descriptions, and personalizes multiple subjects; the simplified ReferenceNet achieves a speed of 53.2 ms/img with 2.02 billion parameters. v) For AI practitioners, AnyStory offers a method to generate high-fidelity personalized images with multiple subjects, directly improving the development of applications requiring precise control over subject representation in text-to-image generation.  |
| CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation (Read more on [arXiv](https://arxiv.org/abs/2501.09433) or [HuggingFace](https://huggingface.co/papers/2501.09433))| Junyoung Choi, Jeong A Wi, Seongyeong Lee, Hwan Heo, longshiine | CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation is a framework for generating high-fidelity 3D assets from textual or visual inputs. The main research objective is to develop a method for generating high-quality 3D assets that overcomes challenges like multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. The key methodology involves a two-stage process: (1) a 3D latent diffusion model guided by multi-view inputs to generate geometry and (2) a model-agnostic Spatially Decoupled Attention framework to synthesize high-resolution textures, followed by a 3D-aware occlusion inpainting algorithm. The primary results demonstrate that CaPa generates high-quality 3D assets in under 30 seconds, achieving a CLIP score of 86.34 and an FID score of 47.56, outperforming existing methods. For AI practitioners, CaPa provides an efficient pipeline to generate high-quality textured 3D meshes ready for commercial applications, representing a significant advancement in practical, scalable 3D asset generation.  |
| Do generative video models learn physical principles from watching videos? (Read more on [arXiv](https://arxiv.org/abs/2501.09038) or [HuggingFace](https://huggingface.co/papers/2501.09038))| Priyank Jaini, Laura Culp, rgeirhos, kswersky, sam-motamed | This research investigates whether generative video models acquire an understanding of physical principles from video data. The main research question is: Do generative video models learn the physical principles that underpin reality from passively "watching" videos? The key methodology involves creating a benchmark dataset, Physics-IQ, to test models' ability to predict video continuations that require understanding physics, such as solid mechanics, fluid dynamics, and optics. The primary results show that current video models, including Sora and Runway Gen 3, exhibit limited physical understanding, with the best model achieving only a 24.1% Physics-IQ score, where 100% represents the upper bound based on physical variance in real-world videos. The principal implication for AI practitioners is that generating visually realistic videos does not equate to understanding the underlying physical principles, suggesting a need for new methods to incorporate physics into video generation models.  |
