

## Papers for 2025-01-06

| Title | Authors | Summary |
|-------|---------|---------|
| EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation (Read more on [arXiv](https://arxiv.org/abs/2501.01895) or [HuggingFace](https://huggingface.co/papers/2501.01895))| jzzzzk, Shengcong, lyuukuu, pathcn, SiyuanH | Here's a concise summary of the research paper:  i) ENERVERSE is a comprehensive framework for embodied future space generation designed for robotic manipulation tasks, integrating a novel chunk-wise autoregressive diffusion model with a Free Anchor View (FAV) space and a 4D Gaussian Splatting (4DGS) data engine pipeline. ii) The main research objective is to develop a method for generating embodied future spaces that enhances a robot's ability to perform long-range manipulation tasks by improving predictive capabilities and spatial understanding. iii) The key methodology involves a chunk-wise autoregressive diffusion model with a sparse contextual memory mechanism, a FAV-based 4D future space generation method, and a data flywheel pipeline integrating 4DGS optimization with multi-view video generation. iv) The proposed method achieved a state-of-the-art average success rate of 88.5 on the LIBERO benchmark with a Three Third View configuration. v) For AI practitioners, the principal implication is that integrating ENERVERSE's future space generation prior into policy learning can significantly enhance the performance of robotic systems, particularly in complex, long-range manipulation tasks, by leveraging enhanced spatial understanding and a robust data generation pipeline.  |
| VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction (Read more on [arXiv](https://arxiv.org/abs/2501.01957) or [HuggingFace](https://huggingface.co/papers/2501.01957))| hertin, shenyunhang, yifanzhang114, xiongwang, linhaojia13 | VITA-1.5 is a multimodal large language model designed for real-time vision and speech interaction. The main research objective is to develop a model that integrates vision, language, and speech modalities without compromising performance due to modality differences. The key methodology involves a three-stage training process: vision-language training, audio input tuning, and audio output tuning, progressively incorporating each modality. The primary results show that VITA-1.5 achieves a Character Error Rate (CER) of 2.2 on the aishell-1 Mandarin speech recognition benchmark and maintains comparable performance to state-of-the-art models in vision tasks after audio training. The principal implication for AI practitioners is that VITA-1.5 provides an effective framework for building multimodal AI systems with near real-time vision and speech interaction capabilities, eliminating the need for separate ASR and TTS modules.  |
| Virgo: A Preliminary Exploration on Reproducing o1-like MLLM (Read more on [arXiv](https://arxiv.org/abs/2501.01904) or [HuggingFace](https://huggingface.co/papers/2501.01904))| jrwen, whenfra, yifanli, JohnCage, Richard1999 | Virgo is a multimodal slow-thinking system developed by fine-tuning a capable MLLM with a small amount of textual long-form thought data. The main research question is whether slow-thinking ability can be transferred across modalities through fine-tuning with text-based long-thought data and if this ability is comparable to that distilled from multimodal slow-thinking systems. The key methodology involves fine-tuning Qwen2-VL-72B-Instruct with textual and visual long-thought instruction datasets, including data distilled from other slow-thinking models. The primary result is that Virgo-72B, fine-tuned with 5K textual instructions, achieved 48.4% accuracy on MathVerse, which is comparable to or surpasses commercial reasoning systems. The principal implication for AI practitioners is that fine-tuning MLLMs with textual long-form thought data can effectively transfer slow-thinking capacities, suggesting a simpler approach to developing such systems.  |
| VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation (Read more on [arXiv](https://arxiv.org/abs/2412.21059) or [HuggingFace](https://huggingface.co/papers/2412.21059))| Jiajun Xu, Yuanming Yang, Jiale Cheng, Yu Huang, xujz0703 | Here is a concise summary of the research paper "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation":  i) The paper introduces VisionReward, a fine-grained, multi-dimensional reward model for aligning visual generation models with human preferences, and a Multi-Objective Preference Optimization (MPO) algorithm for stable model tuning. ii) The main research objective is to develop a reward model that accurately and interpretably predicts human preferences in both image and video generation, addressing the limitations of existing reward models and optimization methods. iii) The key methodology involves decomposing human preferences into multiple dimensions, represented by a series of judgment questions, linearly weighted and summed to produce an interpretable score, and using a multi-objective preference learning algorithm to address confounding factors in preference data. iv) The primary results show that VisionReward surpasses existing methods in video preference prediction, outperforming VideoScore by 17.2% in accuracy. v) The principal implication for AI practitioners is that they can use VisionReward to better align image and video generation models with human preferences, leading to more satisfactory outputs in visual content creation.  |
| Graph Generative Pre-trained Transformer (Read more on [arXiv](https://arxiv.org/abs/2501.01073) or [HuggingFace](https://huggingface.co/papers/2501.01073))| XiaolinXu, y6q9, RArchered, Spony, xchen16 | 1. Summary: The paper introduces the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that generates graphs as sequences of nodes and edges, utilizing a transformer decoder for next-token prediction, and explores fine-tuning for goal-oriented generation and property prediction. 2. Main research question or objective: The main objective is to develop an efficient graph generative model that leverages a novel sequence-based representation and auto-regressive transformer architecture. 3. Key methodology used: The key methodology involves representing graphs as sequences, training a transformer decoder on these sequences using next-token prediction, and applying fine-tuning strategies such as rejection sampling and reinforcement learning for downstream tasks. 4. Primary results: G2PT achieves superior performance on generic graph and molecule datasets; for instance, on the MOSES dataset, G2PT achieves a validity score of 97.2 and an FCD score of 1.02. 5. Principal implication for AI practitioners: AI practitioners can utilize G2PT as a versatile framework for graph generation and property prediction tasks, benefiting from its strong adaptability and superior performance demonstrated across multiple datasets.  |
| LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.00874) or [HuggingFace](https://huggingface.co/papers/2501.00874))| anoperson, Franck-Dernoncourt, ryanrossi, ntnghia1811, Hieuman | LUSIFER is a zero-shot approach that enhances multilingual embeddings of English-centric large language models (LLMs) without requiring multilingual training data. The main research objective is to adapt LLM-based embedding models for multilingual tasks without requiring explicit multilingual supervision. The key methodology involves integrating a multilingual encoder (XLM-R) with an English-centric LLM (Mistral-7B) using a connector with minimal trainable parameters, trained in two stages: alignment and representation finetuning. The primary result is that LUSIFER achieved a state-of-the-art average score of 62.63 across 14 languages on five embedding tasks, outperforming the previous best baseline by 3.19 points. For AI practitioners, LUSIFER offers an effective method to enhance multilingual performance of English-centric LLM embedding models without the need for multilingual training data or architectural modifications, significantly improving performance in medium and low-resource languages.  |
| BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery (Read more on [arXiv](https://arxiv.org/abs/2501.01540) or [HuggingFace](https://huggingface.co/papers/2501.01540))| Louise Li, Lyle Goodyear, ngoodman, michaelyli, obiwan96 | BoxingGym is a benchmark for evaluating AI agents on scientific reasoning tasks. **Main research question or objective:** How well can current language models perform automated experimental design and model discovery in a variety of scientific domains? **Key methodology used:** The authors introduce BoxingGym, a benchmark with 10 environments based on real-world scientific models, where agents interact by proposing experiments, observing outcomes, and refining models, evaluated using expected information gain (EIG) and a communication-based model discovery metric. **Primary results:** GPT-4o struggles with both experimental design and model discovery, with an average standardized prediction error of 0.74 on the hyperbolic discounting choice task after 10 experiments. Augmenting the agent with an explicit statistical model does not reliably improve these results. **Principal implication for AI practitioners:** The benchmark highlights significant limitations of current large language models (LLMs) in performing scientific reasoning, suggesting a need for developing new methods for automated experimental design and model discovery.  |
