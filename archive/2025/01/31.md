

## Papers for 2025-01-31

| Title | Authors | Summary |
|-------|---------|---------|
| GuardReasoner: Towards Reasoning-based LLM Safeguards (Read more on [arXiv](https://arxiv.org/abs/2501.18492) or [HuggingFace](https://huggingface.co/papers/2501.18492))| lakxtxue, JunXia97, zsf, HongchengGao, yueliu1998 | GuardReasoner is a reasoning-based safeguard for large language models (LLMs) that improves performance, explainability, and generalizability. The main research objective is to develop a guard model that can effectively moderate LLM inputs and outputs by incorporating reasoning capabilities. The key methodology involves creating a new dataset, GuardReasonerTrain, with 127K samples and 460K reasoning steps, and using reasoning supervised fine-tuning (R-SFT) and hard sample direct preference optimization (HS-DPO) to train the model. The primary result is that GuardReasoner 8B surpasses GPT-40+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average across 13 benchmarks. The principal implication for AI practitioners is that incorporating explicit reasoning steps into guard models can significantly enhance their ability to detect and mitigate harmful content, offering a more robust and explainable safeguard mechanism for LLMs.  |
| MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding (Read more on [arXiv](https://arxiv.org/abs/2501.18362) or [HuggingFace](https://huggingface.co/papers/2501.18362))| Zhangren Chen, Yifei Li, Yuxin Zuo, stingning, lindsay-qu | MedXpertQA is introduced, a new benchmark for evaluating expert-level medical reasoning and understanding in AI systems.  i)  MedXpertQA, a challenging and comprehensive medical benchmark, is introduced to evaluate expert-level medical knowledge and advanced reasoning in AI. ii) The main research objective is to create a benchmark, MedXpertQA, that addresses limitations of existing medical AI benchmarks by incorporating specialty board questions, improving clinical relevance, and mitigating data leakage. iii) The key methodology involves curating a large-scale question bank from professional medical exams and textbooks, filtering questions using AI and human expert evaluation, augmenting data via model-based rewriting, and conducting multiple rounds of expert reviews to ensure quality. iv) The primary results show that leading AI models, such as GPT-4o, achieve limited performance on MedXpertQA, with GPT-4o achieving 35.96% average accuracy, indicating the benchmark's difficulty. v) The principal implication for AI practitioners is that MedXpertQA provides a rigorous tool for evaluating and improving medical AI systems, particularly in complex reasoning tasks, driving advancements towards more reliable and clinically applicable AI in healthcare.  |
| Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs (Read more on [arXiv](https://arxiv.org/abs/2501.18585) or [HuggingFace](https://huggingface.co/papers/2501.18585))| yudian, freesunshine0316, zwhe99, Jiahao004, Dennis364 | Large language models (LLMs) termed "o1-like" exhibit a tendency to switch reasoning strategies prematurely, leading to a phenomenon called "underthinking." The main research question is whether o1-like LLMs are thinking deeply enough when solving complex reasoning tasks. The key methodology involved analyzing thought-switching patterns in model responses and introducing a decoding strategy with thought-switching penalties. Primary results showed that incorrect answers from o1-like models had 418% more frequent thought-switching behaviors than correct answers. The principal implication for AI practitioners is that addressing underthinking through techniques like the proposed thought-switching penalty can improve the accuracy of o1-like LLMs on challenging datasets without requiring model fine-tuning.  |
| PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding (Read more on [arXiv](https://arxiv.org/abs/2501.16411) or [HuggingFace](https://huggingface.co/papers/2501.16411))| Vitor Guizilini, Daniel Seita, Jiageng Mao, Boyiliee, WeiChow | PhysBench is a benchmark for evaluating vision-language models' (VLMs) understanding of the physical world through analysis of video, image, and text data. The main research question is whether existing VLMs possess an understanding of the physical world and how this understanding can be enhanced to improve embodied agent performance. The key methodology used involves the development of the PhysBench dataset, comprising 10,002 video-image-text entries across four physical domains, and a novel framework called PhysAgent that integrates vision foundation models and a physics knowledge memory to enhance VLMs. Primary results show that while state-of-the-art VLMs like GPT-4o achieve an average accuracy of 49.49% on PhysBench, the proposed PhysAgent framework improves GPT-4o's performance by 18.4%. The principal implication for AI practitioners is that enhancing VLMs with specialized vision models and physics knowledge can significantly improve their physical world understanding, thereby facilitating the development of more capable embodied agents.  |
| Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch (Read more on [arXiv](https://arxiv.org/abs/2501.18512) or [HuggingFace](https://huggingface.co/papers/2501.18512))| Zachary Charles, Satyen Kale, Keith Rush, Yanislav Donchev, Arthur Douillard | Training large language models (LLMs) can be distributed across non-colocated devices with reduced communication bandwidth using Streaming DiLoCo. The main research question is how to minimize peak bandwidth requirements and mitigate worker-blocking during distributed training of LLMs without compromising learning efficiency. The key methodology involves synchronizing subsets of model parameters in sequence, overlapping communication with computation, and quantizing the exchanged data. The primary results show that Streaming DiLoCo achieves similar performance to data-parallel training while reducing the required bandwidth by two orders of magnitude; for instance, a 1 billion parameter model achieved an evaluation loss of 2.50 with Streaming DiLoCo versus 2.49 with Data-Parallel. The principal implication for AI practitioners is that they can train LLMs across distributed devices with significantly lower bandwidth requirements, enabling more geographically distributed training setups and potentially reducing infrastructure costs.  |
| WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training (Read more on [arXiv](https://arxiv.org/abs/2501.18511) or [HuggingFace](https://huggingface.co/papers/2501.18511))| Chinmay Hegde, penfever | WILDCHAT-50M is a large-scale dataset of synthetic chat transcripts for improving language model post-training. The main research question is how the choice of data-generating model (DGM) impacts the synthetic data quality (SDQ) and downstream performance of language models (LLMs) after supervised fine-tuning (SFT). The key methodology involves generating chat transcripts using 50 different open-weight models ranging from 0.5B to 104B parameters and evaluating the performance of LLMs fine-tuned on these synthetic datasets using a mix of ground-truth and LLM-judge benchmarks. The primary results show that the choice of DGM significantly affects downstream benchmark performance, with fine-tuning on the RE-WILD data mix outperforming the Tulu-3 SFT mix by an average of 0.039 points across nine benchmarks. The principal implication for AI practitioners is that carefully selecting a high-quality DGM for generating synthetic data can compensate for a smaller dataset size and improve the performance of LLMs on generalist chat and instruction-following tasks.  |
| o3-mini vs DeepSeek-R1: Which One is Safer? (Read more on [arXiv](https://arxiv.org/abs/2501.18438) or [HuggingFace](https://huggingface.co/papers/2501.18438))| Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta | Here is a concise summary of the research paper "o3-mini vs DeepSeek-R1: Which One is Safer?":  i) This paper presents a comparative analysis of the safety alignment of two large language models, OpenAI's o3-mini and DeepSeek-R1, using the automated safety testing tool ASTRAL. ii) The main research objective was to determine which of the two models exhibits a higher level of safety when responding to unsafe prompts. iii) The key methodology involved generating 1260 unsafe test inputs using ASTRAL and evaluating the safety of the models' responses through automated and manual assessment. iv) Primary results indicate that DeepSeek-R1 responded unsafely to 11.98% of the prompts, while o3-mini responded unsafely to only 1.19%. v) The principal implication for AI practitioners is that DeepSeek-R1 may require further refinement to improve its safety alignment, and practitioners should be aware of the potential for unsafe responses when deploying this model.  |
| Large Language Models Think Too Fast To Explore Effectively (Read more on [arXiv](https://arxiv.org/abs/2501.18009) or [HuggingFace](https://huggingface.co/papers/2501.18009))| Robert C. Wilson, xhb120633, louanna | Summary of the research paper is: The study investigates exploration capabilities of Large Language Models (LLMs) in an open-ended task, revealing that most LLMs underperform compared to humans due to a tendency to make premature decisions. The main research question is whether LLMs can explore effectively in an open-ended task, comparable to humans. The key methodology involves using the game Little Alchemy 2 as a paradigm, applying regression models to analyze exploration strategies, and using Sparse Autoencoders (SAE) to probe latent representations of exploration-related values. The primary results show that o1 significantly outperformed humans (t = 9.71, p < 0.001), while other LLMs performed worse, with most models relying primarily on uncertainty-driven strategies. The principal implication for AI practitioners is that the current architecture of traditional LLMs may hinder effective exploration in open-ended tasks due to their tendency to process uncertainty and choices much earlier than empowerment values.  |
