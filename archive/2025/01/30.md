

## Papers for 2025-01-30

| Title | Authors | Summary |
|-------|---------|---------|
| Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate (Read more on [arXiv](https://arxiv.org/abs/2501.17703) or [HuggingFace](https://huggingface.co/papers/2501.17703))| Xiang Yue, wenhu, ubowang | Critique Fine-Tuning (CFT) is more effective than Supervised Fine-Tuning (SFT) for enhancing mathematical reasoning in language models. The main research question is whether training language models to critique noisy responses is more effective than traditional imitation learning for improving mathematical reasoning. The key methodology involves constructing a 50K-sample dataset from WebInstruct and training models to provide critiques on query-response pairs using GPT-4o as a teacher. The primary result is that the Qwen2.5-Math-7B-CFT model achieved 56.0% average accuracy on mathematical reasoning benchmarks, outperforming the best SFT-trained model by 5.7%. The principal implication for AI practitioners is that CFT offers a more data-efficient and effective alternative to SFT for enhancing reasoning capabilities in large language models, as evidenced by the model trained on just 50K samples outperforming others trained on over 2M samples.  |
| Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts (Read more on [arXiv](https://arxiv.org/abs/2501.14334) or [HuggingFace](https://huggingface.co/papers/2501.14334))| Simon Gosset, Caroline Vateau, Louis Ladan, Neyri56, clementdesroches | This paper proposes a methodology to estimate the environmental impact of a company's AI portfolio, focusing on Generative AI's increasing energy consumption. The main research objective is to develop a simplified yet exhaustive methodology for estimating the operational and embodied environmental impacts of AI solutions at a company level. The key methodology involves four interconnected models: life cycle impacts of primary components, life cycle impacts of AI use cases, an AI company portfolio model, and 2030 AI Landscape projections. The primary results indicate that large generative AI models consume up to 4600 times more energy than traditional models, and under a high adoption scenario, AI electricity use is projected to rise by a factor of 24.4 by 2030. The principal implication for AI practitioners is the need to adopt standardized environmental assessment frameworks and the "Return on Environment" metric to align AI development with net-zero goals due to the significant environmental impact of generative AI.  |
| Atla Selene Mini: A General Purpose Evaluation Model (Read more on [arXiv](https://arxiv.org/abs/2501.17195) or [HuggingFace](https://huggingface.co/papers/2501.17195))| Kyle Dai, Jackson Golden, Henry Broomfield, Andrei Alexandru, NinaCalvi | Atla Selene Mini is a state-of-the-art small language model fine-tuned for general-purpose evaluation. The main research objective was to develop a small language model-as-a-judge (SLMJ) that outperforms existing SLMJs and GPT-40-mini on diverse evaluation tasks. The key methodology involved curating a training dataset of 577k data points from 16 public datasets, augmented with synthetically generated critiques, filtered for quality, and fine-tuning a Llama 3.1 8B Instruct model using a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss. The primary results showed that Selene Mini achieved an overall task-average performance of 0.756, outperforming other SLMJs and GPT-40-mini. The principal implication for AI practitioners is that Selene Mini provides a high-performing, promptable, and efficient model for automated evaluation, demonstrating strong performance in real-world scenarios and robustness to prompt variations.  |
| Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation (Read more on [arXiv](https://arxiv.org/abs/2501.17749) or [HuggingFace](https://huggingface.co/papers/2501.17749))| Miriam Ugarte, ssegura, japarejo, pablovalle, aitorarrieta | Here is a concise summary of the AI research paper:  The paper presents an external safety evaluation of OpenAI's o3-mini large language model (LLM) using the automated testing tool ASTRAL. The main research objective is to assess the safety of the o3-mini model by generating and executing a large number of unsafe test inputs. The key methodology involved using ASTRAL to automatically generate 10,080 unsafe test inputs (prompts) across 14 safety categories, with variations in writing style and persuasion techniques, and then evaluating the model's responses. The primary results showed that ASTRAL identified 87 unsafe LLM outcomes after manual verification, with the most unsafe outcomes found in the "controversial topics and politics" category. The principal implication for AI practitioners is that automated tools like ASTRAL can effectively identify safety issues in LLMs, but the effectiveness of safety measures may vary across different categories, highlighting the importance of comprehensive testing.  |
| Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation (Read more on [arXiv](https://arxiv.org/abs/2501.17433) or [HuggingFace](https://huggingface.co/papers/2501.17433))| ling1119, sftekin25, tawreos, SihaoHu, TianshengHuang | This paper introduces a novel attack method called Virus that bypasses guardrail moderation in fine-tuning large language models (LLMs). The main research question is whether a harmful fine-tuning attack can bypass guardrail moderation and degrade the safety alignment of victim LLMs. The key methodology is a dual-goal data optimization scheme that optimizes harmful data to simultaneously bypass the guardrail and maintain attack effectiveness. The primary result is that Virus achieves up to a 100% leakage ratio through the guardrail and increases the victim model's harmful score by up to 21.8%. The principal implication for AI practitioners is that relying solely on guardrail moderation for filtering harmful data during fine-tuning is insufficient to maintain the safety alignment of LLMs, and other robust defenses are needed.  |
