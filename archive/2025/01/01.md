

## Papers for 2025-01-01

| Title | Authors | Summary |
|-------|---------|---------|
| Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization (Read more on [arXiv](https://arxiv.org/abs/2412.18525) or [HuggingFace](https://huggingface.co/papers/2412.18525))| Tao Yuan, Yuxin Song, Yifan Sun, Xiu-Shen Wei, axxkaya | The paper introduces Explanatory Instructions, a method for defining computer vision (CV) tasks through natural language descriptions of transformations between input and output images, to improve zero-shot generalization. The main research question is whether Explanatory Instructions can enable vision-language models (VLMs) to genuinely understand and generalize to unseen CV tasks. The key methodology involves constructing a dataset (DECVT) with 12 million triplets of "image input → explanatory instruction → output" and training an auto-regressive-based VLM on these instructions. The primary results show that the trained model achieved instruction-level zero-shot capabilities and promising task-level zero-shot capabilities on certain tasks; for instance, it achieved a F1 score of 20.69 on the zero-shot Canny-to-Image task using the MultiGen-20M dataset. The principal implication for AI practitioners is that Explanatory Instructions can enhance VLMs' ability to perform novel vision tasks without explicit training, although the model's task-level zero-shot generalization ability remains unstable and requires further development.  |
| On the Compositional Generalization of Multimodal LLMs for Medical Imaging (Read more on [arXiv](https://arxiv.org/abs/2412.20070) or [HuggingFace](https://huggingface.co/papers/2412.20070))| Yonglin Deng, Weihong Wang, Rongsheng Wang, Junying Chen, Zhenyang Cai | This paper investigates the compositional generalization (CG) capabilities of Multimodal Large Language Models (MLLMs) for medical imaging. The main research question is whether MLLMs can leverage CG to understand unseen medical images by recombining learned elements (Modality, Anatomical area, and Task). The key methodology involved constructing a dataset called Med-MAT from 106 medical datasets, defining the MAT-Triplet, and evaluating MLLMs' ability to generalize to unseen combinations of these elements through multi-task training and controlled variable experiments. A primary result is that MLLMs trained on multiple tasks achieved 96% accuracy on subset 02 in the in-distribution dataset, significantly outperforming single-task training and demonstrating the effectiveness of CG. The principal implication for AI practitioners is that leveraging CG in MLLMs by training with diverse datasets sharing MAT-Triplets can significantly enhance the models' ability to understand and generalize to unseen medical images, which has a direct impact on the development of robust medical imaging applications.  |
| Bringing Objects to Life: 4D generation from 3D objects (Read more on [arXiv](https://arxiv.org/abs/2412.20422) or [HuggingFace](https://huggingface.co/papers/2412.20422))| Gal Chechik, Dvir Samuel, Ori Malca, Ohad Rahamim | This paper introduces 3to4D, a novel method for generating 4D content from static 3D objects and text prompts. The main research question is how to animate user-provided 3D objects while maintaining their identity and adhering to textual prompts that describe the desired motion. The key methodology involves first converting a 3D mesh into a static 4D Neural Radiance Field (NeRF), then animating it using an Image-to-Video diffusion model conditioned on the initial object and text prompt, with an incremental viewpoint selection protocol and masked Score Distillation Sampling (SDS) loss for improved motion realism. The primary results show that 3to4D outperforms baseline methods, achieving a threefold improvement in identity preservation measured using LPIPS scores (15.0 ±0.1 for 3to4D vs. 44.3 ± 0.2 for the best-performing baseline). The principal implication for AI practitioners is that 3to4D provides a method for creating custom 4D animations from existing 3D assets, leveraging text prompts to guide the desired motion while preserving the original object's visual characteristics.  |
| Efficiently Serving LLM Reasoning Programs with Certaindex (Read more on [arXiv](https://arxiv.org/abs/2412.20993) or [HuggingFace](https://huggingface.co/papers/2412.20993))| Zhongdongming Dai, Zheyu Fu, Siqi Zhu, Junda Chen, Yichao Fu | Dynasor is a system designed to optimize inference-time compute for Large Language Model (LLM) reasoning queries by dynamically allocating resources based on model certainty. The main research question is how to efficiently serve LLM reasoning programs that refine outputs by exploring multiple solution paths. The key methodology involves tracking and scheduling requests within reasoning queries using certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor reduces compute by up to 50% in batch processing and sustains 3.3x higher query rates or 4.7x tighter latency SLOs in online serving compared to prior state-of-the-art systems. The principal implication for AI practitioners is that Dynasor enables more efficient deployment of LLM reasoning algorithms in real-world applications by optimizing resource use and improving response times.  |
| TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2412.21037) or [HuggingFace](https://huggingface.co/papers/2412.21037))| Rafael Valle, Ambuj Mehrish, Zhifeng Kong, Navonil Majumder, Chia-Yu Hung | TangoFlux is a text-to-audio model that uses flow matching and CLAP-ranked preference optimization for fast and high-quality audio generation. The main research objective is to develop an efficient text-to-audio (TTA) generative model that addresses the challenges of aligning TTA models due to the difficulty of creating preference pairs. The key methodology used is CLAP-Ranked Preference Optimization (CRPO), which iteratively generates and optimizes preference data using a CLAP model as a proxy reward model. The primary results show that TangoFlux achieves state-of-the-art performance with a CLAP score of 0.480 and an FD score of 75.1 in just 3.7 seconds using 515M parameters. The principal implication for AI practitioners is that TangoFlux provides a fast and efficient method for generating high-quality audio with fewer trainable parameters, which can be particularly useful in scenarios where inference time and computational resources are constrained.  |
| Edicho: Consistent Image Editing in the Wild (Read more on [arXiv](https://arxiv.org/abs/2412.21079) or [HuggingFace](https://huggingface.co/papers/2412.21079))| Ceyuan Yang, Qiuyu Wang, Yinghao Xu, Hao Ouyang, Qingyan Bai | The paper introduces Edicho, a training-free method for consistent image editing across multiple images using diffusion models. The main research question is how to achieve consistent image editing across diverse in-the-wild images without requiring training. The key methodology involves leveraging pre-estimated explicit image correspondence to guide a modified attention mechanism and classifier-free guidance during the denoising process of diffusion models. The primary results show that Edicho achieves a text alignment score of 0.3228 and an editing consistency score of 0.9355 in global image editing tasks, outperforming existing methods. For AI practitioners, Edicho offers a plug-and-play solution for consistent image editing that can be integrated with existing diffusion-based editing models, enabling applications like generating consistent image sets and 3D reconstruction of edits.  |
| Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs (Read more on [arXiv](https://arxiv.org/abs/2412.21187) or [HuggingFace](https://huggingface.co/papers/2412.21187))| Jianhui Pang, Zhiwei He, Tian Liang, Jiahao Xu, Xingyu Chen | This paper investigates the phenomenon of "overthinking" in o1-like large language models (LLMs), where these models expend excessive computational resources on simple tasks. The main research question is how to quantify and mitigate overthinking in o1-like LLMs during inference. The key methodology involves analyzing solution distributions and proposing outcome and process efficiency metrics, alongside self-training strategies to optimize response generation. A primary result is that the o1-like model QwQ-32B-Preview used 1,953% more tokens than conventional models for the simple query "what is the answer of 2 plus 3?". The principal implication for AI practitioners is the need to optimize inference efficiency in o1-like LLMs by addressing overthinking, potentially reducing computational overhead without compromising accuracy using methods like self-training with response simplification.  |
| Facilitating large language model Russian adaptation with Learned Embedding Propagation (Read more on [arXiv](https://arxiv.org/abs/2412.21140) or [HuggingFace](https://huggingface.co/papers/2412.21140))| Daniil Chernyshev, RefalMachine | This paper introduces Learned Embedding Propagation (LEP) as a cost-effective method for adapting large language models (LLMs) to new languages, specifically Russian, without full retraining. The main research objective is to address the limitations of language adaptation posed by restricted access to high-quality instruction-tuning data and the computational expense of full LLM retraining. The key methodology involves training a new tokenization vocabulary, initializing new embeddings by averaging existing ones, and then propagating these embeddings to an instruction-tuned model using linear transformations derived from fine-tuned variants. The primary results show that LEP applied to LLaMa-3-8B and Mistral-7B achieves competitive performance levels, with the LEP-Extended variant of OpenChat 3.5 achieving a Micro-Avg score of 0.632 on the Darumeru benchmark after calibration. For AI practitioners, the principal implication is that LEP offers a viable and efficient alternative to traditional language-specific instruction-tuning, significantly reducing the costs associated with language adaptation while maintaining or surpassing existing performance benchmarks.  |
| OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System (Read more on [arXiv](https://arxiv.org/abs/2412.20005) or [HuggingFace](https://huggingface.co/papers/2412.20005))| Mengshu Sun, Lin Yuan, Kangwei Liu, Xiangyuan Ru, Yujie Luo | OneKE is a dockerized, schema-guided, large language model (LLM) agent-based knowledge extraction system designed for diverse data types and domains. The main research objective is to develop a comprehensive system that can extract knowledge from various data sources following complex schemas and handle debugging/error correction effectively. The key methodology involves a multi-agent design with a configurable knowledge base, utilizing Schema, Extraction, and Reflection Agents to process data, extract information, and refine results, respectively. The primary results show that using the Case Retrieval method, the Extraction Agent achieved significant performance improvements on both CrossNER and NYT-11-HRL datasets, with F1 scores increasing substantially compared to the vanilla method. The principal implication for AI practitioners is that OneKE provides a flexible and adaptable framework for knowledge extraction tasks, supporting various LLMs and data formats without requiring fine-tuning, while the Case Repository enables continuous improvement through error correction.  |
| Slow Perception: Let's Perceive Geometric Figures Step-by-step (Read more on [arXiv](https://arxiv.org/abs/2412.20631) or [HuggingFace](https://huggingface.co/papers/2412.20631))| Liang Zhao, Jia Wang, Yumeng Li, Youyang Yin, Haoran Wei | The paper introduces "Slow Perception," a novel approach for parsing geometric figures in images by mimicking human-like gradual perception.  **Main research question or objective:** How to improve the accuracy of geometric figure parsing in images by Large Vision Language Models (LVLMs)?  **Key methodology used:** The authors propose a two-stage "Slow Perception" (SP) framework: a) perception decomposition, breaking down complex figures into basic units (points and lines); and b) perception flow, using a "perceptual ruler" to trace lines stroke-by-stroke, avoiding "long visual jumps."  **Primary results:** SP improves the F1-score of geometric parsing by 6.1% over the baseline when using a perceptual ruler length of 4 in the test set. Slow perception also exhibits an inference time scaling law, where shorter perceptual ruler lengths lead to longer inference times but improved performance.  **Principal implication for AI practitioners:** AI practitioners can leverage the slow perception framework to enhance the accuracy of geometric figure parsing, particularly in applications requiring precise spatial reasoning, and this framework may offer a new pathway to better performance in other visual tasks.  |
| PERSE: Personalized 3D Generative Avatars from A Single Portrait (Read more on [arXiv](https://arxiv.org/abs/2412.21206) or [HuggingFace](https://huggingface.co/papers/2412.21206))| Hanbyul Joo, Inhee Lee, Hyunsoo Cha | PERSE is a method for creating animatable 3D avatars from a single portrait image with controllable facial attributes. The main research question is how to build a 3D personalized generative avatar from a single reference portrait image that allows for continuous and disentangled control over various facial attributes while preserving the individual's identity. The key methodology involves synthesizing large-scale 2D video datasets with facial attribute editing, and training a 3D Gaussian Splatting-based avatar model with a novel latent space regularization technique using interpolated 2D faces as supervision. The primary result is that PERSE generates high-quality avatars with an FID score of 214.46 on interpolated renderings. The principal implication for AI practitioners is that PERSE provides a novel approach for creating personalized 3D avatars with controllable attributes from a single image, offering a valuable tool for applications in VR/AR environments.  |
| Training Software Engineering Agents and Verifiers with SWE-Gym (Read more on [arXiv](https://arxiv.org/abs/2412.21139) or [HuggingFace](https://huggingface.co/papers/2412.21139))| Navdeep Jaitly, Graham Neubig, Xingyao Wang, alsuhr, Jiayi-Pan | SWE-Gym is a new benchmark for evaluating software engineering agents on real-world coding tasks. The main research objective is to develop and assess a training environment, SWE-Gym, for improving the performance of language model-based software engineering agents. The key methodology involves fine-tuning language models on agent trajectories sampled from SWE-Gym and employing verifiers trained on these trajectories for inference-time scaling. Primary results show that fine-tuning on SWE-Gym improves agents' performance, achieving a 32.0% resolve rate on the SWE-Bench Verified test set. The principal implication for AI practitioners is that SWE-Gym can be used to train and improve software engineering agents through scalable learning methods.  |
| HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation (Read more on [arXiv](https://arxiv.org/abs/2412.21199) or [HuggingFace](https://huggingface.co/papers/2412.21199))| Xiao-Ping Zhang, Arman Cohan, Yilun Zhao, Zhaojian Yu | The paper introduces HumanEval Pro and MBPP Pro, benchmarks for evaluating large language models (LLMs) on self-invoking code generation tasks. The main research question is how well LLMs can generate code that solves a complex problem by invoking their own solution to a related, simpler base problem. The key methodology involves generating new, more complex versions of existing benchmarks (HumanEval and MBPP) by creating self-invoking problems that require using the solution of a base problem and evaluating over twenty LLMs using metrics like pass@1. The primary result is that most LLMs experience a significant performance drop on self-invoking tasks compared to traditional code generation; for example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. The principal implication for AI practitioners is that current LLMs, while proficient in generating code for isolated tasks, still struggle with more complex, multi-step reasoning required for self-invoking code generation, highlighting a crucial area for further development in code-generating models.  |
