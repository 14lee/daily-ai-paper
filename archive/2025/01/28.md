

## Papers for 2025-01-28

| Title | Authors | Summary |
|-------|---------|---------|
| Baichuan-Omni-1.5 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2501.15368) or [HuggingFace](https://huggingface.co/papers/2501.15368))| Song Chen, Tao Zhang, Tao Zhang, Jun Liu, AdamLee1 | Baichuan-Omni-1.5 is a unified omni-modal large language model designed to process text, image, audio, and video inputs, achieving seamless cross-modal interactions.  The research objective was to develop an omni-modal model with fluent and high-quality cross-modal interaction capabilities, particularly including end-to-end audio generation.  The methodology involved a multi-stage training strategy using a high-quality 500B multimodal dataset, an audio-tokenizer, and progressive multimodal alignment. Results showed Baichuan-Omni-1.5 outperforming leading omni-modal models like VITA-1.5 and MiniCPM-0 2.6 on various benchmarks, including an average score of 73.3 across ten image understanding benchmarks. This work provides AI practitioners with a state-of-the-art open-source omni-modal model exhibiting superior performance across multiple modalities, particularly in medical image understanding.  The details of some training hyperparameters are not explicitly stated in the provided excerpt, therefore a complete evaluation is difficult.  |
| Qwen2.5-1M Technical Report (Read more on [arXiv](https://arxiv.org/abs/2501.15383) or [HuggingFace](https://huggingface.co/papers/2501.15383))| Fei Huang, Dayiheng Liu, Chengyuan Li, Bowen Yu, An Yang | Qwen2.5-1M is a series of models that extend the context length to 1 million tokens, enhancing long-context capabilities. The main research objective is to develop and optimize models that can effectively process and understand sequences up to 1 million tokens long. Key methodologies include long data synthesis, progressive pre-training, multi-stage supervised fine-tuning, a training-free length extrapolation method, and a sparse attention mechanism. The Qwen2.5-14B-Instruct-1M model achieved 92.2 accuracy on 128k sequences in the RULER benchmark. For AI practitioners, the principal implication is that the provided inference framework and models, particularly Qwen2.5-14B-Instruct-1M, offer a robust solution for developing applications requiring long-context processing, with a remarkable 3x to 7x prefill speedup in scenarios with 1 million tokens of context.  |
| Towards General-Purpose Model-Free Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2501.16142) or [HuggingFace](https://huggingface.co/papers/2501.16142))| Michael Rabbat, Yuandong Tian, Amy Zhang, Pierluca D'Oro, Scott Fujimoto | This paper investigates the development of a unified model-free deep reinforcement learning algorithm applicable across diverse environments.  The research objective is to identify a single model-free deep RL algorithm that performs well across multiple benchmarks without requiring hyperparameter tuning for each task.  The methodology involves leveraging model-based representations to approximately linearize the value function, using a single set of hyperparameters across four benchmarks and 118 environments.  Results demonstrate competitive performance against domain-specific and general baselines, with MR.Q achieving competitive performance on the DMC benchmarks. The principal implication is that a single, well-designed model-free algorithm can achieve competitive performance on diverse tasks, reducing the need for extensive hyperparameter tuning and potentially speeding up AI development cycles.  Certain aspects of the ablation study results are unclear or lack sufficient detail for complete summarization.  |
| ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer (Read more on [arXiv](https://arxiv.org/abs/2501.15570) or [HuggingFace](https://huggingface.co/papers/2501.15570))| Peter Yue, Li Zhiyuan, Lin Yueyu, xiaol | ARWKV introduces an RNN-based language model derived from a Transformer via knowledge distillation, aiming to enhance expressiveness and efficiency.  Main research question or objective: How to effectively transform a Transformer-based language model into an RNN-based model while preserving performance and improving efficiency.  Key methodology used: A three-stage process involving aligning the hidden state output of the Transformer with an RWKV-7 time mixing module, followed by word-level KL-Divergence knowledge distillation, and concluding with supervised fine-tuning (SFT) and Direct Preference Optimization (DPO).  Primary results: The ARWKV model achieved a score of 62.41 on the MMLU benchmark after stage-2 training, demonstrating the feasibility of the transformation. The paper does not clarify whether the ARWKV model outperformed the teacher model on the MMLU benchmark.  Principal implication for AI practitioners: Knowledge distillation can be used to transform Transformer models into RNN-based architectures, potentially offering a pathway to developing more efficient language models without extensive pretraining.  |
| Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation (Read more on [arXiv](https://arxiv.org/abs/2501.15907) or [HuggingFace](https://huggingface.co/papers/2501.15907))| Yicheng Gu, Xuyuan Li, Chaoren Wang, Zengqiang Shang, Haorui He | Here is a concise summary of the research paper:  The paper introduces Emilia-Pipe, an open-source pipeline for creating speech generation datasets, and Emilia/Emilia-Large, large-scale multilingual datasets derived from in-the-wild speech data. The main research objective is to address the limitations of existing speech generation models trained on audiobook datasets by developing a diverse, spontaneous, and human-like speech dataset. The key methodology involves a six-step preprocessing pipeline (Emilia-Pipe) including standardization, source separation, speaker diarization, fine-grained segmentation, automated speech recognition, and filtering to process raw in-the-wild multilingual speech data. The primary results show that the Emilia dataset, comprising 101k hours of speech across six languages, significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, with the Emilia-Test set achieving a DNSMOS score of 3.26. The principal implication for AI practitioners is that the Emilia dataset and Emilia-Pipe provide valuable resources for training speech generation models capable of producing more natural and human-like speech, particularly in diverse real-world contexts.  |
| iFormer: Integrating ConvNet and Transformer for Mobile Application (Read more on [arXiv](https://arxiv.org/abs/2501.15369) or [HuggingFace](https://huggingface.co/papers/2501.15369))| Chuanyang Zheng | iFormer is a new family of mobile hybrid vision networks designed for optimized latency and accuracy in mobile applications. The main research objective is to develop a lightweight network that effectively integrates the local representation capacity of convolution and the global modeling ability of self-attention for mobile devices. The key methodology involves transforming a standard convolutional network (ConvNeXt) into a lightweight mobile network and introducing a novel mobile modulation attention mechanism that removes memory-intensive operations in multi-head attention (MHA). The primary result is that iFormer achieves a Top-1 accuracy of 80.4% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13. The principal implication for AI practitioners is that they can deploy the iFormer architecture to achieve state-of-the-art balance between latency and accuracy in vision tasks on resource-constrained mobile devices.  |
| Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (Read more on [arXiv](https://arxiv.org/abs/2501.16295) or [HuggingFace](https://huggingface.co/papers/2501.16295))| Luke Zettlemoyer, Ning Dong, Genghan Zhang, Junhong Shen, Weixin Liang | This paper introduces Mixture-of-Mamba, a novel state-space model architecture that enhances multi-modal learning through modality-aware sparsity. The main research question is how to improve the performance and efficiency of multi-modal state-space models (SSMs) by incorporating modality-specific parameterization. The key methodology involves extending the Mixture-of-Transformers approach to SSMs by selectively decoupling projection components in the Mamba block based on input modality, creating a sparse architecture. Primary results show that in the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B parameter scale compared to dense Mamba models. For AI practitioners, Mixture-of-Mamba offers a more computationally efficient architecture for multi-modal pretraining, allowing for significant reductions in training costs while maintaining or improving performance compared to existing dense models.  |
| Feasible Learning (Read more on [arXiv](https://arxiv.org/abs/2501.14912) or [HuggingFace](https://huggingface.co/papers/2501.14912))| Meraj Hashemizadeh, Jose Gallego-Posada, Juan Elenter, Ignacio Hounie, Juan Ramirez | Feasible Learning (FL) is a novel learning paradigm that formulates training machine learning models as a feasibility problem where the loss for each training sample is bounded. The main research question is whether deep networks trained via FL can achieve comparable average performance to Empirical Risk Minimization (ERM) while providing improved tail behavior. The key methodology is a primal-dual approach that dynamically re-weights the importance of each sample during training, and a relaxation called Resilient Feasible Learning (RFL) is introduced to handle potential infeasibility. Primary results show that on CIFAR10, models trained with FL achieved a test accuracy of 0.932 ± 0.002, comparable to ERM's 0.932 ± 0.002, with FL achieving a minimum Conditional Value at Risk (CVaR) across all loss percentiles, implying better performance on outlier samples. The principal implication is that AI practitioners can use FL as an alternative to ERM to achieve more consistent model performance across all data points, particularly when robustness to outliers is important, without significantly sacrificing average performance.  |
