

## Papers for 2025-01-07

| Title | Authors | Summary |
|-------|---------|---------|
| STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2501.02976) or [HuggingFace](https://huggingface.co/papers/2501.02976))| yingtai, zhenheny, chenzhao, yinhongliu, SherryX | STAR introduces a novel approach for real-world video super-resolution using text-to-video models.  The research objective was to enhance spatio-temporal quality in restored videos by addressing artifacts from complex degradations and mitigating fidelity loss from powerful generative models.  The methodology involved a Local Information Enhancement Module (LIEM) and a Dynamic Frequency (DF) Loss.  Results showed STAR outperforming state-of-the-art methods, achieving a 0.5422 DOVER score on the UDM10 dataset. This research highlights the significant potential of integrating text-to-video models and specifically designed loss functions for improving the fidelity and temporal consistency of real-world video super-resolution.  |
| BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning (Read more on [arXiv](https://arxiv.org/abs/2501.03226) or [HuggingFace](https://huggingface.co/papers/2501.03226))| lindahua, yhcao, KennyUTC, yuhangzang, BeichenZhang | Here's a concise summary of the paper:  i)  BoostStep improves large language models' mathematical reasoning by enhancing single-step reasoning through step-level in-context learning. ii)  The main objective is to address the granularity mismatch and negative-effect noise in in-context learning examples to improve the reasoning quality within each step of a multi-step mathematical problem-solving process. iii)  The key methodology is step-level in-context learning with a "first-try" strategy, which aligns the granularity between retrieving and reasoning on a step-by-step basis using an example problem bank constructed with step-level granularity. iv)  Quantitatively, BoostStep improves GPT-4o's performance on various mathematical benchmarks by 3.6% and Qwen2.5-Math-72B by 2.0%. v)  For AI practitioners, BoostStep provides a method to enhance the mathematical reasoning ability of large language models without additional training, demonstrating the importance of fine-grained, step-level guidance in complex problem-solving.  |
| Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction (Read more on [arXiv](https://arxiv.org/abs/2501.03218) or [HuggingFace](https://huggingface.co/papers/2501.03218))| myownskyW7, lindahua, yhcao, yuhangzang, Mar2Ding | Dispider is a novel system designed for active real-time interaction with streaming video using large language models (LLMs). The main research objective is to enable video LLMs to process and respond to streaming video input continuously and in real-time, unlike existing offline models. The key methodology is a disentangled architecture that separates perception, decision, and reaction into asynchronous modules operating in parallel, with a lightweight proactive streaming video processing module and an asynchronous interaction module. Primary results show that Dispider outperforms VideoLLM-online in the Proactive Output task with a score of 25.3, and achieves a leading performance of 55.6 on the EgoSchema benchmark. The principal implication for AI practitioners is that Dispider's disentangled and asynchronous design enables more efficient and responsive real-time video interaction, making it ideal for long-duration video streams and maintaining strong performance in conventional video QA tasks.  |
| Test-time Computing: from System-1 Thinking to System-2 Thinking (Read more on [arXiv](https://arxiv.org/abs/2501.02497) or [HuggingFace](https://huggingface.co/papers/2501.02497))| Jia Xu, Kaixin Wu, Hai Ye, douvleplus, Yisam | This paper surveys test-time computing methods, focusing on their role in enabling the transition from System-1 to System-2 thinking in AI models. The main research question is how test-time computing can enhance the robustness, generalization, and reasoning ability of AI models, particularly large language models (LLMs). The methodology involves a comprehensive review and categorization of existing literature on test-time computing techniques, including test-time adaptation and test-time reasoning, applied to both System-1 and System-2 models. A primary result highlighted is that self-consistency Chain-of-Thought prompting can improve accuracy by 18% over vanilla Chain-of-Thought in math reasoning tasks. The principal implication for AI practitioners is that leveraging test-time computing strategies can significantly enhance model performance on downstream tasks, particularly in complex reasoning scenarios, without the need for retraining.  |
| Personalized Graph-Based Retrieval for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.02157) or [HuggingFace](https://huggingface.co/papers/2501.02157))| Franck-Dernoncourt, namyongp, Ojasmitha17, Tobilee, StevenAu | Personalized Graph-Based Retrieval for Large Language Models introduces a framework called PGraphRAG to enhance personalized text generation. The main research question is how to improve the performance of large language models (LLMs) in generating personalized text, especially in cold-start scenarios with sparse user data. The key methodology is PGraphRAG, a framework that leverages user-centric knowledge graphs to augment prompts with user-relevant context during the retrieval process. Primary results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, with a +32.1% improvement in ROUGE-1 for Hotel Experience Generation using the LLaMA-3.1-8B model. The principal implication for AI practitioners is that integrating structured user knowledge via PGraphRAG enhances the ability of LLMs to generate personalized and contextually appropriate text, particularly when user history is limited.  |
| METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring (Read more on [arXiv](https://arxiv.org/abs/2501.02045) or [HuggingFace](https://huggingface.co/papers/2501.02045))| willieneis, oliu-io, upup-ashton-wang, Johannes, oliu-io | METAGENE-1: A 7-billion parameter autoregressive transformer model is pretrained on a novel metagenomic dataset for pandemic monitoring.  The research aimed to pretrain a foundation model on diverse metagenomic DNA and RNA sequences from human wastewater samples.  Byte-pair encoding (BPE) tokenization was used for the dataset, and the model was pretrained using a decoder-style architecture.  METAGENE-1 achieved state-of-the-art results on pathogen detection benchmarks, with a 92.96% average MCC score across four datasets.  The successful pretraining of a large-scale metagenomic language model demonstrates the potential of this technology for applications in public health and opens up avenues for AI practitioners to develop and deploy similar models for diverse genomic tasks.  |
| TransPixar: Advancing Text-to-Video Generation with Transparency (Read more on [arXiv](https://arxiv.org/abs/2501.03006) or [HuggingFace](https://huggingface.co/papers/2501.03006))| Yijun Li, yingcongchen, HeZhang, zhifeichen097, wileewang | TransPixar introduces a method for generating RGBA videos from text prompts, addressing the challenge of producing transparent visual effects in text-to-video models.  The research objective was to extend pretrained video models to generate RGBA videos while preserving original RGB capabilities.  The methodology involved incorporating alpha-specific tokens and using LoRA-based fine-tuning within a diffusion transformer architecture, optimizing attention mechanisms to align RGB and alpha channels. A user study revealed a significant preference for TransPixar's RGBA alignment (93.3%) over a comparable method (6.7%). This work demonstrates that high-quality RGBA video generation is achievable with limited training data using a modified DiT architecture, offering a practical advancement for creating realistic video effects with transparency for applications such as VFX.  |
| Ingredients: Blending Custom Photos with Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2501.01790) or [HuggingFace](https://huggingface.co/papers/2501.01790))| Di Qiu, MichaelFan, Changqian, Debang, onion | This paper introduces Ingredients, a framework for customizing video generation by incorporating multiple specific identity (ID) photos with video diffusion Transformers. The main research question is how to achieve multi-ID customization in video generation while preserving high-fidelity identity, enhancing content flexibility, and ensuring natural video generation. The key methodology involves a facial extractor for versatile facial feature capture, a multi-scale projector to map embeddings into the contextual space of image query in video diffusion Transformers, and an ID router for dynamically combining and allocating multiple ID embeddings to corresponding space-time regions, trained through a multi-stage protocol. The primary results show that the proposed Ingredients method achieved a face similarity score of 77.1% in multi-ID video generation, significantly outperforming baselines. The principal implication for AI practitioners is that Ingredients provides a training-free framework for multi-ID customization in video generation based on diffusion Transformers, enabling the preservation of multiple IDs while supporting precise textual control signals.  |
| DepthMaster: Taming Diffusion Models for Monocular Depth Estimation (Read more on [arXiv](https://arxiv.org/abs/2501.02576) or [HuggingFace](https://huggingface.co/papers/2501.02576))| Ruijie Zhu, Hao Zhang, Bo Li, Zerong Wang, Ziyang Song | DepthMaster is a single-step diffusion model designed for improved monocular depth estimation by adapting generative features to this discriminative task. The main research question is how to adapt generative features in diffusion models to enhance the performance of discriminative depth estimation while maintaining efficiency. The key methodology involves a Feature Alignment module to incorporate high-quality semantic features into the denoising network and a Fourier Enhancement module to balance low-frequency structure and high-frequency details in a single forward pass, using a two-stage training strategy. The primary results show that DepthMaster achieves state-of-the-art zero-shot performance, with an 8.2% AbsRel on the KITTI dataset. The principal implication for AI practitioners is that DepthMaster provides an effective way to leverage diffusion models for depth estimation with improved generalization and detail preservation, which is particularly beneficial for applications such as autonomous driving.  |
| Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2501.03059) or [HuggingFace](https://huggingface.co/papers/2501.03059))| Yaniv Taigman, Shelly Sheynin, Amit Zohar, Yuval Kirstain, GuyYariv | Through-The-Mask proposes a two-stage image-to-video generation framework using mask-based motion trajectories.  The research objective was to improve the accuracy and consistency of object motion in generated videos, especially in multi-object scenarios.  The methodology involved generating mask-based motion trajectories as an intermediate representation, conditioned on the input image, segmentation mask, and text prompt, followed by video generation conditioned on this representation.  Results demonstrated state-of-the-art performance on several benchmarks, including a FVD score of 925.39 (U-Net) on the SA-V-128 benchmark. This work provides AI practitioners with a novel two-stage framework for I2V generation that significantly improves motion realism and consistency, particularly in complex scenes.  |
| GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking (Read more on [arXiv](https://arxiv.org/abs/2501.02690) or [HuggingFace](https://huggingface.co/papers/2501.02690))| Yijin Li, Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, wangfuyun | GS-DiT advances video generation by enabling 4D video control using pseudo 4D Gaussian fields and efficient dense 3D point tracking. The main research objective is to enable precise 4D control in video generation, such as multi-camera shooting and dolly zoom, without requiring expensive multi-view videos. The key methodology involves constructing a pseudo 4D Gaussian field with a novel dense 3D point tracking method (D3D-PT) and finetuning a pretrained Diffusion Transformer (DiT) to generate videos guided by the rendered videos from this field. The primary result is that D3D-PT outperforms SpatialTracker in accuracy and accelerates dense 3D point tracking by two orders of magnitude, achieving a 3D-AJ score of 9.0 on the TAPVid-3D minival split. The principal implication for AI practitioners is that GS-DiT enables 4D controllable video generation from monocular videos, broadening the applicability of advanced cinematic techniques in AI-driven video content creation.  |
| Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.01830) or [HuggingFace](https://huggingface.co/papers/2501.01830))| Weiqiang Wang, Huijia Zhu, Yaojie Lu, Shuhen Zhou, Yanjiang Liu | AUTO-RT is a reinforcement learning framework for automatically exploring and optimizing attack strategies to uncover security vulnerabilities in large language models (LLMs). The main research objective is to develop an automated red-teaming approach that can efficiently identify complex vulnerabilities in LLMs without relying on predefined safety flaws or fixed attack strategies. The key methodology involves two mechanisms: Early-terminated Exploration, which focuses on high-potential attack strategies, and a Progressive Reward Tracking algorithm that uses intermediate downgrade models to refine the search trajectory. The primary result is that AUTO-RT achieved a 16.63% higher success rate in detecting vulnerabilities compared to existing methods. The principal implication for AI practitioners is that they can use AUTO-RT to improve the efficiency of discovering vulnerabilities in LLMs, enabling more robust and secure language model development.  |
| Samba-asr state-of-the-art speech recognition leveraging structured state-space models (Read more on [arXiv](https://arxiv.org/abs/2501.02832) or [HuggingFace](https://huggingface.co/papers/2501.02832))| Kartik-angadi, kruthika, SyedAbdul | Samba-ASR is a novel speech recognition model utilizing state-space models (SSMs) for improved accuracy and efficiency. The main research objective is to develop an Automatic Speech Recognition (ASR) model that outperforms existing transformer-based models by leveraging the Mamba architecture. The key methodology involves replacing transformer encoders with Mamba's state-space modeling in both the encoder and decoder, using a Mamba-cross-connection mechanism, and training on a combined dataset of LibriSpeech, GigaSpeech, and SPGISpeech. The primary result is that Samba-ASR achieved a Word Error Rate (WER) of 3.65% on average across multiple benchmark datasets, including a 1.17% WER on LibriSpeech Clean. For AI practitioners, Samba-ASR offers a new state-of-the-art model for speech recognition, demonstrating that SSMs can surpass transformers in accuracy and efficiency, particularly for long audio sequences.  |
| ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use (Read more on [arXiv](https://arxiv.org/abs/2501.02506) or [HuggingFace](https://huggingface.co/papers/2501.02506))| Yufei Xu, Xuesong Yao, Zhengyin Du, Junjie Ye, maverick1994 | Here is a concise summary of the research paper "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use":  ToolHop is a new benchmark for evaluating large language models (LLMs) on multi-hop tool use, focusing on their ability to decompose complex queries and utilize multiple tools sequentially. The main research objective is to assess LLMs' capabilities in understanding, reasoning, and function-calling within a multi-hop tool-use context. The key methodology involves a query-driven data construction process that includes tool creation, document refinement, and code generation, resulting in 995 multi-hop queries and 3,912 associated tools. The primary result is that the leading model, GPT-4o, achieved an accuracy of only 49.04% in the mandatory tool use scenario, highlighting significant limitations in current LLMs' multi-hop tool-use abilities. The principal implication for AI practitioners is that there is substantial room for improvement in developing LLMs that can effectively handle complex multi-hop reasoning and tool-use tasks, as evidenced by the leading model's relatively low performance.  |
| Scaling Laws for Floating Point Quantization Training (Read more on [arXiv](https://arxiv.org/abs/2501.02423) or [HuggingFace](https://huggingface.co/papers/2501.02423))| Kan Wu, Weidong Han, Ruobing Xie, Shuaipeng Li, Xingwu Sun | This paper explores scaling laws for floating-point quantization training in large language models (LLMs) to optimize low-precision training. The main research question is how do factors like data size, model size, exponent bits, mantissa bits, and block size of scaling factors affect the performance of LLMs under floating-point quantization training. The key methodology involves training 366 LLMs with various configurations and analyzing the relationships between these factors and model loss to formulate a unified scaling law. The primary result is a unified scaling law that accurately predicts LLM performance under different floating-point quantization settings, with the optimal floating-point quantization precision being directly proportional to computational power. The principal implication for AI practitioners is that they can use the derived scaling law to optimize the trade-off between computational cost and performance when training LLMs with floating-point quantization, particularly that the best cost-performance precision lies between 4-8 bits within a wide computational power range.  |
