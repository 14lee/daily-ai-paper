

## Papers for 2025-01-08

| Title | Authors | Summary |
|-------|---------|---------|
| REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.03262) or [HuggingFace](https://huggingface.co/papers/2501.03262))| chuyi777 | REINFORCE++ is a novel variant of the REINFORCE algorithm designed to enhance the alignment of large language models (LLMs) with human preferences. The main research objective is to develop a more efficient and stable reinforcement learning from human feedback (RLHF) algorithm by simplifying the REINFORCE framework and removing the need for a critic network. Key methodologies include a token-level Kullback-Leibler (KL) penalty, Proximal Policy Optimization (PPO)-clip integration, mini-batch updates, and reward normalization. Primary results demonstrate that REINFORCE++ achieves comparable or superior performance to PPO and Group Relative Policy Optimization (GRPO), with a specific quantitative finding showing a reduction in training time from 60 hours (for PPO) to 42 hours on NVIDIA H100 with the LLaMA3 8b model. Principal implication for AI practitioners is that REINFORCE++ provides a simpler and more computationally efficient method for aligning LLMs, making it a valuable alternative to more complex RLHF approaches like PPO.  |
| MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.02955) or [HuggingFace](https://huggingface.co/papers/2501.02955))| Lefan Wang, Weihan Wang, Zhuoyi Yang, LiquidAmmonia, wenyi | MotionBench: A comprehensive benchmark for evaluating fine-grained video motion understanding in vision-language models (VLMs).  The research objective was to assess the capability of VLMs in understanding fine-grained video motion and to improve VLM performance in this area.  The key methodology involved creating a new benchmark, MotionBench, with diverse video sources and question types focusing on motion-level perception, along with proposing a novel Through-Encoder (TE) Fusion method for enhancing video feature representation.  The primary results indicated that existing VLMs perform poorly in understanding fine-grained motions, achieving accuracies below 60% on MotionBench; TE Fusion yielded improvements in motion understanding.  The paper does not clearly specify the improvement magnitude.  The principal implication is that MotionBench provides a valuable resource for evaluating and improving video understanding VLMs, highlighting a significant deficiency in current models' ability to handle fine-grained motion and offering a novel architectural approach to address this limitation.  |
| Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos (Read more on [arXiv](https://arxiv.org/abs/2501.04001) or [HuggingFace](https://huggingface.co/papers/2501.04001))| Shilin Xu, Zilong Huang, Tao Zhang, Xiangtai Li, HarborYuan | Sa2VA is a unified model for dense grounded understanding of images and videos, integrating SAM-2 and LLaVA-like models.  The research objective was to create a model capable of handling a wide range of image and video tasks, including referring segmentation and conversation, within a single framework.  The methodology involved a one-shot visual instruction tuning approach, unifying text, image, and video into a shared LLM token space.  Sa2VA achieved state-of-the-art results on multiple benchmarks, exceeding GLaMM-7B by 2.1, 3.6, and 4.5 cIoU on RefCOCO, RefCOCO+, and RefCOCOg respectively. For AI practitioners, this work provides a unified, highly effective architecture and demonstrates that integrating powerful visual foundation models with LLMs is highly effective for a broad range of vision-language tasks, offering a superior approach to the design of multi-modal models.  |
| Cosmos World Foundation Model Platform for Physical AI (Read more on [arXiv](https://arxiv.org/abs/2501.03575) or [HuggingFace](https://huggingface.co/papers/2501.03575))| Yogesh Balaji, Maciej Bala, Arslan Ali, Niket Agarwal, NVIDIA | The Cosmos World Foundation Model Platform facilitates Physical AI development by providing pre-trained world models and tools for customization.  The research objective was to create a platform for building and fine-tuning world foundation models (WFMs) for Physical AI applications.  The methodology involved developing video data curation, pre-trained WFMs using diffusion and autoregressive models, video tokenizers, and post-training techniques.  Results showed Cosmos Tokenizer achieved a 4dB PSNR improvement over existing tokenizers on the DAVIS dataset at 8Ã— spatial compression. The platform's open-source nature and model availability empower AI practitioners to build and deploy customized WFMs for their specific Physical AI systems, potentially accelerating development in various applications.  |
| LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token (Read more on [arXiv](https://arxiv.org/abs/2501.03895) or [HuggingFace](https://huggingface.co/papers/2501.03895))| Yang Feng, Zhe Yang, Qingkai Fang, Shaolei Zhang | LLaVA-Mini introduces an efficient large multimodal model using a single vision token to represent images and videos.  The research objective was to develop efficient large multimodal models (LMMs) by minimizing the number of vision tokens while maintaining performance.  The key methodology involved modality pre-fusion to fuse visual information into text tokens before feeding them into the LLM backbone, along with a compression module to reduce vision token quantity.  Results show LLaVA-Mini outperforms LLaVA-v1.5 with only one vision token instead of 576, achieving a 77% reduction in FLOPs.  This research demonstrates the feasibility of building highly efficient LMMs with significantly reduced computational costs,  potentially leading to faster inference times and wider accessibility for real-time multimodal applications.  |
| Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control (Read more on [arXiv](https://arxiv.org/abs/2501.03847) or [HuggingFace](https://huggingface.co/papers/2501.03847))| Zhiyang Dou, Jiahao Lu, Rui Yan, Zekai Gu, pengHTYX | Diffusion as Shader (DaS) is a 3D-aware video diffusion model that enables versatile control over video generation by utilizing 3D tracking videos as conditional inputs. The main research objective is to develop a unified framework for video generation that supports multiple control tasks, such as mesh-to-video generation, camera control, motion transfer, and object manipulation. The key methodology involves using 3D tracking videos, which represent the motion trajectories of 3D points, as control inputs to a video diffusion model that acts as a shader to compute shaded appearances. The primary results demonstrate that DaS outperforms baseline methods on camera control, achieving a rotation error of 10.40 degrees and a translation error of 5.97 degrees on large camera movements, compared to 39.86 and 67.05 for MotionCtrl. For AI practitioners, the principal implication is that leveraging 3D tracking videos as control signals enables more precise and temporally consistent control over video generation compared to methods that rely solely on 2D control signals.  |
| MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2501.03714) or [HuggingFace](https://huggingface.co/papers/2501.03714))| Jihyong Oh, Won-Sik Cheong, Jun Young Jeong, Joonsoo Kim, Sangwoon Kwak | MoDec-GS is a memory-efficient 3D Gaussian splatting framework for reconstructing novel views from dynamic videos with complex motions.  The research objective was to develop a method for efficiently representing and rendering dynamic scenes with complex motions, addressing limitations in existing methods regarding storage and representation of complex movements.  MoDec-GS uses Global-to-Local Motion Decomposition (GLMD) and Temporal Interval Adjustment (TIA) to model complex motions effectively and efficiently.  The results demonstrate a 70% average reduction in model size compared to state-of-the-art methods while maintaining or improving rendering quality; specifically, on the iPhone dataset, MoDec-GS achieved a 0.7dB PSNR gain and a 94% storage reduction compared to the second-best method.  This work provides a highly compact and efficient approach for dynamic scene representation relevant to AI practitioners working on real-time video processing and novel view synthesis.  |
| PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides (Read more on [arXiv](https://arxiv.org/abs/2501.03936) or [HuggingFace](https://huggingface.co/papers/2501.03936))| Hongyu Lin, Jia Zheng, Hao Kong, Xinyan Guan, Forceless | PPTAgent is a novel two-stage, edit-based framework for automatic presentation generation that leverages reference presentations and LLMs.  The research aimed to improve presentation generation by addressing the limitations of existing text-to-slide methods.  PPTAgent utilizes a two-stage process: presentation analysis (clustering slides and extracting schemas) and presentation generation (iterative editing of reference slides).  Experiments showed that PPTAgent significantly outperformed baselines across three dimensions (Content, Design, Coherence), achieving an average score of 3.67 and a 97.8% success rate. This work provides a new approach for AI practitioners to generate high-quality presentations, improving efficiency and visual effectiveness in communication.  |
| MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control (Read more on [arXiv](https://arxiv.org/abs/2501.02260) or [HuggingFace](https://huggingface.co/papers/2501.02260))| Guoying Zhao, Huai-Qian Khor, Xingxun Jiang, Tuomas Varanka, Mengting Wei | MagicFace: High-fidelity facial expression editing using action unit (AU) variations as conditions within a Stable Diffusion framework.  The research objective was to develop a method for high-fidelity facial expression editing that is both interpretable and controllable by adjusting AU variations.  The methodology involved a diffusion model conditioned on AU variations, an ID encoder for identity preservation, and an Attribute Controller for maintaining background and pose consistency.  The model was trained on a dataset of 30,000 image pairs.  The primary result showed that MagicFace achieved a mean squared error (MSE) of 0.261 for AU intensity, outperforming other methods.  The main implication for AI practitioners is the demonstration of precise and controllable facial expression editing using AU variations within a diffusion model framework; this offers improvements for generating photorealistic facial expressions for applications like virtual characters and avatars.  |
| Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2501.03931) or [HuggingFace](https://huggingface.co/papers/2501.03931))| Zexin Yan, Bohao Peng, Bin Xia, Yaoyang Liu, julianjuaner | Magic Mirror: A novel framework for generating high-fidelity identity-preserved videos using video diffusion transformers.  The research objective is to develop a method for generating high-quality, identity-preserved videos with dynamic motion, addressing the challenge of maintaining consistent identity while producing natural motion in existing text-to-video generation models.  The methodology involves a dual-branch facial feature extractor, a lightweight cross-modal adapter with Conditioned Adaptive Normalization (CAN) for efficient identity integration, and a two-stage training strategy.  The primary results demonstrate that Magic Mirror outperforms existing methods, achieving an average ID similarity of 0.911 while maintaining high video quality metrics and dynamic motion.  The overall preference score from a user study was 7.315. The paper does not explicitly specify if the user study is statistically significant.  The most impactful finding is the successful integration of identity preservation into a video diffusion transformer architecture without person-specific fine-tuning, offering a more efficient and scalable approach to personalized video generation.  This has direct relevance for AI practitioners working with video diffusion models, as it provides a more efficient and effective method for identity-preserved video generation.  |
| Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback (Read more on [arXiv](https://arxiv.org/abs/2501.03916) or [HuggingFace](https://huggingface.co/papers/2501.03916))| Tao Chen, Botian Shi, Xiangchao Yan, Jiakang Yuan, BoZhang | DOLPHIN is a closed-loop open-ended auto-research framework automating the scientific research process.  The research aims to create a fully automated scientific research system capable of generating research ideas, performing experiments, and iteratively refining ideas based on results.  DOLPHIN employs LLMs for idea generation and code generation, incorporating an exception-traceback-guided debugging process.  Experiments across three benchmark datasets demonstrated DOLPHIN generating methods comparable to state-of-the-art in some tasks; for example, a 2.9% improvement in ModelNet40 accuracy over the baseline.  This work provides a significant advancement for AI practitioners in automating the scientific research process, though the paper lacks information regarding certain experimental setup details.  |
