

## Papers for 2025-01-03

| Title | Authors | Summary |
|-------|---------|---------|
| 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining (Read more on [arXiv](https://arxiv.org/abs/2501.00958) or [HuggingFace](https://huggingface.co/papers/2501.00958))| Yongliang Shen, Jiashuo Sun, Xin Li, Hang Zhang, Wenqi Zhang | A high-quality multimodal textbook corpus, constructed from 2.5 years of instructional videos, is introduced for vision-language model (VLM) pretraining.  The research aimed to create a more coherent, knowledge-rich interleaved corpus than existing web-crawled datasets.  The methodology involved LLM-based video collection and filtering, followed by progressive extraction and refinement of visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos.  Experiments demonstrated significantly improved pretraining performance, with VLMs achieving an average gain of +4.6% across seven benchmarks in 0-4 shot settings (e.g., +20% improvement on ScienceQA). The resulting textbook dataset offers superior interleaved context awareness, beneficial for improving VLM knowledge and reasoning capabilities.  |
| VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control (Read more on [arXiv](https://arxiv.org/abs/2501.01427) or [HuggingFace](https://huggingface.co/papers/2501.01427))| Xiang Bai, Sihui Ji, Xi Chen, Hao Luo, Yuanpeng Tu | VideoAnydoor is a zero-shot video object insertion framework achieving high-fidelity detail preservation and precise motion control.  The research objective was to develop a method for accurately preserving object identity and precisely controlling object motion during video insertion.  The methodology involved an end-to-end framework utilizing an ID extractor, a pixel warper for fine-grained motion control, and a reweighted reconstruction loss.  Quantitative results showed VideoAnydoor outperforming existing methods, achieving a 37.7 PSNR score, exceeding previous state-of-the-art techniques. This work provides AI practitioners with a robust, end-to-end framework for high-fidelity video object insertion and precise motion control, applicable to various downstream tasks without task-specific fine-tuning.  |
| CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings (Read more on [arXiv](https://arxiv.org/abs/2501.01257) or [HuggingFace](https://huggingface.co/papers/2501.01257))| Dayiheng Liu, Bo Zheng, Bowen Yu, Jiaxi Yang, Shanghaoran Quan | CODEELO is a benchmark for evaluating large language models (LLMs) on competition-level code generation using human-comparable Elo ratings. The main research objective is to develop a standardized benchmark that addresses limitations of existing benchmarks, such as the unavailability of private test cases and misaligned execution environments, to effectively assess LLMs' coding abilities at a competitive level. The key methodology involves submitting LLM-generated code to the CodeForces platform for judging and calculating Elo ratings based on the performance, aligned with the platform's system but with lower variance. The primary results show that the 01-mini model achieved the highest Elo rating of 1578, surpassing nearly 90% of human participants, while most other models struggled, with many falling in the lowest 20th percentile of human competitors. The principal implication for AI practitioners is that enhancing the length of the chain-of-thought (CoT) presents a promising avenue for improving LLMs' reasoning abilities in code generation, as evidenced by the significant performance of 01-mini and QwQ-32B-Preview.  |
| VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM (Read more on [arXiv](https://arxiv.org/abs/2501.00599) or [HuggingFace](https://huggingface.co/papers/2501.00599))| Boqiang Zhang, Zesen Cheng, Wentong Li, Hang Zhang, Yuqian Yuan | VideoRefer Suite introduces a benchmark and model for fine-grained spatial-temporal video understanding.  The research objective was to improve Video LLMs' ability to understand fine-grained spatial and temporal details in videos.  A multi-agent data engine created a large-scale object-level video instruction dataset (VideoRefer-700K), and a VideoRefer model with a versatile spatial-temporal object encoder was developed.  VideoRefer achieved a 3.46 average score on the VideoRefer-BenchD benchmark (a multi-dimensional evaluation of description generation), exceeding existing methods.  This work provides a valuable resource (dataset, model, benchmark) for advancing Video LLM capabilities, particularly in applications requiring fine-grained object-level understanding.  |
| Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2501.01423) or [HuggingFace](https://huggingface.co/papers/2501.01423))| Xinggang Wang, Jingfeng Yao | Latent diffusion models with high-dimensional visual tokenizers exhibit an optimization dilemma: improved reconstruction quality comes at the cost of degraded generation performance.  The research objective is to address the optimization dilemma in latent diffusion models by improving the training efficiency and generative performance of high-dimensional visual tokenizers.  The key methodology is to align the latent space of the visual tokenizer with pre-trained vision foundation models during training, using a novel vision foundation model alignment loss (VF Loss).  The primary result shows a significant improvement in training speed; achieving an FID score of 2.11 in just 64 epochsâ€”a 21x speedup compared to the original DiT.  Additionally, the integrated system achieved state-of-the-art performance on ImageNet 256x256 generation with an FID score of 1.35.  The principal implication for AI practitioners is that the proposed VA-VAE and LightningDiT framework offers a practical solution to a common problem in latent diffusion models, enabling faster convergence and improved generation performance with higher-dimensional tokenizers.  |
| ProgCo: Program Helps Self-Correction of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.01264) or [HuggingFace](https://huggingface.co/papers/2501.01264))| Wenbo Su, Jiaheng Liu, Weixun Wang, Yanan Wu, Xiaoshuai Song | ProgCo improves large language model (LLM) self-correction by integrating program-driven verification and refinement.  The research aimed to enhance LLM self-correction, particularly for complex reasoning tasks, where existing methods often fail.  ProgCo uses self-generated and self-executed verification pseudo-programs to achieve more robust verification, followed by dual refinement of both responses and programs.  Experiments showed ProgCo achieved significant improvements, for example, a 5.8% accuracy increase on the MATH dataset with one round of self-correction.  This work suggests that incorporating program-driven techniques can significantly improve LLM self-correction capabilities, impacting development of more reliable and robust AI systems.  |
| A3: Android Agent Arena for Mobile GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2501.01149) or [HuggingFace](https://huggingface.co/papers/2501.01149))| Guozhi Wang, Liang Liu, Jiayu Zhang, Hanhao Li, Yuxiang Chai | Android Agent Arena (A3) introduces a novel evaluation platform for mobile GUI agents.  The research aims to address limitations of existing datasets and benchmarks by providing a comprehensive, interactive evaluation platform for mobile GUI agents operating in real-world scenarios.  A3 employs a dynamic evaluation approach incorporating 201 tasks across 21 widely used third-party apps and leverages business-level LLMs for automated task evaluation.  Results showed GPT-40 achieved 84% accuracy in LLM-based evaluation of task completion. A3 offers AI practitioners a more realistic and scalable evaluation framework for assessing the performance of mobile GUI agents.  |
| MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2501.00316) or [HuggingFace](https://huggingface.co/papers/2501.00316))| Md Hasebul Hasan, Md Tanvir Parvez, Md Tanvir Hassan, Mahir Labib Dihan, eunus | MAPEVAL is a benchmark for evaluating geo-spatial reasoning in foundation models. The main research objective is to assess foundation models' ability to handle diverse and complex map-based user queries requiring geo-spatial reasoning. The key methodology used is a new benchmark called MAPEVAL, comprising 700 unique multiple-choice questions across three task types (textual, API-based, and visual) that test spatial relationships, map infographics, travel planning, and navigation. The primary result is that Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro performed competitively, but Claude-3.5-Sonnet agents outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21% respectively in the MAPEVAL-API task. The principal implication for AI practitioners is that MAPEVAL provides a critical tool for advancing general-purpose foundation models with stronger geo-spatial understanding, as evidenced by the significant performance gaps observed even among the most advanced models.  |
| Dynamic Scaling of Unit Tests for Code Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2501.01054) or [HuggingFace](https://huggingface.co/papers/2501.01054))| Sijia Luo, Jifan Yu, Jing Zhang, Xiaokang Zhang, KAKA22 | This paper investigates improving code generation accuracy by scaling the number of unit tests used for reward modeling.  The research objective was to determine if increasing unit test quantity enhances reward signal quality, leading to better code selection.  A unit test-based majority voting framework was employed, coupled with a novel unit test generator (CodeRM-8B) and dynamic scaling based on problem difficulty.  Results show a positive correlation between unit test quantity and reward signal quality, with a specific finding of an 18.43% performance gain for Llama3-8B on HumanEval Plus.  This research indicates that scaling unit tests, particularly using CodeRM-8B and dynamic scaling, can significantly enhance code generation performance in LLMs, providing a practical method for improving model accuracy.  |
| MLLM-as-a-Judge for Image Safety without Human Labeling (Read more on [arXiv](https://arxiv.org/abs/2501.00192) or [HuggingFace](https://huggingface.co/papers/2501.00192))| Felix Juefei-Xu, Xiaowen Lin, Shiyu Zhao, Shuming Hu, Zhenting Wang | This paper investigates zero-shot image safety judgment using pre-trained Multimodal Large Language Models (MLLMs).  The main objective is to determine if unsafe images can be detected without human labeling, solely by querying MLLMs using a predefined safety constitution.  The proposed method, CLUE, involves objectifying safety rules, assessing rule-image relevance, using debiased token probabilities for judgment, and employing cascaded chain-of-thought reasoning.  Experiments demonstrate high effectiveness, achieving 95.9% recall and 94.8% accuracy with InternVL2-76B on a complex safety constitution.  This work suggests a scalable, human-labeling-free approach for image safety assessment, potentially significantly reducing costs associated with existing methods.  |
| MapQaTor: A System for Efficient Annotation of Map Query Datasets (Read more on [arXiv](https://arxiv.org/abs/2412.21015) or [HuggingFace](https://huggingface.co/papers/2412.21015))| Md Rizwan Parvez, Mohammed Eunus Ali, mahirlabibdihan | MapQATOR is a web application designed to efficiently create reproducible map-based question-answering datasets for evaluating large language models' geospatial reasoning capabilities.  The research objective was to develop a system for streamlined annotation of map-based QA datasets, overcoming challenges in creating reliable geospatial QA data.  The methodology involved building a plug-and-play web application integrating with multiple map APIs, incorporating data visualization tools, and utilizing a caching mechanism to ensure data consistency.  Results demonstrated a 30x speedup in annotation compared to manual methods. The principal implication for AI practitioners is that MapQATOR significantly accelerates the creation of high-quality, reproducible geospatial datasets crucial for training and benchmarking LLMs on complex reasoning tasks.  |
| Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing (Read more on [arXiv](https://arxiv.org/abs/2501.00658) or [HuggingFace](https://huggingface.co/papers/2501.00658))| Jiajun Zhu, Yuehao Wang, Ruisi Cai, Peihao Wang, pragsri8 | Structured State Space Models (SSMs) are investigated for their limitations in capturing long-range dependencies.  The research aims to understand and mitigate bottlenecks in SSMs, focusing on recency bias and over-smoothing.  A novel polarization technique, modifying state transition matrices, is proposed and empirically evaluated.  Results show that polarization consistently improves associative recall accuracy of long-range tokens (e.g., a 93.43% average accuracy in one experiment), unlocking the benefits of deeper architectures in SSMs.  This work highlights the inherent limitations of SSMs regarding recency and over-smoothing, directly impacting their scalability and robustness for long sequence processing and suggesting design modifications for improved performance.  |
| SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration (Read more on [arXiv](https://arxiv.org/abs/2501.01320) or [HuggingFace](https://huggingface.co/papers/2501.01320))| Ceyuan Yang, Yang Zhao, Meng Wei, Zhijie Lin, Jianyi Wang | SeedVR: a novel diffusion transformer for generic video restoration.  The research objective was to develop a diffusion transformer model capable of handling real-world video restoration with arbitrary length and resolution.  The key methodology involved a shifted window attention mechanism within a diffusion transformer, a causal video variational autoencoder (CVVAE) for efficient compression, and a multi-stage progressive training strategy.  SeedVR demonstrated impressive restoration capabilities; for example, it outperformed existing methods on several benchmark datasets, achieving a 10.508 DOVER score on the SPMCS dataset.  The most impactful finding, relevant for AI practitioners, is SeedVR's superior efficiency compared to existing diffusion-based video restoration approaches, achieving over 2x faster inference speed despite having a larger parameter count.  The details regarding the comparison of training time are unclear.  |
| SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization (Read more on [arXiv](https://arxiv.org/abs/2501.01245) or [HuggingFace](https://huggingface.co/papers/2501.01245))| Haozhou Sun, Zihan Jia, Zhenbang Xu, Haodong Chen, Yongle Huang | SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization proposes a novel semi-supervised learning framework for fine-grained action recognition.  The research objective is to develop a robust method for fine-grained action recognition using limited labeled data, addressing challenges inherent in existing large language models.  The methodology incorporates dual-level temporal element modeling, moderate temporal perturbation as a strong augmentation strategy, and adaptive regulation to stabilize the learning process.  SeFAR achieves state-of-the-art performance on fine-grained datasets, outperforming other methods by margins such as 7.8% to 8.4% increase in accuracy on FineDiving depending on the labeling rate.  This research demonstrates a significant improvement in semi-supervised fine-grained action recognition and provides AI practitioners with a novel framework applicable to vision-based tasks involving nuanced temporal dynamics and limited data.  |
