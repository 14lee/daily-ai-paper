

## Papers for 2025-01-15

| Title | Authors | Summary |
|-------|---------|---------|
| MiniMax-01: Scaling Foundation Models with Lightning Attention (Read more on [arXiv](https://arxiv.org/abs/2501.08313) or [HuggingFace](https://huggingface.co/papers/2501.08313))| Bangwei Gong, Aonian Li, MiniMax, Hannnnnxd, enochzhang | MiniMax-01 introduces a series of large language models featuring efficient scaling via lightning attention and Mixture of Experts, achieving comparable performance to top-tier models with significantly longer context windows. The main research objective is to develop models that match the performance of leading commercial models while offering context windows longer by an order of magnitude using an optimized architecture and training framework. The key methodology involves a hybrid architecture employing lightning attention, a variant of linear attention, combined with softmax attention and a Mixture of Experts (MoE) model, alongside optimized parallel strategies and computation-communication overlap techniques. Primary results show that MiniMax-Text-01, with 456 billion parameters, achieves an 88.5% accuracy on the MMLU benchmark, comparable to leading models, while supporting context windows up to 4 million tokens during inference. The principal implication for AI practitioners is that the model's architecture and training framework enable efficient training and inference on models with large context windows, which could facilitate the development of more sophisticated AI agents.  |
| Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models (Read more on [arXiv](https://arxiv.org/abs/2501.06751) or [HuggingFace](https://huggingface.co/papers/2501.06751))| Yoad Tewel, Rinon Gal, Hadas Orgad, Ido Galil, Michael Toker | This paper investigates the role of padding tokens in text-to-image (T2I) models. The main research question is how padding tokens, typically used to standardize input prompt lengths, affect the image generation process in T2I models. The key methodology involves two causal intervention techniques, ITE and IDP, to analyze the impact of padding tokens on model components by selectively replacing prompt or padding tokens with "clean" pads and observing the changes in generated images. The primary results show that in models like LDM and LLaMA-UNet, padding tokens encode significant semantic information, achieving a CLIP score of 0.30 when only the first 20% of pad tokens are used, and contribute to image generation, whereas, in models with frozen text encoders, they are largely ignored. The principal implication for AI practitioners is that the choice to include or exclude padding tokens during training and inference can significantly impact model behavior, particularly in models with trainable text encoders or those employing multi-modal attention mechanisms.  |
| MangaNinja: Line Art Colorization with Precise Reference Following (Read more on [arXiv](https://arxiv.org/abs/2501.08332) or [HuggingFace](https://huggingface.co/papers/2501.08332))| Hao Ouyang, Jie Xiao, Xi Chen, Ka Leong Cheng, Zhiheng Liu | MangaNinja is a reference-based line art colorization method that leverages diffusion models to accurately transfer colors from a reference image to a target line art. The main research question is how to achieve precise and controllable line art colorization that preserves character identity and details from a reference image, even with significant variations between the reference and line art. The key methodology involves a dual-branch architecture with a patch shuffling module for correspondence learning between the reference image and line art, and a point-driven control scheme using PointNet for fine-grained color matching. The primary results show that MangaNinja achieves a DINO score of 69.91 and a CLIP score of 90.02, outperforming existing methods on a newly collected benchmark. For AI practitioners, MangaNinja offers a robust method for automating line art colorization, potentially accelerating the animation and comics production workflow.  |
| A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2501.08187) or [HuggingFace](https://huggingface.co/papers/2501.08187))| Jingyang Qian, Kangwei Liu, Xinle Deng, Ningyu, Fangyinfff | A multi-modal AI copilot, INSTRUCTCELL, is introduced for single-cell analysis using natural language instructions. Main research question or objective: How can a multi-modal AI copilot be developed to effectively integrate natural language instructions with single-cell RNA sequencing (scRNA-seq) data to perform various analytical tasks? Key methodology used: A multi-modal instruction dataset was constructed, pairing text-based instructions with scRNA-seq profiles, and a multi-modal cell language model was developed, featuring a Q-Former module, a pre-trained language model (LM), and a cell reconstruction block, tuned via instruction tuning. Primary results: INSTRUCTCELL achieved an accuracy exceeding 99.97% in answer extraction using the xFinder tool and demonstrated robust performance in cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction, outperforming existing single-cell foundation models in several benchmarks. Principal implication for AI practitioners: AI practitioners can leverage INSTRUCTCELL's architecture and training methodology to develop multi-modal AI tools that integrate diverse data types and natural language processing, enhancing the interpretability and accessibility of complex biological data analysis.  |
| Diffusion Adversarial Post-Training for One-Step Video Generation (Read more on [arXiv](https://arxiv.org/abs/2501.08316) or [HuggingFace](https://huggingface.co/papers/2501.08316))| Xuefeng Xiao, Ceyuan Yang, Yuxi Ren, Xin Xia, PeterL1n | Diffusion Adversarial Post-Training (APT) accelerates one-step video generation using diffusion models.  The research objective was to develop a method for high-quality, real-time one-step video generation, overcoming limitations of existing diffusion distillation techniques.  The methodology employed adversarial post-training against real data, following diffusion pre-training, incorporating several architectural and training improvements, and an approximated R1 regularization objective.   The model, Seaweed-APT, generated 2-second, 1280x720, 24fps videos in real time using a single forward pass;  it achieved image generation quality comparable to state-of-the-art methods.   This research directly impacts AI practitioners by providing a method for generating high-resolution videos in real-time with a single forward pass, potentially improving efficiency and application across various domains; however, text alignment quality was lower than the original 25-step diffusion model.  |
| PokerBench: Training Large Language Models to become Professional Poker Players (Read more on [arXiv](https://arxiv.org/abs/2501.08328) or [HuggingFace](https://huggingface.co/papers/2501.08328))| Zhengyu Li, Aniket Rahane, Richard Yang, Richard Zhuang, akshat57 | POKERBENCH is a new benchmark for evaluating large language models' (LLMs) ability to play poker. The main research objective is to assess how well LLMs can learn and apply game theory optimal poker strategies. The key methodology involves creating a dataset (POKERBENCH) of 11,000 poker scenarios, evaluating various LLMs on this dataset, and fine-tuning them using a subset of this data. The primary results show that GPT-4 achieved the highest accuracy of 53.55% among pre-trained models, but fine-tuned models like Llama-3-8B surpassed it, reaching 80.64% accuracy. For AI practitioners, POKERBENCH provides a valuable benchmark for training and evaluating LLMs on complex decision-making tasks, with the most impactful finding being that supervised fine-tuning can significantly improve LLM performance in strategic game environments like poker, but may have limitations.  |
| Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens (Read more on [arXiv](https://arxiv.org/abs/2501.07730) or [HuggingFace](https://huggingface.co/papers/2501.07730))| Xiaohui Shen, Chenglin Yang, Qihang Yu, Dongwon Kim, turkeyju | This paper introduces TA-TiTok, a text-aware one-dimensional image tokenizer, and MaskGen, a text-to-image masked generative model, designed for efficient and accessible text-to-image generation. The main research question is: Can an efficient and effective text-to-image generative model be developed using only open data, enabling reproducibility? The key methodology involves a novel text-aware 1D tokenizer (TA-TiTok) that integrates textual information during de-tokenization and a simplified one-stage training process for masked generative models. Primary results show that MaskGen-XL achieves a generation FID of 7.51 on the MJHQ-30K benchmark using discrete tokens, surpassing several recent models while using only open-source datasets. The principal implication for AI practitioners is that high-quality text-to-image generation can be achieved with reduced computational resources and publicly available data, facilitating broader access and research in this area.  |
| Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks (Read more on [arXiv](https://arxiv.org/abs/2501.08326) or [HuggingFace](https://huggingface.co/papers/2501.08326))| Subhashree Radhakrishnan, Sifei Liu, De-An Huang, Min-Hung Chen, Miran Heo | Omni-RGPT unifies image and video region-level understanding using token marks for consistent spatio-temporal comprehension. The main research question is how to achieve consistent region representation across spatio-temporal dimensions in images and videos for multimodal large language models (MLLMs). The key methodology involves introducing Token Mark, a set of tokens highlighting target regions within the visual feature space, and an auxiliary task that guides Token Mark by leveraging the consistency of the tokens for stable region interpretation across video frames. Primary results show that Omni-RGPT achieves 88.5% accuracy on the Visual Commonsense Reasoning (VCR) validation set, demonstrating state-of-the-art performance in image-based commonsense reasoning. The principal implication for AI practitioners is that using Token Mark for region-level understanding enhances the performance of MLLMs on tasks requiring detailed visual comprehension, offering a more robust method for integrating region-specific information in both image and video domains.  |
| OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training (Read more on [arXiv](https://arxiv.org/abs/2501.08197) or [HuggingFace](https://huggingface.co/papers/2501.08197))| Ran Chen, Wei Wang, Zekun Wang, Ziyun Dai, yuyijiong | OpenCSG Chinese Corpus introduces four high-quality Chinese datasets for LLM training.  The research objective was to address the scarcity of high-quality Chinese datasets for LLM training by creating a series of datasets with diverse characteristics.  The methodology involved combining automated filtering techniques with synthetic data generation and domain-focused curation.  Results demonstrated significant performance improvements using a 2B parameter model trained on Fineweb-Edu-Chinese (achieving an accuracy increase of approximately 0.08 over the baseline on the CMMLU benchmark).  This work provides publicly available high-quality datasets that are directly applicable to improving the performance of Chinese LLMs, particularly in educational contexts.  |
| Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2501.07888) or [HuggingFace](https://huggingface.co/papers/2501.07888))| Yuan Lin, Yuchen Zhang, Haomiao Sun, Jiawei Wang, Liping Yuan | Tarsier2 is a state-of-the-art large vision-language model for video understanding, especially detailed video description. The main research objective is to develop a model that can generate detailed and accurate video descriptions and exhibit superior general video understanding capabilities. The key methodology involves scaling pre-training data to 40 million video-text pairs, performing fine-grained temporal alignment during supervised fine-tuning, and using model-based sampling with Direct Preference Optimization (DPO). The primary results show that Tarsier2-7B outperforms GPT-40 by 2.8% in F1 score on the DREAM-1K benchmark for detailed video description. The principal implication for AI practitioners is that scaling training data and incorporating fine-grained temporal alignment, along with DPO, significantly enhances the performance of vision-language models on video understanding tasks, particularly in generating detailed and accurate video descriptions.  |
| Enhancing Automated Interpretability with Output-Centric Feature Descriptions (Read more on [arXiv](https://arxiv.org/abs/2501.08319) or [HuggingFace](https://huggingface.co/papers/2501.08319))| Mor Geva, Chen Agassy, Roy Mayan, Yoav Gur-Arieh, atticusg | This paper introduces output-centric methods for automatically generating feature descriptions in large language models (LLMs).  The research objective was to improve automated interpretability pipelines by addressing the limitations of input-centric approaches.  Two output-centric methods, VocabProj and TokenChange, were developed and compared to the existing input-centric MaxAct method using input- and output-based evaluations.  Results showed that ensemble methods combining input and output-centric approaches consistently outperformed MaxAct on both evaluations, with a significant improvement of 6-10% observed in Gemma-2.  This work provides AI practitioners with improved methods for generating feature descriptions, leading to more effective model interpretability and steering capabilities, particularly by enabling efficient discovery of previously "dead" features.  |
| Potential and Perils of Large Language Models as Judges of Unstructured Textual Data (Read more on [arXiv](https://arxiv.org/abs/2501.08167) or [HuggingFace](https://huggingface.co/papers/2501.08167))| Satya Kapoor, Sreyoshi Bhaduri, Natalie Perez, Rewina Bedemariam, amanchadha | This research investigates the effectiveness of LLMs as judge models for evaluating thematic alignment in summaries generated by other LLMs using open-ended survey data.  The main objective was to determine if LLMs could replicate human judgment in thematic alignment evaluations and the implications of higher inter-model agreement compared to human-model agreement.  A three-stage methodology was used, employing human evaluation as a baseline, followed by LLM evaluation using several models (Claude, Titan Express, Nova Pro, and Llama) and statistical analysis (Cohen's kappa, Spearman's rho, Krippendorff's alpha).  Results showed that while LLMs offered a scalable alternative to human raters, achieving moderate agreement (Cohen's kappa = 0.44) with human ratings, humans demonstrated superior ability in detecting subtle nuances.  This highlights the need for cautious consideration when generalizing LLM judge models across various contexts and reinforces the importance of human oversight in ensuring fair and accurate AI-assisted text analysis.  |
| HALoGEN: Fantastic LLM Hallucinations and Where to Find Them (Read more on [arXiv](https://arxiv.org/abs/2501.08292) or [HuggingFace](https://huggingface.co/papers/2501.08292))| Yejin Choi, David Wadden, Shrusti Ghela, Abhilasha Ravichander | HALOGEN is a benchmark for evaluating hallucinations in long-form text generated by large language models (LLMs).  Main research question or objective: To construct a comprehensive benchmark for measuring and analyzing hallucination behavior in long-form generations of LLMs across diverse domains.  Key methodology used: Development of the HALOGEN benchmark, comprising 10,923 prompts across nine domains and automatic high-precision verifiers that decompose LLM generations into atomic units and verify them against external knowledge sources.  Primary results: Evaluation of 14 LLMs revealed that even the best-performing models produce hallucinations in 4% to 86% of generated atomic facts, depending on the task, with GPT-4 demonstrating better refusal behavior than other models.  Principal implication for AI practitioners: AI practitioners should leverage diverse, multi-domain benchmarks like HALOGEN to evaluate and mitigate LLM hallucinations, as no single domain is highly predictive of hallucination behavior in others, highlighting the complexity of this issue.  |
| AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages (Read more on [arXiv](https://arxiv.org/abs/2501.08284) or [HuggingFace](https://huggingface.co/papers/2501.08284))| Ibrahim Said Ahmad, David Ifeoluwa Adelani, Abinew Ali Ayele, Idris Abdulmumin, Shamsuddeen Hassan Muhammad | AfriHate is a new dataset for hate speech and abusive language detection in 15 African languages. The main research objective is to address the lack of high-quality data for hate speech and abusive language in African languages and evaluate the effectiveness of current models. The key methodology involves collecting tweets, crowdsourcing keywords, manually annotating data for hate speech, abusive language, or neutral content, and conducting experiments with various pre-trained language models (PLMs), few-shot learning, and prompting large language models (LLMs). The primary results show that fine-tuning multilingual models yields the best performance, with AfroXLMR-76L achieving an average macro F1-score of 78.16 across all languages. The principal implication for AI practitioners is that multilingual fine-tuning on AfriHate is currently the most effective approach for hate speech detection in the studied African languages, emphasizing the importance of multilingual and context-specific models for low-resource settings.  |
