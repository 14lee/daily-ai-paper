

## Papers for 2025-01-10

| Title | Authors | Summary |
|-------|---------|---------|
| The GAN is dead; long live the GAN! A Modern GAN Baseline (Read more on [arXiv](https://arxiv.org/abs/2501.05441) or [HuggingFace](https://huggingface.co/papers/2501.05441))| jamestompkin, kuleshov, Skylion007, Eva1209 | Here is a concise summary of the paper:  i) The paper introduces R3GAN, a new baseline for Generative Adversarial Networks (GANs) that achieves state-of-the-art results without relying on ad-hoc tricks common in previous GAN architectures. ii) The main research objective is to develop a more principled and stable GAN baseline by addressing mode dropping and non-convergence issues in existing GAN training. iii) The key methodology involves proposing a novel regularized relativistic GAN loss (RpGAN + R1 + R2) and modernizing the network backbone using ResNet design principles and grouped convolutions. iv) The primary results show that R3GAN surpasses StyleGAN2 on FFHQ-256, achieving an FID score of 7.05 compared to StyleGAN2's 7.52, and matches or exceeds state-of-the-art GANs and diffusion models on various datasets. v) The principal implication for AI practitioners is that R3GAN provides a robust and efficient baseline for image generation tasks, demonstrating that GANs remain competitive with modern architectures and can be trained reliably without complex, ad-hoc techniques.  |
| An Empirical Study of Autoregressive Pre-training from Videos (Read more on [arXiv](https://arxiv.org/abs/2501.05453) or [HuggingFace](https://huggingface.co/papers/2501.05453))| Ilija Radosavovic, jitendra1995, yossig, rravishankar, brjathu | This paper empirically studies autoregressive pre-training of transformer models on videos for visual representation learning. The main research question is how effective is autoregressive pre-training on videos for learning visual representations across various downstream tasks. The key methodology involves training a series of autoregressive video models, called Toto, to predict future tokens in videos and images, using a diverse dataset of over 1 trillion visual tokens and evaluating these models on downstream tasks. The primary result is that autoregressive pre-training leads to competitive performance across all benchmarks, with the Toto-1b model achieving 75.3% top-1 accuracy on ImageNet classification. The principal implication for AI practitioners is that autoregressive pre-training on videos is a viable method for learning visual representations, achieving strong performance on various tasks despite minimal inductive biases.  |
| Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives (Read more on [arXiv](https://arxiv.org/abs/2501.04003) or [HuggingFace](https://huggingface.co/papers/2501.04003))| ZwwWayne, Chonghao, THUdyh, ldkong, shaoyuanxie | DriveBench, a benchmark dataset, evaluates the reliability of Vision-Language Models (VLMs) in autonomous driving across various tasks and conditions. The main research question is: Are existing VLMs capable of providing reliable explanations grounded on visual cues for driving? The methodology involves evaluating 12 VLMs on a dataset with 19,200 frames and 20,498 QA pairs across 17 settings (clean, corrupted, and text-only inputs), using metrics like accuracy, traditional language metrics, and GPT scores. Primary results indicate that under clean image inputs, the GPT-4 model achieved a GPT score of 75.75 in the planning task, but VLMs often generated plausible yet fabricated responses under degraded or missing visual inputs. The principal implication for AI practitioners is that current VLMs are not yet reliable for autonomous driving applications due to their tendency to provide fabricated responses under degraded visual conditions, emphasizing the need for improved datasets and evaluation protocols.  |
| On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis (Read more on [arXiv](https://arxiv.org/abs/2501.04377) or [HuggingFace](https://huggingface.co/papers/2501.04377))| Yingyu Liang, Xiaoyu Li, Zhenmei, JamesSand, keyekun | Visual Autoregressive (VAR) models' computational complexity and efficiency for image generation are analyzed in this paper. The main research question is whether the computations of VAR models can be performed faster than O(n⁴) time. The key methodology involves analyzing the computation of VAR models under the Strong Exponential Time Hypothesis (SETH) and using low-rank approximations to develop efficient algorithms. A primary result is that when the hidden dimension d = O(log n) and the bound of the entries of the input matrices R = o(√log n), there is an algorithm that approximates the VAR model up to 1/poly(n) additive error in O(n²⁺⁰⁽¹⁾) time. The principal implication for AI practitioners is that VAR models can be computed in almost quadratic time under specific conditions, offering a more efficient approach to image generation than previous O(n⁴) methods.  |
| Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model (Read more on [arXiv](https://arxiv.org/abs/2501.05122) or [HuggingFace](https://huggingface.co/papers/2501.05122))| Radu Timofte, Chris Biemann, Carolin Holtermann, Florian Schneider, Gregor Geigle | Centurio is a 100-language large vision-language model (LVLM) that offers state-of-the-art performance across 14 tasks and 56 languages. The main research question is what are the optimal training strategies for developing massively multilingual LVLMs, focusing on the number of training languages, data distribution across languages, and techniques for improving multilingual text-in-image understanding. The key methodology involves a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically varying the training data composition and evaluating performance. A primary result is that including up to 100 training languages simultaneously with as little as 25-50% of non-English data greatly improves multilingual performance while retaining strong English performance, with negligible performance degradation compared to fewer languages. The principal implication for AI practitioners is that massively multilingual LVLMs can be effectively trained with a balanced mix of English and multilingual data, even for low-resource languages, and incorporating synthetic OCR data can significantly enhance multilingual text-in-image understanding.  |
| Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models (Read more on [arXiv](https://arxiv.org/abs/2501.04828) or [HuggingFace](https://huggingface.co/papers/2501.04828))| Ece Elif Adak, tcTHEBESTMAN, fatihburakkaragoz, temretiras, sbozates | The paper introduces new resources and models for natural language processing (NLP) of historical Turkish, a previously underexplored area. The main research objective is to develop foundational resources and models for NLP tasks in historical Turkish, including named entity recognition (NER), dependency parsing, and part-of-speech (POS) tagging. The key methodology involves creating and annotating datasets (HisTR, OTA-BOUN), compiling a clean text corpus (Ottoman Text Corpus - OTC), and fine-tuning transformer-based language models (BERTurk, mBERT, TURNA) on these resources. Primary results indicate that the BERTurk model fine-tuned on both MilliyetNER and HisTR achieved a 90.07 F1 score on the HisTR development set for NER. The principal implication for AI practitioners is that fine-tuning language-specific pre-trained models on domain-specific datasets is a viable approach for historical Turkish NLP, but challenges remain in adapting to out-of-domain data.  |
| Entropy-Guided Attention for Private LLMs (Read more on [arXiv](https://arxiv.org/abs/2501.03489) or [HuggingFace](https://huggingface.co/papers/2501.03489))| Brandon Reagen, nandan523 | This paper introduces an information-theoretic framework to optimize transformer architectures for privacy-preserving language model inference. The main research question is how the removal of nonlinearities in decoder-only language models impacts their training dynamics and expressiveness, particularly in the context of private inference (PI). The key methodology involves using Shannon's entropy to analyze the dual role of nonlinearities in maintaining training stability and attention head diversity, and exploring PI-friendly alternatives like weight normalization and entropy regularization. A primary result is that the proposed entropy-guided attention mechanism with a Softmax-only model reduces communication overhead by 3.94x and improves end-to-end PI latency by 1.72x, compared to a baseline GPT-2 model with GELU and LayerNorm. The principal implication for AI practitioners is that entropy-guided attention can enable more efficient and scalable privacy-preserving inference for large language models by reducing reliance on computationally expensive nonlinear operations.  |
