

## Papers for 2025-01-20

| Title | Authors | Summary |
|-------|---------|---------|
| Evolving Deeper LLM Thinking (Read more on [arXiv](https://arxiv.org/abs/2501.09891) or [HuggingFace](https://huggingface.co/papers/2501.09891))| Shumeet Baluja, Dave Marwood, Yueh-Hua Wu, Ian Fischer, Kuang-Huei Lee | Mind Evolution, an evolutionary search strategy, improves large language model (LLM) problem-solving.  The research aimed to enhance LLM problem-solving abilities by leveraging inference time compute.  Mind Evolution uses an LLM to generate, recombine, and refine candidate solutions based on evaluator feedback, avoiding formal problem representation.  Results show Gemini 1.5 Flash achieving a 95.6% success rate on the TravelPlanner benchmark using Mind Evolution, significantly outperforming other methods.  This approach enables efficient exploration of the solution space in natural language tasks, offering a valuable strategy for LLM application development.  |
| PaSa: An LLM Agent for Comprehensive Academic Paper Search (Read more on [arXiv](https://arxiv.org/abs/2501.10120) or [HuggingFace](https://huggingface.co/papers/2501.10120))| Yuchen Zhang, Yuan Lin, Peiyuan Feng, Guanhua Huang, Yichen He | PaSa is a large language model (LLM) based agent designed for comprehensive academic paper search. The main research question is whether an LLM agent can autonomously conduct comprehensive and accurate academic paper searches, mimicking human-like behavior. The key methodology involves using two LLM agents, a "Crawler" and a "Selector," optimized with reinforcement learning on a synthetic dataset, AutoScholarQuery, containing 35k fine-grained academic queries. The primary results show that PaSa-7B surpasses the Google with GPT-40 baseline by 37.78% in recall@20 and 39.90% in recall@50 on the RealScholarQuery benchmark. The principal implication for AI practitioners is that PaSa provides a more effective tool for academic literature search, significantly improving search accuracy and recall compared to existing search engines and other LLM-based approaches.  |
| Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions (Read more on [arXiv](https://arxiv.org/abs/2501.10020) or [HuggingFace](https://huggingface.co/papers/2501.10020))| Liefeng Bo, Jianqiang Ren, Chao He | Textoon generates diverse, animatable 2D cartoon characters from text descriptions using a novel Live2D-based framework.  The research objective is to develop a method for generating high-quality, interactive 2D cartoon characters from text prompts, overcoming the limitations of existing Live2D creation methods.  The methodology combines a fine-tuned large language model (LLM) for accurate text parsing, a text-to-image diffusion model (Stable Diffusion) for controllable appearance generation, an image editing technique for re-editing, and a component completion and repair module.  ARKit's face blendshapes are integrated for improved animation.  The primary result is achieving >90% accuracy in parsing component categories from complex input text at millisecond speeds using 4GB of memory (RTX 4090).  The system can generate a new character within one minute.  The most impactful finding is the creation of a method for generating Live2D characters from text prompts in under one minute, enhancing efficiency in 2D character creation and potentially impacting workflows for game developers, animators, and other creative professionals.  |
| Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong (Read more on [arXiv](https://arxiv.org/abs/2501.09775) or [HuggingFace](https://huggingface.co/papers/2501.09775))| Pedro Reviriego, Gonzalo Martínez, Javier Conde, Tairan Fu, mariagrandury | This paper investigates how prompting techniques affect LLM confidence in multiple-choice question responses.  The research objective was to determine if LLMs exhibit altered confidence levels when prompted to provide reasoning before selecting an answer, compared to directly answering.  The study employed two prompting methods: direct answer and chain-of-thought (CoT), evaluating seven different LLMs on the MMLU benchmark.  Results indicated that LLMs demonstrated higher confidence (average probability of selected option increased) with CoT prompts, regardless of answer correctness.  For example, the increase in average confidence was larger for incorrect answers than for correct answers.  The principal implication is that LLM-estimated probabilities may have intrinsic limitations, impacting their use in evaluation procedures and highlighting a potential mismatch between confidence and accuracy.  Further research is needed to clarify how to leverage LLM confidence estimates effectively.  |
| HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2501.10045) or [HuggingFace](https://huggingface.co/papers/2501.10045))| Chong Zhang, Yukun Ma, Zexu Pan, Kun Zhou, Shengkui Zhao | HiFi-SR proposes a unified generative adversarial network for high-fidelity speech super-resolution.  The research objective was to improve speech super-resolution (SR) by addressing limitations of existing methods that use independently trained networks.  The methodology involved a unified transformer-convolutional generator trained end-to-end, incorporating a multi-band, multi-scale time-frequency discriminator and mel-reconstruction loss.  Results showed HiFi-SR significantly outperformed existing methods, achieving an average log-spectral distance (LSD) of 0.82 on the VCTK test set, improving upon the baseline NVSR model's LSD of 0.85. This demonstrates the effectiveness of a unified network architecture for high-fidelity speech SR, providing a more robust and generalizable approach for AI practitioners developing speech enhancement technologies.  |
| X-Dyna: Expressive Dynamic Human Image Animation (Read more on [arXiv](https://arxiv.org/abs/2501.10021) or [HuggingFace](https://huggingface.co/papers/2501.10021))| Zhengfei Kuang, Yipeng Gao, You Xie, Hongyi Xu, Boese0601 | X-Dyna introduces a zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements from a driving video.  The research objective was to create a method for realistic, context-aware dynamic human image animation addressing shortcomings in existing approaches.  The methodology employed a diffusion UNet backbone with a novel Dynamics-Adapter module integrating reference appearance context into spatial attentions, coupled with a local face control module for expression transfer.  Quantitative results demonstrated that X-Dyna outperforms state-of-the-art methods, achieving a 0.900 FG-DTFVD score compared to scores ranging from 1.753 to 2.639 for other methods.  This research significantly advances the field of human image animation offering a more efficient and effective method for realistic video generation which directly improves the quality and realism of animated videos.  |
| GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor (Read more on [arXiv](https://arxiv.org/abs/2501.09978) or [HuggingFace](https://huggingface.co/papers/2501.09978))| Yuan Liu, Qi Zhang, Heng Li, Kunming Luo, Xiangyue Liu | GaussianAvatar-Editor introduces a novel framework for text-driven editing of animatable 3D Gaussian head avatars.  The research objective was to develop a method for fully controllable text-driven editing of animatable Gaussian head avatars, addressing challenges of motion occlusion and spatiotemporal inconsistency.  The methodology employed a Weighted Alpha Blending Equation (WABE) for anti-occlusion and conditional adversarial learning to ensure 4D consistency.  Quantitative results demonstrated that the proposed method achieved superior CLIP-S scores (0.275) compared to baselines (e.g., INSTA+I-N2N, 0.181) in novel view rendering. This work provides AI practitioners with a novel approach to high-quality, consistent 4D Gaussian head avatar editing, directly applicable to applications such as virtual and augmented reality.  |
| ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario (Read more on [arXiv](https://arxiv.org/abs/2501.10132) or [HuggingFace](https://huggingface.co/papers/2501.10132))| Jie Tang, Haiyi Hu, Xiaohan Zhang, Zhengxiao Du, Lucen Zhong | ComplexFuncBench is a benchmark for evaluating large language models' (LLMs) complex function-calling capabilities.  The research aimed to evaluate LLMs' ability to handle multi-step, constrained function calls within a long-context (128k tokens) setting.  The authors developed ComplexEval, an automated evaluation framework using a multi-dimensional matching approach to assess function call correctness. Results showed that even leading closed-source models achieved only a 61% success rate on complex function calls.  This highlights a significant deficiency in current LLMs' ability to manage complex real-world API interactions, emphasizing the need for further research into robust and efficient LLM function-calling capabilities for production-level applications.  |
| Bridging Language Barriers in Healthcare: A Study on Arabic LLMs (Read more on [arXiv](https://arxiv.org/abs/2501.09825) or [HuggingFace](https://huggingface.co/papers/2501.09825))| Ronnie Rajan, Marco AF Pimentel, Clément Christophe, Tathagata Raha, Nada Saadi | This paper investigates the challenges of developing effective Arabic LLMs for clinical tasks.  The main objective was to determine optimal strategies for training LLMs proficient in both multilingual understanding and medical knowledge, focusing on Arabic.  The researchers employed a methodology combining translation of existing English medical datasets into Arabic, synthetic data generation, and fine-tuning Llama 3.1 with varying ratios of Arabic and English data.  Results showed that Llama 3.1 achieved significantly lower accuracy on Arabic medical benchmarks (29.5% on MedQA) compared to English (62.0% on MedQA); optimal language ratios varied across tasks.  For AI practitioners, the study highlights the limitations of solely relying on translation and fine-tuning for low-resource languages in specialized domains; more computationally intensive pretraining techniques may be necessary for optimal multilingual medical LLM performance.  |
