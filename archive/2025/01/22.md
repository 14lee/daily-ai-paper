

## Papers for 2025-01-22

| Title | Authors | Summary |
|-------|---------|---------|
| Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training (Read more on [arXiv](https://arxiv.org/abs/2501.11425) or [HuggingFace](https://huggingface.co/papers/2501.11425))| Zhengyin Du, Zhiheng Xi, Junjie-Ye, lovesnowbest, siyuyuan | Agent-R is an iterative self-training framework that enables language agents to reflect on and correct their actions in interactive environments. The main research question is whether language model agents can be trained to reflect on their behavior and improve performance via iterative self-training without relying on human or expert model supervision. The key methodology involves using Monte Carlo Tree Search (MCTS) to construct training samples that recover correct trajectories from erroneous ones and a model-guided critique mechanism for timely error revision. The primary result is that agents trained with Agent-R achieved a 70.71% average success rate across three interactive environments, outperforming baseline methods by 5.59%. The principal implication for AI practitioners is that Agent-R offers a method to develop language agents with enhanced self-reflection and error correction capabilities, enabling more robust performance in interactive and agentic environments.  |
| MMVU: Measuring Expert-Level Multi-Discipline Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2501.12380) or [HuggingFace](https://huggingface.co/papers/2501.12380))| Lujing Xie, Yilun Zhao, Phil-01, entropyhu, freesky | MMVU is a benchmark for evaluating the expert-level, multi-discipline video understanding capabilities of foundation models. The main research question is how well current multimodal foundation models can understand and reason about specialized-domain videos requiring expert knowledge across multiple disciplines. The key methodology involves creating a dataset of 3,000 expert-annotated examples from 1,529 specialized-domain videos, spanning 27 subjects across four core disciplines, with each example including expert-annotated reasoning rationales and relevant domain knowledge. The primary results show that the best performing model, ol, achieved an accuracy of 77.0% on the test set, significantly below the human expert performance of 86.8% in an open-book setting. The principal implication for AI practitioners is that while current models show promise in expert-level video understanding, there remains a substantial gap compared to human expertise, indicating a need for further development in integrating domain-specific knowledge and reasoning into multimodal models for specialized domains.  |
| Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models (Read more on [arXiv](https://arxiv.org/abs/2501.11873) or [HuggingFace](https://huggingface.co/papers/2501.11873))| Kaiyue Wen, Bo Zheng, Zeyu Huang, Zihan Qiu, Losin94 | This paper revisits the implementation of Load-balancing Loss (LBL) in Mixture-of-Experts (MoEs) models. The main research question is how the calculation scope of LBL (micro-batch vs. global-batch) affects the performance and expert specialization of MoE-based large language models (LLMs). The key methodology involves synchronizing expert selection frequency across parallel groups to calculate LBL at the global-batch level and comparing it with the traditional micro-batch approach. The primary results show that global-batch LBL significantly improves model performance, for example by 0.1 in pre-training perplexity in the MoE-3.4A0.6B model, and enhances domain specialization of experts. The principal implication for AI practitioners is that using global-batch LBL can lead to more performant and specialized MoE models during training.  |
| UI-TARS: Pioneering Automated GUI Interaction with Native Agents (Read more on [arXiv](https://arxiv.org/abs/2501.12326) or [HuggingFace](https://huggingface.co/papers/2501.12326))| Shihao Liang, Haoming Wang, Junjie Fang, Yining Ye, Yujia Qin | UI-TARS introduces a native GUI agent model that solely uses screenshots as input to perform human-like GUI interactions.  The research objective was to develop an end-to-end GUI agent model surpassing existing framework-based models.  UI-TARS employed enhanced perception, unified action modeling, system-2 reasoning, and iterative training with reflective online traces.  Results showed UI-TARS achieving state-of-the-art performance on multiple benchmarks, including a score of 24.6 on the OSWorld benchmark with 50 steps. This work demonstrates the potential of native GUI agents, suggesting that data-driven approaches can outperform framework-based methods for GUI interaction.  |
| Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks (Read more on [arXiv](https://arxiv.org/abs/2501.11733) or [HuggingFace](https://huggingface.co/papers/2501.11733))| Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy, mikewang | Mobile-Agent-E is a hierarchical multi-agent mobile assistant framework with a self-evolution module that improves task performance and efficiency on complex real-world mobile tasks.  The research objective was to address limitations of existing mobile agents, namely their struggles with reasoning-intensive tasks and lack of learning from experience.  Mobile-Agent-E employs a hierarchical architecture separating high-level planning from low-level action execution and a self-evolution module learning reusable shortcuts and general tips.  Results showed a 22% absolute improvement in satisfaction score over previous state-of-the-art approaches using GPT-40.  The most impactful finding, a substantial performance gain, directly suggests the efficacy of hierarchical multi-agent frameworks and self-evolution mechanisms for improving mobile agent capabilities.  |
| TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space (Read more on [arXiv](https://arxiv.org/abs/2501.12224) or [HuggingFace](https://huggingface.co/papers/2501.12224))| Shiran Zada, Omer Tov, Roni Paiss, Shahar Yadin, Daniel Garibi | TokenVerse is a method for multi-concept personalization in text-to-image diffusion models, enabling disentangled control over diverse visual elements extracted from single or multiple images. The main research question is how to achieve versatile and disentangled multi-concept personalization and composition in diffusion transformers. The key methodology involves optimizing per-token directions in the modulation space of a Diffusion Transformer (DiT) model to learn and compose visual concepts described by text tokens. Primary results show that TokenVerse outperforms existing methods, achieving a Concept Preservation score of 0.470108 and Prompt Fidelity score of 0.688061 in the composition task, while other methods score lower on at least one of these metrics. The principal implication for AI practitioners is that TokenVerse provides a more effective way to personalize and control the generation of complex images with multiple concepts, offering advantages in creative control and content customization compared to existing methods, especially for those working with DiT-based text-to-image models.  |
| Video Depth Anything: Consistent Depth Estimation for Super-Long Videos (Read more on [arXiv](https://arxiv.org/abs/2501.12375) or [HuggingFace](https://huggingface.co/papers/2501.12375))| Zilong Huang, Feihu Zhang, Shengnan Zhu, Hengkai Guo, Sili Chen | Video Depth Anything is a new method for producing temporally consistent depth estimations for arbitrarily long videos. The main research question is whether it is possible to achieve temporal stability in depth estimation for arbitrarily long videos while inheriting the capabilities of existing depth foundation models. The key methodology involves replacing the head of the Depth Anything V2 model with a spatial-temporal head and using a temporal gradient matching loss during training, along with a key-frame-based strategy for inference. The primary results show that the proposed model, Video Depth Anything, achieves state-of-the-art zero-shot video depth estimation, outperforming all baselines on temporal consistency across five datasets and achieving a Temporal Alignment Error (TAE) of 0.570 on the NYUv2 dataset. The principal implication for AI practitioners is that this model offers a new state-of-the-art approach for video depth estimation that maintains quality, consistency, and generalization ability without sacrificing efficiency, even for videos of several minutes in length.  |
| Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation (Read more on [arXiv](https://arxiv.org/abs/2501.12202) or [HuggingFace](https://huggingface.co/papers/2501.12202))| Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Zibo Zhao | Hunyuan3D 2.0 is an open-source system for generating high-resolution textured 3D assets from images using diffusion models. The main research objective is to develop a scalable 3D asset creation system that outperforms existing models in geometry details, condition alignment, and texture quality. The key methodology involves a two-stage pipeline: first, a shape generation model (Hunyuan3D-DiT) based on a flow-based diffusion transformer creates a bare mesh from an input image; second, a texture synthesis model (Hunyuan3D-Paint) generates a high-resolution texture map for the mesh. Primary results show that Hunyuan3D-ShapeVAE achieved a 93.6% volume Intersection of Union (V-IoU) in shape reconstruction, surpassing other models. The principal implication for AI practitioners is that Hunyuan3D 2.0 provides a strong foundation for large-scale 3D generative models, offering pre-trained weights and code for practical application in generating high-fidelity 3D assets.  |
| Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments (Read more on [arXiv](https://arxiv.org/abs/2501.10893) or [HuggingFace](https://huggingface.co/papers/2501.10893))| Tao Yu, Pengcheng Yin, Jinsung Yoon, Ruoxi Sun, Hongjin Su | Learn-by-interact is a data-centric framework for training LLM-based agents without human annotations. The main research question is how to adapt large language models (LLMs) to new environments without human annotations. The key methodology used is "backward construction," which synthesizes agent-environment interaction trajectories from documentation and constructs instructions by summarizing interaction histories. Primary results show that using this method, the baseline results are improved by up to 12.2% for in-context learning (ICL) with Claude-3.5-sonnet and 19.5% for training with Codestral-22B. The principal implication for AI practitioners is that they can use this framework to adapt LLMs to new environments efficiently, significantly reducing the reliance on manually annotated data.  |
| Reasoning Language Models: A Blueprint (Read more on [arXiv](https://arxiv.org/abs/2501.11223) or [HuggingFace](https://huggingface.co/papers/2501.11223))| Afonso Catarino, Ales Kubicek, Eric Schreiber, Julia Barth, Maciej Besta | Reasoning Language Models (RLMs) integrate large language models (LLMs) with reasoning mechanisms to enhance AI problem-solving. The main research question is: What is the detailed design of an RLM, and how can it achieve effectiveness, low cost, and scalability? The key methodology is a modular blueprint organizing RLM components, including reasoning structures (chains, trees, graphs), strategies (e.g., Monte Carlo Tree Search), reinforcement learning concepts, and supervision schemes, along with mathematical formulations and algorithmic specifications. A primary result is that the blueprint can model various existing RLMs, such as LLaMA-Berry and QwQ, as special cases, although specific quantitative performance metrics are not provided in the summary. The principal implication for AI practitioners is that the blueprint and the x1 framework provide tools for RLM development, experimentation, and analysis, potentially democratizing advanced reasoning capabilities.  |
| Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement (Read more on [arXiv](https://arxiv.org/abs/2501.12273) or [HuggingFace](https://huggingface.co/papers/2501.12273))| Chuyu Zhang, Mo Li, Taolin Zhang, Maosong Cao, zsytony | Condor is a two-stage framework for generating synthetic data to enhance the conversational capabilities of large language models (LLMs). The main research question is whether a novel knowledge-driven data synthesis and refinement framework can improve LLM alignment and performance on human-preference benchmarks. The key methodology involves constructing a World Knowledge Tree to generate diverse prompts, synthesizing question-answer pairs, and using Self-Reflection Refinement to improve response quality. The primary results show that a model fine-tuned on 20K Condor-generated samples achieved an average human-preference score of 61.29, judged by GPT4o-0806, surpassing the official model's score of 58.02. The principal implication for AI practitioners is that leveraging the Condor framework to generate high-quality synthetic data can significantly enhance LLM performance in subjective chat evaluations, even with relatively small datasets.  |
| EMO2: End-Effector Guided Audio-Driven Avatar Video Generation (Read more on [arXiv](https://arxiv.org/abs/2501.10687) or [HuggingFace](https://huggingface.co/papers/2501.10687))| Liefeng Bo, Bang Zhang, Qi Wang, Siqi Hu, Linrui Tian | EMO2 proposes a novel two-stage audio-driven talking head video generation method focusing on co-speech gesture generation.  The research objective was to address the weak correspondence between audio and full-body gestures by generating hand poses directly from audio in the first stage, followed by video frame synthesis using a diffusion model in the second stage.  The proposed method outperformed state-of-the-art approaches, such as CyberHost and Vlogger, in terms of visual quality and synchronization accuracy, with specific quantitative results showing an improvement in Diversity (DIV) scores. This work provides a robust framework for creating expressive and natural talking head animations, particularly relevant for AI practitioners working on audio-visual synchronization and diffusion model applications.  The paper does not provide a clear description of the specific quantitative improvement in all metrics across all datasets.  |
| GPS as a Control Signal for Image Generation (Read more on [arXiv](https://arxiv.org/abs/2501.12390) or [HuggingFace](https://huggingface.co/papers/2501.12390))| Andrew Owens, Alexei A. Efros, Aleksander Holynski, Ziyang Chen, chfeng | The paper introduces GPS conditioning as a novel control signal for image generation and 3D reconstruction using diffusion models. The main research question is whether GPS tags in photo metadata can be used to generate images that accurately reflect location-specific visual characteristics and to extract 3D models from 2D images. The key methodology involves training diffusion models conditioned on GPS coordinates and text prompts, and using GPS-guided score distillation sampling for 3D reconstruction. The primary results show that the method achieves an average CLIP score and GPS score of 18.02, outperforming baseline methods, and that angle-to-image diffusion models achieve 22.36% accuracy in generating images with the correct azimuth. The principal implication for AI practitioners is that GPS conditioning offers a new and effective way to control image generation and perform 3D reconstruction, leveraging the readily available geospatial information in photo metadata.  |
| MSTS: A Multimodal Safety Test Suite for Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.10057) or [HuggingFace](https://huggingface.co/papers/2501.10057))| Alicia Parrish, Janis Goldzycher, Felix Friedrich, Giuseppe Attanasio, Paul Röttger | This paper introduces MSTS, a Multimodal Safety Test Suite for evaluating the safety of Vision-Language Models (VLMs). The main research question is how to assess the novel safety risks posed by VLMs due to their multimodal inputs. The key methodology is the creation of 400 multimodal test prompts across 40 hazard categories, where each prompt's unsafe meaning is only evident when both image and text are combined. A primary result is that commercial VLMs were found to be very safe with less than 0.5% unsafe responses on average, whereas the least safe open VLM, xGen-MM, responded unsafely to 14.0% of test prompts. The principal implication for AI practitioners is that MSTS can be used to identify safety issues in VLMs, particularly highlighting safety disparities between open and commercial models and across different languages.  |
| InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model (Read more on [arXiv](https://arxiv.org/abs/2501.12368) or [HuggingFace](https://huggingface.co/papers/2501.12368))| Ziyu Liu, Yuhang Cao, Pan Zhang, Xiaoyi Dong, Yuhang Zang | InternLM-XComposer2.5-Reward is a multi-modal reward model designed to align large vision-language models (LVLMs) with human preferences. The main research question is how to create an effective multi-modal reward model for LVLMs that can handle diverse modalities and domains. The key methodology involves constructing a multi-modal preference dataset and training the model on this data by augmenting an existing LVLM (InternLM-XComposer2.5) with a scoring head. A primary result is that InternLM-XComposer2.5-Reward achieved a 70.0% Macro Accuracy on the VL-RewardBench benchmark. The principal implication for AI practitioners is that they can use this model to improve the quality of multi-modal chat, follow user instructions, and filter noisy or low-quality samples from pre-training and post-training datasets.  |
