

## Papers for 2025-03-05

| Title | Authors | Summary |
|-------|---------|---------|
| MPO: Boosting LLM Agents with Meta Plan Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.02682) or [HuggingFace](https://huggingface.co/papers/2503.02682))| sujianli, songff, Adagio, Rsy24, xwm | The paper introduces Meta Plan Optimization (MPO), a framework that enhances large language model (LLM) agents' planning capabilities by incorporating optimized, high-level meta plans. The main research objective is to improve LLM-based agents' performance on interactive planning tasks without requiring retraining for each new agent, while addressing planning hallucinations. MPO leverages a meta planner that generates abstract task strategies, optimized via a combination of supervised fine-tuning, Monte Carlo sampling, and Direct Preference Optimization (DPO) using agent feedback. Experiments on ALFWorld and ScienceWorld benchmarks demonstrate that MPO significantly outperforms existing baselines, with performance improvements of up to 100% for some agents. For AI practitioners, MPO offers a plug-and-play solution to boost agent performance and generalization in planning tasks, by incorporating general guidance that is improvable.  |
| Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.02846) or [HuggingFace](https://huggingface.co/papers/2503.02846))| Kai Chen, Chengqi Lyu, lindahua, ZwwWayne, vanilla1116 | Mask-DPO is a fine-grained factuality alignment method for LLMs that leverages sentence-level factuality to improve preference learning and reduce hallucinations. The main research objective is to develop a more effective and generalizable method for aligning LLMs with factual correctness, addressing limitations of response-level preference learning. The key methodology, Mask-DPO, incorporates sentence-level factuality annotations as mask signals in Direct Preference Optimization (DPO), selectively learning from correct sentences in preferred responses and avoiding penalties on factual content in non-preferred responses. Primary results show that Mask-DPO improved the factuality score of Llama3.1-8B-Instruct on the ANAH test set from 49.19% to 77.53%. Principal implication for AI practitioners is that Mask-DPO provides a more precise alignment technique that enhances factuality and generalization in LLMs, enabling the development of more reliable and trustworthy AI assistants.  |
| Wikipedia in the Era of LLMs: Evolution and Risks (Read more on [arXiv](https://arxiv.org/abs/2503.02879) or [HuggingFace](https://huggingface.co/papers/2503.02879))| Yao Wan, fjchendp, mgeng, sdzzxyl, hsm316 | This paper analyzes the impact of Large Language Models (LLMs) on Wikipedia, examining its evolution and potential risks to the broader NLP community. The primary research objective is to determine if and how LLMs have already impacted Wikipedia, and how this might influence the NLP community. The key methodology involves analyzing Wikipedia page views, article content, and simulating LLM impact on machine translation benchmarks and Retrieval-Augmented Generation (RAG) systems. Primary results indicate that Wikipedia articles have been influenced by LLMs, with an estimated impact of 1%-2% in certain categories and simulations show potential score inflations in machine translation benchmarks and performance reduction in RAG systems using LLM generated content. The principal implication for AI practitioners is that reliance on Wikipedia for training and evaluating NLP models may be affected by LLM-generated content, necessitating careful consideration of data provenance and potential biases.  |
| LADDER: Self-Improving LLMs Through Recursive Problem Decomposition (Read more on [arXiv](https://arxiv.org/abs/2503.00735) or [HuggingFace](https://huggingface.co/papers/2503.00735))| akiray1, TamasSimonds | LADDER is a framework enabling large language models (LLMs) to autonomously improve problem-solving through self-guided learning by recursively generating and solving simpler problem variants. The main research objective is to develop a method for LLMs to improve their mathematical integration capabilities without curated datasets or human feedback. The key methodology, LADDER, involves recursive generation of simpler problem variants, solution verification via numerical integration, and reinforcement learning (using GRPO) on the variant trees. LADDER improved a Llama 3.2 3B model's accuracy on undergraduate-level integration problems from 1% to 82%, and, with test-time reinforcement learning (TTRL) a Qwen 2.5 7B model achieved 90% on MIT Integration Bee. AI practitioners can leverage self-improving systems like LADDER and TTRL to enhance model capabilities in verifiable domains without extensive human supervision or data curation, demonstrating a practical path to developing more autonomous and capable AI.  |
| MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents (Read more on [arXiv](https://arxiv.org/abs/2503.01935) or [HuggingFace](https://huggingface.co/papers/2503.01935))| mikewang, ShuyiGuo, Thomas-X-Yang, zhaochenhong, Leozkl | MultiAgentBench is a benchmark designed to evaluate LLM-based multi-agent systems across diverse interactive scenarios, measuring task completion and the quality of collaboration and competition. The main research objective is to assess how well LLM-based multi-agent systems perform in collaborative and competitive environments, using novel milestone-based key performance indicators. The methodology involves evaluating various coordination protocols (star, chain, tree, graph) and strategies (group discussion, cognitive planning) in six interactive scenarios, including research, Minecraft, database, coding, bargaining, and Werewolf, developed using the MARBLE framework. Results show gpt-4o-mini achieves the highest average task score, graph structure performs best in research, and cognitive planning improves milestone achievement rates by 3%. For AI practitioners, the framework and benchmark provide a means to systematically evaluate and improve multi-agent coordination, which is critical in developing more effective and collaborative AI systems.  |
| PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.01328) or [HuggingFace](https://huggingface.co/papers/2503.01328))| Min Lin, Xinyi Wan, JialinLi, huanggx-sea, QPHutu | PipeOffload enhances pipeline parallelism (PP) scalability for large language models (LLMs) by optimizing activation memory usage through offloading. The main research objective is to address the activation memory bottleneck in PP that limits its scalability. The key methodology involves selectively offloading activations to host memory, prioritizing those with longer lifespans, and integrating a generalized interleaving strategy for balancing memory and throughput. The primary result is that PipeOffload reduces per-device activation memory in a better-than-linear manner, enabling up to a 19% acceleration compared to tensor parallelism (TP), while using less memory in applicable cases. For AI practitioners, PipeOffload provides a more scalable PP method, especially beneficial when full activation offload is feasible (k <= 1), allowing for more efficient training of large models.  |
| Iterative Value Function Optimization for Guided Decoding (Read more on [arXiv](https://arxiv.org/abs/2503.02368) or [HuggingFace](https://huggingface.co/papers/2503.02368))| Ruizhe Chen, jokephp, ab3223323, lljhbxt, zhliu | Iterative Value Function Optimization (IVO) is a novel framework for guided decoding that improves the accuracy of value estimation in language models without retraining the base model. The main research objective is to address the limitations of existing value-guided decoding methods, which suffer from inaccurate value estimation due to high variance and distribution shift. The key methodology involves two components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Primary results show that IVO achieves 77.52% GPT-4 win rates on the Multi-turn Dialogue task against the base policy, significantly outperforming baseline methods in terms of reward scores across various tasks. Principal implication for AI practitioners is that IVO offers a computationally efficient way to align language models with human values and task requirements, improving control over model outputs without expensive retraining.  |
| FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling (Read more on [arXiv](https://arxiv.org/abs/2502.14856) or [HuggingFace](https://huggingface.co/papers/2502.14856))| yuxuanli, zwl96, hyx21, ThonyPan, Achazwl | FR-Spec accelerates large-vocabulary language models by optimizing draft candidate selection in speculative sampling. The main research objective is to address the increased computational overhead of the LM Head in speculative sampling when using models with large vocabularies. The key methodology is frequency-ranked speculative sampling, which constrains the draft search to a frequency-prioritized token subset, reducing LM Head computation. Primary results show an average 1.12x speedup over the state-of-the-art speculative sampling method EAGLE-2 on multiple datasets, with optimized drafting reducing computation by 75%. For AI practitioners, this method provides a plug-and-play solution to accelerate existing speculative sampling techniques without retraining, directly improving inference speed for large-vocabulary language models.  |
| SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking (Read more on [arXiv](https://arxiv.org/abs/2503.00955) or [HuggingFace](https://huggingface.co/papers/2503.00955))| Thanh T. Tran, ThanhDi, TienAnh, xuandin, DavidNguyen | SemViQA is a Vietnamese language fact-checking system that enhances accuracy and efficiency through semantic understanding. The main research objective is to develop a robust fact-checking system for Vietnamese, a low-resource language, addressing challenges like semantic ambiguity and long-token sequences. The key methodology integrates Semantic-based Evidence Retrieval (SER), combining TF-IDF and a Question Answering Token Classifier (QATC), with a Two-step Verdict Classification (TVC) using Focal Loss and Cross-Entropy Loss. The system achieves a strict accuracy of 80.82% on the ViWikiFC dataset and 78.97% on the ISE-DSC01. The principal implication is that AI practitioners can leverage SemViQA's framework, particularly its SER and TVC components, to develop more efficient, robust, and effective fact-checking systems that handle complex linguistic structures, especially in low-resource languages.  |
| UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface (Read more on [arXiv](https://arxiv.org/abs/2503.01342) or [HuggingFace](https://huggingface.co/papers/2503.01342))| windmillknight, Shawnee-bxy, Haiyang-W, chenweix7, kanashi6 | UFO unifies fine-grained visual perception tasks through an open-ended language interface, achieving state-of-the-art performance without task-specific decoders. The main research objective is to effectively integrate fine-grained perception tasks (like detection and segmentation) into multimodal large language models (MLLMs) without relying on complex, task-specific designs. The key methodology involves transforming all perception targets into the language space and using a novel embedding retrieval approach for segmentation, relying solely on the language interface. After multi-task training, UFO outperforms previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. AI practitioners can leverage UFO's unified framework to simplify architectural design and training, seamlessly integrating fine-grained perception capabilities into MLLMs for enhanced visual understanding and enabling more challenging vision-language tasks.  |
| ATLaS: Agent Tuning via Learning Critical Steps (Read more on [arXiv](https://arxiv.org/abs/2503.02197) or [HuggingFace](https://huggingface.co/papers/2503.02197))| Yuxuan Huang, Ming Li, Zhixun Chen, zhoutianyi, YaliDU | ATLAS finetunes large language model (LLM) agents on critical steps within expert trajectories to improve generalization and reduce training costs. The main research objective is to develop a more efficient and effective agent tuning method by identifying and focusing on critical steps in expert trajectories. The key methodology, ATLAS, uses an oracle LLM to select critical steps based on criteria like plan creation, critical observation, critical action, and self-correction, then finetunes the agent's LLM solely on these steps. Results show that an LLM finetuned on only ~30% critical steps selected by ATLAS outperforms the LLM finetuned on all steps and recent open-source LLM agents. The principal implication is that AI practitioners can achieve better agent generalization and performance with reduced training costs by focusing LLM finetuning on semantically critical steps identified by an oracle LLM.  |
| Language Models can Self-Improve at State-Value Estimation for Better Search (Read more on [arXiv](https://arxiv.org/abs/2503.02878) or [HuggingFace](https://huggingface.co/papers/2503.02878))| rittera, emendes3 | Self-taught lookahead (STL) enables language model-based value functions to improve without ground truth rewards by leveraging state-transition dynamics. The main research objective is to demonstrate that an LLM-based value function can self-improve without labels or rewards, outperforming computationally expensive methods. The key methodology, STL, fine-tunes a value model by predicting the next best action, resulting state, and value rationale, bootstrapping from an initial value function using lookahead in tree search. Results show that STL-improved models match the performance of a GPT-4 value model, improving performance by 20% while reducing inference costs 37x compared to prior LLM-based tree search. Principal implication is that AI practitioners can utilize STL to train efficient and effective value models for search-based tasks, reducing reliance on expensive closed-source models and ground truth rewards.  |
| RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification (Read more on [arXiv](https://arxiv.org/abs/2503.02537) or [HuggingFace](https://huggingface.co/papers/2503.02537))| Liang Hou, dizhang, wileewang, PaulSHEN1, YZCS | RectifiedHR is a training-free method for generating high-resolution images with diffusion models by addressing energy decay and employing noise refresh. The main objective is to enable diffusion models to efficiently generate images at resolutions higher than their training resolution without additional training. The key methodology involves a noise refresh strategy to progressively increase resolution during sampling and an energy rectification strategy that adjusts classifier-free guidance to mitigate image blurriness. The primary result is that RectifiedHR achieves a FID score of 25.347 and a CLIP score of 33.756 at 2048x2048 resolution, outperforming several baselines in image quality while using less computing time. The principal implication is that AI practitioners can generate high-quality, high-resolution images using pre-trained diffusion models without costly retraining or complex modifications, by using noise refresh and energy rectification steps during image generation.  |
| SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models (Read more on [arXiv](https://arxiv.org/abs/2503.02876) or [HuggingFace](https://huggingface.co/papers/2503.02876))| Ekaterina Ivanova, alpchel, mgvz | SPIDER is a new multi-organ histopathology dataset with baseline models for patch-level classification and whole-slide image segmentation. The main research objective is to create and evaluate a large, high-quality, multi-organ, patch-level histopathology dataset with comprehensive class coverage, along with baseline classification models. Key methodology used is a semi-automatic annotation pipeline, expert pathologist verification, feature extraction with Hibou-L foundation model, and an attention-based classification head. Primary results of SPIDER's evaluation include, on the thorax test set, model achieved an accuracy of 0.962, precision of 0.958, and F1 score of 0.960. AI practitioners can use this dataset and models to improve digital pathology tasks like tissue classification and rapid identification, providing a new benchmark for future developments in this field.  |
| Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content (Read more on [arXiv](https://arxiv.org/abs/2503.02357) or [HuggingFace](https://huggingface.co/papers/2503.02357))| Zicheng Zhang, GTZhai, a9108, sl2782087, wcain | The paper introduces Q-Eval-100K, a large-scale dataset, and Q-Eval-Score, a unified model, for evaluating visual quality and text-image/video alignment in text-to-vision generation. The main research objective is to develop a comprehensive benchmark and method for assessing both the visual quality and text-alignment of content generated by text-to-vision models. The key methodology involves collecting 100K instances (images and videos) with 960K human annotations of Mean Opinion Scores (MOS) and developing Q-Eval-Score, a Large Multimodal Model (LMM) fine-tuned using a context-prompt format. The primary results show that Q-Eval-Score achieves a 0.943 SRCC for image visual quality at the model-level, outperforming existing methods, it also introduces Vague-to-Specific Strategy for long prompt alignment. AI practitioners can use Q-Eval-100K and Q-Eval-Score as a reliable benchmark and evaluation metric to assess and improve the performance of text-to-vision generative models, focusing on both visual quality and text-alignment.  |
| IterPref: Focal Preference Learning for Code Generation via Iterative Debugging (Read more on [arXiv](https://arxiv.org/abs/2503.02783) or [HuggingFace](https://huggingface.co/papers/2503.02783))| Ruihang, yangyu90, Jianwen2003, CharonBony, Ringo1110 | IterPref is a new preference alignment framework for code generation that improves Code LLMs through iterative debugging. The research objective is to address the limitation of existing preference learning methods that do not pinpoint specific code errors, hindering the learning of informative error correction patterns. The key methodology is IterPref, which involves creating the CodeFlow dataset where code is iteratively refined until passing tests, and using a tailored DPO algorithm to align corresponding tokens for error regions. Primary result is that, equipped with IterPref, Qwen2.5-Coder-7B achieved a 29.7% pass@1 score on BigCodeBench Complete Hard, on par with some much larger models. For AI practitioners, this implies an effective way to enhance code generation models that leverages an iterative debugging process for precise preference learning, focusing model's learning on correcting critical errors.  |
| AppAgentX: Evolving GUI Agents as Proficient Smartphone Users (Read more on [arXiv](https://arxiv.org/abs/2503.02268) or [HuggingFace](https://huggingface.co/papers/2503.02268))| Chi Zhang, Wenjia Jiang, xuyang, ChenxiSong, yyzhuang2 | AppAgentX introduces an evolutionary framework for GUI agents that improves operational efficiency on smartphones while maintaining adaptability. The main research objective is to address the inefficiency of LLM-based GUI agents in performing routine tasks by enabling them to learn and evolve high-level actions. The key methodology involves a memory mechanism that records task execution history, allowing the agent to identify repetitive action sequences and replace them with abstract, high-level actions represented as "shortcut nodes". Primary results show that on the AppAgent benchmark, AppAgentX reduced the average steps per task from 9.1 to 5.7 and increased the success rate from baseline 16.9% to 71.4% . For AI practitioners, this evolutionary framework offers a method to develop GUI agents that execute routine operations more efficiently while using LLM only to optimize new behavior, thus improving the balance between intelligence and efficiency in practical applications.  |
