

## Papers for 2025-03-06

| Title | Authors | Summary |
|-------|---------|---------|
| Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers (Read more on [arXiv](https://arxiv.org/abs/2503.00865) or [HuggingFace](https://huggingface.co/papers/2503.00865))| LidongBing, maljunied, jhying, lukecq, Yiran0924 | Babel is an open multilingual large language model that supports 25 languages, covering over 90% of global speakers. The main objective is to develop an open-source multilingual LLM that addresses the underrepresentation of many widely spoken languages in existing models. The key methodology is layer extension, adding new layers to an existing model (Qwen2.5) and pre-training on a curated dataset emphasizing under-resourced languages. Babel-83B-Base achieves an average score of 73.2 across six multilingual benchmarks, outperforming comparable open models like Qwen2.5-72B (69.8). AI practitioners can use Babel as a strong base or chat model for multilingual applications, benefiting from enhanced performance, especially in low-resource languages, and from the use of layer extension in scaling the model.  |
| ABC: Achieving Better Control of Multimodal Embeddings using VLMs (Read more on [arXiv](https://arxiv.org/abs/2503.00329) or [HuggingFace](https://huggingface.co/papers/2503.00329))| Florian Kerschbaum, Benjamin Schneider, wenhu | ABC is a multimodal embedding model that uses a vision-language model (VLM) backbone to integrate natural language instructions with visual inputs for improved control over embeddings. The main research objective is to develop a model that can effectively utilize user instructions to control and refine multimodal embeddings, overcoming limitations of existing CLIP-based models. The key methodology involves a two-stage training process: contrastive pretraining with mined negatives and instruction fine-tuning using synthetic instructions generated from image captions. The model achieves best-for-size performance on MSCOCO image-to-text retrieval with a R@1 score of 69.2 and outperforms all other models on the Massive Multimodal Embedding Benchmark (MMEB) for classification and VQA tasks. AI practitioners can use ABC's architecture and training approach to create multimodal embedding models with enhanced control via natural language, resulting in a flexible tool that improves performance of visual retrieval, classification, and VQA, as well as the ability to complete unique, instruction-specific tasks.  |
| Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions (Read more on [arXiv](https://arxiv.org/abs/2503.03278) or [HuggingFace](https://huggingface.co/papers/2503.03278))| Cosmin I. Bercea, Rossella Arcucci, Wenjia Bai, Jun Li, che111 | This paper introduces a method to improve medical abnormality grounding in vision-language models (VLMs) using decomposed knowledge descriptions. The main research objective is to enhance the performance of VLMs in detecting and localizing medical abnormalities in images by improving the alignment between textual descriptions and visual features. The key methodology involves decomposing medical concepts into fundamental attributes and visual patterns, and using these attribute-based descriptions as prompts during VLM training. The proposed method, trained on only 1.5% of the data used by larger models, achieved a RoDeO score of 54.38% on the VinDr-CXR dataset, comparable to 7B parameter models like RadVLM. AI practitioners can use this knowledge-enhanced approach to achieve competitive performance in medical image abnormality grounding with significantly smaller VLMs and less training data, and improve zero-shot generalization.  |
| GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control (Read more on [arXiv](https://arxiv.org/abs/2503.03751) or [HuggingFace](https://huggingface.co/papers/2503.03751))| Yifan Lu, Huan Ling, Jiahui Huang, Tianchang Shen, xrenaa | GEN3C is a generative video model with precise camera control and temporal 3D consistency. The main research objective is to develop a video generation model that allows for precise camera control and maintains 3D consistency across generated frames. The key methodology involves constructing a 3D cache (point clouds from depth estimates) and rendering it with user-provided camera trajectories to condition a fine-tuned video diffusion model. The results demonstrate that GEN3C achieves a PSNR of 18.66 and an SSIM of 0.67 on the Tanks-and-Temples dataset for single-view video generation, outperforming baselines. For AI practitioners, GEN3C offers a method for generating 3D-consistent videos with precise camera control by conditioning video generation on 3D renderings, improving controllability and consistency compared to prior video generation models.  |
| KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding (Read more on [arXiv](https://arxiv.org/abs/2503.02951) or [HuggingFace](https://huggingface.co/papers/2503.02951))| Radha Poovendran, mingyuanzhou, yyqoni, nlpyang, flydust | KODCODE is a synthetic dataset of 447K coding problems with verified solutions and unit tests, designed to enhance code LLM training. The main research objective is to create a large-scale, diverse, and verifiable coding dataset that addresses limitations in existing resources for training large language models (LLMs) for code. The methodology involves a three-step pipeline: coding question synthesis from 12 sources, solution and test generation with self-verification, and post-training data synthesis via question rewriting and test-based rejection sampling using DeepSeek-R1. Models fine-tuned on KODCODE-SFT achieved a 61.26% average score across five coding benchmarks, outperforming models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B. The principal implication is that AI practitioners can use KODCODE to improve the performance of code LLMs in supervised fine-tuning and potentially RL training, with verified solutions and tests offering advantages for various code-related tasks.  |
| CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom (Read more on [arXiv](https://arxiv.org/abs/2503.01836) or [HuggingFace](https://huggingface.co/papers/2503.01836))| Pan Zhou, Wenxuan Shen, Lingfeng Yang, shuaishuaicdp, yisenL | CROWDSELECT, a novel synthetic instruction data selection framework, leverages multi-LLM responses and reward scores for improved instruction tuning. The main research objective is to investigate whether multi-dimensional signals derived from multiple LLMs can enhance the selection of synthetic instruction-response pairs for instruction tuning. The key methodology involves calculating three metrics (Difficulty, Separability, Stability) from multiple LLM responses and reward model assessments, and then integrating these with a clustering-based approach for diverse data selection. Primary results show that CROWDSELECT achieves state-of-the-art performance, improving instruction tuning by 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. The principal implication for AI practitioners is that leveraging multi-LLM wisdom through the proposed metrics and framework can lead to more efficient and effective instruction tuning, improving the performance of distilled smaller models.  |
| QE4PE: Word-level Quality Estimation for Human Post-Editing (Read more on [arXiv](https://arxiv.org/abs/2503.03044) or [HuggingFace](https://huggingface.co/papers/2503.03044))| Malvina Nissim, Ana Guerberof-Arenas, Grzegorz Chrupała, Vilém Zouhar, gsarti | The QE4PE study investigates the impact of word-level quality estimation (QE) on professional machine translation post-editing, finding that factors beyond QE accuracy influence its real-world usefulness. The main research objective was to measure the effect of word-level QE error span highlighting on the editing quality, productivity, and usability in a realistic post-editing workflow. The methodology involved 42 professional translators post-editing machine-translated texts in English-Italian and English-Dutch, using four highlight modalities (supervised, unsupervised, oracle, and no highlights) and logging their editing behavior. Results showed that highlight modalities are not solely predictive of editing time and that cross-modality highlight overlap ranged between 15% and 39%. This implies that AI practitioners should consider factors beyond accuracy, such as domain, language, and user-specific factors, to improve the integration of word-level QE in post-editing tools and enhance their real-world usability.  |
| Exploring Rewriting Approaches for Different Conversational Tasks (Read more on [arXiv](https://arxiv.org/abs/2502.18860) or [HuggingFace](https://huggingface.co/papers/2502.18860))| Xiang Chen, Mike Rimer, Ryan A. Rossi, Md Mehrab Tanjim, Franck-Dernoncourt | This paper systematically investigates query rewriting and fusion approaches for conversational AI tasks. The main research question is whether a single LLM-based query rewrite module can be universally effective across diverse conversational scenarios or if specialized modules are needed. The key methodology involves evaluating two parameterized query rewriting approaches (query rewrite and query fusion) on three datasets: conversational text-based Q&A and two text-to-visualization tasks (short and long conversations). The primary result is that for the conversational text-based Q&A task, the query rewrite approach achieved a 3.9% higher mean cosine similarity than query fusion, while for long text-to-vis tasks, query fusion had 7.6% high mean cosine similarity. The principal implication is that AI practitioners should select a query rewriting approach (either query rewrite and query fusion) that aligns with the specific conversational task and data characteristics, as no single approach is universally superior.  |
| Process-based Self-Rewarding Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.03746) or [HuggingFace](https://huggingface.co/papers/2503.03746))| Zheheng Luo, Junxiao Liu, Xin Zhang, Shimao Zhang, lx865712528 | The paper introduces Process-based Self-Rewarding Language Models, enhancing mathematical reasoning by incorporating step-wise evaluations and preference optimization. The main research objective is to improve the mathematical reasoning capabilities of large language models (LLMs) using a self-rewarding paradigm without external human feedback. The key methodology involves iterative training with step-wise LLM-as-a-Judge evaluations and step-wise preference optimization using Direct Preference Optimization (DPO). The primary result is that the 72B model, after four iterations, achieved an average accuracy of 60.6 across several math benchmarks, an improvement over the starting accuracy. The principal implication is that AI practitioners can improve LLMs' mathematical reasoning performance, through iterative self-improvement without human-annotated data.  |
| Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective (Read more on [arXiv](https://arxiv.org/abs/2503.01933) or [HuggingFace](https://huggingface.co/papers/2503.01933))| KartikAngadi, kruthika, SyedAbdul, RakshitAralimatti | The paper introduces the Shakti series of Small Language Models (SLMs) designed for efficient on-device AI, focusing on domain-specific applications. The main objective is to develop SLMs that can overcome resource constraints of edge devices while maintaining high performance in specialized domains. Key methodologies include a combination of efficient transformer architectures, quantization-aware training, supervised fine-tuning, and preference alignment (RLHF or DPO). Primary results show that Shakti-500-Q4 achieves 583.88 tokens per second (TPS) on an NVIDIA L40s GPU and the Shakti-250M model, after fine-tuning, achieves 0.86 answer relevance score in finance domain. The paper's principal implication is that carefully engineered and fine-tuned compact models can effectively be deployed on edge devices, offering a practical approach for real-world, domain-specific AI applications with limited computational resources.  |
| Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases (Read more on [arXiv](https://arxiv.org/abs/2502.20317) or [HuggingFace](https://huggingface.co/papers/2502.20317))| Ryan A. Rossi, Haoyu Han, Yongjia Lei, mhalappa, Franck-Dernoncourt | This paper proposes a Mixture of Structural-and-Textual Retrieval (MoR) framework for answering queries over text-rich graph knowledge bases (TG-KBs). The main research objective is to develop a retrieval method that effectively combines both textual and structural information from TG-KBs to improve query answering performance. The key methodology is a Planning-Reasoning-Organizing framework, where the Planning stage generates textual planning graphs, the Reasoning stage interweaves structural traversal and textual matching, and the Organizing stage reranks candidates based on their structural trajectory. The primary result shows that MoR achieved an average Hit@1 score of 48.93%, outperforming other baselines on three TG-KB datasets. The principal implication is that AI practitioners can leverage MoR's mixture-of-experts approach to improve retrieval performance in applications that use the graph knowledge bases by harmonizing textual and structural signals, especially useful to combine and rank structural knowledge from graph data with traditional text features.  |
| Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.01763) or [HuggingFace](https://huggingface.co/papers/2503.01763))| Shuaiqiang Wang, Pengjie Ren, Lingyong Yan, Yuhan Wang, Zhengliang Shi | The paper introduces TOOLRET, a new benchmark for evaluating information retrieval (IR) models on tool retrieval tasks for large language models (LLMs). The main research objective is to assess the performance of existing IR models in retrieving relevant tools for LLMs in diverse, real-world scenarios, and to analyze the impact of retrieval quality on end-to-end task performance. The key methodology involves collecting and curating a large-scale dataset of 7.6k retrieval tasks and 43k tools from existing datasets, evaluating various IR models (sparse, dense, and re-ranking) on this benchmark, and contributing a large scale training dataset (TOOLRET-train) to improve retrieval performance. A primary result is that the best-performing model (NV-embedd-v1) achieves an nDCG@10 of only 33.83 on the benchmark, indicating existing IR models struggle with tool retrieval. The principal implication is that AI practitioners need to develop new retrieval methods tailored for tool retrieval, or improve upon current methods using target-aware reasoning and large-scale training data, as shown in the paper using TOOLRET-train, since current strong IR models are not effective for tool retrieval.  |
| FLAME: A Federated Learning Benchmark for Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2503.01729) or [HuggingFace](https://huggingface.co/papers/2503.01729))| Danica Kragic, Yuchong Zhang, Miguel Vasco, Alberta Longhini, Santiago Bou Betran | FLAME is a new benchmark for federated learning in robotic manipulation, providing datasets and a framework for distributed training. The main objective is to evaluate federated learning (FL) strategies for training robotic manipulation policies in a distributed, privacy-preserving manner. The key methodology involves creating a large-scale dataset of diverse manipulation tasks across multiple simulated environments and integrating it into a FL framework using FLOWER, where local models are trained and aggregated. Primary results show that Federated Averaging (FedAvg) achieves a 2.64 ± 0.13 RMSE on the Slide Block to Target task, but performance varies significantly across tasks and FL methods. The principal implication for AI practitioners is that FLAME provides a standardized benchmark for evaluating and developing scalable, adaptive, and privacy-aware robotic learning systems, although further development in FL algorithms are necessary.  |
| Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection (Read more on [arXiv](https://arxiv.org/abs/2503.01449) or [HuggingFace](https://huggingface.co/papers/2503.01449))| Hung Nguyen, Martin Weyssow, Yindu Su, Chengran Yang, Ting Zhang | This paper presents a comprehensive empirical study evaluating large language models (LLMs) on software vulnerability detection (SVD) across multiple programming languages. The main research objective is to investigate the effectiveness of various LLMs in predicting software vulnerabilities, comparing them with smaller language models (SLMs) and static application security testing (SAST) tools, and exploring strategies to improve LLM performance. The key methodology involves compiling a multi-language dataset (Python, Java, JavaScript) of vulnerable functions, evaluating five open-source LLMs using prompt engineering, instruction tuning, and sequence classification fine-tuning, and comparing them against SLMs and SAST tools. The results show that fine-tuned LLMs achieved the best F1-score of 0.443 on the JavaScript dataset, with performance varying significantly across programming languages and adaptation strategies. The principal implication for AI practitioners is that while LLMs show promise for SVD, particularly in JavaScript with fine-tuning, performance is highly dependent on data characteristics, requiring careful consideration of language, model selection, and adaptation strategies.  |
| CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs (Read more on [arXiv](https://arxiv.org/abs/2503.01378) or [HuggingFace](https://huggingface.co/papers/2503.01378))| Artyom Myshlyaev, Oleg Sautenkov, Muhammad Haris Khan, Valerii Serpiva, Artem Lykov | CognitiveDrone, a Vision-Language-Action (VLA) model and benchmark for real-time cognitive task solving in UAVs, is introduced. The main research objective is to develop and evaluate a UAV control system capable of performing complex cognitive tasks, including human recognition, symbol understanding, and reasoning, based on visual input and textual instructions. The methodology combines a 7B-parameter VLA model (adapted from OpenVLA) trained on a dataset of over 8,000 simulated flight trajectories with an optional 7B-parameter VLM reasoning module (Qwen2.5-VL based) for task refinement, and evaluates performance within a Gazebo-based simulation benchmark (CognitiveDroneBench). The CognitiveDrone-R1 model, incorporating the reasoning module, achieved a 77.2% overall success rate, outperforming the base CognitiveDrone model (59.6%) and a racing-oriented model (RaceVLA, 31.3%). AI practitioners can utilize the provided open-source dataset, benchmark environment, and model weights to develop and evaluate VLA models for UAVs that incorporate cognitive capabilities beyond basic navigation and control.  |
| Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions (Read more on [arXiv](https://arxiv.org/abs/2503.00502) or [HuggingFace](https://huggingface.co/papers/2503.00502))| Peng Hang, Chen Lv, Chengkai Xu, Jiaqi Liu, FanGShiYuu | This paper introduces an LLM-driven Actor-Reasoner framework for autonomous vehicles (AVs) to improve bidirectional interactions with human-driven vehicles (HVs). The main objective is to enhance AVs' real-time decision-making and intent expression capabilities in complex driving scenarios with heterogeneous HVs. The methodology involves a parallel Actor-Reasoner architecture; the Reasoner uses an LLM with Chain-of-Thought (CoT) reasoning to infer HV driving styles and generate eHMI displays, while the Actor employs a two-layer memory retrieval mechanism from a database constructed during training with simulated HVs. Results show that the proposed framework achieves a 94% success rate in intersection scenarios, and a memory partition module improves retrieval speed by an average of 12%. AI practitioners can use this framework as a method to integrate LLMs into real-time decision-making systems, addressing LLM inference speed limitations by combining reasoning capabilities with memory-based fast retrieval.  |
| SwiLTra-Bench: The Swiss Legal Translation Benchmark (Read more on [arXiv](https://arxiv.org/abs/2503.01372) or [HuggingFace](https://huggingface.co/papers/2503.01372))| Yingqiang Gao, Sina Ahmadi, Luka Nenadic, Jakob Merane, Joel Niklaus | SwiLTra-Bench introduces a multilingual benchmark for evaluating LLM-based translation systems on Swiss legal texts, comprising 180K aligned translation pairs across five languages. The main research objective was to evaluate the performance of frontier LLMs and fine-tuned open SLMs on Swiss legal translations in zero-shot and fine-tuning settings, including the development of an LLM-based evaluation metric. Key methodology included systematic evaluation using lexical and model-based metrics, fine-tuning open SLMs, human expert validation, and developing a specialized LLM evaluation system (SwiLTra-Judge). Primary results showed that frontier models like Claude-3.5-Sonnet outperformed others, achieving a GEMBA-MQM score of 80.66, while fine-tuned open SLMs improved but still lagged behind. For AI practitioners, this benchmark and the associated evaluations highlight that while frontier models provide superior legal text translation, fine-tuning offers significant improvement for open SLMs, and SwiLTra-Judge can serve as a reliable automated evaluation tool that aligns well with human experts.  |
