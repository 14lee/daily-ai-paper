

## Papers for 2025-03-31

| Title | Authors | Summary |
|-------|---------|---------|
| AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through
  Lightweight Vocabulary Adaptation (Read more on [arXiv](https://arxiv.org/abs/2503.19693) or [HuggingFace](https://huggingface.co/papers/2503.19693))| Roi Reichart, ehoffer, eyalbd, nitay, itaynakash | AdaptiVocab enhances Large Language Model (LLM) efficiency in focused domains through lightweight vocabulary adaptation. Its objective is to reduce latency and computational costs in domain-specific, low-resource settings by optimizing the LLM's vocabulary. The methodology involves replacing low-frequency general tokens with high-frequency domain-specific n-gram tokens based on a token-saving score, initializing new embeddings using exponential weighting, and performing lightweight fine-tuning on embedding and adjacent layers. Results across two 7B LLMs and three niche domains show over a 25% reduction in token usage for both input processing and output generation, without compromising generation quality or end-task performance. For AI practitioners, this offers a resource-efficient technique to improve the inference speed and reduce the operational cost of LLMs deployed for specialized applications, particularly in settings with limited data or computational budgets. |
| Exploring Data Scaling Trends and Effects in Reinforcement Learning from
  Human Feedback (Read more on [arXiv](https://arxiv.org/abs/2503.22230) or [HuggingFace](https://huggingface.co/papers/2503.22230))| amusingchao, qingping95, zhengwu07, glnbyte, Swtheking | This paper investigates data scaling challenges in RLHF, proposing data construction and training strategies to mitigate reward hacking and improve response diversity. The primary objective is to identify and overcome data-driven bottlenecks hindering RLHF performance scaling. Methodology involves a hybrid reward system combining Reasoning Task Verifiers (RTV) and Generative Reward Models (GenRM) with ground truth, alongside a Pre-PPO prompt selection method prioritizing challenging prompts and early-stage math/coding task training. Results demonstrate the proposed 'Data Scale' approach significantly outperforms baseline PPO, achieving a +1.4 overall score improvement on the challenging TestSet V2.0 for the large model, and RTV exhibited the strongest resistance to reward hacking. For AI practitioners, this work highlights that strategic data curation and robust reward mechanisms (like RTV/GenRM-GT) are critical for enhancing RLHF performance and scalability, offering practical methods to address reward hacking and diversity issues. |
| Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation (Read more on [arXiv](https://arxiv.org/abs/2503.22675) or [HuggingFace](https://huggingface.co/papers/2503.22675))| Xu Chen, Jun Xu, TengShi, KID-22, TangJiakai5704 | This paper introduces ReaRec, an inference-time framework that enhances sequential recommendation (SeqRec) models by incorporating multi-step implicit reasoning. The objective is to overcome the limitations of traditional direct forward inference in capturing complex user preference dynamics, especially for long-tail items. ReaRec achieves this by autoregressively feeding the last hidden state back into the SeqRec model, using specialized reasoning position embeddings, and employs two learning strategies: Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL). Empirical results show ReaRec improves performance by an average of 7.49% across metrics on five datasets, and notably, post-hoc analysis reveals it can raise the performance ceiling of backbone SeqRec models by approximately 30-50%. For AI practitioners, ReaRec presents a model-agnostic method to potentially improve existing SeqRec systems by strategically increasing computation during inference rather than solely relying on model parameter scaling. |
| A Survey of Efficient Reasoning for Large Reasoning Models: Language,
  Multimodality, and Beyond (Read more on [arXiv](https://arxiv.org/abs/2503.21614) or [HuggingFace](https://huggingface.co/papers/2503.21614))| Elliott, weigao266, Warrieryes, yaful, Xiaoye08 | This survey reviews methods to enhance the computational efficiency of reasoning processes in Large Reasoning Models (LRMs) throughout their development lifecycle. The paper's objective is to categorize patterns of reasoning inefficiency, such as excessive token generation and overthinking simple problems, and provide a comprehensive overview of techniques aiming to improve reasoning efficiency. Methodologically, it defines reasoning efficiency η(M) = E[Q(M,D) / C(M,D)] and systematically surveys literature, classifying techniques across pretraining, SFT, RL, and inference stages, including length budgeting, model switching, reasoning chain compression, and architectural modifications. Primary results highlight significant inefficiencies, exemplified by an LRM (QwQ-32B) using nearly 40 times more tokens than an instruction-tuned model for a simple math problem, and detail various strategies to reduce computational cost, often involving a trade-off with performance accuracy. The principal implication for AI practitioners is the catalog of techniques (e.g., length budgeting, SFT compression, latent-space reasoning) that can be applied to mitigate excessive token usage and latency, enabling more cost-effective and resource-aware deployment of LRMs, especially in applications like agent-based systems. |
| ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.22194) or [HuggingFace](https://huggingface.co/papers/2503.22194))| Jihyun Lee, Minhyuk, 32V, daehyeonchoi, myhong | ORIGEN introduces the first zero-shot method for grounding 3D orientation for multiple objects in text-to-image generation. The main research objective is to enable controllable 3D orientation in generated images without requiring specific training data or being limited to single objects or synthetic data. The key methodology involves a reward-guided sampling approach using a pretrained orientation estimation model (OrientAnything) and a one-step generative flow model, optimized via Langevin dynamics with adaptive time rescaling. Quantitative results on the MS-COCO-Single benchmark show ORIGEN achieves significantly better orientation alignment (e.g., 87.1% Acc.@22.5° azimuth accuracy) compared to prior orientation-conditioned models and training-free guidance methods. For AI practitioners, this provides a training-free mechanism to impose precise 3D orientation constraints on generated objects, improving spatial controllability in text-to-image synthesis for complex scenes. |
| Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2503.20785) or [HuggingFace](https://huggingface.co/papers/2503.20785))| skhu101, GuangcongWang, FrozenBurning, Inso, tqliu | Free4D introduces a tuning-free framework for generating spatially-temporally consistent 4D scenes from a single image or text input. The primary objective is to produce high-quality, controllable 4D scene representations from limited observations without expensive training or finetuning, ensuring spatial-temporal consistency. Key methodologies involve initializing 4D geometry using image-to-video diffusion and dynamic reconstruction, generating consistent multi-view videos via adaptive guidance and latent replacement strategies, and optimizing a final 4D Gaussian Splatting representation using a coarse-to-fine strategy with modulation-based refinement. Compared to the text-to-4D baseline 4Real on VBench, Free4D demonstrates improved performance in Dynamics (47.4% vs 32.3%) and Aesthetics (64.7% vs 50.9%). For AI practitioners, this work offers an efficient pipeline for generating dynamic 4D scenes directly from single images, reducing reliance on large-scale 4D datasets or model tuning for applications in immersive media and virtual environments. |
| PHYSICS: Benchmarking Foundation Models on University-Level Physics
  Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2503.21821) or [HuggingFace](https://huggingface.co/papers/2503.21821))| armanc, jsous, henryL7, yilunzhao, Carrie777 | This paper introduces PHYSICS, a benchmark with 1,297 university-level physics problems to evaluate foundation models' advanced problem-solving skills. The primary objective is to assess foundation models' abilities in multi-step reasoning, mathematical derivation, and domain-specific knowledge application in physics. The methodology involves expert annotation of PhD-qualifying exam problems and a robust automated evaluation system combining SymPy-based verification with GPT-4o assessment. Results show significant limitations even for top models, with the best proprietary model (o3-mini) achieving only 59.9% accuracy, revealing persistent challenges in calculation, assumption validity, and knowledge integration. For AI practitioners, this highlights the substantial gap remaining for models to reach expert-level scientific reasoning, necessitating further research into robust mathematical handling and effective knowledge grounding. |
| Perceptually Accurate 3D Talking Head Generation: New Definitions,
  Speech-Mesh Representation, and Evaluation Metrics (Read more on [arXiv](https://arxiv.org/abs/2503.20308) or [HuggingFace](https://huggingface.co/papers/2503.20308))| taehyunoh, akasha9890, backryun, Han-EunGi, Chae-Yeon | This paper defines criteria and introduces a speech-mesh representation and metrics for perceptually accurate 3D talking head generation. The research aims to define and improve the perceptual accuracy of lip movements in speech-driven 3D talking heads, focusing on Temporal Synchronization, Lip Readability, and Expressiveness. A speech-mesh synchronized representation is developed using a two-stage training process, leveraging large-scale 2D audio-visual data before aligning with 3D mesh data, and is applied as a perceptual loss and metric (PLRS), alongside two new physical metrics (MTM for synchronization, SLCC for expressiveness). Experiments show that incorporating the proposed perceptual loss significantly improves existing models across all three criteria; for instance, applying it to FaceFormer on the VOCASET dataset improved the Perceptual Lip Readability Score (PLRS) from 0.368 to 0.463. AI practitioners can utilize the proposed perceptual loss to enhance the realism of 3D talking heads and employ the introduced metrics (MTM, PLRS, SLCC) for a more comprehensive, perceptually-grounded evaluation beyond traditional geometric error metrics like LVE. |
| Segment Any Motion in Videos (Read more on [arXiv](https://arxiv.org/abs/2503.22268) or [HuggingFace](https://huggingface.co/papers/2503.22268))| Nan Huang, qianqian68, akanazawa, kurtkeutzer, chenfengx | This paper introduces a novel method for Moving Object Segmentation (MOS) by integrating long-range trajectories, semantic features, and foundation model prompting. The objective is to accurately segment objects based solely on their observable motion within a video, even in challenging scenarios like occlusions or complex deformations. The methodology combines long-range point tracks with DINO semantic features using specialized Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding, followed by an iterative prompting strategy with SAM2 to generate dense masks from sparse tracks. The proposed approach achieves state-of-the-art results on multiple benchmarks, including a 91.0 F-score on the DAVIS2016 MOS task, outperforming previous methods. For AI practitioners, this work demonstrates a powerful technique for video understanding tasks, showcasing how combining long-term motion cues, semantic context, and large segmentation models like SAM2 can yield robust and precise segmentation of moving objects where traditional optical flow or VOS methods might fail. |
| Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal
  Bridging (Read more on [arXiv](https://arxiv.org/abs/2503.22236) or [HuggingFace](https://huggingface.co/papers/2503.22236))| Xiaoyang Guo, Jiahao Chang, Yushuang Wu, Chongjie Ye, LUZITENG | Hi3DGen introduces a novel framework for high-fidelity 3D geometry generation from single images by leveraging normal maps as an intermediate bridge. The primary objective is to accurately reproduce fine-grained geometric details from 2D images, addressing limitations like domain gaps and inherent RGB ambiguities in existing methods. Key methodology involves a noise-injected, dual-stream image-to-normal estimator (NiRNE) for sharp normal prediction, and a normal-to-geometry latent diffusion learner (NoRLD) with explicit normal map regularization, supported by a high-quality synthetic 3D dataset (DetailVerse). The framework demonstrates superior performance, with NiRNE achieving a Normal Error (NE) of 21.837 on the LUCES-MV dataset, significantly outperforming prior state-of-the-art methods, and user studies confirm higher perceived fidelity. For AI practitioners, this work presents a technique using normal maps as an explicit intermediate representation with regularization in latent diffusion to significantly enhance the geometric detail and fidelity of single-image 3D model generation pipelines. |
| ReFeed: Multi-dimensional Summarization Refinement with Reflective
  Reasoning on Feedback (Read more on [arXiv](https://arxiv.org/abs/2503.21332) or [HuggingFace](https://huggingface.co/papers/2503.21332))| jasoncai, hwany-j, Myyhlee, hyang0503, hamzzi | This paper introduces ReFeed, a pipeline employing reflective reasoning on feedback to refine text summaries across multiple quality dimensions simultaneously. The primary objective is to enhance summarization refinement beyond single dimensions like faithfulness, addressing inter-dimensional trade-offs, feedback ordering bias, and sensitivity to noisy LLM-generated feedback. ReFeed utilizes a novel dataset, SumFeed-CoT, containing Long-CoT reflective reasoning distilled from a large reasoning model, to fine-tune a lightweight model (LLaMA-3.1-8B) capable of backtracking and validating feedback during refinement. Experiments show ReFeed significantly outperforms baselines, improving average summary quality by 8.4 points over initial summaries and specifically boosting completeness by 13.6 points, while demonstrating robustness to feedback noise and order. For AI practitioners, ReFeed offers a method and dataset to build lightweight yet effective multi-dimensional refinement models that mitigate quality trade-offs by incorporating distilled reflective reasoning, crucial for robust real-world deployment. |
| OThink-MR1: Stimulating multimodal generalized reasoning capabilities
  via dynamic reinforcement learning (Read more on [arXiv](https://arxiv.org/abs/2503.16081) or [HuggingFace](https://huggingface.co/papers/2503.16081))| Changwang Zhang, Feng Liu, Yuting Zhang, Zhiyuan Liu, jwanglux | OThink-MR1 introduces GRPO-D, a dynamic reinforcement learning strategy, to enhance the generalized multimodal reasoning capabilities of MLLMs beyond standard fine-tuning. The primary objective is to overcome the limitations of SFT and static RL by developing a dynamic RL approach (GRPO-D) that fosters better same-task performance and cross-task generalization for multimodal reasoning. The key methodology is GRPO-D, which employs a dynamically adjusted Kullback-Leibler (KL) divergence weight during reinforcement learning fine-tuning to optimally balance policy exploration and exploitation based on verifiable multimodal task rewards. GRPO-D demonstrated superior same-task and cross-task performance, achieving over a 61.63% relative improvement versus SFT in cross-task generalization evaluations where SFT showed poor transferability. For AI practitioners, GRPO-D provides a superior fine-tuning technique for MLLMs, enabling the development of models with stronger, transferable reasoning abilities across diverse multimodal tasks without requiring retraining for each specific task. |
| Your ViT is Secretly an Image Segmentation Model (Read more on [arXiv](https://arxiv.org/abs/2503.19108) or [HuggingFace](https://huggingface.co/papers/2503.19108))| Giuseppe Averta, Narges Norouzi, Alexander Hermans, Niccolò Cavagnero, Tommie Kerssies | This paper introduces the Encoder-only Mask Transformer (EoMT), demonstrating that a plain Vision Transformer (ViT) can perform image segmentation without task-specific components like adapters or decoders. The study investigates if these components are essential for state-of-the-art ViT-based segmentation, hypothesizing their relevance diminishes with larger models and extensive pre-training. By systematically removing components from a ViT-Adapter + Mask2Former baseline and repurposing the ViT encoder blocks to process learnable queries alongside patch tokens, supplemented by a mask annealing strategy for efficient inference, EoMT is developed. Results show that EoMT with ViT-L achieves comparable Panoptic Quality (56.0 PQ) to the baseline (57.1 PQ) on COCO while being 4.4x faster (128 FPS vs 29 FPS). For AI practitioners, this implies that investing compute in scaling ViT models and pre-training, rather than adding architectural complexity, can yield simpler, faster, and highly accurate segmentation models that readily benefit from foundation model advancements. |
| 4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.17827) or [HuggingFace](https://huggingface.co/papers/2503.17827))| mhelhoseiny, ajhamdi, TonNew, bing-li-ai, vxuanz | This paper introduces 4D-Bench, the first benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in understanding dynamic 4D objects through question answering and captioning tasks. The objective is to assess current MLLM performance in multi-view spatial-temporal reasoning for 4D assets, addressing the lack of standardized evaluation in this domain. The methodology involved creating a dataset from rendered dynamic 3D objects (Objaverse-XL) into multi-view videos, curating data via motion and quality filters, and generating challenging QA pairs and human-annotated captions, followed by evaluating multiple MLLMs using accuracy and diverse captioning metrics, including GPT-4o assessment. Key results show MLLMs significantly underperform humans, with the state-of-the-art GPT-4o achieving only 62.98% overall accuracy on the 4D object QA task compared to a 91.08% human baseline, demonstrating particular weakness in object counting (37.29% average accuracy) and temporal reasoning. For AI practitioners, this highlights substantial MLLM limitations in integrating complex spatial-temporal information for 4D objects and handling counterfactual data, indicating a need for developing more robust models for applications involving dynamic 3D assets. |
| A Refined Analysis of Massive Activations in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.22329) or [HuggingFace](https://huggingface.co/papers/2503.22329))| Fabian Güra, akanyaani, nilabhra, louisowen6 | This paper analyzes massive activations across diverse LLMs, challenging prior assumptions and evaluating mitigation strategies. The research objective is to systematically assess the characteristics, impact, and mitigation of massive activations across a broader range of GLU and non-GLU based LLM architectures than previously studied. Methodology involves intervention analysis (setting activations to zero/mean) on pre-trained models and retraining LLaMA-1B/GPT-2 with mitigation techniques (Attention KV Bias, TVR, DyT, hybrids), evaluating perplexity and downstream task performance. Primary results contradict prior claims, showing not all massive activations are detrimental, Attention KV bias mitigation is ineffective for architectures like LLaMA-1B, and hybrid strategies such as TVR + KV Bias successfully mitigate activations in LLaMA-1B (mean downstream task accuracy 52.0 vs 50.3 baseline) while preserving performance. The principal implication for AI practitioners is that mitigating massive activations, crucial for quantization and numerical stability, requires architecture-specific analysis and potentially hybrid approaches like TVR+KV Bias or TVR+DyT, as universal solutions are ineffective. |
| SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.21732) or [HuggingFace](https://huggingface.co/papers/2503.21732))| Lp256, pookiefoof, bennyguo, zouzx, XianglongHe | SparseFlex introduces a sparse-structured isosurface representation for high-resolution, arbitrary-topology 3D shape modeling. The primary objective is to create high-fidelity 3D meshes (up to 1024³) with complex geometries, open surfaces, and interiors directly from rendering supervision, overcoming limitations of existing methods. Key methodologies involve adapting Flexicubes within a sparse voxel structure and employing a novel frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering to drastically reduce memory consumption. Experiments demonstrate state-of-the-art reconstruction accuracy, evidenced by an ~82% reduction in Chamfer Distance and an ~88% increase in F-score compared to previous methods on tested benchmarks. For AI practitioners, this work provides a memory-efficient pathway to train high-resolution, differentiable mesh reconstruction and generation models using only rendering losses, facilitating the creation of detailed 3D assets with arbitrary topology without costly watertight preprocessing. |
| MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via
  Reasoning Agentic Workflow (Read more on [arXiv](https://arxiv.org/abs/2503.18968) or [HuggingFace](https://huggingface.co/papers/2503.18968))| Yueming Jin, Chang Han Low, morson, ZiyueWang | MedAgent-Pro introduces a reasoning agentic workflow for evidence-based, multi-modal medical diagnosis. The primary objective is to enhance diagnostic reliability and explainability compared to standard MLLMs by strictly adhering to retrieved clinical criteria and enabling quantitative analysis. The methodology utilizes a hierarchical agentic workflow: a task-level planner uses RAG to generate diagnostic plans based on medical knowledge, while case-level tool agents (specialized vision/VQA models, coding agent) execute steps on patient data, followed by a decider agent integrating findings. MedAgent-Pro significantly outperformed baselines, achieving 90.4% mACC on Glaucoma diagnosis using its MOE decider, a 32.3% absolute improvement over the best single foundation model tested (BioMedClip). For AI practitioners, this work implies that augmenting MLLMs with structured agentic workflows, external specialized tools, and explicit knowledge retrieval is crucial for building reliable and interpretable systems in domains requiring rigorous, evidence-based quantitative reasoning like medical diagnosis. |
| X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time
  Tomographic Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2503.21779) or [HuggingFace](https://huggingface.co/papers/2503.21779))| yixuanyuan, XGGNet, Fanzhiwen, CaiYuanhao, vortex778 | X²-Gaussian presents a novel framework for continuous-time 4D computed tomography (CT) reconstruction using dynamic radiative Gaussian splatting. The objective is to reconstruct 4D CT volumes at arbitrary time points directly from projections, eliminating discrete phase binning and the need for external respiratory gating devices. The methodology integrates dynamic radiative Gaussian splatting, modeled via a spatiotemporal encoder-decoder for continuous deformation prediction, with a self-supervised, physiology-driven periodic consistency loss to learn respiratory cycles directly from projection data. Results demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR improvement over traditional methods and a 2.25 dB gain over prior Gaussian splatting approaches on the DIR dataset. For AI practitioners, this provides a hardware-free method for high-fidelity, continuous dynamic medical image reconstruction, potentially enhancing motion analysis in clinical applications like image-guided radiotherapy. |
| On Large Multimodal Models as Open-World Image Classifiers (Read more on [arXiv](https://arxiv.org/abs/2503.21851) or [HuggingFace](https://huggingface.co/papers/2503.21851))| Yiming Wang, Enrico Fini, paolorota, massimilianom, altndrr | This paper evaluates Large Multimodal Models (LMMs) for open-world image classification beyond predefined categories. The objective was to assess LMM performance in an unconstrained classification setting and analyze prediction errors using novel metrics. The methodology involved evaluating 13 LMMs on 10 benchmarks using four metrics (Text Inclusion, Llama Inclusion, Semantic Similarity, Concept Similarity) to measure alignment between generated text and ground truth labels. Results indicate LMMs outperform open-world contrastive baselines (e.g., CaSED) on inclusion metrics but significantly underperform closed-world models (e.g., CLIP), with notable errors in granularity (e.g., predicting "dog" instead of "pug") and fine-grained discrimination; for instance, even the best models struggled significantly on very fine-grained datasets, often achieving near 0% Text Inclusion. AI practitioners should recognize current LMMs' limitations in specific open-world classification, noting that while promising, tailored prompting and reasoning only partially alleviate errors related to granularity and fine-grained distinctions compared to traditional closed-world approaches. |
| Reconstructing Humans with a Biomechanically Accurate Skeleton (Read more on [arXiv](https://arxiv.org/abs/2503.21751) or [HuggingFace](https://huggingface.co/papers/2503.21751))| Qixing Huang, Etienne Vouga, Xiaowei Zhou, geopavlakos, IsshikiHugh | This paper presents HSMR, a method for single-image 3D human reconstruction using the biomechanically accurate SKEL model. The main objective is to estimate SKEL parameters directly from an image, overcoming the lack of paired image-SKEL training data. HSMR utilizes a transformer network trained with iteratively refined pseudo-ground truth SKEL parameters generated by converting existing SMPL datasets and optimizing against 2D keypoints ("SKELify"). HSMR achieves competitive performance on standard benchmarks compared to SMPL-based methods like HMR2.0, while significantly outperforming them (by >10mm PA-MPJPE) on datasets with extreme poses like MOYO and reducing unnatural joint rotations. For AI practitioners, this offers a way to generate more physically plausible 3D human models directly from images, which is crucial for biomechanics, robotics, and simulation applications where joint limits and skeletal accuracy are paramount. |
