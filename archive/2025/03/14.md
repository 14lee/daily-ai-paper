

## Papers for 2025-03-14

| Title | Authors | Summary |
|-------|---------|---------|
| CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing (Read more on [arXiv](https://arxiv.org/abs/2503.10613) or [HuggingFace](https://huggingface.co/papers/2503.10613))| Dang Nguyen, zhoutianyi, nandakiran09, advaitgupta | CoSTA* is a cost-sensitive toolpath agent that finds the optimal tool sequence for multi-turn image editing by combining LLMs and A* search. The main research question is how to combine the strengths of large language models (LLMs) and graph search to find cost-efficient tool paths for multi-turn image editing. The key methodology is a three-stage approach called CoSTA* that uses LLMs to create a subtask tree, prunes a graph of AI tools, and then conducts A* search on the subgraph to find a tool path, guided by a combination of cost and quality metrics. CoSTA* achieved an overall accuracy of 0.94 across all tasks, outperforming baselines such as GenArtist (0.73) and CLOVA (0.63) and offers dynamic trade-offs between the computational cost and quality. This implies that AI practitioners can leverage CoSTA* to build more efficient and adaptable image editing systems that can handle complex, multi-turn editing instructions, allowing for dynamic parameter adjustments of quality-cost trade-offs.  |
| World Modeling Makes a Better Planner: Dual Preference Optimization for
  Embodied Task Planning (Read more on [arXiv](https://arxiv.org/abs/2503.10480) or [HuggingFace](https://huggingface.co/papers/2503.10480))| xpqiu, Jinlan, CyberDJ, ngc7293, sinwang | D²PO jointly optimizes state prediction and action selection in LVLMs for embodied task planning, improving performance and efficiency. The research objective is to develop a learning framework, Dual Preference Optimization (D²PO), that enhances embodied task planning in large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The key methodology involves a tree search mechanism for automatic data collection and a dual preference learning approach using preference pairs for both action and state prediction. Primary results show that D²PO significantly outperforms existing methods and GPT-4o on the VoTa-Bench, achieving a 31.4% relative improvement in success rate and a 33.0% improvement in planning efficiency compared to SFT baselines on a 7B-parameter model. The principal implication for AI practitioners is that incorporating world modeling objectives through D²PO substantially enhances the planning capabilities of LVLMs in embodied AI, offering a more effective approach for developing agents that can perform complex tasks with higher success and efficiency.  |
| Silent Branding Attack: Trigger-free Data Poisoning Attack on
  Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.09669) or [HuggingFace](https://huggingface.co/papers/2503.09669))| Sung Ju Hwang, kiminle2, harryjo97, wchoi403, agwmon | This paper introduces a novel data poisoning attack, called Silent Branding Attack, that manipulates text-to-image diffusion models to generate images with specific brand logos, without requiring any text triggers. The main research objective is to develop and validate a data poisoning method that unobtrusively embeds target logos into images generated by text-to-image diffusion models, operating without explicit text triggers. The key methodology involves an automated algorithm that personalizes logos, generates masks for logo placement, and uses inpainting and refinement techniques to seamlessly integrate logos into existing images. The attack achieved a logo inclusion rate (LIR) of 45.00% on the Midjourney dataset and 39.68% on the Tarot dataset with a 100% poisoning ratio, demonstrating successful logo embedding without specific text triggers. AI practitioners should be aware that text-to-image diffusion models are vulnerable to data poisoning attacks that can subtly embed unwanted visual elements, even without trigger words, necessitating safeguards against such manipulations.  |
| Charting and Navigating Hugging Face's Model Atlas (Read more on [arXiv](https://arxiv.org/abs/2503.10633) or [HuggingFace](https://huggingface.co/papers/2503.10633))| yedid, LielAmar, jonkahana, nitzankur, Eliahu | The paper introduces a method for charting and navigating the vast model repository of Hugging Face by constructing a model atlas represented as a directed acyclic graph. The main research objective is to develop a method for recovering the undocumented evolutionary relationships between models in large repositories, and to explore the use cases of such an atlas. The key methodology involves representing models by their weights, calculating pairwise distances, and using temporal and structural priors to predict directed edges, accounting for model merging and quantization. The results show the proposed method recovers 78.87% of the model relations on a Qwen connected component dataset, substantially outperforming baseline methods, and reveal that 99.41% of quantised models in hugging face are leafs (don't have children). The principal implication is that AI practitioners can use the constructed atlas to improve model discovery, attribute prediction, and heritage tracing, enabling more efficient model reuse and analysis.  |
| GoT: Unleashing Reasoning Capability of Multimodal Large Language Model
  for Visual Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.10639) or [HuggingFace](https://huggingface.co/papers/2503.10639))| zengxingyu, shilinyan, LjHuang, gogoduan, LucasFang | This paper introduces Generation Chain-of-Thought (GoT), a new paradigm for visual generation and editing that leverages multimodal large language models (MLLMs) to perform explicit semantic-spatial reasoning before outputting images. The main research objective is to integrate reasoning mechanisms into visual generation and editing to improve the alignment of generated content with human intentions. The key methodology involves formulating GoT as a multimodal reasoning chain, constructing large-scale GoT datasets with 9M+ samples, and developing a unified framework integrating Qwen2.5-VL with a Semantic-Spatial Guidance Module enhanced diffusion model. The GoT framework achieved a 0.64 overall score on the GenEval benchmark for text-to-image generation, outperforming existing methods. For AI practitioners, GoT offers a framework to build visual generation and editing systems with enhanced reasoning capabilities, enabling improved control, more accurate results, and interactive generation based on modified reasoning steps.  |
| Transformers without Normalization (Read more on [arXiv](https://arxiv.org/abs/2503.10622) or [HuggingFace](https://huggingface.co/papers/2503.10622))| Zhuang Liu, Kaiming He, ylecun, endernewton, JiachenZhu | This paper introduces Dynamic Tanh (DyT) as a replacement for normalization layers in Transformers, achieving comparable or superior performance. The main research question is whether normalization layers are indispensable in Transformers, and can they be replaced with a simpler alternative. The key methodology involves replacing normalization layers (LayerNorm and RMSNorm) with a proposed element-wise operation, DyT(x) = tanh(αx), where α is a learnable parameter, and empirically evaluating the modified architectures. Primary results show that Vision Transformers (ViT-B) with DyT achieved 82.5% top-1 accuracy on ImageNet-1K, surpassing the 82.3% accuracy of the LN-based model. Principal implication for AI practitioners that normalization layers in Transformers may not be necessary, and simpler, computationally efficient alternatives such as DyT can provide same or better performance across multiple tasks.  |
| GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding (Read more on [arXiv](https://arxiv.org/abs/2503.10596) or [HuggingFace](https://huggingface.co/papers/2503.10596))| wenyuliu, steelozazala, wondervictor, LianghuiZhu, RuiHu | GroundingSuite introduces a new benchmark and framework for evaluating and improving pixel-level visual grounding in complex and diverse scenarios. The main research objective is to address limitations in existing pixel grounding datasets, specifically their limited object categories, textual diversity, and annotation quality. The key methodology involves an automated annotation framework (GSSculpt) leveraging multiple VLM agents for entity localization, text generation, and noise filtering, alongside a curated evaluation benchmark (GSEval). A model trained on the new dataset (GSTrain-10M) achieved a cIoU of 68.9 on gRefCOCO, outperforming models trained on other datasets. AI practitioners can use GroundingSuite to train and evaluate models for more robust and generalizable pixel grounding, applicable across diverse granularities and complex referential expressions.  |
| New Trends for Modern Machine Translation with Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.10351) or [HuggingFace](https://huggingface.co/papers/2503.10351))| acecamel1977, longyuewang, minghaowu, ChenyangLyu, SNF | Large Reasoning Models (LRMs) substantially transform traditional machine translation (MT) by reframing it as a dynamic reasoning task.  The main research objective is to explore the potential of LRMs in redefining MT systems and identify the foundational shifts, new opportunities, and challenges they introduce.  The key methodology involves a conceptual analysis and empirical case studies of LRM capabilities in various translation scenarios, including stylized, document-level, and multimodal translation.  Primary results show LRMs can perform self-reflection to correct errors, automatically utilize pivot translation, and struggle with complex encoded text; experiments on commonMT showed similar BLEURT (73.0-74.2) and COMET (84.1-84.8) scores for both the reasoning and non-reasoning models.  AI practitioners should consider LRMs as a means to develop MT systems that function as multilingual cognitive agents capable of reasoning about meaning, context, culture, and intent, beyond simple text conversion.  |
| Shifting Long-Context LLMs Research from Input to Output (Read more on [arXiv](https://arxiv.org/abs/2503.04723) or [HuggingFace](https://huggingface.co/papers/2503.04723))| mingshan, tsq2000, Zhiqiang007, bys0318, mozhu | This paper advocates for a shift in long-context large language model (LLM) research, prioritizing long-output generation capabilities over the current focus on long-input processing. The main research objective is to define and address the challenges of developing LLMs capable of generating high-quality, coherent, and contextually relevant long-form text outputs. The key methodology involves analyzing existing datasets, benchmarks, and models, and identifying limitations in long-output generation through statistical analysis and qualitative assessment of model outputs. Primary results show that the demand for long-output generation (exceeding 4,000 tokens) is 2-3 times greater than for equivalent-length inputs in real-world applications, while only 2 out of 104 papers on long-context tasks at major ML/NLP conferences in 2024 directly addressed long-output generation. The principal implication for AI practitioners is the need to develop new datasets, training techniques, and evaluation metrics specifically designed for long-output LLMs to meet real-world demands in areas like creative writing and complex reasoning.  |
| VisualWebInstruct: Scaling up Multimodal Instruction Data through Web
  Search (Read more on [arXiv](https://arxiv.org/abs/2503.10582) or [HuggingFace](https://huggingface.co/papers/2503.10582))| Bo Li, Xiang Yue, wenhu, jiachenli-ucsb, jymmmmm | VisualWebInstruct introduces a method for creating large-scale, multimodal instruction datasets by leveraging web search. The main research objective is to address the scarcity of high-quality, diverse training data for reasoning-focused multimodal tasks. The key methodology involves using Google Image Search with 30,000 seed images to collect over 700K unique URLs, extracting QA pairs from HTML accessibility trees, and refining the data using GPT-4O for answer synthesis and consistency filtering. Fine-tuning MAmmoTH-VL on this dataset (named VisualWebInstruct) achieves a state-of-the-art performance of 50.4% average accuracy across seven visual reasoning benchmarks. The principal implication is that AI practitioners can leverage web-scale data to improve the reasoning abilities of vision-language models, particularly on tasks requiring multi-step deliberation with visual context.  |
| DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture
  Design in Text to Image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10618) or [HuggingFace](https://huggingface.co/papers/2503.10618))| Rui Qian, Chen Chen, yinfeiy, tsujuifu, wenzehu | The paper introduces DiT-Air, a streamlined Diffusion Transformer architecture for text-to-image generation that achieves state-of-the-art performance with improved parameter efficiency. The main research objective is to empirically investigate the impact of architectural choices, text-conditioning strategies, and training protocols on the performance and efficiency of Diffusion Transformers (DiTs). The key methodology involves a comparative analysis of vanilla DiT, PixArt-style, and MMDiT variants, along with ablations of text encoders, layer-wise parameter sharing, and a progressive VAE training approach. Primary results show that DiT-Air achieves GenEval and T2I CompBench scores of 82.9 and 59.5, respectively, outperforming existing models while using significantly fewer parameters (66% reduction compared to MMDiT). For AI practitioners, DiT-Air offers a more parameter-efficient architecture for text-to-image diffusion models, enabling competitive performance with reduced computational resources.  |
| Do I look like a `cat.n.01` to you? A Taxonomy Image Generation
  Benchmark (Read more on [arXiv](https://arxiv.org/abs/2503.10357) or [HuggingFace](https://huggingface.co/papers/2503.10357))| Ekaterina Neminova, Alina Lobanova, lilaspourpre, apanc, VityaVitalich | This paper introduces a benchmark for evaluating text-to-image models' ability to generate images representing taxonomic concepts from WordNet. The main research objective is to assess how well text-to-image models can visualize concepts of varying abstraction levels within a hierarchical taxonomy. The key methodology involves evaluating 12 text-to-image models using 9 taxonomy-related metrics, human feedback, and pairwise evaluation with GPT-4 feedback. The primary results show that Playground-v2 and FLUX consistently outperform other models across metrics, with Playground ranking first in all preference-based evaluations, but the model ranking differs significantly from standard text-to-image tasks. AI practitioners can use this benchmark to evaluate and improve text-to-image models for generating images reflecting structured, hierarchical data, with a clear indication that specific models are much better at reflecting taxonomic data.  |
| Open-Sora 2.0: Training a Commercial-Level Video Generation Model in
  $200k (Read more on [arXiv](https://arxiv.org/abs/2503.09642) or [HuggingFace](https://huggingface.co/papers/2503.09642))| Xinying Guo, Tom Young, Chenhui Shen, Zangwei Zheng, Xiangyu Peng | Open-Sora 2.0 is a commercially viable video generation model trained for $200k, demonstrating cost-effective techniques for high-quality video synthesis. The main research objective is to develop a top-performing video generation model at a highly controlled cost, much lower than comparable existing models. Key methodologies used include a hierarchical data filtering system, a deeply compressed video autoencoder (Video DC-AE), a diffusion transformer (DiT) architecture leveraging full attention, and an image-to-video training approach. The model achieves a win rate favorably against other top-performing models in all three aspects of human preference evaluation (visual quality, prompt adherence, and motion quality); specifically it is 5-10x cheaper to train ($200k) than comparables like MovieGen and Step-Video-T2V. Principal implication for AI practitioners is that high-quality video generation models are achievable with significantly reduced training costs through optimized data curation, model architecture, and training strategies.  |
| Long Context Tuning for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10589) or [HuggingFace](https://huggingface.co/papers/2503.10589))| lindahua, zhenheny, Ikuinen, Brightmzb, ziyany | Long Context Tuning (LCT) extends pre-trained video diffusion models to generate coherent multi-shot scenes by expanding their context window. The main research objective is to enable scene-level video generation with visual and dynamic consistency across multiple shots. The key methodology involves adapting full attention mechanisms to encompass all shots in a scene, incorporating interleaved 3D positional embedding, and using an asynchronous noise strategy for training. The primary results show that LCT-trained models achieve superior semantic alignment compared to baseline methods, with a user study score of 3.79 versus baselines ranging from 1.57 to 2.50. For AI practitioners, LCT offers a training paradigm to directly adapt single-shot video models for coherent, multi-shot video generation without additional parameters, enabling applications like short film production and interactive video editing.  |
| 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.10437) or [HuggingFace](https://huggingface.co/papers/2503.10437))| hpfister, Qmh, wrencanfly, rpzhou, EthanTaylor | 4D LangSplat learns 4D language fields for efficient, time-sensitive, open-vocabulary querying of dynamic scenes. The main research objective is to develop a method for constructing precise 4D language fields that enable both time-agnostic and time-sensitive open-vocabulary queries in dynamic scenes. The key methodology involves using Multimodal Large Language Models (MLLMs) to generate object-wise video captions, encoding these captions into sentence embeddings for supervision, and employing a status deformable network to model continuous state changes. Results show that on the HyperNeRF dataset, for time-sensitive querying the proposed method achieves an accuracy of 89.42% and a vIoU of 66.07%. AI practitioners can use 4D LangSplat to build systems that enable open vocabulary text-based queries, which are time agnostic and time-sensitive, of the evolution and interaction of objects within a dynamic scene.  |
| SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2503.09641) or [HuggingFace](https://huggingface.co/papers/2503.09641))| Yuyang Zhao, Shuchen Xue, Junsong Chen, xieenze, sayakpaul | SANA-Sprint is a text-to-image diffusion model that achieves fast, high-quality image generation through hybrid distillation. The main research objective is to develop an efficient diffusion model capable of one-step high-quality text-to-image (T2I) generation while maintaining multi-step sampling flexibility. The key methodology involves transforming a pre-trained flow-matching model for continuous-time consistency distillation (sCM), combined with latent adversarial distillation (LADD), and includes QK-normalization and dense time-embedding. The primary results show SANA-Sprint achieves a 7.59 FID and 0.74 GenEval in only one step, outperforming FLUX-schnell while being 10x faster (0.1s vs 1.1s on H100). The principal implication for AI practitioners is that they can leverage SANA-Sprint for applications requiring real-time or near real-time image generation with significantly reduced computational overhead compared to prior diffusion models.  |
| UniGoal: Towards Universal Zero-shot Goal-oriented Navigation (Read more on [arXiv](https://arxiv.org/abs/2503.10630) or [HuggingFace](https://huggingface.co/papers/2503.10630))| Ziwei Wang, Lingqing Zhao, jiwenlu, xuxw98, hangyin | UniGoal is a framework for universal zero-shot goal-oriented navigation that unifies different goal types within a single model. The main research objective is to develop a general framework capable of handling multiple navigation tasks (object, instance-image, and text-based goals) without task-specific training or fine-tuning. The key methodology involves representing both the scene and goals as graphs, performing graph matching, and using a multi-stage exploration policy guided by the matching score and a blacklist mechanism. Results show that UniGoal achieves a 60.2% success rate on instance-image goal navigation on the HM3D benchmark, outperforming prior zero-shot methods. AI practitioners can use UniGoal to deploy navigation agents in new environments with varied goal specifications without needing environment-specific or task-specific retraining.  |
| Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and
  Beyond (Read more on [arXiv](https://arxiv.org/abs/2503.10460) or [HuggingFace](https://huggingface.co/papers/2503.10460))| tanglifu, JunchenLiu, yyy99, duan901010, cizhenshi | Light-R1 presents a training recipe for long chain-of-thought (COT) reasoning models, achieving state-of-the-art math performance with efficient training. The main research objective was to develop a method for training compact long-COT models from scratch, overcoming limitations of existing approaches. The key methodology involved a curriculum training recipe comprising two-stage supervised fine-tuning (SFT) with a curated dataset and semi-on-policy direct preference optimization (DPO), followed by reinforcement learning (specifically GRPO). The Light-R1-32B model, trained from Qwen2.5-32B-Instruct, achieved 76.6% on the AIME24 benchmark, surpassing DeepSeek-R1-Distill-Qwen-32B. AI practitioners can use this open-sourced approach, including models, data, and code, to efficiently train and deploy long-COT reasoning capabilities in resource-constrained environments, particularly for mathematical problem-solving.  |
| CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance (Read more on [arXiv](https://arxiv.org/abs/2503.10391) or [HuggingFace](https://huggingface.co/papers/2503.10391))| brotherhuang, u302117, BestWishYsh, angtian, dyf | CINEMA is a framework for generating videos featuring multiple subjects, guided by reference images and text, using a Multimodal Large Language Model (MLLM) for improved coherence. The main research objective is to generate coherent multi-subject videos that maintain visual consistency of individual subjects and follow textual prompts, addressing limitations of existing methods that rely on ambiguous keyword mapping. The key methodology involves leveraging an MLLM (specifically Qwen2-VL) to encode multimodal conditions, an AlignerNet to align MLLM outputs with text features, and VAE encoding of reference images for fine-grained visual detail preservation, all integrated within a Multimodal Diffusion Transformer (MM-DiT) framework. The model was trained on 1.46 million video clips, each paired with 1 to 6 human/object references, achieving results shown qualitatively in Figures 5 and 6 with a training configuration using 128 NVIDIA H100 GPUs. For AI practitioners, CINEMA offers a scalable approach for multi-subject video generation that eliminates the need for explicit subject-text correspondences, improving subject consistency, which is beneficial for applications like personalized video content creation.  |
| Quantization for OpenAI's Whisper Models: A Comparative Analysis (Read more on [arXiv](https://arxiv.org/abs/2503.09905) or [HuggingFace](https://huggingface.co/papers/2503.09905))| allisonandreyev | Whisper and its variants are evaluated for speech recognition, focusing on quantization's impact on model size, latency, and accuracy. The main research objective is to analyze the similarities, differences, and capabilities of three Whisper models (Whisper, Whisper_Streaming, and whisper-timestamped) and quantify the impact of quantization on latency and its viability for edge deployment. The key methodology involves qualitative comparisons of the three models and quantitative evaluation of word error rate (WER) and latency using the LibriSpeech dataset with three quantization methods (INT4, INT5, INT8) in whispercpp. Quantization with INT4 reduced model size by 45% (from 141.11MB to 44.33MB) and decreased latency by 19%, while slightly improved word error rate (0.0199 to 0.0159). Quantization is a viable method for deploying Whisper on resource-limited devices, maintaining accuracy while significantly reducing model size and improving deployment efficiency.  |
| Distilling Diversity and Control in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.10637) or [HuggingFace](https://huggingface.co/papers/2503.10637))| David Bau, RohitGandikota | Distilled diffusion models can retain the control and regain/exceed the diversity of their base models through strategic timestep management. The paper investigates how to distill both diversity and control capabilities from base diffusion models to their efficient distilled variants. The key methodology involves introducing DT-Visualization to analyze latent representations, and a hybrid inference approach that utilizes the base model for the first critical timestep and the distilled model subsequently. The primary results reveal that the hybrid approach achieves a FID score of 10.79 on COCO-30k, better than both the base (12.74) and distilled (15.52) models, while maintaining the distilled model's inference speed. The principal implication is that AI practitioners can achieve both high diversity and efficiency in image generation using distilled diffusion models without additional training by leveraging the hybrid inference approach.  |
| R1-Onevision: Advancing Generalized Multimodal Reasoning through
  Cross-Modal Formalization (Read more on [arXiv](https://arxiv.org/abs/2503.10615) or [HuggingFace](https://huggingface.co/papers/2503.10615))| Xiaoxuan He, Yi Yang, twilightsnow, dcyin, Emilia515 | R1-Onevision introduces a multimodal reasoning model, dataset, and benchmark to improve visual-language understanding and reasoning. The main research objective is to bridge the gap between visual perception and deep reasoning in large language models by employing a cross-modal reasoning pipeline. Key methodologies used include a cross-modal reasoning pipeline that transforms images into formal textural representations and a two-stage post-training strategy (supervised fine-tuning and reinforcement learning). R1-Onevision achieved 29.9% accuracy on MathVision, comparable to the closed-source model GPT-4o. The principal implication for AI practitioners is that formalizing visual information into textual representations, combined with specialized training, can significantly enhance the multimodal reasoning capabilities of large language models, as demonstrated through performance in visual reasoning benchmarks.  |
| Autoregressive Image Generation with Randomized Parallel Decoding (Read more on [arXiv](https://arxiv.org/abs/2503.10568) or [HuggingFace](https://huggingface.co/papers/2503.10568))| Huan Wang, Guoqi Li, Jinyue Yang, hp-l33 | ARPG is a visual autoregressive model that enables random-order, parallel image generation. The research objective is to develop an autoregressive image generation model that overcomes the limitations of raster-order approaches in inference efficiency and zero-shot generalization. The methodology involves a "guided decoding" framework that decouples positional guidance (queries) from content representation (key-value pairs) within the causal attention mechanism, to specify the output image token. On ImageNet-1K 256x256, ARPG achieves an FID of 1.94 with 64 sampling steps, attaining over 20x throughput increase and reducing memory use by over 75% compared to autoregressive models of similar scale. AI practitioners can use ARPG as a more efficient and versatile framework for autoregressive image generation, enabling faster and more flexible image synthesis applications.  |
| The Curse of Conditions: Analyzing and Improving Optimal Transport for
  Conditional Flow-Based Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10636) or [HuggingFace](https://huggingface.co/papers/2503.10636))| Alexander Schwing, hkchengrex | Conditional optimal transport (C²OT) improves conditional flow-based generative models by addressing a train-test discrepancy caused by standard optimal transport. The main research objective is to analyze and mitigate the performance degradation of minibatch optimal transport (OT) in conditional flow matching when conditions are introduced. The key methodology is the introduction of a conditional weighting term in the OT cost matrix calculation, along with adaptive weight finding and oversampling techniques. The primary results demonstrate C²OT outperforms flow matching (FM) and OT in conditional generation, e.g. achieving a 2-Wasserstein distance of 0.013±0.003 on 8gaussians→moons with continuous conditions vs FM (0.028±0.010) and OT (2.143±1.993). AI practitioners can use C²OT as a drop-in replacement for standard OT in flow matching to achieve better performance in conditional generative modeling, avoiding skewed priors during training.  |
| VisualPRM: An Effective Process Reward Model for Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.10291) or [HuggingFace](https://huggingface.co/papers/2503.10291))| Einsiedler, Yeshenglong, Decaux, chenlj22, Weiyun1025 | VisualPRM is an 8B parameter multimodal Process Reward Model (PRM) that improves reasoning in Multimodal Large Language Models (MLLMs) using Best-of-N evaluation. The research introduces VisualPRM and evaluates its effectiveness as a critic model for enhancing MLLM reasoning. The authors construct a multimodal process supervision dataset (VisualPRM400K) and a benchmark (VisualProcessBench) with human-annotated step-wise correctness labels, then train VisualPRM on the dataset. Applying VisualPRM to InternVL2.5-78B achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. AI practitioners can utilize VisualPRM as an effective critic model to enhance the reasoning performance of MLLMs through Test-Time Scaling, particularly with the Best-of-N strategy.  |
| "Silent Is Not Actually Silent": An Investigation of Toxicity on Bug
  Report Discussion (Read more on [arXiv](https://arxiv.org/abs/2503.10072) or [HuggingFace](https://huggingface.co/papers/2503.10072))| Jaydeb Sarker, imranraad | This study investigates toxicity in GitHub bug report discussions, revealing its negative impacts on collaboration and resolution. The main research objective was to analyze how toxicity manifests in bug reports and impacts developers' bug resolution. The researchers performed a qualitative analysis of 203 bug threads (including 81 toxic ones) from GitHub, selected using stratified sampling and toxicity detection tools (ToxiCR and LLaMA). A primary result was that only 29.11% of toxic bug report issues were linked with a Pull Request, lower than percentages reported in prior studies. The principal implication for AI practitioners is that automated systems for bug severity/priority management, combined with enhanced toxicity detection tools incorporating domain-specific knowledge, are needed to improve communication and efficiency in software projects.  |
| PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with
  Implicit Hierarchical Masked Image Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.09368) or [HuggingFace](https://huggingface.co/papers/2503.09368))| Daniel Mueller-Gritschneder, Sascha Hauke, HerrSiebert, edukrom, Nikolai10 | PerCoV2 is an open ultra-low bit-rate perceptual image compression system built upon Stable Diffusion 3, enhancing entropy coding through explicit modeling of the discrete hyper-latent image distribution. The main research objective is to improve ultra-low bit-rate image compression while maintaining perceptual quality by using an implicit hierarchical masked image modeling approach. The key methodology involves extending the PerCo framework to Stable Diffusion 3 and comparing autoregressive methods (VAR and MaskGIT) for entropy modeling within a two-stage training protocol. Results on the MSCOCO-30k benchmark show that PerCoV2 achieves higher image fidelity at lower bit-rates than previous methods, with the QLDS masking schedule achieving a 6.34% bit-rate saving over the baseline in the ultra-low bit-rate setting. For AI practitioners, PerCoV2 offers a publicly available, state-of-the-art, ultra low bit-rate image compression approach that, in comparison to previous works, particularly excels at the ultra low-extreme bit rates (0.003-0.03bpp).  |
| On the Limitations of Vision-Language Models in Understanding Image
  Transforms (Read more on [arXiv](https://arxiv.org/abs/2503.09837) or [HuggingFace](https://huggingface.co/papers/2503.09837))| Saquib Sarfraz, Hasnain Ali, Ahmad Mustafa Anis | This paper investigates the limitations of Vision-Language Models (VLMs) in comprehending basic image transformations. The main research question is: "Can Vision Language Embedding Models understand simple Image Transformations?". The researchers created an augmented Flickr8k dataset and evaluated CLIP and SigLIP models' ability to associate image transformations with textual descriptions and classify transformations. Key results showed that SigLIP Base 256 Multilingual achieved only 47.21% accuracy in understanding augmented descriptions (Experiment 1), and all the VLMs model cannot classify the image transformation correctly. For AI practitioners, the principal implication is that current VLMs, despite strong semantic understanding, have significant limitations in understanding fundamental image transformations which can significantly limit downstream applications of image editing.  |
