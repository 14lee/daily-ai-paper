

## Papers for 2025-03-12

| Title | Authors | Summary |
|-------|---------|---------|
| Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural
  Vision-Language Dataset for Southeast Asia (Read more on [arXiv](https://arxiv.org/abs/2503.07920) or [HuggingFace](https://huggingface.co/papers/2503.07920))| davidanugraha, rifqifarhansyah, tackhwa, holylovenia, samuelcahyawijaya | SEA-VL is an open-source initiative to develop a vision-language dataset representing Southeast Asian cultures, addressing their underrepresentation in AI research. The main objective is to create a high-quality, culturally relevant vision-language dataset for Southeast Asian (SEA) languages and assess different data collection strategies. The researchers employ a multi-pronged approach that includes crowdsourcing, crawling existing image corpora, and generating synthetic images using diffusion models, followed by human evaluation. Crawling achieves approximately 85% cultural relevance and is more cost- and time-efficient than crowdsourcing, while image generation models are currently found unreliable for accurately reflecting SEA cultures. AI practitioners can leverage this dataset to develop more inclusive vision-language models and should prioritize crawling over generation for efficient collection of culturally relevant visual data.  |
| LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through
  Two-Stage Rule-Based RL (Read more on [arXiv](https://arxiv.org/abs/2503.07536) or [HuggingFace](https://huggingface.co/papers/2503.07536))| Jie Liu, Zhiyuan You, Miaosen Zhang, Gongrui Zhang, Yingzhe Peng | i) This paper introduces LMM-R1, a two-stage rule-based RL framework for enhancing reasoning abilities in Large Multimodal Models (LMMs). ii) The main objective is to improve the reasoning capabilities of compact 3B-parameter LMMs, particularly in multimodal contexts. iii) The methodology involves Foundational Reasoning Enhancement (FRE) using text-only data and Multimodal Generalization Training (MGT) to extend reasoning to multimodal domains. iv) Results on Qwen2.5-VL-Instruct-3B show LMM-R1 achieves 4.83% and 4.5% average improvements over baselines in multimodal and text-only benchmarks, respectively, and a 3.63% gain in Football Game tasks. v) LMM-R1 provides AI practitioners with a data-efficient approach to enhance reasoning in LMMs by leveraging text-based reasoning enhancement for effective multimodal generalization.  |
| YuE: Scaling Open Foundation Models for Long-Form Music Generation (Read more on [arXiv](https://arxiv.org/abs/2503.08638) or [HuggingFace](https://huggingface.co/papers/2503.08638))| HKUST-Audio, Liam-Liu, dododododo, zhangysk, a43992899 | YuE is a family of open foundation models for long-form, lyrics-to-song music generation based on the LLaMA2 architecture. The main research objective is to develop a system capable of generating high-quality, long-form (up to five minutes) music with coherent structure, lyrical alignment, and engaging vocal melodies from lyrics and other control signals. The key methodology involves a track-decoupled next-token prediction strategy with dual-token output (vocal and accompaniment), structural progressive conditioning using a Chain-of-Thought-like approach, a redesigned music in-context learning framework, and a multitask, multiphase pre-training recipe. Primary results include outperforming or matching several proprietary systems (e.g., Suno, Udio) in human evaluations of musicality, and achieving a mean vocal range of approximately 27 semitones, comparable to closed-source systems. The principal implication for AI practitioners is that YuE provides an open, scalable, and performant approach to full-song lyrics-to-music generation, offering improved controllability and competitive quality to existing proprietary alternatives.  |
| UniF^2ace: Fine-grained Face Understanding and Generation
  with Unified Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2503.08120) or [HuggingFace](https://huggingface.co/papers/2503.08120))| Liya Guo, Linrui Xu, Xuerui Qiu, delinqu, tulvgengenr | UniF²ace is a unified multimodal model designed for fine-grained face understanding and generation tasks, trained on a new specialized dataset. The main research objective is to develop a single model capable of both understanding (image-to-text) and generating (text-to-image) fine-grained facial attributes with high accuracy. The key methodology involves a combination of autoregressive and diffusion models, optimized using a dual discrete diffusion training strategy and a two-level mixture-of-experts architecture, trained on the self-constructed UniF²ace-130K dataset. The primary results show that UniF²ace achieves a FID score of 66.005 and a VLM-score of 88.049 on the UniF²ace-130K test dataset, outperforming existing unified multimodal models and approaching state-of-the-art generative models. The principal implication for AI practitioners is that a unified model, leveraging both score-based and masked generative models with a specialized architecture, can achieve high performance in both detailed facial image understanding and generation, potentially streamlining the development of face-related AI applications.  |
| SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by
  Imitating Human Annotator Trajectories (Read more on [arXiv](https://arxiv.org/abs/2503.08625) or [HuggingFace](https://huggingface.co/papers/2503.08625))| Qingpei Guo, Chunluan Zhou, Hao Chen, Yuzhuo Tian, Z-MU-Z | SegAgent introduces a new segmentation framework where Multimodal Large Language Models (MLLMs) mimic human annotators using interactive tools to enhance pixel-level understanding. The main research objective is to develop and evaluate a method for MLLMs to perform fine-grained pixel-level image segmentation by imitating human annotation trajectories. The key methodology is modeling segmentation as a multi-step Markov Decision Process (HLMAT), where MLLMs generate text-based click points iteratively, and adapting policy improvement methods like StaR and process reward modeling (PRM) guided tree search. The primary result is that SegAgent-LLaVA+SAM achieved a 75.72 cIoU on the refCOCO testB dataset, demonstrating performance comparable to state-of-the-art methods. Principal implication for AI practitioners is a new protocol to train and assess the fine-grained visual understanding capabilities of MLLMs on pixel segmentation and interactive tasks.  |
| MagicInfinite: Generating Infinite Talking Videos with Your Words and
  Voice (Read more on [arXiv](https://arxiv.org/abs/2503.05978) or [HuggingFace](https://huggingface.co/papers/2503.05978))| Jiantong Zhao, Xuancheng Yang, Shitong Shao, Hongwei Yi, Owen777 | MagicInfinite is a diffusion Transformer framework for generating high-fidelity, infinite-length talking head videos controlled by audio and text. The main research objective is to overcome limitations of existing portrait animation methods in handling diverse character styles, achieving accurate lip synchronization, and enabling efficient long video generation. The key methodology involves a 3D full-attention mechanism with a sliding window denoising strategy, a two-stage curriculum learning scheme (integrating audio, text, and reference images), and region-specific masks with adaptive loss functions. Primary results show that MagicInfinite achieves a 20x inference speed boost over the basemodel and can generate a 10-second 540x540p video in 10 seconds on 8 H100 GPUs without quality loss. For AI practitioners, this framework offers an efficient way to generate high-quality, controllable, and arbitrarily long talking head animations with strong temporal coherence.  |
| Seedream 2.0: A Native Chinese-English Bilingual Image Generation
  Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2503.07703) or [HuggingFace](https://huggingface.co/papers/2503.07703))| Liang Li, Fanshi Li, Xiaoxia Hou, Lixue Gong, wujie10 | Seedream 2.0 is a bilingual Chinese-English text-to-image diffusion model that addresses limitations of existing models in cultural understanding, text rendering, and model bias. The main research objective is to develop a foundation model capable of generating high-fidelity images aligned with both Chinese and English prompts, demonstrating superior performance in multiple aspects, including text rendering and understanding of Chinese cultural nuances. The key methodology includes a multi-level optimization framework that integrates a bilingual LLM text encoder, a Glyph-Aligned ByT5 for character-level text rendering, Scaled ROPE, multi-phase post-training (SFT, RLHF), and a data system for continuous knowledge integration. The primary result is that Seedream 2.0 achieves state-of-the-art performance, outperforming models like Midjourney v6.1 and Ideogram 2.0 in human evaluations, with a human evaluation ELO score of 1117, and demonstrating a 78% text accuracy rate and 82% hit rate in Chinese text rendering. Principal implication for AI practitioners is that Seedream 2.0 provides a robust and culturally aware foundation model for bilingual image generation, particularly effective for applications requiring accurate Chinese text rendering and culturally specific content generation, outperforming widely available text-to-image models in the field.  |
| Gemini Embedding: Generalizable Embeddings from Gemini (Read more on [arXiv](https://arxiv.org/abs/2503.07891) or [HuggingFace](https://huggingface.co/papers/2503.07891))| Madhuri Shanbhogue, Daniel Cer, Sahil Dua, Feiyang Chen, Jinhyuk Lee | Gemini Embedding is a new state-of-the-art text embedding model that leverages the Gemini large language model for improved generalizability across languages and tasks. The main research objective is to develop a unified embedding model that achieves state-of-the-art performance across a broad range of multilingual text embedding tasks. The key methodology involves initializing the embedding model from Gemini, curating a high-quality training dataset using Gemini, and employing a two-stage training pipeline (pre-finetuning and finetuning) with a contrastive learning objective, culminating with model souping. The primary result is that Gemini Embedding achieves a mean task score of 68.32 on the Massive Multilingual Text Embedding Benchmark (MMTEB), outperforming prior state-of-the-art models. The principal implication for AI practitioners is that they can leverage Gemini Embedding as a highly generalizable, off-the-shelf solution for various downstream tasks, including classification, similarity, clustering, ranking and retrieval, particularly in multilingual settings.  |
| LightGen: Efficient Image Generation through Knowledge Distillation and
  Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.08619) or [HuggingFace](https://huggingface.co/papers/2503.08619))| Yexin Liu, Harold Haodong Chen, Haoze Zheng, Yajing Bai, Xianfeng Wu | LightGen is an efficient text-to-image generation model that uses knowledge distillation and direct preference optimization to reduce computational costs. The main research objective is to develop a text-to-image generation model that achieves comparable performance to state-of-the-art (SOTA) models with significantly reduced computational resources and dataset size. The key methodology involves distilling knowledge from SOTA text-to-image models into a compact Masked Autoregressive (MAR) architecture using a synthetic dataset and refining the output with Direct Preference Optimization (DPO). The model achieves an overall performance score of 0.62 on the GenEval benchmark at 512x512 resolution using only 0.7B parameters and a 2M image dataset. AI practitioners can use LightGen to develop high-quality image generation models with limited computational resources and smaller datasets, achieving performance similar to much larger and resource intensive models.  |
| Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled
  Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.08605) or [HuggingFace](https://huggingface.co/papers/2503.08605))| Jinwoo Shin, Joon-Young Lee, Jui-Hsien Wang, Seoung Wug Oh, Subin Kim | The paper introduces SynCoS, a tuning-free inference framework for generating multi-event long videos from text prompts using existing text-to-video diffusion models. The main research objective is to extend text-to-video diffusion models for long-form video generation with multiple events while maintaining local smoothness and global coherence. The key methodology, Synchronized Coupled Sampling (SynCoS), combines reverse and optimization-based sampling (DDIM and CSD) with a grounded timestep and fixed baseline noise to synchronize denoising paths across the entire video. SynCoS achieved a subject consistency score of 90.19% on Open-Sora Plan, outperforming baselines. AI practitioners can utilize SynCoS to extend existing diffusion models for high-quality, multi-event, and coherent, long video generation without additional model training.  |
| Implicit Reasoning in Transformers is Reasoning through Shortcuts (Read more on [arXiv](https://arxiv.org/abs/2503.07604) or [HuggingFace](https://huggingface.co/papers/2503.07604))| Deqing Yang, Siyu Yuan, Tianhe Lin, hsaest | Transformers trained for implicit multi-step reasoning rely on shortcuts rather than true step-by-step computation, limiting generalization. The main research question is how language models perform implicit reasoning in multi-step tasks, and why advanced reasoning capabilities observed in explicit reasoning do not emerge in implicit reasoning. The researchers trained GPT-2 models from scratch on a synthetic multi-step mathematical reasoning dataset and used activation patching for analysis. Results showed that models trained on data with unfixed premise order had significantly reduced accuracy; for instance, accuracy dropped to ~40% on 5-step reasoning tasks. The principal implication for AI practitioners is that current language models may achieve high performance on tasks with similar patterns through shortcut learning without genuine generalization, particularly in implicit reasoning scenarios.  |
| OmniMamba: Efficient and Unified Multimodal Understanding and Generation
  via State Space Models (Read more on [arXiv](https://arxiv.org/abs/2503.08686) or [HuggingFace](https://huggingface.co/papers/2503.08686))| Xinggang Wang, Wenyu Liu, Qian Zhang, Bencheng Liao, Jialv Zou | OmniMamba is a Mamba-based unified multimodal model for both understanding and generation tasks. The main research objective is to develop a unified multimodal generation model that achieves both training and inference efficiency with limited training data. The key methodology involves using a linear-architecture-based Mamba-2, decoupled vocabularies, task-specific LoRA, and a decoupled two-stage training strategy. OmniMamba achieves competitive performance with JanusFlow and surpasses Show-o across benchmarks while using only 2M image-text pairs, demonstrating up to 119.2x speedup and 63% GPU memory reduction. AI practitioners can leverage OmniMamba's efficient architecture and training strategies for developing multimodal models with reduced computational cost and data requirements.  |
| Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.07572) or [HuggingFace](https://huggingface.co/papers/2503.07572))| Edward Emanuel Beeching, Lewis Tunstall, Amrith Setlur, Matthew Y. R. Yang, CohenQu | This paper introduces Meta Reinforcement Fine-Tuning (MRT), a method to optimize test-time compute for large language models (LLMs) by minimizing cumulative regret. The main research question is whether current LLMs efficiently utilize test-time compute and whether scaling approaches continue to be effective as budget improves. The key methodology is to formalize test-time compute optimization as a meta-reinforcement learning problem, using a dense reward bonus based on "progress" quantified by the change in likelihood of eventual success. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL. For AI practitioners, MRT provides a new fine-tuning method that improves LLM performance and efficiency by optimizing for progress during inference, enabling better utilization of computational resources.  |
| Video Action Differencing (Read more on [arXiv](https://arxiv.org/abs/2503.07860) or [HuggingFace](https://huggingface.co/papers/2503.07860))| Alejandro Lozano, Anita Rau, Yuhui Zhang, nicholswang, jmhb | This paper introduces Video Action Differencing (VidDiff), a new task and benchmark for identifying subtle differences between videos of the same action. The main research question is how to identify and describe fine-grained differences between two videos of individuals performing the same action. The key methodology is a three-stage agentic workflow (VidDiff Method) that leverages large language models (LLMs) for difference proposal, CLIP for frame localization, and vision-language models (VLMs) for frame differencing. The primary result is that the proposed VidDiff Method achieves a closed-set accuracy of 56.3%, outperforming GPT-40 (53.5%) and Gemini-1.5 Pro (57.7%), and its open set recall@N is 42.1. AI practitioners can use the VidDiffBench dataset and the VidDiff Method as a benchmark and baseline for developing and evaluating models capable of fine-grained video understanding and comparison, essential for applications like skill learning, coaching and automated performance feedback.  |
| ^RFLAV: Rolling Flow matching for infinite Audio Video generation (Read more on [arXiv](https://arxiv.org/abs/2503.08307) or [HuggingFace](https://huggingface.co/papers/2503.08307))| Claudio Ferrari, Tomaso Fontanini, Filippo Botti, Giuseppe Gabriele Tarollo, MaverickAlex | RFLAV is a novel transformer-based architecture for infinite and synchronized audio-video generation. The main research objective is to address the limitations of existing audio-video generation models regarding quality, multimodal synchronization, and duration. The key methodology is a rolling rectified-flow model with a lightweight temporal cross-modality fusion module that processes audio and video in separate branches before combining them. The proposed RFLAV model achieves a FVD score of 38.36 on the AIST++ dataset with 200 denoising steps, surpassing existing state-of-the-art models. For AI practitioners, this model offers an improved method for generating arbitrarily long, high-quality audio-video sequences without the duration constraints of prior methods.  |
| "Principal Components" Enable A New Language of Images (Read more on [arXiv](https://arxiv.org/abs/2503.08685) or [HuggingFace](https://huggingface.co/papers/2503.08685))| Xiaojuan Qi, Jiankang Deng, Ismail Elezi, tennant, xwen99 | "Principal Components" Enable A New Language of Images introduces a visual tokenization framework with a provable PCA-like structure in the latent token space. The main research objective is to create a compact, structured image representation that reduces redundancy while effectively decoupling semantic information from less important low-level details in 1D visual tokenizers. The key methodology involves a dynamic nested classifier-free guidance strategy during training to induce an orderliness bias in tokens, combined with a diffusion-based decoder. The approach achieves a state-of-the-art reconstruction FID score of 0.72 on the ImageNet validation set, a 10% improvement over prior methods. For AI practitioners, this method provides a way to generate more interpretable and efficient visual representations, suitable for tasks such as image reconstruction and auto-regressive generative modeling, with fewer tokens for training and inference.  |
| BiasEdit: Debiasing Stereotyped Language Models via Model Editing (Read more on [arXiv](https://arxiv.org/abs/2503.08588) or [HuggingFace](https://huggingface.co/papers/2503.08588))| Julian McAuley, Ningyu Zhang, Wei Xu, XinXuNLPer | BIASEDIT is a model editing method for debiasing stereotyped language models by modifying model parameters with lightweight editor networks. The main research objective is to develop an efficient method to remove stereotypical biases from language models without significantly impacting their language modeling capabilities. The key methodology involves training editor hyper-networks using a debiasing loss and a retention loss to generate parameter updates that locally modify a language model's parameters related to stereotyped biases. Results show that BIASEDIT reduces Stereotype Score (SS) to less than 57% and more than 46% on various LMs, outperforming baselines, while maintaining language modeling scores with small changes. For AI practitioners, BIASEDIT offers a computationally efficient method to mitigate societal biases within pre-trained language models, enabling the development of fairer and more robust NLP applications, and bias editing on upper blocks of language models had fewer negative impacts.  |
| QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long
  Video Comprehension (Read more on [arXiv](https://arxiv.org/abs/2503.08689) or [HuggingFace](https://huggingface.co/papers/2503.08689))| Shukang Yin, Weizhong Huang, Xiawu Zheng, Wang Chen, Yongdong Luo | QuoTA is a training-free framework for long video understanding that enhances existing LVLMs by assigning visual tokens based on query relevance. The main research objective is to improve long-video comprehension in Large Video-Language Models (LVLMs) by mitigating visual redundancy and aligning visual processing with task-specific requirements. The key methodology involves query-oriented frame-level importance assessment using Chain-of-Thoughts reasoning to decouple the query, parallel video frame evaluation with a scoring LVLM, and dynamic visual token assignment based on the generated scores. Implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six video understanding benchmarks, including Video-MME and MLVU. The principal implication for AI practitioners is that QuoTA offers a plug-and-play module to improve existing LVLMs' long video understanding capabilities without additional training, enabling more effective processing aligned with given query.  |
| Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents (Read more on [arXiv](https://arxiv.org/abs/2503.08684) or [HuggingFace](https://huggingface.co/papers/2503.08684))| Xiao Zhang, Liang Pang, Haiyuan Zhao, Sunhao Dai, Haoyu Wang | PLM-based retrieval models exhibit a "perplexity trap," overrating documents with low perplexity, leading to source bias that favors LLM-generated content. The main research question is why PLM-based retrievers prefer low-perplexity documents, even when semantic quality is comparable to human-written ones. The authors employ causal graphs, two-stage least squares (2SLS) regression, and theoretical analysis linking retrieval and language modeling objectives. Results show a consistently negative causal effect of perplexity on relevance scores across multiple datasets and models; for example, on the TREC-COVID dataset, ANCE showed a coefficient of -0.23 (p=0.15). A causal-inspired debiasing method, Causal Diagnosis and Correction (CDC), is proposed to mitigate this effect, which is valuable for those seeking to remove perplexity-related source bias.  |
| RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow
  Trajectories (Read more on [arXiv](https://arxiv.org/abs/2503.07699) or [HuggingFace](https://huggingface.co/papers/2503.07699))| Xing Wang, Yuxi Ren, Yuhong Yang, Xin Xia, Huiyang Shao | RayFlow is a diffusion model acceleration framework that guides each sample along a unique path to an instance-specific target distribution, improving generation speed and control. The main research objective is to address the slow generation speed, sample quality compromises, and training complexities of existing diffusion model acceleration methods. The key methodology includes guiding each sample along a unique path towards instance-specific target distributions and introducing an importance sampling technique (Time Sampler) for enhanced training efficiency. Primary results show that, on the COCO-5k dataset, the SDXL-Ray model achieved a FID score of 3.90 in a 4-step generation, outperforming several existing methods. A principal implication is that AI practitioners can use RayFlow to generate high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.  |
| Benchmarking AI Models in Software Engineering: A Review, Search Tool,
  and Enhancement Protocol (Read more on [arXiv](https://arxiv.org/abs/2503.05860) or [HuggingFace](https://huggingface.co/papers/2503.05860))| Maliheh Izadi, philippedebekker, RohamKoohestani | This paper reviews AI4SE benchmarks, introduces a search tool (BenchScout) and enhancement protocol (BenchFrame), and demonstrates improvements on HumanEval, resulting in HumanEvalNext. The main research objective is to address challenges in AI4SE benchmarking, including knowledge fragmentation, benchmark selection, lack of standardization, and existing benchmark limitations. The key methodology involves a systematic literature review of 204 benchmarks, development of a semantic search tool using clustering and dimensionality reduction, and a case study applying a proposed framework (BenchFrame) for benchmark enhancement through code review, modifications, and peer review. A primary result shows that on HumanEvalNext, language models exhibited a pass@1 score reduction of 31.22% compared to the original HumanEval. The principal implication for AI practitioners is that using refined and rigorously evaluated benchmarks like HumanEvalNext provides a more accurate assessment of model capabilities and guides future AI4SE research, emphasizing the need for continuous benchmark improvement.  |
| Referring to Any Person (Read more on [arXiv](https://arxiv.org/abs/2503.08507) or [HuggingFace](https://huggingface.co/papers/2503.08507))| Yuda Xiong, Tianhe Ren, Zhaoyang Zeng, Lin Wu, Qing Jiang | This paper introduces "Referring to Any Person," a new task and model (RexSeek) for detecting all individuals in an image that match a natural language description, along with a new dataset (HumanRef). The main research objective is to develop a model capable of multi-instance person referring, overcoming limitations of existing models and datasets that primarily focus on one-to-one object referring. The key methodology involves integrating a multimodal large language model with an object detection framework, trained in a multi-stage process, and creating a new dataset, HumanRef with 103,028 referring statements. The primary result is that RexSeek achieves a DensityF1 score of 82.3 on the HumanRef benchmark, significantly outperforming existing models like Qwen2.5-VL (31.9 DensityF1). The principal implication is that AI practitioners should leverage this model and the HumanRef for robust referring expression comprehension, especially within the task of referring to any person to enable more precise, multi-instance person detection.  |
| AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.08417) or [HuggingFace](https://huggingface.co/papers/2503.08417))| Junyong Noh, Chaelin Kim, Seokhyeon Hong, kwanY | AnyMoLe is a novel method for generating 3D character motion in-betweening without character-specific datasets, by leveraging video diffusion models. The main research objective is to address the scarcity of character-specific datasets in motion in-betweening, enabling animation generation for arbitrary characters. The key methodology involves a two-stage video generation process using a fine-tuned video diffusion model (ICAdapt), and motion-video mimicking optimization with a scene-specific joint estimator. The primary results show that AnyMoLe outperforms baseline methods in all metrics, achieving an HL2Q of 0.0015 for humanoid characters, demonstrating superior motion generation. For AI practitioners, this implies a reduced reliance on extensive character-specific datasets for motion in-betweening, expanding the applicability of animation generation to a wider range of characters.  |
| AI-native Memory 2.0: Second Me (Read more on [arXiv](https://arxiv.org/abs/2503.08102) or [HuggingFace](https://huggingface.co/papers/2503.08102))| Jingbo Shang, Felix Tao, Tao Gao, Xiang Ying, Jiale Wei | SECOND ME is an AI-native memory system that acts as an intelligent, persistent memory offload for users. The main research objective is to develop and evaluate an LLM-based system that can retain, organize, and dynamically utilize user-specific knowledge to improve human-computer interaction. The key methodology involves a multi-layer hybrid architecture integrating supervised fine-tuning (SFT) and direct preference optimization (DPO) with automated data synthesis and evaluation using LLMs. A key result is that using diverse data sources with strong Chain-of-Thought (CoT) normalization achieved a 0.91 score in the Memory (Self) evaluation metric. AI practitioners can leverage this fully localizable, open-sourced system's approach to memory parameterization and multi-agent framework to build more personalized and context-aware AI applications.  |
| Mixture of Experts Made Intrinsically Interpretable (Read more on [arXiv](https://arxiv.org/abs/2503.07639) or [HuggingFace](https://huggingface.co/papers/2503.07639))| Puneet K. Dokania, Christian Schroeder de Witt, Ashkan Khakzar, Constantin Venhoff, Xingyi Yang | This paper introduces MoE-X, a Mixture-of-Experts language model designed for intrinsic interpretability by leveraging sparsity and width. The main research objective is to design an intrinsically interpretable language model architecture that reduces polysemanticity without relying on post-hoc interpretability methods. The key methodology involves rewriting the MoE layer as an equivalent sparse, wide MLP, enforcing sparse activation within each expert using ReLU, and redesigning the routing mechanism to prioritize experts with the highest activation sparsity. MoE-X achieves a perplexity better than GPT-2 and a chess board state reconstruction score of 0.840, surpassing sparse autoencoder-based approaches. AI practitioners can leverage MoE-X's architecture for improved interpretability in language models without sacrificing performance, offering a direct path to more transparent and understandable AI systems.  |
| NullFace: Training-Free Localized Face Anonymization (Read more on [arXiv](https://arxiv.org/abs/2503.08478) or [HuggingFace](https://huggingface.co/papers/2503.08478))| Nicu Sebe, Terence Sim, Tuomas Varanka, hkung | NullFace is a training-free method for localized face anonymization that preserves non-identity facial attributes using diffusion models. The main research objective is to develop a face anonymization technique that balances identity obscuration with the preservation of key non-identity-related attributes, without requiring model training. The key methodology involves inverting an input image using DDPM inversion to recover initial noise, then denoising it through an identity-conditioned diffusion process with modified identity embeddings, and optionally applying segmentation masks for localized control. The method achieved a re-identification rate of 0.34% on the FFHQ dataset, the lowest among compared methods. For AI practitioners, this method offers a flexible and practical approach to face anonymization, achieving competitive performance in privacy-preserving applications without the need for training or fine-tuning, and enabling controllable localized anonymization.  |
| Beyond Decoder-only: Large Language Models Can be Good Encoders for
  Machine Translation (Read more on [arXiv](https://arxiv.org/abs/2503.06594) or [HuggingFace](https://huggingface.co/papers/2503.06594))| Qinghong Zhang, Bei Li, Yongyu Mu, Tong Zheng, luoyingfeng | LaMaTE uses LLMs as encoders within an encoder-decoder architecture for improved machine translation.  The main research objective is to explore combining LLMs with NMT by using LLMs for encoding and NMT decoders for efficient and generalizable translation.  The key methodology is a two-stage training approach: first pre-training the NMT decoder and adaptor with frozen LLM parameters, then fine-tuning all parameters on a multi-task dataset (ComMT).  Primary results show that LaMaTE achieves a COMET score of 82.32 and BLEU score of 33.85, averaging across all tasks in the new ComMT benchmark dataset.  Principal implication for AI practitioners is that using LLMs as encoders in encoder-decoder models offers a strong balance between high translation quality, reduced computational cost (2.4-6.5x faster decoding), and generalizability, suggesting a promising direction of research.  |
| VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large
  Vision-Language Models in Fact-Seeking Question Answering (Read more on [arXiv](https://arxiv.org/abs/2503.06492) or [HuggingFace](https://huggingface.co/papers/2503.06492))| Lixin Liu, Shasha Guo, Xiaodong Chen, Yihan Zhao, WYLing | VisualSimpleQA is a new benchmark for evaluating fact-seeking question-answering capabilities of large vision-language models (LVLMs). The main research objective is to introduce a multimodal fact-seeking benchmark that allows for decoupled evaluation of visual and linguistic modules in LVLMs and incorporates well-defined difficulty criteria. The key methodology involves human annotation of samples with multimodal questions, text-only questions, rationales, and difficulty scores based on visual and linguistic factors. Primary results show that even state-of-the-art LVLMs like GPT-4o achieve only 60%+ correctness on multimodal questions in VisualSimpleQA, and 30%+ on a harder subset. The principal implication for AI practitioners is that there is substantial room for improvement in both the visual and linguistic modules of LVLMs for fact-seeking QA, especially regarding challenging visual recognition tasks and knowledge identification.  |
