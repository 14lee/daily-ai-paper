

## Papers for 2025-03-07

| Title | Authors | Summary |
|-------|---------|---------|
| LLM as a Broken Telephone: Iterative Generation Distorts Information (Read more on [arXiv](https://arxiv.org/abs/2502.20258) or [HuggingFace](https://huggingface.co/papers/2502.20258))| Michalis Vazirgiannis, guokan-shang, mgeng, amr-mohamed | Iterative processing of text by large language models (LLMs) degrades information, similar to the "broken telephone" game. The main research question is whether LLMs distort information through iterative generation, particularly in translation tasks. The key methodology involved simulating iterative translation chains, where an English document was repeatedly translated into and out of other languages using LLMs. Primary results show a gradual decline in factuality and relevance over iterations, with an average FActScore gradient of -0.038 ± 0.02 in the most complex translation chain setting. Principal implication for AI practitioners is that iterative generation with LLMs can lead to information distortion, making control of temperature, prompt design, and understanding the role of intermediary languages necessary when building applications relying on the iterative processing of LLM-generated content.  |
| EgoLife: Towards Egocentric Life Assistant (Read more on [arXiv](https://arxiv.org/abs/2503.03803) or [HuggingFace](https://huggingface.co/papers/2503.03803))| Zzitang, Alarak, fesvhtr, THUdyh, Jingkang | i) EgoLife introduces a comprehensive egocentric dataset and benchmark for developing AI life assistants. ii) The study aims to create life-oriented question-answering tasks designed to provide meaningful assistance in daily life through multimodal egocentric data understanding. iii) Data was collected from six participants living together for a week, using AI glasses to record multimodal egocentric video, supplemented by synchronized third-person video references and annotated for comprehensive data analysis. iv) The EgoLife Dataset comprises 300 hours of egocentric data and introduces EgoLifeQA, a benchmark for long-context question answering, alongside EgoButler, an integrated system, and their experiments verified the mechanisms, critical factors, and bottlenecks, guiding future improvements with EgoGPT achieving state-of-the-art performance on egocentric video understanding. v) The EgoLife dataset, tasks, and models offer AI practitioners a resource for advancing long-term egocentric life assistance through improved multimodal integration, identity recognition, and ultra-long-context question answering.  |
| HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization (Read more on [arXiv](https://arxiv.org/abs/2503.04598) or [HuggingFace](https://huggingface.co/papers/2503.04598))| Ya Wang, Breeze0417, LLIXQ, Taoer, BryceZhuo | HybridNorm, a novel normalization strategy for Transformers, combines QKV normalization in attention and Post-Norm in the feed-forward network to improve training stability and performance. The research objective is to address the trade-offs between training stability and final model performance inherent in existing normalization techniques like Pre-Norm and Post-Norm in Transformer models. The key methodology involves proposing HybridNorm and evaluating it through extensive experiments on large-scale dense and Mixture-of-Experts (MoE) language models. The primary results show that HybridNorm consistently outperforms Pre-Norm and Post-Norm across various benchmarks; for example, HybridNorm* achieved an average accuracy of 64.15% compared to Pre-Norm's 62.99% on downstream tasks for 1.2B dense models. Principal implication: AI practitioners can use HybridNorm to achieve more stable training dynamics and superior performance when training large Transformer models, particularly in language modeling applications.  |
| PokéChamp: an Expert-level Minimax Language Agent (Read more on [arXiv](https://arxiv.org/abs/2503.04094) or [HuggingFace](https://huggingface.co/papers/2503.04094))| Andy Luu Nguyen, chijin, milkkarten | PokéChamp is a minimax language agent that achieves expert-level performance in Pokémon battles by integrating large language models (LLMs) into the tree search algorithm. The main research objective is to develop an agent capable of strategic action proposal, accurate opponent modeling, and effective evaluation of game trajectories in Pokémon battles, without requiring LLM fine-tuning. The key methodology involves replacing three components of minimax tree search—player action sampling, opponent modeling, and value function estimation—with LLM-based generations, leveraging a world model that approximates game transitions. PokéChamp, powered by GPT-4o, achieves a 76% win rate against the best existing LLM-based bot and 84% against the strongest rule-based bot in the Generation 9 OverUsed Meta. AI practitioners can leverage this framework's integration of LLMs with game-theoretic planning algorithms to develop agents for complex, partially observable environments without task-specific training.  |
| FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion (Read more on [arXiv](https://arxiv.org/abs/2503.04222) or [HuggingFace](https://huggingface.co/papers/2503.04222))| passerqxj, OnewayLab, GGLS, Wanfq, AALF | FuseChat-3.0 integrates the strengths of heterogeneous large language models (LLMs) into more compact target LLMs using a two-stage training process. The main objective is to develop a method for effectively fusing knowledge from multiple, diverse source LLMs into smaller target LLMs. The methodology involves a specialized data construction protocol followed by supervised fine-tuning (SFT) and Direct Preference Optimization (DPO), using preference pairs generated from the same source model. When using Llama-3.1-8B-Instruct as the target model, the fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. AI practitioners can use this implicit model fusion technique to enhance the performance of smaller LLMs by leveraging the capabilities of larger, heterogeneous models, without requiring architectural changes.  |
| Token-Efficient Long Video Understanding for Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.04130) or [HuggingFace](https://huggingface.co/papers/2503.04130))| zhiqilinv, MuyangLI, zhijianliu, xiuyul, jdps | i) STORM is a novel architecture for efficient long video understanding in multimodal LLMs. ii) The research aims to improve video understanding in LLMs, particularly with extended temporal contexts. iii) A dedicated temporal encoder using the Mamba State Space Model is introduced between the image encoder and the LLM, enabling token reduction via sampling and spatial/temporal pooling. iv) STORM achieves state-of-the-art results with over 5% improvement on MLVU and LongVideoBench, while reducing computation costs by up to 8x and decoding latency by 2.4-2.9x for fixed input frames. v) Practitioners can leverage STORM to reduce LLM computational demands and latency without sacrificing performance.  |
| The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.04606) or [HuggingFace](https://huggingface.co/papers/2503.04606))| Xu Tan, Kai Shen, Aoxiong Yin, JunchengLi, ustcscallion | LanDiff is a hybrid text-to-video generation framework that combines language models and diffusion models for coarse-to-fine video synthesis. The main research objective is to develop a framework that leverages the strengths of both autoregressive language models (semantic understanding, causal modeling) and diffusion models (high visual quality, progressive refinement) while mitigating their limitations. The key methodology involves a two-stage process: (1) a semantic tokenizer compresses 3D visual features into 1D discrete representations, and an LLM generates semantic tokens; (2) a streaming diffusion model refines these tokens into high-fidelity video features, decoded by a VAE. LanDiff, with a 5B parameter model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing state-of-the-art open-source and commercial models. AI practitioners can use LanDiff architecture as a blueprint of production-level video generation, particularly in scenarios requiring high semantic accuracy, visual quality, and long video generation capabilities.  |
| IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval (Read more on [arXiv](https://arxiv.org/abs/2503.04644) or [HuggingFace](https://huggingface.co/papers/2503.04644))| Mingsheng Shang, yilunzhao, guo9, songtingyu | IFIR is a new benchmark for evaluating instruction-following information retrieval in specialized domains, revealing challenges for current models. The main research objective is to evaluate how well current information retrieval (IR) systems can follow complex, domain-specific instructions in expert fields. Key methodology involves creating a new benchmark (IFIR) with 2,426 examples across finance, law, healthcare, and scientific literature, incorporating three levels of instruction complexity and a novel LLM-based evaluation metric (INSTFOL). Primary results show that while BM25 performs relatively well due to glossary terms, instruction-tuned retrievers like INSTRUCTOR don't significantly outperform their base models, and most models' performance declines with increasing instruction complexity; LLM-based retrievers achieve the highest INSTFOL score, as demonstrated by Promptriever-7B. Principal implication is that current retrieval models, even those fine-tuned for instruction following, struggle with long, complex instructions in specialized domains, indicating a need for improved training methodologies and architectures or hybrid systems, leveraging large language model's superior instruction-following ability.  |
| Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities (Read more on [arXiv](https://arxiv.org/abs/2503.03983) or [HuggingFace](https://huggingface.co/papers/2503.03983))| manocha, rafaelvalle, firecomputer, ZhifengKong, SreyanG-NVIDIA | i) Audio Flamingo 2 (AF2) is a novel audio-language model (ALM) enhancing audio understanding and reasoning. ii) The research aims to develop an ALM with advanced capabilities in understanding and reasoning over both short and long audio segments, including non-speech sounds and music. iii) AF2 leverages a custom CLAP model, synthetic Audio QA data, and a multi-stage curriculum learning strategy. iv) AF2 achieves state-of-the-art performance on over 20 benchmarks, surpassing larger models, with a 3B parameter language model achieving up to 18.9% improvement on the LongAudioBench compared to Gemini F v2. v) AF2's ability to understand long audio segments offers AI practitioners new capabilities for real-world applications requiring contextual auditory cue processing, such as anomaly detection and assistive technologies.  |
| Identifying Sensitive Weights via Post-quantization Integral (Read more on [arXiv](https://arxiv.org/abs/2503.01901) or [HuggingFace](https://huggingface.co/papers/2503.01901))| Weiyu Huang, surfingtomchen, jt-zhang, zcliang22, yuezhouhu | The paper introduces a novel sensitivity metric and quantization framework for compressing large language models (LLMs). The primary research objective is to develop a more accurate sensitivity metric for weight quantization that addresses limitations of existing gradient and Hessian-based methods. The key methodology is Post-quantization Integral (PQI), which estimates the impact of quantized weights on the loss function, along with a Dense-and-Sparse detach framework called ReQuant. Applying ReQuant to Llama 3.2 1B with QTIP quantization reduces perplexity by 2.66, showcasing the improvement. For AI practitioners, this method provides an effective way to improve post-training quantization of LLMs, achieving better compression with minimal accuracy loss.  |
| L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.04725) or [HuggingFace](https://huggingface.co/papers/2503.04725))| Marin Soljačić, Di Luo, Zhuotao Jin, oriolmayne, zhuoc3 | This paper establishes a theoretical framework for understanding and improving long-context language modeling based on a bipartite mutual information scaling law. The main research question is how a language model's capacity to handle long-range dependencies scales with its internal state size and sequence length. The key methodology involves proving a "Long-context Language Modeling (L²M)" condition, theoretically relating model state size to bipartite mutual information, and empirically validating this scaling law using transformer and state space models on text datasets. The primary result is that bipartite mutual information in natural language scales as I ~ L^β (where β is between 0 and 1) and that a model's state size must grow at least as fast as I ~ L^β for effective long-context modeling. The principal implication for AI practitioners is that designing models for long-context tasks requires careful consideration of the history state's scaling, with transformers naturally satisfying this condition and other architectures (like SSMs) needing model size increases to maintain performance at longer sequence lengths.  |
| Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.04378) or [HuggingFace](https://huggingface.co/papers/2503.04378))| Ellie Evans, Daniel Egert, Jiaqi Zeng, Zhilin Wang, odelalleau | Dedicated Feedback and Edit Models enable inference-time scaling for open-ended tasks, achieving state-of-the-art performance by leveraging human feedback.  i) Main research question or objective: How to perform inference-time scaling for open-ended general-domain tasks, inspired by human feedback, using dedicated Feedback and Edit Models.  ii) Key methodology used: Trained dedicated Feedback and Edit Models on a curated dataset, leveraging human-provided feedback and edits.  iii) Primary results: The optimally scaled system, based on 70B models from the Llama 3 family, achieved a state-of-the-art performance on Arena Hard at 92.7, surpassing OpenAI ol-preview-2024-09-12 (90.4) and DeepSeek R1 (92.3).  iv) Principal implication for AI practitioners: This approach demonstrates a viable method for improving model performance on complex, open-ended tasks by using human feedback to train models to improve responses at inference.  |
| Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.02495) or [HuggingFace](https://huggingface.co/papers/2503.02495))| Linhui Li, Jing Lian, yjyangwork | Union-of-Experts (UoE) decomposes transformers into equivalent experts and implements selective routing on input data and experts to improve model performance while maintaining efficiency. The main research objective is to address limitations of existing Mixture-of-Experts (MoE) methods, specifically lack of high-quality expert interactions and inefficient extension to attention blocks. Key methodology involves equivalent expert decomposition on MLP and attention blocks via matrix partition, two routing paradigms (patch-wise data and expert selection), and parallel implementation of routing/computation. Primary results show UoE achieves an average perplexity reduction of 2.38 on language modeling tasks compared to the best-performed MoE method, using only 76% of the FLOPs. Principal implication for AI practitioners is that UoE offers a more efficient and performant approach to building transformer-based models, directly applicable to large-scale language and vision tasks.  |
| Lost in Literalism: How Supervised Training Shapes Translationese in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.04369) or [HuggingFace](https://huggingface.co/papers/2503.04369))| Leyang Cui, Huajian Zhang, Zhilin Wang, Ronghao Zhang, yaful | This paper investigates and mitigates translationese (unnatural translations) in Large Language Models (LLMs) caused by biases introduced during supervised fine-tuning (SFT).  The main research objective is to evaluate the prevalence of translationese in LLM-generated translations and investigate its origins during supervised training.  The key methodology involves human annotation to identify translationese spans, analysis of training data, and mitigation strategies such as refining training references and filtering unnatural instances using perplexity.  The primary results show that even advanced models like GPT-4 exhibit substantial translationese, with over 40% of their translations containing substantial translationese patterns, and that refining training data with LLMs reduces perplexity by 7.8 in the English-Chinese dataset.  Principal implication for AI practitioners is that addressing translationese bias in SFT data, by polishing golden references or filtering, can improve the naturalness of LLM translation outputs.  |
| Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems (Read more on [arXiv](https://arxiv.org/abs/2503.01375) or [HuggingFace](https://huggingface.co/papers/2503.01375))| Ekaterina Muravleva, oseledets, dsherki | The paper introduces a method combining Conditional Flow Matching (CFM) and transformers to efficiently solve Bayesian inverse problems. The main objective is to recover the distribution of model parameters conditioned on observed experimental data, given a series of observations and a forward model. The key methodology involves training a transformer-based CFM architecture to learn the conditional probability distribution from samples, handling a variable number of observations. Results showed that for a SEIR disease model, the average error was 2.05% ± 1.04% using a 4-point MLP model, significantly outperforming MCMC in computational efficiency. AI practitioners can leverage this approach for faster and more scalable sampling from posterior distributions in Bayesian inverse problems, particularly with datasets having variable-length observations.  |
| Understanding and Predicting Derailment in Toxic Conversations on GitHub (Read more on [arXiv](https://arxiv.org/abs/2503.02191) or [HuggingFace](https://huggingface.co/papers/2503.02191))| Rebekah Copeland, Robert Zita, kdamevski, rahat-rizvi, imranraad | This research investigates conversational derailment leading to toxicity in GitHub discussions, aiming to predict and mitigate such occurrences proactively. The main research objective is to understand the characteristics of toxic conversations on GitHub and how these conversations derail into toxicity. The key methodology involves curating a dataset of toxic and non-toxic GitHub conversations, analyzing linguistic and conversational features, and developing a Large Language Model (LLM)-based approach using conversation trajectory summaries. The LLM prompts, tailored to provide summaries of GitHub conversations, achieved a 69% F1-score in predicting conversational derailment. AI practitioners can use this proactive, domain-specific, LLM-based moderation approach to identify and address potentially harmful conversations on platforms like GitHub before they escalate to toxicity.  |
