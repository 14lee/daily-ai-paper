

## Papers for 2025-03-26

| Title | Authors | Summary |
|-------|---------|---------|
| Long-Context Autoregressive Video Modeling with Next-Frame Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.19325) or [HuggingFace](https://huggingface.co/papers/2503.19325))| Mike Zheng Shou, Weijia Mao, Yuchao Gu | This paper introduces Frame AutoRegressive (FAR), a baseline for long-context autoregressive video modeling using next-frame prediction. The research objective is to address challenges in long-context video modeling, namely visual redundancy impacting temporal extrapolation and computational costs associated with long sequences. Key methodologies include FAR trained with a frame-wise flow matching objective and causal attention, stochastic clean context to bridge the train-inference gap, FlexRoPE for improved test-time temporal extrapolation (up to 16x), and long short-term context modeling for efficient training on longer videos. Primary results show FAR achieves state-of-the-art performance, outperforming Token-AR and demonstrating better convergence than video diffusion transformers, achieving an FVD of 279 on UCF-101 (Table 2, FAR-XL). For AI practitioners, FAR provides an effective and simpler baseline framework for autoregressive video generation that naturally supports variable-length context and improves temporal consistency in long videos compared to existing methods. |
| CoMP: Continual Multimodal Pre-training for Vision Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2503.18931) or [HuggingFace](https://huggingface.co/papers/2503.18931))| Yu-Gang Jiang, Zuxuan Wu, Wujian Peng, Lingchen Meng, Row11n | This paper introduces COMP, a continual multimodal pre-training method enhancing Vision Foundation Models (VFMs) for native resolution processing and better language alignment. The objective is to adapt prevailing VFMs, regardless of their original training, to handle diverse image sizes and produce visual features more congruent with Large Language Model (LLM) representations. COMP utilizes Continual Rotary Position Embedding (C-ROPE) for variable resolution inputs and an Alignment Loss for explicit cross-modal feature alignment within a three-stage training framework. Results show COMP-SigLIP achieves significant gains, reaching 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while largely maintaining performance on unimodal tasks like ImageNet-1K classification (87.4%). For AI practitioners, COMP provides a mechanism to upgrade existing VFMs, enabling them to serve as more effective vision encoders for LLMs, particularly in tasks demanding fine-grained visual understanding from native resolution images. |
| Exploring Hallucination of Large Multimodal Models in Video
  Understanding: Benchmark, Analysis and Mitigation (Read more on [arXiv](https://arxiv.org/abs/2503.19622) or [HuggingFace](https://huggingface.co/papers/2503.19622))| Yue Liu, Baolong Bi, Jingyi Tang, Jiashu Qu, Hongcheng Gao | This paper introduces HAVEN, a benchmark to evaluate and mitigate hallucinations in Large Multimodal Models (LMMs) for video understanding. The main objective is to systematically analyze hallucination causes (prior conflict, in-context conflict, capability deficiency) and aspects (object, scene, event) in videos and develop mitigation strategies. Key methodology involves constructing the 6K-question HAVEN benchmark and proposing a thinking-based mitigation approach combining supervised reasoning fine-tuning (SRFT) and thinking-based direct preference optimization (TDPO). Primary results show significant variation in hallucination across 16 LMMs, with the proposed SRFT+TDPO method improving baseline accuracy by 7.65% on hallucination evaluation and reducing the consistency bias score by 4.5%. For AI practitioners, HAVEN offers a standardized tool to assess video LMM reliability regarding hallucinations, while the SRFT+TDPO training strategy presents a method to enhance model factuality and reasoning in video tasks. |
| Inference-Time Scaling for Flow Models via Stochastic Generation and
  Rollover Budget Forcing (Read more on [arXiv](https://arxiv.org/abs/2503.19385) or [HuggingFace](https://huggingface.co/papers/2503.19385))| Minhyuk Sung, Jisung Hwang, Taehoon Yoon, Jaihoon Kim | This paper introduces an inference-time scaling approach for pretrained flow models using stochastic generation and adaptive compute allocation to enhance alignment with user preferences. The main objective is to enable effective inference-time scaling, similar to diffusion models, for deterministic flow models without retraining. The key methodology involves converting the flow model's ODE to an SDE, using a Variance Preserving (VP) interpolant instead of a linear one to increase diversity, and applying Rollover Budget Forcing (RBF) to adaptively allocate computation across timesteps. Results show the VP-SDE with RBF significantly improves compositional alignment, achieving a VQAScore of 0.925, outperforming the base model (0.726) and diffusion models even with fewer computations (NFEs). For AI practitioners, this method allows enhancing existing flow models to better follow complex prompts (e.g., counting, spatial relations) during inference, offering a computationally efficient way to improve output quality and alignment compared to standard generation or diffusion model scaling approaches. |
| Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection
  with Artifact Explanation (Read more on [arXiv](https://arxiv.org/abs/2503.14905) or [HuggingFace](https://huggingface.co/papers/2503.14905))| Zichen Wen, Hengrui Kang, Peilin Feng, Junyan Ye, Siwei Wen | This paper introduces FakeVLM, a specialized large multimodal model for detecting synthetic images and providing artifact explanations, alongside the FakeClue dataset. The primary objective is to create an LMM-based system capable of accurately classifying images as real or synthetic (general and DeepFake) while offering interpretable, natural language explanations for detected artifacts. FakeVLM employs a LLaVA-v1.5 architecture, fine-tuning all parameters on the novel FakeClue dataset (>100k images, 7 categories) which features fine-grained artifact annotations generated via a multi-LMM strategy and category-specific prompts, framing detection as an explanatory visual question answering task. FakeVLM demonstrated superior performance over baseline LMMs, achieving 0.986 Accuracy and 0.981 F1 score on the FakeClue dataset for combined detection and explanation, nearing expert model performance in detection-only tasks without requiring auxiliary classifiers. For AI practitioners, FakeVLM offers a robust, single-model solution for synthetic image detection that inherently provides interpretability, enhancing trust and transparency in authenticity assessment pipelines compared to black-box classifiers or less specialized LMMs. |
| Scaling Vision Pre-Training to 4K Resolution (Read more on [arXiv](https://arxiv.org/abs/2503.19903) or [HuggingFace](https://huggingface.co/papers/2503.19903))| Sifei Liu, Yao Lu, Han Cai, Boyi Li, Baifeng Shi | This paper introduces PS3, a method scaling CLIP-style vision pre-training to 4K resolution with near-constant computational cost by selectively processing local regions instead of entire high-resolution images. The objective is to overcome the prohibitive quadratic/quartic cost of training vision models on high-resolution inputs. PS3 employs a multi-stage architecture involving low-resolution global feature extraction, top-down/bottom-up patch selection based on saliency or text prompts, and multi-scale high-resolution feature extraction on selected patches using localized contrastive learning. Applied within a Multimodal Large Language Model (MLLM) named VILA-HD, PS3 significantly improves performance on high-resolution tasks; on the proposed 4KPro benchmark, VILA-HD achieves 74.2% accuracy, outperforming Qwen2-VL by 3.2% while being 2.96x faster. For AI practitioners, PS3 provides a computationally efficient pre-training framework enabling MLLMs to perceive fine-grained details in 4K images, significantly enhancing capabilities for tasks requiring high-resolution visual understanding with reduced inference latency compared to full-image processing or token pruning methods. |
| Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time
  Thinking (Read more on [arXiv](https://arxiv.org/abs/2503.19855) or [HuggingFace](https://huggingface.co/papers/2503.19855))| Yunjie Ji, Shuaiting Chen, Haotian Wang, Sitong Zhao, Xiaoyu Tian | This paper introduces "Multi-round Thinking," a test-time scaling method enhancing large language model (LLM) reasoning by iteratively refining answers using previous outputs as prompts. The main objective is to improve LLM reasoning performance, especially on complex tasks, by overcoming limitations of single-step reasoning and cognitive inertia without requiring additional training. The key methodology involves repeatedly prompting the LLM with the original question concatenated with the model's final answer from the previous round, using a specific prompt template. Primary results show consistent performance gains across models and benchmarks; for example, QwQ-32B improved pass@1 accuracy on AIME 2024 from 80.3% (Round 1) to 82.1% (Round 2), and DeepSeek-R1 improved from 79.7% to 82.0%. For AI practitioners, this simple, training-free technique offers a practical method to potentially enhance LLM accuracy at inference time simply by re-prompting, although it incurs additional computational cost and latency per round. |
| CoLLM: A Large Language Model for Composed Image Retrieval (Read more on [arXiv](https://arxiv.org/abs/2503.19910) or [HuggingFace](https://huggingface.co/papers/2503.19910))| Son Tran, Mubarak Shah, Ashish Tawari, Jinyu Yang, Chuong Huynh | CoLLM introduces a Large Language Model (LLM) based framework for Composed Image Retrieval (CIR) that synthesizes training triplets dynamically from image-caption pairs. The objective is to overcome CIR data scarcity, enhance multimodal query understanding using LLMs, and improve evaluation benchmark reliability. Key methodology includes synthesizing reference image embeddings using Spherical Linear Interpolation (Slerp) and modification text using template-based interpolation between image-caption pairs, feeding these into an LLM for composed query embedding generation. CoLLM achieves state-of-the-art results on multiple CIR benchmarks, and the introduced MTCIR dataset yields up to 15% performance improvement for baseline models compared to other synthetic datasets. For AI practitioners, the principal implication is a method for supervised CIR model training without expensive manually annotated triplets, providing scalability alongside a large-scale synthetic dataset (MTCIR) and refined evaluation benchmarks. |
| MDocAgent: A Multi-Modal Multi-Agent Framework for Document
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.13964) or [HuggingFace](https://huggingface.co/papers/2503.13964))| Yun Li, Tong Sun, Ruiyi Zhang, Peng Xia, Siwei Han | MDocAgent is a novel multi-modal, multi-agent framework integrating text and image retrieval-augmented generation (RAG) for improved document question answering (DocQA). The primary objective is to address the limitations of single-modal DocQA systems by effectively integrating and reasoning over both textual and visual information in complex documents. The methodology utilizes parallel text and image RAG pipelines feeding context to five specialized agents (General, Critical, Text, Image, Summarizing) that collaborate to extract, analyze, and synthesize information guided by extracted critical cues. Preliminary experiments show MDocAgent achieves an average performance improvement of 12.1% over current state-of-the-art methods on five benchmarks using top-1 retrieval. For AI practitioners, this demonstrates that a structured multi-agent, multi-modal RAG approach can enhance DocQA accuracy on complex documents by enabling detailed cross-modal understanding and synthesis beyond single-modal or basic LVLM capabilities. |
| Latent Space Super-Resolution for Higher-Resolution Image Generation
  with Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.18446) or [HuggingFace](https://huggingface.co/papers/2503.18446))| Seon Joo Kim, Jinwoo Kim, Sangmin Han, Jinho Jeong | This paper proposes LSRNA, a framework combining Latent space Super-Resolution (LSR) and Region-wise Noise Addition (RNA) to improve higher-resolution image generation with diffusion models. The objective is to overcome limitations like manifold deviation (latent upsampling) and smoothness (RGB upsampling) in reference-based high-resolution generation, enabling faster inference and better detail preservation beyond native model resolutions. The methodology involves training an LSR module to map low-resolution latents to the high-resolution manifold and using RNA to inject Canny edge-guided noise adaptively, enhancing high-frequency details without progressive upsampling. Integrating LSRNA into DemoFusion for 16x resolution (4096x4096) reduced generation time to 34% (1507s to 506s) and improved patch-FID from 32.89 to 29.12 compared to the baseline DemoFusion. AI practitioners can leverage LSRNA to accelerate and enhance detail in high-resolution image generation pipelines built on pretrained diffusion models, offering a superior alternative to progressive latent upscaling or RGB-space upsampling methods. |
| ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.19470) or [HuggingFace](https://huggingface.co/papers/2503.19470))| Chenzheng Zhu, Yijie Zhou, Haoze Sun, Tianpeng Li, Mingyang Chen | ReSearch trains Large Language Models (LLMs) to integrate reasoning with external search using reinforcement learning, without supervised data on reasoning steps. The primary objective is to enable LLMs to handle complex multi-hop questions requiring multiple retrieval steps by treating search operations as part of the reasoning chain. The key methodology involves using Group Relative Policy Optimization (GRPO), where the LLM generates text thoughts and search queries, receives retrieval results, and is optimized based solely on rewards derived from final answer correctness and format adherence. Experiments training Qwen2.5 models showed significant improvements over baselines on multi-hop QA benchmarks, with average absolute gains ranging from 8.9% to 22.4% across benchmarks, such as a 17.56% average LLM-as-a-judge improvement for the 7B model. For AI practitioners, this demonstrates a viable approach to train more capable reasoning and multi-step Retrieval-Augmented Generation (RAG) systems using reinforcement learning from final outcomes, reducing the need for costly supervised reasoning data and enhancing model generalizability. |
| LookAhead Tuning: Safer Language Models via Partial Answer Previews (Read more on [arXiv](https://arxiv.org/abs/2503.19041) or [HuggingFace](https://huggingface.co/papers/2503.19041))| Mengshu Sun, Lin Yuan, Yujie Luo, Mengru Wang, Kangwei Liu | This paper introduces LookAhead Tuning, a data modification technique using partial answer previews to preserve large language model (LLM) safety during fine-tuning. The primary objective is to mitigate the degradation of safety alignment caused by fine-tuning, particularly on benign data, without sacrificing downstream task performance. The key methodology involves modifying training data instructions by appending either the initial tokens of the ground-truth answer (Real Answer) or a fixed prefix phrase (Virtual Answer), thereby minimizing perturbations to the model's initial token distributions. Results show LookAhead Tuning (virtual) significantly improves safety metrics (e.g., +20.76% average Jailbreak Safe Rate) compared to vanilla fine-tuning, while maintaining comparable utility (-1.59% average decrease across tasks). For AI practitioners, this presents a simple, low-resource, data-centric method to fine-tune models more safely without requiring architectural changes or significant computational overhead. |
| Frequency Dynamic Convolution for Dense Image Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.18783) or [HuggingFace](https://huggingface.co/papers/2503.18783))| Ying Fu, Chenggang Yan, Liang Li, Lin Gu, CharlesChen2023 | Frequency Dynamic Convolution (FDConv) introduces a novel approach to enhance dynamic convolution by learning frequency-diverse weights within a fixed budget in the Fourier domain. The primary objective is to overcome the limited adaptability and high parameter cost associated with the frequency homogeneity observed in traditional dynamic convolution methods. FDConv employs Fourier Disjoint Weight (FDW) to create diverse parallel weights from frequency-grouped spectral coefficients, Kernel Spatial Modulation (KSM) for fine-grained spatial filter adjustment, and Frequency Band Modulation (FBM) for spatially varying frequency response adaptation. Applied to ResNet-50 for object detection, FDConv achieves an Apbox of 39.4 on COCO with only +3.6M parameters, outperforming prior methods requiring substantially larger parameter increases (e.g., ODConv +65.1M for 39.2 Apbox). For AI practitioners, FDConv provides a parameter-efficient module to improve the adaptability and performance of vision models on dense prediction tasks by explicitly managing weight frequency diversity, integrating readily into existing ConvNet and Transformer architectures. |
| LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary
  Semantic Segmentation (Read more on [arXiv](https://arxiv.org/abs/2503.19777) or [HuggingFace](https://huggingface.co/papers/2503.19777))| Giorgos Tolias, Jiří Matas, Yannis Kalantidis, Vladan Stojnić | This paper presents LPOSS/LPOSS+, a training-free label propagation method for improving open-vocabulary semantic segmentation using Vision-Language and Vision Models. The objective is to enhance coarse initial VLM patch-level predictions and overcome patch-resolution limitations by propagating labels across patches and then pixels. The methodology involves a two-stage label propagation (LP) process: first on a patch graph using Vision Model features for affinities (LPOSS), followed by pixel-level LP initialized with patch-level results (LPOSS+), enabling joint prediction across the entire image. LPOSS+ achieves state-of-the-art performance among training-free methods, attaining an average mIoU of 42.1% across eight datasets with ViT-B/16 backbones. For AI practitioners, LPOSS+ offers a plug-and-play, training-free technique to significantly refine segmentation outputs from existing VLMs, particularly improving accuracy near object boundaries without requiring model retraining. |
| Gumbel-Softmax Flow Matching with Straight-Through Guidance for
  Controllable Biological Sequence Generation (Read more on [arXiv](https://arxiv.org/abs/2503.17361) or [HuggingFace](https://huggingface.co/papers/2503.17361))| Alexander Tong, Yinuo Zhang, Sophia Tang, pranamanam | This paper introduces Gumbel-Softmax Flow Matching and Score Matching, generative frameworks operating on the continuous simplex for biological sequence design. The primary objective is to develop a scalable and controllable method for generating discrete sequences like DNA and proteins by learning smooth interpolations from noise to data using a novel Gumbel-Softmax interpolant with time-dependent temperature. Methodologically, it derives velocity fields for flow matching and score functions for score matching based on this interpolant and introduces Straight-Through Guided Flows (STGFlow), a training-free classifier guidance technique leveraging straight-through estimators. Results demonstrate state-of-the-art performance in conditional DNA promoter design (MSE 0.029), competitive *de novo* protein generation, and effective target-binding peptide design using STGFlow guidance, outperforming existing binders in docking scores. For AI practitioners, this provides a scalable flow-matching framework for discrete data generation on the simplex, offering a modular, training-free guidance mechanism (STGFlow) to control generation towards desired properties using pre-trained classifiers. |
| Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID (Read more on [arXiv](https://arxiv.org/abs/2503.17237) or [HuggingFace](https://huggingface.co/papers/2503.17237))| wish44165 | This paper presents a strong baseline for multi-UAV tracking in thermal infrared video using YOLOv12 and BoT-SORT-ReID. The objective was to establish a straightforward yet effective tracking workflow leveraging recent advances in detection and tracking, evaluated against the Anti-UAV Challenge metrics. The methodology integrates the YOLOv12 detector with the BoT-SORT tracker (including ReID for multi-object tracking), utilizing staged training and tailored inference strategies for SOT and MOT tasks without contrast enhancement or temporal fusion. Results demonstrate competitive performance, significantly improving over official baselines, achieving a MOTA score of 0.7609 on Track 3, with increased image input resolution identified as the most significant factor contributing approximately 0.1 to score improvement. For AI practitioners, this work provides a validated high-performance baseline for thermal UAV tracking, emphasizing the effectiveness of combining state-of-the-art detection/tracking models and highlighting input resolution tuning as crucial for optimizing performance. |
| When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only
  Training For Human-Centered Decision Making (Read more on [arXiv](https://arxiv.org/abs/2503.16965) or [HuggingFace](https://huggingface.co/papers/2503.16965))| Yu Yin, Jing Li, Zhe Hu | This study demonstrates that Visual Language Models (VLMs) can enhance human-centered decision-making capabilities through text-only training, even achieving self-improvement using data from smaller counterpart LLMs. The primary objective was to improve VLM performance on complex decision-making tasks where they initially underperform compared to text-only LLMs. The methodology involved evaluating baseline models on the VIVA benchmark and then fine-tuning VLMs using synthesized text-only situational data generated by either GPT-4o or Llama-3.1 8B. Results show significant accuracy improvements post-training (e.g., Qwen2-VL improved from 80.32% to 83.15% using GPT-4o data) and notably, that training data generated by the smaller Llama 8B yielded comparable gains, demonstrating VLM self-improvement. For AI practitioners, this indicates that VLM reasoning can be effectively and efficiently enhanced for human-centric tasks via text-only data, bypassing the need for costly image-text pairs and enabling improvement using accessible LLM counterparts. |
| Towards a Unified Copernicus Foundation Model for Earth Vision (Read more on [arXiv](https://arxiv.org/abs/2503.11849) or [HuggingFace](https://huggingface.co/papers/2503.11849))| Thomas Dujardin, Adam J. Stewart, Chenying Liu, Zhitong Xiong, Yi Wang | This paper introduces a unified framework for Earth observation (EO) foundation models integrating data from all major Copernicus Sentinel missions. The objective is to develop a single model capable of processing diverse spectral/non-spectral sensor data and metadata, overcoming the limitations of sensor-specific approaches. The methodology involves creating Copernicus-Pretrain (18.7M aligned images), Copernicus-FM (a model using dynamic hypernetworks and Fourier-encoded metadata), and Copernicus-Bench (a 15-task benchmark). Copernicus-FM demonstrates superior performance, significantly improving results on Sentinel-3/5P tasks compared to prior models and supervised training, achieving an RMSE of 789.4 on AQ-O3-S5P compared to 1755.6 for DOFA [69], with metadata integration yielding substantial gains (e.g., +22.4% OA on EuroSAT-S1). For AI practitioners, this work offers a scalable architecture (Copernicus-FM) and resources (Copernicus-Pretrain, Copernicus-Bench) enabling the development of versatile foundation models for multimodal geospatial data, applicable across diverse EO tasks including atmospheric and climate studies. |
