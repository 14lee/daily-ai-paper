

## Papers for 2025-03-20

| Title | Authors | Summary |
|-------|---------|---------|
| φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation (Read more on [arXiv](https://arxiv.org/abs/2503.13288) or [HuggingFace](https://huggingface.co/papers/2503.13288))| Qika, haitengzhao, changma, Meituannnnnn, xufangzhi | Φ-Decoding is a novel inference-time optimization algorithm that balances exploration and exploitation in large language model reasoning. The main research objective is to develop an efficient inference-time strategy that achieves globally optimal step estimation without external auxiliary models. The key methodology is "foresight sampling," which leverages simulated future steps to derive two distributions (advantage and alignment) for optimal step selection, combined with in-width and in-depth pruning strategies for adaptive computation. Primary results show that Φ-Decoding improves the average reasoning performance of LLaMA3.1-Instruct-8B by over 14% across various reasoning benchmarks compared to auto-regressive CoT. For AI practitioners, Φ-Decoding offers a training-free method to improve LLM reasoning performance while balancing computational cost.  |
| DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.15265) or [HuggingFace](https://huggingface.co/papers/2503.15265))| yikaiwang, NTU-yiwen, guangce, yejunliang23, zzzrw | DeepMesh is a framework for generating artist-like 3D triangle meshes conditioned on point clouds and images using an auto-regressive transformer and reinforcement learning. The main research objective is to generate high-quality, aesthetically pleasing meshes with precise topology that align with human preferences, overcoming limitations of existing auto-regressive methods. The key methodology involves an improved mesh tokenization algorithm that reduces sequence length by 72%, a data curation strategy, and Direct Preference Optimization (DPO) with a scoring standard combining 3D metrics and human evaluation. Results show that DeepMesh outperforms state-of-the-art methods, achieving a Chamfer Distance of 0.0884 and a user preference score of 37% on a test dataset. AI practitioners can use DeepMesh’s improved tokenization and DPO implementation to efficiently generate more aesthetically refined 3D meshes, with geometric accuracy for various applications.  |
| TULIP: Towards Unified Language-Image Pretraining (Read more on [arXiv](https://arxiv.org/abs/2503.15485) or [HuggingFace](https://huggingface.co/papers/2503.15485))| XuDong Wang, Seun Eisape, Long Lian, yala, ZinengTang | TULIP is a contrastive image-text model that enhances visual feature learning while preserving language grounding. The main research objective is to improve the learning of general-purpose visual features in contrastive image-text models, addressing limitations in fine-grained visual understanding. The methodology leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization. TULIP achieved a zero-shot ImageNet-1K top-1 accuracy of 85.3%, surpassing existing models like SigLIP 2. AI practitioners can use TULIP as a drop-in replacement for existing CLIP-like models to achieve state-of-the-art performance on tasks requiring fine-grained visual understanding and improved vision-language representation.  |
| Cube: A Roblox View of 3D Intelligence (Read more on [arXiv](https://arxiv.org/abs/2503.15475) or [HuggingFace](https://huggingface.co/papers/2503.15475))| Karun Channa, Nishchaie Khanna, Kiran Bhat, Foundation AI Team, marcelvanworkum | This paper introduces a 3D shape tokenization method for building a foundation model for 3D intelligence on the Roblox platform. The main research objective is to develop a method for converting 3D shapes into discrete tokens that can be used in multi-modal autoregressive sequence models. The key methodology involves a Perceiver-based transformer with Phased-Modulated Positional Encoding, optimal-transport vector quantization, and a stochastic gradient shortcut, trained with a self-supervised loss. Primary results show that the proposed method, Ours-VQ, achieves a 91.7% surface-IoU and 94.5% volumetric-IoU on the Toys4K dataset, surpassing other existing methods such as Craftsman. The principal implication for AI practitioners is that this shape tokenization method enables the development of various 3D generative applications, including text-to-shape, shape-to-text, and text-to-scene generation, allowing for better integration of 3D shapes into large language models.  |
| Efficient Personalization of Quantized Diffusion Model without
  Backpropagation (Read more on [arXiv](https://arxiv.org/abs/2503.14868) or [HuggingFace](https://huggingface.co/papers/2503.14868))| Se Young Chun, Kyungryeol Lee, Wongi Jeong, Agorium | ZOODiP enables memory-efficient personalization of quantized diffusion models using only forward passes. The research objective is to reduce the memory demands of diffusion model personalization on edge devices without relying on backpropagation. The key methodology combines zeroth-order optimization with a quantized diffusion model, subspace gradient projection, and partial uniform timestep sampling. The primary results show that ZOODiP achieves comparable performance to prior methods in image and text alignment scores, while reducing training memory demand up to 8.2x (2.37GB VRAM consumption). AI practitioners can leverage this approach for diffusion model personalization in memory-constrained environments, enabling on-device training with significantly reduced resources.  |
| Temporal Regularization Makes Your Video Generator Stronger (Read more on [arXiv](https://arxiv.org/abs/2503.15417) or [HuggingFace](https://huggingface.co/papers/2503.15417))| Yajing Bai, Yexin Liu, Xianfeng Wu, Haojian Huang, Harold328 | FLUXFLOW enhances temporal coherence and diversity in video generation by applying controlled temporal perturbations during training. The main research question is whether temporal augmentation, specifically the proposed FLUXFLOW strategy, can improve the temporal quality of generated videos while maintaining spatial fidelity. FLUXFLOW introduces frame-level and block-level temporal perturbations to video data during the training of video generation models, without architectural changes. Experiments on UCF-101 and VBench show that FLUXFLOW applied to VideoCrafter2 improves the FVD score by 19.21 while improves Total Score to 82.36, a 1.92 improvement, enhancing both temporal coherence and diversity without reducing spatial fidelity. AI practitioners can integrate FLUXFLOW as a plug-and-play data augmentation strategy to improve the temporal quality of various video generation models.  |
| STEVE: AStep Verification Pipeline for Computer-use Agent Training (Read more on [arXiv](https://arxiv.org/abs/2503.12532) or [HuggingFace](https://huggingface.co/papers/2503.12532))| Chi-Wing Fu, Shu Liu, Ziqin Wei, Zhisheng Zhong, Fanbin Lu | STEVE is a step verification pipeline designed to train computer-use agents using a large, verified instruction set and trajectory data. The main research objective is to develop a scalable training pipeline for computer-use agents that overcomes the limitations of behavior cloning, which requires vast, high-quality trajectories. The key methodology involves establishing a large instruction set, collecting trajectory data with suboptimal agents, using GPT-4o to verify the correctness of each step based on before-and-after screen states, and then employing Kahneman & Tversky Optimization (KTO). A primary result is that the STEVE-trained 7B vision-language model achieved a 23% task success rate on the challenging WinAgentArena live environment using KTO, surpassing the performance of supervised finetuning. The principal implication for AI practitioners is that using step verification with KTO allows training of effective computer-use agents from sub-optimal trajectory data, which scales better and performs better.  |
| LEGION: Learning to Ground and Explain for Synthetic Image Detection (Read more on [arXiv](https://arxiv.org/abs/2503.15264) or [HuggingFace](https://huggingface.co/papers/2503.15264))| Weijia Li, Junyan Ye, Siwei Wen, zichenwen, khr0516 | The paper introduces SynthScars, a new dataset for synthetic image detection, and LEGION, a multimodal large language model-based framework for analyzing and refining synthetic images. The main research objective is to develop a model capable of detecting, localizing, and explaining artifacts in fully synthetic images, and to explore its use as a controller for improving image generation. The key methodology involves using a multimodal large language model (MLLM) to integrate artifact detection, segmentation, and explanation, and then applying this in iterative image regeneration and inpainting pipelines. Primary results show that LEGION outperforms existing methods on SynthScars, achieving a 3.31% higher mIoU and 7.75% higher F1 score than the second-best traditional expert, and demonstrates superior robustness. For AI practitioners, LEGION provides a new approach and benchmark for synthetic image analysis, and suggests how deep learning based image detection models can be integrated into the generative process to achieve higher quality of image synthesis.  |
| MusicInfuser: Making Video Diffusion Listen and Dance (Read more on [arXiv](https://arxiv.org/abs/2503.14505) or [HuggingFace](https://huggingface.co/papers/2503.14505))| Steven M. Seitz, Brian Curless, Ira Kemelmacher-Shlizerman, Susung Hong | MusicInfuser adapts existing text-to-video diffusion models to generate dance videos synchronized to music, while preserving text-based control over style. The main research objective is to adapt pre-trained text-to-video models to condition on music tracks and generate synchronized dance outputs. The key methodology involves introducing lightweight music-video cross-attention and a low-rank adapter within a video diffusion model, trained on dance videos, without requiring motion capture data. The method achieved a Dance Quality Average score of 7.95, outperforming baselines like Mochi (7.70) and MM-Diffusion (7.16) in comprehensive evaluations including factors like style and beat alignment. AI practitioners can adapt pre-existing video diffusion models for music-driven video generation by incorporating audio features via cross-attention and low-rank adapters, without extensive multimodal training.  |
| GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction (Read more on [arXiv](https://arxiv.org/abs/2503.11227) or [HuggingFace](https://huggingface.co/papers/2503.11227))| Jun Liu, haiping Zhu, Shihao Qi, Bifan Wei, VentureZJ | This paper introduces GKG-LLM, a unified framework for constructing generalized knowledge graphs (GKGs), encompassing knowledge graphs, event knowledge graphs, and commonsense knowledge graphs.  The main research objective is to develop a unified framework for constructing generalized knowledge graphs (GKGs) that overcomes task-specific differences and integrates knowledge from various graph types.  The key methodology is a three-stage curriculum learning fine-tuning framework that iteratively injects knowledge from knowledge graphs (KGs), event knowledge graphs (EKGs), and commonsense knowledge graphs (CKGs) into a Large Language Model (LLM), using the LoRA+ technique.  The primary result is that GKG-LLM achieved an average performance of 67.90% across all tasks, outperforming the strongest baseline by 7.49%, and specifically achieved 80.63% on the NYT sentence-level relation extraction task.  AI practitioners can leverage the GKG-LLM framework for improved and generalized knowledge graph construction across various domains, achieving state-of-the-art performance with a single, unified model.  |
| Mitigating Visual Forgetting via Take-along Visual Conditioning for
  Multi-modal Long CoT Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.13360) or [HuggingFace](https://huggingface.co/papers/2503.13360))| Han-Jia Ye, Houwen Peng, Zhun Sun, Allen8 | The paper introduces "Take-along Visual Conditioning" (TVC) to address visual forgetting in multi-modal large language models (MLLMs) during long-chain reasoning. The main research question is how to mitigate the decline in attention to visual information in MLLMs as reasoning progresses. The key methodology involves shifting image input to critical reasoning stages and compressing visual tokens via dynamic pruning, combined with Dynamic Visual Reaffirmation (DVR) and Periodic Visual Calibration (PVC). The primary result shows that the TVC approach achieves state-of-the-art performance, with a +3.4% average improvement over previous methods across five mathematical reasoning benchmarks. For AI practitioners, TVC offers a method to improve multi-modal reasoning performance in MLLMs by sustaining visual attention, applicable to tasks like geometric problem-solving.  |
| Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based
  Spatiotemporal Diffusion for Audio-driven Talking Portrait (Read more on [arXiv](https://arxiv.org/abs/2503.12963) or [HuggingFace](https://huggingface.co/papers/2503.12963))| Chenru Jiang, Yuyao Yan, weiguangzhao, KaiserYaoJM, ChaolongYang | KDTalker is a novel framework that generates audio-driven talking portrait videos using implicit keypoint-based spatiotemporal diffusion. The main research objective is to generate talking head videos with accurate lip synchronization and diverse head poses while maintaining computational efficiency. The methodology combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model and a custom-designed spatiotemporal attention mechanism. Primary results show that KDTalker achieves a LSE-C score of 7.326 and a head pose diversity of 0.760 on the HDTF dataset, outperforming existing methods. For AI practitioners, KDTalker offers a method for creating realistic talking portrait animations suitable for real-time applications with improved pose diversity and lip-sync accuracy.  |
| ELTEX: A Framework for Domain-Driven Synthetic Data Generation (Read more on [arXiv](https://arxiv.org/abs/2503.15055) or [HuggingFace](https://huggingface.co/papers/2503.15055))| Eugene Dmitriev, Julien Capitaine, Sofia Sedlova, Kseniia Murasheva, lavriz | ELTEX is a framework for generating high-quality synthetic training data in specialized domains, like blockchain-related cyberattack detection. The main research objective is to address the scarcity of domain-specific training data in specialized fields like cybersecurity, which limits the performance of Large Language Models (LLMs). ELTEX systematically integrates explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge during the generation process. Fine-tuning Gemma-2B with ELTEX-generated data, combined with real data, achieved an F1-score of 0.81, competitive with GPT-4. The principal implication is that AI practitioners can use domain-driven synthetic data generation to bridge the performance gap between smaller, more efficient models, and larger models, in specialized domains.  |
