

## Papers for 2025-03-25

| Title | Authors | Summary |
|-------|---------|---------|
| I Have Covered All the Bases Here: Interpreting Reasoning Features in
  Large Language Models via Sparse Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2503.18878) or [HuggingFace](https://huggingface.co/papers/2503.18878))| Polina Druzhinina, Andrey Galichin, tlenusik, razzant, therem | This research identifies and validates reasoning-specific features in Large Language Models (LLMs) using Sparse Autoencoders (SAEs). The main research question is how reasoning capabilities are internally encoded within LLMs, specifically the DeepSeek-R1 series. The key methodology involves training SAEs on LLM activations, proposing a "ReasonScore" metric to identify reasoning features, and using feature steering to analyze their impact. Primary results show that steering identified features increases reasoning trace length, such as feature i=46379 increasing the completion length by 29% for the AIME 2024 task. The principal implication is that AI practitioners can use SAEs and feature steering to interpret, and potentially improve, the internal reasoning processes of LLMs.  |
| Position: Interactive Generative Video as Next-Generation Game Engine (Read more on [arXiv](https://arxiv.org/abs/2503.17359) or [HuggingFace](https://huggingface.co/papers/2503.17359))| XihuiLiu, dizhang, Xintao, chehx, VictorYuki | This position paper proposes Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling AI-driven game development. The main research objective is to demonstrate how IGV can overcome current game engine limitations and serve as the core technology for next-generation game development. The key methodology involves extending video generation models with interactivity, user control, memory, physics-awareness, and causal reasoning to create a comprehensive GGE framework. A hierarchical maturity roadmap (L0-L4) is presented, outlining progressive steps from manual game development to self-evolving world ecosystems, including systems with level L2, where the engine continuously generates physics-compliant video based on users interactions. The principal implication for AI practitioners is that IGV offers a viable pathway to create games with unlimited content, realistic physics, and adaptive gameplay, reducing development barriers and expanding creative possibilities.  |
| Video-T1: Test-Time Scaling for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.18942) or [HuggingFace](https://huggingface.co/papers/2503.18942))| Hanyang Wang, duanyueqi, xhangzhan, iseesaw, Liuff23 | The paper introduces Video-T1, a framework for improving video generation quality by scaling computation at test time. The main research question is how much video generation quality can be improved by allowing a model to use more inference-time compute, given a challenging text prompt. The key methodology involves reinterpreting test-time scaling as a search problem and using test-time verifiers and heuristic algorithms, including random linear search and Tree-of-Frames (ToF), to sample better trajectories from Gaussian noise. Experiments on text-conditioned video generation benchmarks show that increasing test-time compute consistently improves video quality; for example, the CogVideoX-5B model with Test-Time Scaling (TTS) achieved a total score of 84.42, a 3.44% increase. AI practitioners can use this framework to significantly enhance the quality of generated videos without retraining, by scaling inference-time computation.  |
| Aether: Geometric-Aware Unified World Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.18945) or [HuggingFace](https://huggingface.co/papers/2503.18945))| Junyichen, lizizun, AmberHeart, ZhouTimeMachine, HaoyiZhu | AETHER is a unified world model that integrates 4D reconstruction, action-conditioned video prediction, and visual planning using synthetic data. The main research objective is to develop a framework that enables geometry-aware reasoning in world models by jointly optimizing reconstruction, prediction, and planning capabilities. The key methodology involves post-training a video diffusion model with synthetic 4D data, utilizing a robust camera pose annotation pipeline, and integrating cross-task and cross-modal conditioning signals. Primary results show AETHER achieved a zero-shot Absolute Relative error (Abs Rel) of 0.056 on the KITTI dataset for video depth estimation, surpassing prior methods. Principal implication for AI practitioners is that AETHER provides an effective framework for post-training world models with scalable synthetic data, achieving strong zero-shot transfer to real-world tasks and enabling actionable planning.  |
| SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for
  Open Base Models in the Wild (Read more on [arXiv](https://arxiv.org/abs/2503.18892) or [HuggingFace](https://huggingface.co/papers/2503.18892))| jxhe, HelicHe, SivilTaram, yuzhen17, AndrewZeng | Zero reinforcement learning (zero RL) training can significantly improve the reasoning abilities of open base language models. The paper investigates how zero RL training impacts the reasoning capabilities of diverse open base language models. The methodology involves training 10 base models (e.g., Llama3-8B, Mistral-7B, Qwen2.5 series) using the GRPO algorithm, with rule-based rewards based solely on answer correctness, on the training sets of GSM8K and MATH datasets. Results show that zero RL training consistently improves accuracy and response length, with Qwen-2.5-32B's Pass@1 on AIME 24 increasing from 10.0 to 36.7. The study provides AI practitioners with key design factors and empirical findings to enable successful zero RL training, emphasizing alignment of data difficulty with model capability and avoiding overly restrictive format rewards.  |
| OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.18033) or [HuggingFace](https://huggingface.co/papers/2503.18033))| Nir Darshan, ramiben, galchechik, m98levy, Dvir | OmnimatteZero is a training-free approach for video object removal, extraction, and layer composition using pre-trained video diffusion models. The main research objective is to adapt zero-shot image inpainting techniques for efficient and high-quality video omnimatte without requiring model training or optimization. The key methodology leverages self-attention maps from video diffusion models to identify object footprints and effects, then uses latent arithmetic for object layer isolation and blending. OmnimatteZero achieves a PSNR of 39.09 and LPIPS of 0.012 on the Movie dataset for background reconstruction, outperforming all existing methods, and runs at 0.04 seconds per frame on an A100 GPU. AI practitioners can utilize this method for real-time video editing applications like object removal and layer composition without any fine-tuning, requiring only a pre-trained video diffusion model.  |
| LEMMA: Learning from Errors for MatheMatical Advancement in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.17439) or [HuggingFace](https://huggingface.co/papers/2503.17439))| mingchenlin2025, Word2Li, QizhiPei, LHL3341, panzs | LEMMA is a framework that enhances LLMs' mathematical reasoning by learning from error-corrective trajectories. The main research objective is to improve LLMs' reflective reasoning capabilities by constructing and learning from data consisting of incorrect solutions, erroneous steps, and reflection connections to correct solutions. The key methodology involves an error-type grounded mistake augmentation method to collect diverse errors, constructing paired reflection data via "Fix & Continue" and "Fresh & Restart" mechanisms, and connecting trajectories with model-aware reflection links. Primary results show that models fine-tuned with LEMMA achieved a 62.4% average accuracy on in-distribution and out-of-distribution math datasets using LLaMA3-8B, outperforming strong baselines. Principal implication is that AI practitioners can significantly improve LLMs' mathematical reasoning abilities by systematically constructing and learning from structured error data, without reliance on complex external critique models.  |
| Equivariant Image Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.18948) or [HuggingFace](https://huggingface.co/papers/2503.18948))| Li Li, Zigang Geng, hanhu2, Mendel192, dongruixiao | The paper introduces an equivariant image modeling framework that aligns optimization targets across subtasks in image generation. The core research question is: Can a task decomposition framework be established to inherently align optimization targets across subtasks in image generation? The method uses column-wise tokenization and windowed causal attention to enhance translational symmetry and enforce consistent contextual relationships. When evaluated on class-conditioned ImageNet generation at 256x256 resolution, the proposed approach achieves a generative FID (gFID) of 5.57, comparable to state-of-the-art AR models with fewer computational resources. The principal implication is that, AI practitioners can improve model efficiency and zero-shot generalization in generative modeling by leveraging inherent equivariance properties of visual data.  |
| Training-free Diffusion Acceleration with Bottleneck Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.18940) or [HuggingFace](https://huggingface.co/papers/2503.18940))| lazybone128, Lingaaaaaaa, xiaoxuefeng, renyuxi, tyfeld | The paper introduces Bottleneck Sampling, a training-free framework to accelerate inference in diffusion models by leveraging low-resolution priors. The main research objective is to reduce the computational cost of high-resolution image and video generation in diffusion models without sacrificing output quality. The key methodology is a high-low-high denoising workflow that performs high-resolution denoising at initial and final stages and low-resolution denoising in intermediate steps, with adaptive resolution transition points and timestep shifting. Primary results show that Bottleneck Sampling accelerates inference by up to 3x for image generation and 2.5x for video generation, while maintaining comparable output quality to standard full-resolution sampling. For AI practitioners, Bottleneck Sampling provides a plug-and-play acceleration strategy for existing diffusion models that does not require retraining or architectural modifications, enhancing deployment in resource-constrained environments.  |
| Judge Anything: MLLM as a Judge Across Any Modality (Read more on [arXiv](https://arxiv.org/abs/2503.17489) or [HuggingFace](https://huggingface.co/papers/2503.17489))| shuang72, Frywind, NiuniuWang, yuhangchen, fjchendp | This paper introduces TASKANYTHING and JUDGEANYTHING benchmarks to evaluate Multimodal LLMs (MLLMs) as judges across various modalities for multimodal understanding and generation tasks.  The main research objective is to evaluate whether MLLMs can serve as a unified judge for assessing the understanding and generation ability of any-to-any modality tasks.  The key methodology involves constructing two benchmarks: TASKANYTHING, with 1,500 open-ended queries across 15 any-to-any modality categories, and JUDGEANYTHING, evaluating MLLMs' judging abilities using Pair Comparison and Score Evaluation settings against human annotations.  The primary results show that MLLMs align more closely with human preferences on Pair Comparison than Score Evaluation, with Gemini-1.5-Pro achieving an average of 70.6% accuracy on Pair Comparison for Multimodal Understanding tasks.  Principal implication for AI practitioners: Current MLLM-as-a-Judge systems show promise but face limitations, especially in Multimodal Generation tasks, highlighting the need for refined evaluation protocols and improved alignment with human preferences in model development.  |
| FFN Fusion: Rethinking Sequential Computation in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.18908) or [HuggingFace](https://huggingface.co/papers/2503.18908))| geifmany, AmnonGeifman, omripuny, mdabbah-nvidia, abercovich | FFN Fusion is a novel architectural optimization that reduces sequential computation in large language models by parallelizing Feed-Forward Network (FFN) layers. The main research objective is to investigate whether sequences of FFN layers in transformers can be parallelized to reduce inference latency while preserving model accuracy. The key methodology involves identifying and fusing consecutive FFN layers into wider, parallel layers, supported by a block-wise dependency analysis and a distillation-based refinement. The primary result is that Ultra-253B-Base, created using FFN Fusion, achieves a 1.71x speedup in inference latency and 35x reduction of the per-token cost compared to its parent Llama-3.1-405B model, while maintaining or exceeding its performance. AI practitioners can apply FFN Fusion to significantly improve the inference efficiency of large language models, particularly in resource-constrained deployment scenarios.  |
| CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models (Read more on [arXiv](https://arxiv.org/abs/2503.18886) or [HuggingFace](https://huggingface.co/papers/2503.18886))| Ziwei Liu, Raymond A. Yeh, Amber Yijia Zheng, weepiess2383 | CFG-Zero* enhances classifier-free guidance for flow matching models by addressing inaccuracies in early-stage velocity estimation. The main research objective is to improve the sample quality and controllability of flow matching models during generation when the learned velocity is underfitted. The key methodology involves introducing an optimized scale to correct for velocity inaccuracies and a "zero-init" technique that zeros out the first few steps of the ODE solver. Primary results show that CFG-Zero* achieves the best FID Score of 2.10 and sFID Score of 4.59 on ImageNet-256, outperforming existing methods. Principal implication for AI practitioners is that CFG-Zero* can be readily integrated into flow matching models to improve image fidelity and text alignment, particularly during the early stages of training or when models are underfitted.  |
| Video SimpleQA: Towards Factuality Evaluation in Large Video Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.18923) or [HuggingFace](https://huggingface.co/papers/2503.18923))| Pengfei Hu, zhangysk, Drexubery, grejioh, mengcao | Video SimpleQA, a new benchmark, evaluates the factual accuracy of large video language models (LVLMs). The main research objective is to develop and introduce a comprehensive benchmark for assessing the factuality of LVLMs in video contexts. The key methodology involves creating a dataset of 2030 question-answer pairs derived from 1293 videos, with questions requiring external knowledge, designed to be fact-seeking, and having definitive, short-form, and externally verified answers. Primary results indicate that the best-performing model, Gemini-1.5-Pro, achieves an F-score of only 54.4%, and open-source models perform notably worse. The principal implication for AI practitioners is the need to address significant deficiencies in factual adherence of current LVLMs, highlighting a critical area for improvement in developing models that can accurately and reliably process video information.  |
| AgentRxiv: Towards Collaborative Autonomous Research (Read more on [arXiv](https://arxiv.org/abs/2503.18102) or [HuggingFace](https://huggingface.co/papers/2503.18102))| Samuel Schmidgall, mdmoor | Here's a summary of the paper "AgentRxiv: Towards Collaborative Autonomous Research" by Schmidgall and Moor, following the provided guidelines:  1.  AgentRxiv is a framework enabling LLM agent laboratories to collaboratively conduct research by sharing and building upon findings via a centralized preprint server.  2.  **Main research question/objective:** To determine if autonomous LLM agents can collaboratively improve research performance by sharing and building upon each other's work.  3.  **Key methodology:** Agent laboratories developed reasoning/prompting techniques, uploading/retrieving reports on a shared server, with performance evaluated on benchmarks like MATH-500.  4.  **Primary results:** Agents with access to prior research achieved higher performance improvements (11.4% relative improvement on MATH-500) compared to isolated agents. Multiple labs using the System were able to reach a best performance of 79.8%  5.  **Principal implication for AI practitioners:** AgentRxiv demonstrates a viable path for accelerating AI research through agent collaboration, potentially leading to faster discovery and improved generalization of techniques.  |
| MagicComp: Training-free Dual-Phase Refinement for Compositional Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.14428) or [HuggingFace](https://huggingface.co/papers/2503.14428))| Hongyu Zhang, ClownRat, Pengjin, BestWishYsh, dyf | MagicComp is a training-free framework that improves compositional text-to-video generation through dual-phase refinement during conditioning and denoising. The main research objective is to address challenges in compositional video generation, such as attribute binding, spatial relationships, and interactions between multiple subjects, without additional training. The key methodology involves Semantic Anchor Disambiguation (SAD) to resolve inter-subject ambiguity during conditioning, and Dynamic Layout Fusion Attention (DLFA) for spatial-attribute binding during denoising. Results on T2V-CompBench show that MagicComp achieves a Consist-attr score of 0.7665, outperforming the baseline CogVideoX-2B's score of 0.6775. The principal implication for AI practioners is that MagicComp can be integrated into existing text-to-video architectures to enhance compositional video generation quality without requiring additional training or significant increases in inference time.  |
| Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models
  via Vision-Guided Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.18013) or [HuggingFace](https://huggingface.co/papers/2503.18013))| Fan Yang, Hongyin Zhao, Shurong Zheng, Yousong Zhu, Yufei Zhan | Vision-R1 is a vision-guided reinforcement learning algorithm that improves object localization in Large Vision-Language Models (LVLMs) using only curated instruction data. The main research objective is to enhance LVLM capabilities in object localization tasks without relying on human-annotated preference data or specialized reward models. The key methodology involves a criterion-driven reward function based on visual feedback and a progressive rule refinement strategy that dynamically adjusts reward criteria during training. Results show that fine-tuning a 7B LVLM with Vision-R1 achieved up to 50% improvement, and specifically, increased the Average Precision (mAP) on the ODINW-13 benchmark by 9.0 points compared to supervised fine tuning for Qwen2.5-VL-7B. AI practitioners can utilize Vision-R1 to improve object localization performance in LVLMs without the need for costly human-annotated preference data, leading to substantial gains in model accuracy.  |
| Reasoning to Learn from Latent Thoughts (Read more on [arXiv](https://arxiv.org/abs/2503.18866) or [HuggingFace](https://huggingface.co/papers/2503.18866))| Tatsunori Hashimoto, cmaddis, nband, ryoungj | This paper introduces "reasoning to learn," an approach for improving language model (LM) pretraining data efficiency by explicitly modeling and inferring the latent human thoughts underlying text generation. The main research objective is to investigate whether augmenting observed text data with inferred latent thoughts can improve data efficiency in LM pretraining, particularly in a data-constrained regime. The key methodology involves training LMs to jointly model the distribution of observed text and synthesized latent thoughts, using an EM algorithm (BoLT) to iteratively improve latent thought quality and LM capability. Primary results show that a 1.1B LM pretrained with GPT-4o-mini synthesized latent thoughts achieves 25.4% accuracy on MATH, significantly outperforming the 5.74% accuracy achieved by training on raw data alone. For AI practitioners, this implies that incorporating synthesized latent thoughts during pretraining can lead to substantial data efficiency improvements, enabling the development of more capable models with limited data.  |
| Defeating Prompt Injections by Design (Read more on [arXiv](https://arxiv.org/abs/2503.18813) or [HuggingFace](https://huggingface.co/papers/2503.18813))| Tianqi Fan, ftramer, carlini, iliashum, dedeswim | CaMeL is a system designed to protect Large Language Model (LLM) agents from prompt injection attacks by enforcing explicit security policies. The main research question is how to design a robust defense that prevents prompt injection attacks in LLM agents interacting with untrusted data, without modifying the underlying model. The key methodology involves extracting control and data flows from user queries, representing them as pseudo-Python code, and enforcing security policies via a custom Python interpreter that tracks provenance and capabilities. The primary results demonstrate that CaMeL solves 67% of tasks with provable security in the AgentDojo benchmark, with some utility degradation on specific task suites, and eliminates almost all prompt injection attacks when combined with capabilities and policy. The principal implication for AI practitioners is that using capability-based security, explicit isolation, and a custom interpreter to manage data and control flows can significantly enhance the security of LLM agent systems against prompt injections, without relying solely on inherent model robustness.  |
| Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid
  Question Answering (Read more on [arXiv](https://arxiv.org/abs/2503.15879) or [HuggingFace](https://huggingface.co/papers/2503.15879))| Yunho Maeng, Hyeonseo Nam, Ahjeong Park, keirahrlee, oneonlee | Typed-RAG is a framework for non-factoid question answering that improves response quality by classifying questions and decomposing multi-aspect queries. The main research objective is to address the limitations of existing retrieval-augmented generation (RAG) systems in handling the complexity and diversity of non-factoid questions (NFQs). The key methodology is Typed-RAG, a type-aware, multi-aspect decomposition approach that integrates question type classification and aspect-based decomposition into the RAG pipeline. Experimental results on the Wiki-NFQA dataset show that Typed-RAG outperforms baselines, achieving a Mean Reciprocal Rank (MRR) of 0.8413 with a GPT-40 mini scorer and Mistral-7B base model configuration. Principal implication is that AI practitioners can create NFQA models by leveraging type-aware and multi-aspect decomposition strategies to create a more comprehensive RAG system.  |
| AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and
  Symbolic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.18769) or [HuggingFace](https://huggingface.co/papers/2503.18769))| Bui Quang Huy, Dinh Bach Vu, alandao | AlphaSpace enhances spatial reasoning in language models for 3D robotic manipulation using semantic tokenization and symbolic reasoning. The main objective is to improve the ability of language models to perform precise object manipulation in 3D Cartesian space without relying on vision-based embeddings. The key methodology involves a hierarchical semantics-based tokenization strategy that encodes spatial information (including height) and object attributes, combined with synthetic reasoning data for training. AlphaSpace achieves a total accuracy of 66.67% on the EmbodiedBench Manipulation Subtask, significantly outperforming GPT-4o (37.5%) and Claude 3.5 Sonnet (29.17%). AI practitioners can leverage this approach to develop more efficient and accurate robotic control systems that rely less on computationally expensive visual processing and more on structured spatial representations.  |
| AMD-Hummingbird: Towards an Efficient Text-to-Video Model (Read more on [arXiv](https://arxiv.org/abs/2503.18559) or [HuggingFace](https://huggingface.co/papers/2503.18559))| Dong Zhou, He Cui, Takashi Isobe, ebarsoum, gemengmeng | AMD-Hummingbird is a lightweight text-to-video (T2V) generation framework that balances computational efficiency with high visual quality. The main research objective is to develop a T2V model suitable for resource-constrained devices by addressing the trade-off between model size and visual fidelity. The key methodology involves a two-stage diffusion model distillation pipeline: first pruning the U-Net architecture and then enhancing visual quality via visual feedback learning, combined with a data processing pipeline using LLMs and VQA models. The primary result is that Hummingbird achieves a 31x speedup compared to VideoCrafter2 and reduces U-Net parameters from 1.4 billion to 0.7 billion, while attaining the highest overall VBench score. For AI practitioners, this provides a practical and efficient solution for T2V generation, combining performance, scalability, and flexibility, especially beneficial for deployment on devices with limited computational resources.  |
| Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural
  Contexts? (Read more on [arXiv](https://arxiv.org/abs/2503.18018) or [HuggingFace](https://huggingface.co/papers/2503.18018))| Bhoomika Lohana, jaswindersingh2, 55mv, Abdul084, abedk | Large Language Models (LLMs) demonstrate reduced mathematical reasoning performance when presented with culturally adapted math word problems, despite the underlying mathematical structure remaining constant. The research investigates whether LLMs' mathematical reasoning abilities persist across different cultural contexts. Six culturally adapted datasets were synthesized from the GSM8K benchmark by modifying cultural elements (names, foods, places) while preserving mathematical logic. Fourteen LLMs were evaluated, revealing that models performed worse on culturally adapted problems compared to the original GSM8K, with Meta LLaMA 3.1-8B showing the largest accuracy drop (5.9%) on the Somalia dataset. AI practitioners should prioritize diverse and representative training data to improve LLMs' robustness in real-world applications across various cultural contexts.  |
| Variance Control via Weight Rescaling in LLM Pre-training (Read more on [arXiv](https://arxiv.org/abs/2503.17500) or [HuggingFace](https://huggingface.co/papers/2503.17500))| gueraf, nilabhra, akanyaani, louisowen6 | This paper introduces weight initialization and variance control techniques to improve LLM pre-training. The main research objective is to investigate how controlling weight variance, both at initialization and during training, impacts LLM stability and downstream task performance. The key methodology involves proposing Layer Index Rescaling (LIR) for weight initialization and Target Variance Rescaling (TVR) for variance control during training, and evaluating these on a 1B parameter LLaMA model using various benchmarks. Primary results show that the combined use of LIR and TVR improves downstream task performance, with up to a 4.6% increase on common pre-training benchmarks, while also reducing extreme activation values. Principal implication for AI practioners is that managing weight variance using LIR and TVR during LLM pre-training can lead to improved model performance and stability, while mitgating some issues as massive activations.  |
| V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V
  Platforms (Read more on [arXiv](https://arxiv.org/abs/2503.17422) or [HuggingFace](https://huggingface.co/papers/2503.17422))| Luca Benini, Daniele Jahier Pagliari, Alessio Burrello, Mohamed Amine Ahmdi, Javier J. Poveda Rodrigo | This paper optimizes LLM inference on a many-core RISC-V CPU, achieving significant speedups compared to baseline implementations. The main research objective is to optimize the performance of LLM inference on the Sophon SG2042 RISC-V platform. Key methodologies include developing optimized quantized kernels, choosing a suitable compilation toolchain (Xuantie GCC 10.4 for kernels, Clang 19 for the framework), and optimizing model mapping with NUMA policies. On a DeepSeek R1 Distill Llama 8B model, the authors achieved 4.32 tokens/s for token generation and 6.54 tokens/s for prompt processing, representing speedups of up to 2.9x/3.0x over the baseline. The principal implication is to use, on RISC-V architecture, Clang 19 compiler, to disable NUMA Balancing and activate Memory Interleaving, to improve LLM inference performance.  |
| MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse (Read more on [arXiv](https://arxiv.org/abs/2503.18470) or [HuggingFace](https://huggingface.co/papers/2503.18470))| Han Liu, zhenyupan | MetaSpatial is an RL-based framework that enhances 3D spatial reasoning in vision-language models (VLMs) for 3D scene generation. The main research objective is to address the lack of internalized 3D spatial reasoning in VLMs and the limitations of supervised fine-tuning for 3D layout generation. The key methodology is a multi-turn reinforcement learning (RL) optimization that uses format detection, physical detection, and rendering-based evaluation to provide reward signals, optimized via Group Relative Policy Optimization (GRPO). Results show that on a Qwen-VL 7B model, MetaSpatial improves format correctness from 0.85 to 0.98 and reduces the object collision rate by 24.5%. For AI practitioners, this provides a method to train VLMs to generate coherent, physically plausible 3D scenes without needing extensive "perfect" layout annotations or manual post-processing.  |
| Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.18352) or [HuggingFace](https://huggingface.co/papers/2503.18352))| Junjie Liu, Jinjin Zhang, dihuang, xiefan-guo, qiuyuhuang | Diffusion-4K introduces a framework for direct ultra-high-resolution (4K) image synthesis using latent diffusion models. The main research objective is to enable direct training and generation of 4K images with diffusion models, addressing the lack of a 4K image synthesis benchmark. The key methodology involves a wavelet-based fine-tuning approach for latent diffusion models and the creation of a new benchmark, Aesthetic-4K, including a curated 4K dataset with GPT-40-generated captions. Results show that Diffusion-4K, particularly when powered by models like SD3-2B and Flux-12B, achieves a FID score of 39.49 and the model performs well and improves GLCM Score up to 0.79 on the Aesthetic-Eval@2048 benchmark, outperforming the previous scores. AI practitioners can use Diffusion-4K and the Aesthetic-4K benchmark for training and evaluating models capable of generating high-quality, ultra-high-resolution images with detailed textures and improved text prompt adherence.  |
| RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame
  Animated Sticker Generation (Read more on [arXiv](https://arxiv.org/abs/2503.17735) or [HuggingFace](https://huggingface.co/papers/2503.17735))| Yeshuang Zhu, Jiapei Zhang, Ying Deng, Ting Zhang, Zhiqiang Yuan | i) This paper introduces RDTF, a resource-efficient training framework for generating multi-frame animated stickers using a dual-mask approach and curriculum learning. ii) The main research objective is to demonstrate that training a smaller video generation model from scratch with limited data can outperform parameter-efficient tuning of larger models under resource constraints. iii) Key methodologies include a discrete frame generation network with a spatial-temporal interaction layer, a dual-mask data utilization strategy (condition mask and loss mask), and a difficulty-adaptive curriculum learning method. iv) On the I&T->V task, RDTF achieved an FVD of 442.18 and a VQA of 0.502, outperforming methods like I2V-Adapter and SimDA. v) For AI practitioners, RDTF shows that effective data utilization and curriculum strategies can enable smaller models trained from scratch to achieve superior performance in resource-constrained settings, suggesting an alternative to fine-tuning large pre-trained models.  |
| Optimized Minimal 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2503.16924) or [HuggingFace](https://huggingface.co/papers/2503.16924))| Jong Hwan Ko, epark, maincold2 | Optimized Minimal 3D Gaussian Splatting (OMG) significantly reduces the storage and computational costs of 3D Gaussian Splatting while maintaining rendering quality. The main objective is to minimize the number of Gaussian primitives and storage requirements for 3D Gaussian Splatting (3DGS) without significantly degrading rendering quality. The key methodology involves using a compact attribute representation with sub-vector quantization, integrating per-Gaussian features with a lightweight neural field, and introducing a local distinctiveness metric for Gaussian pruning. The primary result is that OMG achieves nearly a 50% storage reduction compared to the previous state-of-the-art on the Mip-NeRF 360 dataset, requiring only 4.06 MB while preserving comparable rendering quality. The principal implication for AI practitioners is that they can utilize OMG for real-time, high-fidelity rendering on resource-constrained devices and accelerate training through reduced Gaussians and optimized attribute representation.  |
| Verbal Process Supervision Elicits Better Coding Agents (Read more on [arXiv](https://arxiv.org/abs/2503.18494) or [HuggingFace](https://huggingface.co/papers/2503.18494))| Jui-Ming Yao, Cheng-Pong Huang, MarkChenX | CURA, a novel code reasoning agent with verbal process supervision (VPS), enhances code generation performance. The main research objective is to examine if iterative verbal process supervision, combined with an agentic reasoning pipeline like Code Understanding and Reasoning Agent (CURA), improves code generation over baseline models. The key methodology involves a process-supervised reasoning framework called CURA, using VPS to generate verbal reward signals at each reasoning step, incorporating iterative feedback within a code-testing sandbox. The primary result is that CURA with VPS achieved a 3.65% improvement over baseline models on BigCodeBench. For AI practitioners, integrating agentic reasoning with iterative, step-level verbal process supervision offers a new, effective approach for enhancing code generation and software engineering tasks, with a direct, measurable performance improvement.  |
