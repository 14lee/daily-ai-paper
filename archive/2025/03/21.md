

## Papers for 2025-03-21

| Title | Authors | Summary |
|-------|---------|---------|
| One-Step Residual Shifting Diffusion for Image Super-Resolution via
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2503.13358) or [HuggingFace](https://huggingface.co/papers/2503.13358))| agoxandr, skushneryuk, ngushchin, kekchpek, apryc1 | This paper introduces RSD, a distillation method for accelerating diffusion-based super-resolution models, achieving single-step image restoration. The main research objective is to develop a computationally efficient distillation method for ResShift that maintains high perceptual quality while significantly reducing inference time. The key methodology is based on training a student network to produce images such that a fake ResShift model trained on them coincides with the teacher model, incorporating multistep training and additional supervised losses. Primary results show that RSD outperforms the teacher ResShift model and SinSR on RealSR with a MUSIQ score of 69.172 compared to the teacher's 61.330. Principal implication for AI practitioners is that RSD offers a way to deploy diffusion-based super-resolution models in real-time applications on consumer devices by providing faster inference and lower computational requirements.  |
| Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.16419) or [HuggingFace](https://huggingface.co/papers/2503.16419))| andrewwen, HongyiLiuAI, jy-yuan, JiamuZhang, yangsui | This survey systematically investigates and explores the current progress toward achieving efficient reasoning in Large Language Models (LLMs), particularly addressing the "overthinking phenomenon". The main research question is how to optimize reasoning length in LLMs while preserving or even enhancing their reasoning capabilities. Key methodologies used include model-based (RL with length reward, SFT with varied-length CoT data), reasoning output-based (latent representation compression, dynamic reasoning), and input prompt-based (prompt-guided, attribute-driven routing) approaches. Primary results across multiple works demonstrate the feasibility of significantly shortening LLM reasoning paths, with one example, O1-Pruner, showing the effectiveness of the Length-Harmonizing reward for shortening CoT length. Principal implication for AI practitioners is that efficient reasoning strategies can substantially reduce computational costs and improve the responsiveness of LLM-based applications without significantly compromising, and sometimes improving accuracy.  |
| Unleashing Vecset Diffusion Model for Fast Shape Generation (Read more on [arXiv](https://arxiv.org/abs/2503.16302) or [HuggingFace](https://huggingface.co/papers/2503.16302))| Huiwenshi, wangfuyun, cocacola, qikahh, ZeqiangLai | FlashVDM is a framework for accelerating 3D shape generation using Vecset Diffusion Models (VDMs) by optimizing both diffusion sampling and VAE decoding. The main research objective is to address the slow inference speed of VDMs in generating high-resolution 3D shapes. The key methodology involves Progressive Flow Distillation for diffusion sampling, and a lightning vecset decoder with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design for VAE acceleration. Primary results show a 45x speedup in VAE decoding (from 22.33s to 0.491s) and an overall 32x speedup in shape generation, achieving comparable quality to state-of-the-art with significantly reduced inference time. AI practitioners can leverage FlashVDM to enable significantly faster 3D shape generation with VDMs, opening possibilities for real-time interactive applications.  |
| Survey on Evaluation of LLM-based Agents (Read more on [arXiv](https://arxiv.org/abs/2503.16416) or [HuggingFace](https://huggingface.co/papers/2503.16416))| Yilun Zhao, Guy Uziel, Lilach Eden, lihaoxin2020, Asaf-Yehudai | This paper provides a comprehensive survey of evaluation methodologies for LLM-based agents across capabilities, applications, and frameworks. The main research objective is to systematically analyze existing benchmarks and frameworks for evaluating LLM-based agents across four critical dimensions: fundamental agent capabilities, application-specific benchmarks, generalist agent benchmarks, and agent evaluation frameworks. The key methodology involves a systematic review and categorization of existing literature, benchmarks, and evaluation methods for LLM-based agents, highlighting emerging trends and research gaps. Primary results include the identification of trends toward more realistic and challenging evaluations (e.g., some top-performing models scoring as low as 2% on complex benchmarks), the continuous updating of "live benchmarks," and a lack of standardized metrics for cost-efficiency, safety, and granular performance evaluation. A principal implication for AI practitioners is the need to adopt and develop more granular, dynamic, and safety-focused evaluation frameworks to ensure robust and responsible development of LLM-based agents, shifting beyond coarse-grained metrics to include fine-grained trajectory analysis and security aspects.  |
| DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2503.14487) or [HuggingFace](https://huggingface.co/papers/2503.14487))| Mingwu Zheng, Xintao Wang, Haotian Yang, Ziyang Yuan, MingleiShi | DiffMoE introduces a Mixture-of-Experts (MoE) architecture for diffusion transformers that enables dynamic token selection and global token accessibility. The main research objective is to address the limitations of existing MoE approaches in diffusion models, specifically their restricted token accessibility and fixed computational patterns. The key methodology incorporates a batch-level global token pool during training and a capacity predictor for dynamic resource allocation during inference. DiffMoE achieves a state-of-the-art FID score of 2.13 on ImageNet 256x256 class-conditional generation with classifier-free guidance (cfg=1.5), surpassing dense models with 1.5x the number of activated parameters. The principle implication is that AI practitioners can leverage DiffMoE to scale diffusion models more efficiently, achieving superior performance while maintaining computational efficiency compared to dense models and previous MoE implementations.  |
| Scale-wise Distillation of Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.16397) or [HuggingFace](https://huggingface.co/papers/2503.16397))| Dmitry Baranchuk, Artem Babenko, Denis Kuznedelev, Nikita Starodubcev | Scale-wise Distillation (SWD) is a novel method that improves diffusion model efficiency by progressively increasing spatial resolution during sampling. The paper's main objective is to investigate whether generating images scale-by-scale across the diffusion process can improve the efficiency of diffusion distillation methods. The key methodology involves integrating a scale-wise generation approach into existing diffusion distillation frameworks, specifically DMD2, and introducing a patch distribution matching (PDM) loss. A primary result is that, within SD3.5 medium, the 6-step scale-wise configuration achieves a FID score of 23.0 on COCO 2014, while its full-scale 6-step counterpart reaches 20.4. AI practitioners can leverage SWD to achieve a balance between generation speed and quality in diffusion models, offering a practical technique to accelerate inference by operating at lower resolutions during initial sampling steps.  |
| Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.15558) or [HuggingFace](https://huggingface.co/papers/2503.15558))| Hannah Brandon, Alisson Azzolini, NVIDIA, zhuoliny, fferroni | Cosmos-Reason1 is a family of multimodal large language models developed by NVIDIA, trained to integrate physical common sense and embodied reasoning. The main research objective is to develop models capable of understanding the physical world and generating appropriate embodied decisions using natural language through long chain-of-thought reasoning. The key methodology involves defining ontologies for physical common sense and embodied reasoning, curating datasets based on these ontologies, and training models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL). Evaluation results show that the Cosmos-Reason1-56B model achieves 60.2% accuracy on the physical common sense benchmark, and Physical AI RL improves performance across most benchmark components. For AI practitioners, using Physical AI SFT and RL, this work will make the code and models open-source to expedite the progress of building Physical AI systems that understand and perform complex tasks.  |
| MathFusion: Enhancing Mathematic Problem-solving of LLM through
  Instruction Fusion (Read more on [arXiv](https://arxiv.org/abs/2503.16212) or [HuggingFace](https://huggingface.co/papers/2503.16212))| Honglin Lin, Yu Li, Zhuoshi Pan, Lijun Wu, Qizhi Pei | MathFusion enhances LLM mathematical problem-solving by synthesizing new training instructions from existing problem pairs. The main research objective is to improve LLMs' mathematical reasoning capabilities through cross-problem instruction synthesis, overcoming limitations of instance-level data augmentation. The key methodology, MathFusion, employs three fusion strategies—sequential, parallel, and conditional—to combine existing mathematical problems into new, more complex ones. Experiments using DeepSeekMath-7B, Mistral-7B, and Llama3-8B show that MathFusion increases accuracy by 18.0 points on average across diverse benchmarks with only 45K additional synthetic instructions. The principal implication is that AI practitioners can improve mathematical reasoning performance in LLMs efficiently using this data synthesis technique.  |
| InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity (Read more on [arXiv](https://arxiv.org/abs/2503.16418) or [HuggingFace](https://huggingface.co/papers/2503.16418))| Hao Kang, Zichuan Liu, Yumin Jia, Qing Yan, Liming Jiang | InfiniteYou (InfU) is a Diffusion Transformer (DiT)-based framework for identity-preserved image generation that recrafts photos using text descriptions while maintaining facial identity. The main research objective is to address limitations of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality when using DiTs. The key methodology involves InfuseNet, a generalization of ControlNet, which injects identity features into the DiT base model via residual connections, combined with a multi-stage training strategy using synthetic single-person-multiple-sample (SPMS) data. Primary results showed that InfU achieved a lower ID Loss (0.209) compared to PuLID-FLUX (0.225) and FLUX.1-dev IPA (0.772), while also achieves the highest CLIPScore and PickScore. A principal implication for AI practitioners is that they can utilize InfU's plug-and-play design, as well as the method of residual feature connections demonstrated, to create high-fidelity and text-aligned identity-preserved images, and extend use cases beyond those presented in the paper.  |
| Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.16257) or [HuggingFace](https://huggingface.co/papers/2503.16257))| Huan Wang, Can Qin, Yang Sui, Haoxuan You, KD-TAO | VidKV, a plug-and-play KV cache quantization method, compresses the KV cache in Video Large Language Models (VideoLLMs) to 1.x-bit precision with minimal performance loss. The main research question is how to effectively quantize the KV cache in VideoLLMs to lower than 2 bits while preserving model performance. The key methodology involves mixed-precision quantization for the key cache (2-bit for anomalous channels, 1-bit with FFT for normal channels) and 1.58-bit quantization with optional token protection for the value cache, applied per-channel. Primary results show that VidKV compresses the KV cache to 1.5-bit and 1.58-bit precision on LLaVA-OV-7B and Qwen2.5-VL-7B, achieving a VideoChat-GPT average score of 3.06 and 3.00 respectively, which is a close to no loss to the FP16 counterparts. The principal implication for AI practitioners is that they can significantly reduce the memory footprint and computational cost of VideoLLM inference using VidKV, enabling efficient deployment of these models.  |
| JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play
  Visual Games with Keyboards and Mouse (Read more on [arXiv](https://arxiv.org/abs/2503.16365) or [HuggingFace](https://huggingface.co/papers/2503.16365))| Yitao Liang, Xiaojian Ma, Kaichen He, Zihao Wang, Muyao Li | JARVIS-VLA introduces a new training paradigm, ActVLP, that enhances vision-language-action (VLA) models for decision-making in open-world environments like Minecraft. The main research objective is to investigate whether integrating visual-language tasks into the post-training phase of VLA models improves their performance. The key methodology, ActVLP, involves a three-stage training pipeline: post-training language models on text-only world knowledge, post-training both vision encoder and language models on multimodal vision-language alignment and spatial grounding datasets, then post-training language models on multimodal instruction following datasets. The primary result is that post-training on non-trajectory tasks leads to a 40% improvement over the best agent baseline in Minecraft on a diverse set of atomic tasks. For AI practitioners, this demonstrates that incorporating visual-language post-training significantly improves VLA model performance in complex decision-making tasks, offering a new, effective training approach.  |
| CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners (Read more on [arXiv](https://arxiv.org/abs/2503.16356) or [HuggingFace](https://huggingface.co/papers/2503.16356))| Shumin Deng, Jia-Chen Gu, Jizhan Fang, Yunzhi Yao, Ningyu | CaKE improves the generalization of knowledge editing in large language models by aligning edits with the models' reasoning circuits. The main research objective is to address the poor performance of existing knowledge editing (KE) methods on downstream reasoning tasks involving updated knowledge. The key methodology, CaKE, involves generating circuit-aware training data that explicitly requires reasoning with updated knowledge and training the model to construct robust reasoning circuits integrating the new information. Experimental results show CaKE improves multi-hop reasoning accuracy on the MQUAKE dataset by an average of 20% compared to existing KE methods. AI practitioners can use CaKE to create language models that not only store updated facts but also effectively apply this knowledge in downstream reasoning tasks, improving generalizability.  |
| Ultra-Resolution Adaptation with Ease (Read more on [arXiv](https://arxiv.org/abs/2503.16322) or [HuggingFace](https://huggingface.co/papers/2503.16322))| Xinchao Wang, Zhenxiong Tan, Songhua Liu, Ruonan Yu | URAE facilitates adapting text-to-image diffusion models to ultra-high resolutions with limited data and computation. The main research objective is to identify efficient guidelines for adapting existing text-to-image models to ultra-high resolutions (2K and 4K) when training data and computational resources are limited. The key methodology involves theoretically and empirically investigating data efficiency (using synthetic data from teacher models) and parameter efficiency (tuning minor components of weight matrices), alongside examining the impact of classifier-free guidance. Primary results include that URAE achieves comparable 2K generation performance to FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K resolution generation. The principal implication for AI practitioners is that they can adapt diffusion models to ultra-high resolutions efficiently by using synthetic data when available, tuning minor weight matrix components, and disabling classifier-free guidance during adaptation.  |
| Expert Race: A Flexible Routing Strategy for Scaling Diffusion
  Transformer with Mixture of Experts (Read more on [arXiv](https://arxiv.org/abs/2503.16057) or [HuggingFace](https://huggingface.co/papers/2503.16057))| Xun Zhou, Defa Zhu, Ziyu Wang, FetchFortune, yyk-wew | Race-DiT introduces a flexible routing strategy for scaling diffusion transformers with Mixture of Experts (MoE). The main research objective is to enhance the scalability and performance of diffusion transformers by integrating MoE methods with a new routing strategy called Expert Race. The key methodology involves allowing tokens and experts to compete and selecting the top candidates, along with per-layer regularization and router similarity loss. The primary result is that Race-DiT achieves a 7.2x speedup in iterations when reaching the same training loss compared to DiT-XL, with an equal number of activated parameters. Principal implication for AI practioners is that it provides a method to improve performance gains and scaling in diffusion models while maintaining good expert utilization, with superior ImageNet validation and image quality.  |
| MagicMotion: Controllable Video Generation with Dense-to-Sparse
  Trajectory Guidance (Read more on [arXiv](https://arxiv.org/abs/2503.16421) or [HuggingFace](https://huggingface.co/papers/2503.16421))| Qi Dai, Hui Zhang, Rui Wang, Zhen Xing, quanhaol | MagicMotion is a novel image-to-video generation framework that enables trajectory control through three levels of conditions (masks, bounding boxes, and sparse boxes). The main objective is to develop a trajectory-controllable video generation model that overcomes limitations of existing methods, such as imprecise trajectory adherence and compromised visual quality, and supports multiple trajectory control formats. The key methodology involves a progressive training strategy using a Trajectory ControlNet architecture (similar to ControlNet) to inject trajectory conditions into a diffusion model, alongside a novel latent segment loss. The primary results demonstrate that MagicMotion outperforms previous methods on the MagicBench benchmark, achieving a Mask\_IoU of 91.57% and a Box\_IoU of 87.75% in Stage 1, and Mask\_IoU=76.61 and Box\_IoU=81.45 in Stage 2. AI practitioners can use MagicMotion for improved controllable video generation, allowing more precise control over object motion and facilitating the creation of high-quality videos with user-specified trajectories.  |
| M3: 3D-Spatial MultiModal Memory (Read more on [arXiv](https://arxiv.org/abs/2503.16413) or [HuggingFace](https://huggingface.co/papers/2503.16413))| Jianglong Ye, Xuanbin Peng, Ri-Zhao Qiu, Yuchen Song, Xueyan Zou | M3 is a multimodal memory system that integrates 3D Gaussian Splatting with foundation models to store and render multimodal representations of medium-sized static scenes. The main research objective is to develop a spatial memory system that efficiently stores and retrieves multi-granularity information about static scenes from video sources, addressing computational constraints and information loss in existing feature splatting methods. The key methodology involves storing high-dimensional feature maps from foundation models in a memory bank (principal scene components) and using low-dimensional queries from 3D Gaussians as indices, applying Gaussian memory attention to render foundation model embeddings. The primary results show that M3 outperforms previous methods in feature similarity and downstream tasks; for example, M3 achieved a cosine similarity of 0.6074 on the Playroom dataset using CLIP, compared to 0.4867 for F-Splat. For AI practitioners, M3 provides a more effective framework to integrate foundation models with 3D scene representations, enabling efficient memorization and query of visual and semantic information in spatial contexts.  |
| Why Do Multi-Agent LLM Systems Fail? (Read more on [arXiv](https://arxiv.org/abs/2503.13657) or [HuggingFace](https://huggingface.co/papers/2503.13657))| Bhavya Chopra, Lakshya A. Agrawal, Shuyi Yang, Melissa Z. Pan, Mert Cemri | This paper presents a comprehensive study of failure modes in Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). The main research question is: Why do Multi-Agent LLM Systems fail, and what is the taxonomy of these failure modes? The key methodology involves grounded theory analysis of 150+ conversation traces from five popular MAS frameworks, with human expert annotation and iterative refinement to establish a failure taxonomy. The primary result is a taxonomy (MASFT) of 14 failure modes grouped into 3 categories, with the "Poor Specification" category appearing in 37.17% of analyzed traces. AI practitioners should use this taxonomy to identify and mitigate failures in MAS designs, focusing on enhanced specification, inter-agent coordination, and task verification, rather than relying solely on base LLM improvements.  |
| 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering (Read more on [arXiv](https://arxiv.org/abs/2503.16422) or [HuggingFace](https://huggingface.co/papers/2503.16422))| Xinchao Wang, Xingyi Yang, Qiuhong Shen, nopyyh | 4DGS-1K achieves over 1000 FPS in dynamic scene rendering by addressing temporal redundancy in 4D Gaussian Splatting. The main research objective is to reduce the storage requirements and improve the rendering speed of 4D Gaussian Splatting (4DGS) for dynamic scenes. The key methodology involves a two-step pruning approach: first, pruning short-lifespan Gaussians using a spatial-temporal variation score, and second, filtering inactive Gaussians using a key-frame based temporal filter. The method achieves a 41x reduction in storage and 9x faster rasterization speed compared to vanilla 4DGS on complex dynamic scenes, while maintaining comparable visual quality. For AI practitioners, this implies that they can render high-fidelity, complex dynamic scenes, in real-time with significantly less storage requirements through the implementation of temporal-aware filtering and pruning.  |
| XAttention: Block Sparse Attention with Antidiagonal Scoring (Read more on [arXiv](https://arxiv.org/abs/2503.16428) or [HuggingFace](https://huggingface.co/papers/2503.16428))| Song Han, Junxian Guo, Guangxuan Xiao, Ruyi Xu, songhan | XAttention is a plug-and-play framework that accelerates long-context Transformer inference by using block-sparse attention based on antidiagonal scoring. The paper's main research question is: Can a block-sparse attention mechanism be designed to accelerate long-context Transformers without accuracy loss? XAttention's methodology sums antidiagonal values in the attention matrix to estimate block importance, enabling selective computation. Evaluations on language and video benchmarks show XAttention achieves comparable accuracy to full attention, with up to 13.5x acceleration in attention computation during pre-filling. This suggests AI practitioners can deploy more efficient long-context Transformer models in real-world applications by adopting XAttention to reduce computational costs.  |
| Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on
  Compressed Spatial Tokens (Read more on [arXiv](https://arxiv.org/abs/2503.16278) or [HuggingFace](https://huggingface.co/papers/2503.16278))| Zhifeng Gao, Lin Yao, Haowei Lin, Shuqi Lu, guolinke | Uni-3DAR is a unified framework for 3D structural generation and understanding that uses autoregressive prediction on compressed spatial tokens. The main research objective is to develop a unified framework that seamlessly integrates 3D generation and understanding (3D GU) tasks via autoregressive prediction. The key methodology involves a hierarchical tokenization using an octree to compress 3D space, a two-level subtree compression strategy, and a masked next-token prediction mechanism. Primary results show that Uni-3DAR surpasses previous state-of-the-art diffusion models on microscopic 3D GU tasks, achieving up to 256% relative improvement on PXRD-guided crystal structure prediction and up to 21.8x faster inference speeds. AI practitioners can use Uni-3DAR as a more efficient and versatile framework for unifying diverse 3D GU tasks, potentially leading to faster and more accurate models in areas like materials science and drug discovery.  |
| CLS-RL: Image Classification with Rule-Based Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.16188) or [HuggingFace](https://huggingface.co/papers/2503.16188))| Kaipeng Zhang, Jike Zhong, Ming Li, yuxianglai117, stzhao | This paper introduces CLS-RL, a rule-based reinforcement learning approach for fine-tuning Multimodal Large Language Models (MLLMs) for image classification, demonstrating improved performance and generalization compared to supervised fine-tuning. The main research objective is to explore few-shot MLLM classification fine-tuning and address catastrophic forgetting issues observed with supervised fine-tuning (SFT). The key methodology involves using verifiable signals (class names) as rewards to fine-tune MLLMs and formatting the reward to encourage "thinking" before answering, and comparing the proposed method to No-Thinking-CLS-RL. The primary results show CLS-RL outperforms SFT in most of 11 datasets, with a base-to-new generalization setting achieving 81.17% accuracy on base classes and 79.15% on new classes for CLS-RL, compared to 67.4% and 70.73% for SFT. For AI practitioners, using rule-based reinforcement learning for fine-tuning MLLMs can lead to improved image classification performance and better generalization to new classes, even with limited labeled data.  |
| LHM: Large Animatable Human Reconstruction Model from a Single Image in
  Seconds (Read more on [arXiv](https://arxiv.org/abs/2503.10625) or [HuggingFace](https://huggingface.co/papers/2503.10625))| Weichao Shen, Peihao Li, Xiaodong Gu, Lingteng Qiu, DyrusQZ | LHM is a feed-forward transformer model that generates animatable 3D human avatars from single images in seconds. The main objective is to create a generalizable model for high-fidelity 3D human reconstruction from a single image that supports real-time rendering and animation. The method utilizes a multimodal transformer architecture with a head feature pyramid encoding scheme to fuse 3D point features and 2D image features and represents the avatar as 3D Gaussian splatting. Trained on a large-scale video dataset, LHM achieves a PSNR of 25.183 on synthetic data, outperforming existing methods. For AI practitioners, LHM offers an efficient solution for generating animatable 3D human models from single images, reducing reliance on extensive optimization or post-processing.  |
| Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.15851) or [HuggingFace](https://huggingface.co/papers/2503.15851))| Chua Tat-Seng, Fan Hehe, Ma Fan, zhenglin | Zero-1-to-A is a method for generating animatable 4D head avatars from a single image using video diffusion models. The main research objective is to generate high-fidelity 4D head avatars from a single image input, overcoming the spatial and temporal inconsistencies of video diffusion models. The key methodology, Zero-1-to-A, employs Symbiotic GENeration (SymGEN) to iteratively construct a consistent video dataset and optimize the avatar, alongside a Progressive Learning strategy that separates spatial and temporal learning. Results show that Zero-1-to-A achieves an average CLIP score of 0.285 (ViT-L/14) and 0.322(ViT-B/32), and improves ID consistency and rendering speed compared to prior methods. AI practitioners can leverage this method for efficient and data-sparse creation of high-fidelity, animatable head avatars from single images, eliminating the need for extensive training data.  |
| Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.15567) or [HuggingFace](https://huggingface.co/papers/2503.15567))| Kenji Kawaguchi, Sihang Li, Yi Zhao, Zhiyuan Liu, Yanchen Luo | The paper introduces UAE-3D and UDM-3D, a VAE and latent diffusion model, for 3D molecule generation using a unified latent space. The main research question is whether a unified generative model can seamlessly integrate all modalities of 3D molecule generation (atom types, bonds, 3D coordinates). The key methodology is a multi-modal VAE (UAE-3D) that compresses 3D molecules into a unified latent space, using a Relational Transformer encoder and SE(3) augmentations, combined with a Diffusion Transformer (DiT) for latent diffusion modeling. The results show that UDM-3D achieves 100.0% atom and bond accuracy and 0.0002 coordinate RMSD in reconstruction, and 9.89E-03 bond length distribution in GEOM-Drugs in comparison with the second-best result of 3.91E-01. For AI practitioners, this offers a way to generate 3D molecules with improved efficiency and accuracy by leveraging a unified latent space, simplifying the complexities of handling multi-modality and equivariance.  |
| Tokenize Image as a Set (Read more on [arXiv](https://arxiv.org/abs/2503.16425) or [HuggingFace](https://huggingface.co/papers/2503.16425))| Shuyang Gu, Han Hu, Mengde Xu, Zigang Geng | This paper introduces TokenSet, a new image generation paradigm using set-based tokenization and distribution modeling to improve context aggregation and robustness. The main research objective is to develop a more effective image representation that dynamically allocates coding capacity based on regional semantic complexity, unlike fixed-position latent codes. The key methodology involves representing images as unordered token sets, using a dual transformation to convert sets into fixed-length sequences, and applying a novel Fixed-Sum Discrete Diffusion model for distribution modeling. Primary results show that the TokenSet achieves a reconstruction rFID of 2.74 on ImageNet, with an token overlap of 87.6% after adding a level-10 Gaussian noise (Signal-to-Noise Ratio (dB)), which is a superior performance as compared to prior state of arts. AI practitioners can use TokenSet's representation and modeling approach to create image generation models that better capture global context and exhibit robustness to image perturbations for a variety of computer vision applications.  |
| NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.16375) or [HuggingFace](https://huggingface.co/papers/2503.16375))| Angel X. Chang, Qinghong Han, rexleeppp | NuiScene explores efficient generation of unbounded outdoor scenes using a novel vector set representation and explicit outpainting. The main research objective is to develop an efficient method for generating large, unbounded outdoor scenes with varying heights and diverse styles. The key methodology involves compressing scene chunks into uniform vector sets using 3DShape2VecSet, training an explicit outpainting diffusion model for unbounded generation, and curating a dataset (NuiScene43) of 43 scenes with unified scales and cleaned ground geometries. The vector set diffusion model achieves an FPD score of 0.571 and KPD score of 0.951, outperforming the triplane baseline. For AI practitioners, this method provides a more efficient approach for representing and generating unbounded 3D outdoor scenes compared to methods using spatially structured latents.  |
| Fin-R1: A Large Language Model for Financial Reasoning through
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.16252) or [HuggingFace](https://huggingface.co/papers/2503.16252))| Jinyi Niu, Lingfeng Zeng, Fangqi Lou, Xin Guo, Zhaowei Liu | Fin-R1 is a 7-billion parameter large language model designed specifically for financial reasoning, addressing data fragmentation, reasoning uncontrollability, and generalization challenges. The main research objective was to develop a model that can effectively handle complex financial problems and improve performance in financial reasoning tasks. The key methodology involved constructing a high-quality dataset (Fin-R1-Data) with 60,091 chain-of-thought entries, followed by Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) using Group Relative Policy Optimization (GRPO). Fin-R1 achieved an average score of 75.2 across multiple financial benchmarks, outperforming other similar-sized models and ranking second overall. The principal implication is that AI practitioners can leverage Fin-R1's two-stage training framework and specialized dataset to build more accurate and interpretable decision-making tools for financial AI applications, particularly in areas like compliance and robo-advisory.  |
| SALT: Singular Value Adaptation with Low-Rank Transformation (Read more on [arXiv](https://arxiv.org/abs/2503.16055) or [HuggingFace](https://huggingface.co/papers/2503.16055))| Mohammad Yaqub, Hu Wang, Mohammed Elseiagy, Abdelrahman Elsayed, Sarim-Hash | SALT is a parameter-efficient fine-tuning method for adapting the Segment Anything Model (SAM) to medical image segmentation. The main research objective is to develop a method that effectively adapts foundation models to the medical domain while minimizing trainable parameters and preserving pre-trained knowledge. The key methodology, SALT, combines SVD-based adaptation of dominant singular values with low-rank updates for the remaining subspace, using trainable scale, shift, and low-rank matrices. SALT outperformed state-of-the-art PEFT methods (LoRA and SVD) by 2% to 5% in Dice score on five medical datasets, with only 3.9% trainable parameters. AI practitioners can use SALT for efficient and robust adaptation of large foundation models to specialized domains like medical imaging, achieving high accuracy with significantly reduced computational overhead compared to full fine-tuning or other PEFT methods.  |
| MotionStreamer: Streaming Motion Generation via Diffusion-based
  Autoregressive Model in Causal Latent Space (Read more on [arXiv](https://arxiv.org/abs/2503.15451) or [HuggingFace](https://huggingface.co/papers/2503.15451))| Liang Pan, Ke Fan, Huaijin Pi, Shunlin Lu, lxxiao | MotionStreamer is a framework for text-conditioned streaming motion generation that uses a diffusion-based autoregressive model in a causal latent space. The main research objective is to address the challenge of generating human motion sequences incrementally while dynamically adapting to online text inputs and maintaining semantic coherence. The key methodology involves incorporating a continuous causal latent space into a probabilistic autoregressive model with a diffusion head, utilizing a Causal Temporal AutoEncoder (TAE) for motion compression and online decoding, and employing Two-Forward and Mixed training strategies. The method achieves a Frechet Inception Distance (FID) of 10.724 on the HumanML3D test set, outperforming existing approaches. For AI practioners, MotionStreamer provides an effective model to generate realistic and diverse human motions that directly respond to progressive input text prompts, with low latency.  |
| Make Your Training Flexible: Towards Deployment-Efficient Video Models (Read more on [arXiv](https://arxiv.org/abs/2503.14237) or [HuggingFace](https://huggingface.co/papers/2503.14237))| Yi Wang, Xiangyu Zeng, Tianxiang Jiang, Kunchang Li, Chenting Wang | FluxViT enhances video model efficiency by optimizing input token selection and sampling for varied computational budgets. The main research question is how to maximize input information across budgets, addressing sub-optimal accuracy-computation trade-offs in video models. The key methodology, termed Flux, uses flexible video sampling and token selection, integrated with a masked alignment strategy in a teacher-student training framework. FluxViT-S outperforms InternVideo2-S by 2.2% on K400 with standard computation and achieves comparable performance with only 10% of the inference cost. AI practitioners can leverage Flux for training robust video models adaptable to diverse deployment scenarios, achieving state-of-the-art performance with significantly reduced computational requirements.  |
| MagicID: Hybrid Preference Optimization for ID-Consistent and
  Dynamic-Preserved Video Customization (Read more on [arXiv](https://arxiv.org/abs/2503.12689) or [HuggingFace](https://huggingface.co/papers/2503.12689))| Hongwei Yi, Tianyang Wang, Xi Xiao, Lifan Jiang, Hengjia Li | MagicID is a framework for generating personalized videos that maintain consistent identity and exhibit natural dynamics based on user-provided reference images. The main research objective is to address identity degradation and reduced dynamics in customized video generation caused by reliance on self-reconstruction training with static images. The key methodology involves constructing pairwise preference video data with explicit identity and dynamic rewards, and a hybrid sampling strategy that prioritizes identity preservation and then enhances dynamic motion. The primary results show MagicID achieves a mean identity similarity score of 0.600, outperforming existing methods while preserving motion dynamics. The principal implication for AI practitioners is that using hybrid preference optimization with tailored rewards can improve the quality of identity-preserved video customization, enabling more realistic and personalized video generation.  |
| Reinforcement Learning for Reasoning in Small LLMs: What Works and What
  Doesn't (Read more on [arXiv](https://arxiv.org/abs/2503.16219) or [HuggingFace](https://huggingface.co/papers/2503.16219))| Chris Ngo, quyanh | This study investigates reinforcement learning (RL) for improving reasoning in small language models (LLMs) under resource constraints. The main research question is how small LLMs behave when fine-tuned with RL under strict computational and time limitations, and whether their reasoning performance can be improved using an RL approach similar to DeepSeek-R1. The key methodology involves adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, then training a 1.5-billion-parameter model (DeepSeek-R1-Distill-Qwen-1.5B) on 4 GPUs within 24 hours. A primary result is that the model achieved an AIME24 score of 46.7% with only 7,000 training samples and a $42 training cost, surpassing the o1-preview model. This implies AI practitioners can achieve substantial reasoning gains in small LLMs using RL with limited data and computational resources, offering a cost-effective alternative to large-scale approaches.  |
| Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.16194) or [HuggingFace](https://huggingface.co/papers/2503.16194))| Michael Qizhe Shieh, Kaipeng Zhang, Ziyao Guo | This paper introduces a coarse-to-fine framework for autoregressive image generation that alleviates vocabulary redundancy in large codebooks. The main research objective is to maintain the benefits of large codebooks for high-quality image reconstruction while simplifying the autoregressive modeling task. The key methodology involves clustering similar VQ-VAE codebook tokens into coarse labels, predicting coarse labels autoregressively, and then predicting fine-grained tokens in parallel using full attention. The primary results include an average improvement of 59 points in Inception Score compared to baselines, reduced FID, and faster sampling speeds despite adding an auxiliary network. For AI practitioners, this method allows more efficient autoregressive image generation by reducing the effective vocabulary size, facilitating faster training and improved image quality when using large codebooks.  |
| Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging
  Fabricated Claims with Humorous Content (Read more on [arXiv](https://arxiv.org/abs/2503.16031) or [HuggingFace](https://huggingface.co/papers/2503.16031))| Sunil Saumya, Shankar Biradar, UVSKKR | Here's a concise summary of the research paper, adhering strictly to your guidelines:  This paper introduces the Deceptive Humor Dataset (DHD), a new synthetic multilingual benchmark for studying humor derived from fabricated claims and misinformation. The main research objective is to establish a structured foundation for analyzing humor in deceptive contexts and to understand how humor influences the perception and spread of misinformation. The key methodology involves generating 9,000 humor-infused comments using ChatGPT-4o, labeled with satire levels (1-3) and humor attributes (Irony, Absurdity, Social Commentary, Dark Humor, Wordplay) across multiple languages and code-mixed variants. Primary results show that mBART achieved the best performance for Satire Level Classification with an accuracy of 51.00%, while BERT performed best on Humor Attribute Classification with an accuracy of 40.44%. The principal implication for AI practitioners is the availability of a structured dataset and established baselines to benchmark and advance deceptive humor detection models, a critical aspect in mitigating the spread of harmful narratives.  |
| VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting
  Generation with Flexible Pose and Multi-View Joint Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.15855) or [HuggingFace](https://huggingface.co/papers/2503.15855))| Hyungjin Chung, Byung-Hoon Kim, Hyelin Nam, Byeongjun Park, Hyojun Go | VideoRFSplat is a text-to-3D Gaussian Splatting model that generates real-world scenes with flexible camera poses and multi-view image consistency, eliminating the need for per-scene optimization or external refinement models. The main objective is to develop a direct text-to-3D generation model capable of handling diverse camera poses and unbounded scenes without relying on score distillation sampling (SDS) refinement. The methodology utilizes a dual-stream architecture with a video generation model and a side-attached pose generation model, communicating via cross-attention and employing an asynchronous sampling strategy. The primary result is that VideoRFSplat achieves a FID of 30.33 and CLIP score of 33.0 on MVImgNet, outperforming existing direct text-to-3D methods that use SDS refinement. The principal implication is that AI practitioners can directly generate realistic and coherent 3D scenes from text prompts without needing post-hoc refinement, simplifying the 3D generation pipeline and potentially improving efficiency.  |
| Sonata: Self-Supervised Learning of Reliable Point Representations (Read more on [arXiv](https://arxiv.org/abs/2503.16429) or [HuggingFace](https://huggingface.co/papers/2503.16429))| Chris Xie, Tianwei Shen, Duncan Frost, Daniel DeTone, Xiaoyang Wu | Sonata is a self-supervised learning framework for 3D point cloud representations that addresses limitations of existing approaches. The main research question is whether a reliable self-supervised point cloud model can be developed for diverse 3D tasks via simple linear probing, even with limited data. The key methodology involves a point self-distillation framework that obscures spatial information and emphasizes input features, training on 140k point cloud scenes. A primary result is that Sonata triples linear probing accuracy on ScanNet semantic segmentation compared to previous methods, achieving 72.5% mIoU with less than 0.2% learnable parameters. The principal implication is that AI practitioners can leverage Sonata as a reliable foundation model for various 3D perception tasks, achieving strong performance and data efficiency, even with limited labeled data, by using it as initialization and then employing simple linear probing.  |
| BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space
  Complexity? (Read more on [arXiv](https://arxiv.org/abs/2503.15242) or [HuggingFace](https://huggingface.co/papers/2503.15242))| Gabriel Synnaeve, Benoit Sagot, Baptiste Roziere, pierrechambon | BIGO(BENCH) is a new benchmark for evaluating the ability of large language models (LLMs) to generate code with specified time and space complexity constraints. The main objective is to assess LLMs' capacity to understand and control computational complexity in code generation. The methodology involves a dynamic complexity inference framework to analyze Python functions, a dataset of 3,105 coding problems and 1,190,250 solutions with inferred complexity labels, and evaluations of LLMs on complexity prediction, generation, and coefficient ranking. The results show that DEEPSEEK-R1 LLAMA 70B achieved 4.8% and 3.4% All@1 on time and space complexity generation, respectively, revealing challenges in handling complexity requirements. The main implication for AI practitioners is that while LLMs show proficiency in program synthesis, controlling and reasoning about time and space complexity remains a significant challenge, indicating a need to improve models on abstract thinking about code.  |
| See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language
  Balance to Mitigate Dominant Modality Bias (Read more on [arXiv](https://arxiv.org/abs/2503.13834) or [HuggingFace](https://huggingface.co/papers/2503.13834))| YoungBin Kim, Juhwan Choi, Eunju Lee, MiHyeon Kim, JuneHyoung Kwon | Vision-language (VL) models exhibit a "dominant modality bias," disproportionately relying on one modality, which BALGRAD mitigates by reweighting and projecting gradients. The research analyzes model behavior under dominant modality bias, showing how unaligned gradients and differences in gradient magnitudes hinder balanced loss convergence. The proposed BALGRAD framework employs inter-modality gradient reweighting (adjusting KL divergence gradient based on modality contribution) and inter-task gradient projection. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets demonstrate BALGRAD's effectiveness; on UPMC Food-101, BALGRAD improved performance on the weak (text) modality by 12.5%p compared to the baseline. AI practitioners can use BALGRAD to create more robust VL models that effectively utilize both modalities, even when one is impaired, reducing reliance on a single dominant modality.  |
| AIMI: Leveraging Future Knowledge and Personalization in Sparse Event
  Forecasting for Treatment Adherence (Read more on [arXiv](https://arxiv.org/abs/2503.16091) or [HuggingFace](https://huggingface.co/papers/2503.16091))| Hassan Ghasemzadeh, Diane J. Cook, ab9mamun | AIMI, a knowledge-guided system, forecasts medication adherence by leveraging sensor data, medication history, and future knowledge. The main research objective was to determine the impact of future knowledge and personalization on the accuracy of sparse event forecasting for treatment adherence. The key methodology involved training and evaluating CNN and LSTM models with various combinations of input features, including sensor data, adherence history, and "future knowledge" (prescribed medication times), along with an incremental learning algorithm. The LSTM models achieved an accuracy of 0.932 and an F-1 score of 0.936, and leveraging future knowledge improved the F-1 score by almost 112% when only high-sampled features and future knowledge data were used. For AI practitioners, the results demonstrate that incorporating readily available future knowledge, such as scheduled events, can significantly enhance the performance of sparse event forecasting models in time-series prediction, especially in resource-constrained environments.  |
