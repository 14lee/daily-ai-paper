

## Papers for 2025-03-27

| Title | Authors | Summary |
|-------|---------|---------|
| Dita: Scaling Diffusion Transformer for Generalist
  Vision-Language-Action Policy (Read more on [arXiv](https://arxiv.org/abs/2503.19757) or [HuggingFace](https://huggingface.co/papers/2503.19757))| TTTTTony, MIASANMIA, robot-haonan, TianyiZhang0213, zhihou | Dita introduces a scalable Diffusion Transformer architecture for generalist vision-language-action robot policies. The primary objective is to develop a versatile, open-source VLA model capable of zero-shot or few-shot generalization across diverse robotic embodiments, tasks, and environments, particularly addressing long-horizon tasks and environmental variations. The key methodology involves using a causal Transformer to directly denoise continuous action sequences via a diffusion process, conditioned in-context on raw visual tokens (from DINOv2 and Q-Former) and language instructions (from CLIP). Dita achieves state-of-the-art or competitive performance on simulation benchmarks, notably attaining an 82.4% average success rate on LIBERO (a ~6% improvement over prior methods), and demonstrates robust real-world adaptation with 10-shot finetuning on complex, long-horizon tasks under varying conditions. For AI practitioners, Dita provides a lightweight (334M parameters) and effective open-source framework that integrates Transformer scalability with inherent diffusion denoising via in-context conditioning, offering a strong baseline for developing adaptable robot policies requiring minimal task-specific data. |
| Qwen2.5-Omni Technical Report (Read more on [arXiv](https://arxiv.org/abs/2503.20215) or [HuggingFace](https://huggingface.co/papers/2503.20215))| JialinWang, chenkq, bluelike, jinzheng-he, ZhifangGuo | Qwen2.5-Omni is an end-to-end multimodal model processing text, image, audio, and video to generate streaming text and speech responses. The primary objective is to develop a unified model capable of perceiving diverse streaming inputs, synchronizing temporal modalities like audio and video, and concurrently generating both text and low-latency speech outputs. Key methodologies include block-wise processing for input encoders, Time-aligned Multimodal RoPE (TMROPE) for audio-video synchronization, and a Thinker-Talker architecture separating text generation (Thinker LLM) from streaming speech token generation (Talker), using a sliding-window DiT for audio decoding. Primary results demonstrate state-of-the-art performance on benchmarks like OmniBench (56.13% average score), comparable end-to-end speech instruction following capabilities to text input on tasks like GSM8K (88.7% speech accuracy vs 91.6% text accuracy for Qwen2.5-7B), and robust streaming speech generation with 6.54% WER on the seed-tts-eval test-hard set after reinforcement learning. For AI practitioners, this work offers the Thinker-Talker architecture and TMROPE as a framework for building unified streaming multimodal systems that handle synchronized inputs and generate real-time text and speech, enabling more natural human-AI interaction. |
| LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2503.19990) or [HuggingFace](https://huggingface.co/papers/2503.19990))| Leoxing, KennyUTC, zengyh1900, favourisnotyou, KexianTang | This paper introduces LEGO-Puzzles, a benchmark designed to evaluate multi-step spatial reasoning in Multimodal Large Language Models (MLLMs). The objective is to assess MLLMs' capabilities in both spatial understanding and sequential reasoning through diverse LEGO construction-based tasks. The methodology involves a curated dataset of over 1,100 visual question-answering (VQA) pairs across 11 tasks, alongside image generation evaluations, tested on 20 state-of-the-art MLLMs. Results reveal significant limitations; even the best MLLM (GPT-40) achieved only 57.7% overall accuracy, far below human performance (93.6%), with particular weaknesses in multi-step sequential reasoning and spatially grounded image generation. For AI practitioners, this highlights critical deficiencies in current MLLMs' spatial intelligence, underscoring the need for advancements in models intended for complex real-world applications like robotics and automated assembly that demand robust sequential spatial reasoning. |
| Wan: Open and Advanced Large-Scale Video Generative Models (Read more on [arXiv](https://arxiv.org/abs/2503.20314) or [HuggingFace](https://huggingface.co/papers/2503.20314))| HermanZ, chenweix7, chaojiemao, baoleai, ang-annng | This paper introduces Wan, an open-source suite of advanced large-scale video generative models based on the Diffusion Transformer paradigm. The objective is to push video generation boundaries by developing high-performance, efficient, and comprehensive open-source models (1.3B and 14B parameters) trained on billions of images/videos. Key methodologies include a novel spatio-temporal VAE, scalable pre-training with Flow Matching, large-scale data curation, and extensions to tasks like I2V, editing, personalization, and real-time generation. The 14B model achieved a leading Wan-Bench score of 0.724, outperforming competitors, while the 1.3B model demonstrated consumer-grade efficiency requiring only 8.19 GB VRAM for 480p inference. For AI practitioners, Wan provides open-source access to powerful (14B) and efficient (1.3B) foundation models, code, and training details, enabling the development of diverse video generation applications, including potential deployment on consumer GPUs with the 1.3B model. |
| Unconditional Priors Matter! Improving Conditional Generation of
  Fine-Tuned Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.20240) or [HuggingFace](https://huggingface.co/papers/2503.20240))| Jaihoon Kim, Minhyuk, phillipinseoul, prinphunya | This paper introduces a training-free method to enhance conditional generation from fine-tuned diffusion models by utilizing stronger unconditional priors from base models. The primary objective is to address the degradation in conditional generation quality caused by poor unconditional noise predictions learned during Classifier-Free Guidance (CFG) based fine-tuning. The key methodology involves replacing the unconditional noise prediction term in the CFG sampling process of the fine-tuned model with the corresponding prediction from its original base model or another pretrained model with robust unconditional generation capabilities. Results demonstrate significant improvements; for example, applying this method to Zero-1-to-3 novel view synthesis using SD2.1 as the unconditional prior improved LPIPS from 0.182 to 0.158 and PSNR from 16.647 to 17.801. For AI practitioners, this implies that during inference with CFG-based fine-tuned diffusion models, leveraging the unconditional prior from a separate, well-trained unconditional model can substantially boost conditional output quality without requiring model retraining or architectural changes. |
| Open Deep Search: Democratizing Search with Open-source Reasoning Agents (Read more on [arXiv](https://arxiv.org/abs/2503.20201) or [HuggingFace](https://huggingface.co/papers/2503.20201))| speedyarda, ljirwin, pchiniya, cabxyz, salzubi401 | Open Deep Search (ODS) is introduced as an open-source framework augmenting LLMs with reasoning agents and web search tools to rival proprietary search AI. The primary objective is to bridge the performance gap between open-source and closed-source search AI solutions by enhancing LLM reasoning with real-time web information. ODS employs two main components: an Open Search Tool for improved web context retrieval and an Open Reasoning Agent (using ReAct or CodeAct) to orchestrate tool use, including the search tool, calculator, and code interpreter, based on user queries. Key results show ODS-v2 paired with DeepSeek-R1 achieves 75.3% accuracy on the FRAMES benchmark, outperforming GPT-4o Search Preview by 9.7%, and 88.3% on SimpleQA. For AI practitioners, ODS offers a modular, open-source system to integrate advanced search and reasoning into any base LLM, enabling state-of-the-art performance on fact-based question answering without dependence on closed systems. |
| GenHancer: Imperfect Generative Models are Secretly Strong
  Vision-Centric Enhancers (Read more on [arXiv](https://arxiv.org/abs/2503.19480) or [HuggingFace](https://huggingface.co/papers/2503.19480))| yshan2u, yxgeee, aether25, tttoaster, msj9817 | GenHancer enhances CLIP's fine-grained visual representations using lightweight generative models without requiring perfect reconstruction or pre-trained denoisers. The objective is to explore how imperfect generative models can effectively transfer fine-grained visual knowledge to discriminative models like CLIP, investigating optimal conditioning, denoising configurations, and generation paradigms. The key methodology involves a two-stage post-training approach using lightweight, randomly initialized continuous or discrete denoisers conditioned solely on CLIP's global ([CLS]) token for self-supervised reconstruction, employing techniques like LoRA and scaled Logit-Normal timestamp sampling. GenHancer consistently outperforms prior methods, achieving a 6.0% improvement over the baseline OpenAICLIP on the MMVP-VLM benchmark, demonstrating that perfect generation is not necessary for representation enhancement. For AI practitioners, this implies that fine-grained visual capabilities of CLIP-based systems (like MLLMs) can be significantly and efficiently improved post-hoc using lightweight generative models focused on specific conditioning (global token only) and training strategies, avoiding computationally expensive heavy denoisers. |
| BizGen: Advancing Article-level Visual Text Rendering for Infographics
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.20672) or [HuggingFace](https://huggingface.co/papers/2503.20672))| YuanYuhui, kevinlin311tw, bohanChen, Marseclipse, wukeming11 | BizGen introduces a framework for generating high-quality infographics and slides with accurate article-level visual text rendering and adherence to ultra-dense layouts. The primary objective is to overcome the challenges of significantly longer text contexts and the scarcity of high-quality business content data compared to standard text-to-image tasks. Key methodologies include the creation of a large-scale dataset (INFOGRAPHICS-650K) via retrieval-augmented generation and a novel layout-guided cross-attention mechanism with layout-conditional Classifier-Free Guidance (CFG) for region-wise control. BizGen significantly outperforms models like FLUX and SD3 on the BizEval benchmark, achieving over 25% absolute improvement in visual text spelling accuracy (OCR) on infographics with more than 20 layers compared to FLUX. For AI practitioners, BizGen offers a scalable data generation strategy and a controllable diffusion model architecture to produce complex, text-rich business graphics demanding high fidelity to dense layouts and long-form textual content. |
| Gemini Robotics: Bringing AI into the Physical World (Read more on [arXiv](https://arxiv.org/abs/2503.20020) or [HuggingFace](https://huggingface.co/papers/2503.20020))| abalakrishna123, TravisAStrong, montse90, jalayrac, saminda | This paper introduces Gemini Robotics, a family of AI models based on Gemini 2.0 designed to bridge AI capabilities into the physical world via robotics. The main objective is to endow large multimodal models with robust embodied reasoning and dexterous physical interaction capabilities for general-purpose robot control. Key methodologies include enhancing Gemini 2.0's embodied reasoning (Gemini Robotics-ER), evaluated on a new ERQA benchmark, and fine-tuning a Vision-Language-Action (VLA) model (Gemini Robotics) on extensive robot action data for direct, low-latency control. The generalist Gemini Robotics VLA achieved high proficiency out-of-the-box, succeeding on 50% of 20 diverse dexterous manipulation tasks with over 80% success rate, and demonstrated strong generalization and rapid adaptation to new tasks and embodiments. For AI practitioners, this work shows that large multimodal foundation models, when specifically trained for embodied reasoning and grounded with robot interaction data, provide a viable foundation for developing more general-purpose, dexterous, and adaptable robotic agents. |
| MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree
  Search (Read more on [arXiv](https://arxiv.org/abs/2503.20757) or [HuggingFace](https://huggingface.co/papers/2503.20757))| armanc, chenzhao, yilunzhao, AlexCCtop | MCTS-RAG integrates Monte Carlo Tree Search (MCTS) with Retrieval-Augmented Generation (RAG) to improve reasoning capabilities of small language models (SLMs) on knowledge-intensive tasks. The research aims to overcome SLM limitations in accessing and utilizing external knowledge by dynamically combining structured reasoning search with adaptive retrieval. The methodology employs MCTS to explore reasoning paths, introducing specific RAG actions (Retrieval Reasoning, Retrieval Decompose) at decision points, guided by UCT, and evaluates paths using retrieved information. Key results show MCTS-RAG enabled Llama 3.1-8B to achieve over 20% absolute accuracy improvement on ComplexWebQA and roughly 15% on GPQA compared to baseline methods. For AI practitioners, this work presents an effective inference-time compute scaling method to significantly enhance the performance of smaller LMs on complex, knowledge-reliant tasks without model retraining, offering a pathway to achieve higher accuracy with more resource-efficient models. |
| AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset (Read more on [arXiv](https://arxiv.org/abs/2503.19462) or [HuggingFace](https://huggingface.co/papers/2503.19462))| Yunhong Wang, XihuiLiu, YaohuiW, AriaChen, aejion | AccVideo accelerates video diffusion models through distillation using a synthetic dataset of denoising trajectories. The research objective is to reduce the extensive inference steps required by video diffusion models while maintaining output quality by avoiding distillation on irrelevant data points. Key methodology involves generating a synthetic dataset (SynVid) with full denoising trajectories from a pretrained teacher model, training a student model using trajectory-based few-step guidance on keyframes from these trajectories, and employing an adversarial training strategy with timestep-aware discriminators. The primary result is an 8.5x reduction in inference time compared to the teacher model (HunyuanVideo), generating 720x1280 videos in 380s vs 3234s with comparable quality. For AI practitioners, this demonstrates an effective technique to significantly speed up high-resolution video generation from diffusion models, making them more feasible for real-world deployment by leveraging synthetic data distillation. |
| ViLBench: A Suite for Vision-Language Process Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.20271) or [HuggingFace](https://huggingface.co/papers/2503.20271))| cihangxie, xianft, alihiker, Helicopt, PahaII | This paper introduces VILBENCH, a benchmark suite for vision-language process reward modeling, alongside a new dataset (ViLReward-73K) and a trained process reward model (ViLPRM). The main objective is to evaluate the effectiveness of vision-language large models (VLLMs) as process reward models (PRMs) and output reward models (ORMs), and to develop improved PRMs for tasks requiring step-wise reasoning. Key methodologies include benchmarking seven VLLMs on five VL datasets, filtering data to create VILBENCH emphasizing step-wise rewards, collecting preference data using an enhanced MCTS algorithm, and training a 3B parameter ViLPRM based on QwenVL-2.5. Primary results show neither ORM nor PRM consistently outperforms the other across tasks using general VLLMs, while the trained ViLPRM achieves an average improvement of 3.3% over standard Chain-of-Thought evaluation on VILBENCH. For AI practitioners, this indicates that specialized PRMs trained on process supervision data, like ViLPRM, can better evaluate complex vision-language reasoning steps than general VLLMs or ORMs, highlighting a pathway to improve model alignment and evaluation for multi-step multimodal tasks. |
| LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior
  Accuracy Preservation (Read more on [arXiv](https://arxiv.org/abs/2503.19950) or [HuggingFace](https://huggingface.co/papers/2503.19950))| Pingyi Luo, Bingsheng He, deciding, Zicong99, Concyclics | LogQuant introduces a log-distributed 2-bit quantization method for LLM KV Caches, improving accuracy preservation over existing techniques. The objective is to reduce KV Cache memory usage via 2-bit quantization while mitigating the associated accuracy loss by selectively preserving important tokens based on a log-distributed attention pattern. The methodology involves applying a base-2 logarithmic filtering strategy to retain tokens with decreasing density further from the current position, quantizing less critical tokens to 2-bits while keeping a dynamic window of recent tokens (2W to 3W) at full precision. LogQuant demonstrated superior performance, improving accuracy by 40%-200% on Math and Code tasks compared to KiVi at similar compression ratios, and boosting throughput by 25% over a BF16 baseline. For AI practitioners, LogQuant offers a way to deploy LLMs with long contexts more efficiently on memory-constrained hardware by significantly reducing KV Cache size with better accuracy retention than prior 2-bit quantization approaches. |
| ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving
  Systems (Read more on [arXiv](https://arxiv.org/abs/2503.20756) or [HuggingFace](https://huggingface.co/papers/2503.20756))| xzwnlp, bozhong, xiangchen-dvi, JizhanFang, Chenxiwang | This paper introduces ADS-Edit, a multimodal benchmark dataset for evaluating knowledge editing techniques applied to Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS). The research objective is to assess how effectively knowledge editing can update LMMs with domain-specific ADS knowledge (addressing traffic knowledge gaps, complex conditions, dynamic states) without requiring full retraining. The methodology involves constructing the ADS-Edit benchmark from existing ADS datasets (LingoQA, DriveLM, CODA-LM) with video, multi-view, and single-image data across perception, understanding, and decision-making scenarios, and evaluating four editing baselines (Prompt, AdaLora, GRACE, WISE) on reliability, generality, and locality. Primary results demonstrate that memory-based methods achieve high reliability (e.g., GRACE reached 100% reliability on single edits), but differ significantly in generality (GRACE <30%, WISE ~85-95%), with WISE showing strong locality (~100%). For AI practitioners, ADS-Edit provides a framework to evaluate and select knowledge editing methods for efficiently updating LMMs in ADS, indicating WISE offers a balanced trade-off for update reliability, generalization, and parameter preservation. |
| Beyond Words: Advancing Long-Text Image Generation via Multimodal
  Autoregressive Models (Read more on [arXiv](https://arxiv.org/abs/2503.20198) or [HuggingFace](https://huggingface.co/papers/2503.20198))| Min Li, Lijuan, zyang39, linjieli222, Awiny | This paper presents LongTextAR, a multimodal autoregressive model enabling high-fidelity long-text image generation. It addresses the challenge of accurately rendering extensive textual content in images, a limitation of current generative models. The methodology identifies Vector Quantization (VQ) tokenization bottlenecks and introduces TextBinarizer, a novel text-focused binary tokenizer, integrated into a Llama2-based autoregressive architecture trained on text-rich data. LongTextAR significantly outperforms models like SD3.5 Large, achieving 69.5% OCR accuracy on long texts (>10 words) versus 52.3% for SD3.5 Large, and offers controllable text rendering (font, size, color, alignment). For AI practitioners, this work demonstrates that specialized tokenization within an autoregressive framework provides a strong alternative to diffusion models for generating images requiring accurate, controllable long text, impacting applications like automated document and presentation creation. |
| Attention IoU: Examining Biases in CelebA using Attention Maps (Read more on [arXiv](https://arxiv.org/abs/2503.19846) or [HuggingFace](https://huggingface.co/papers/2503.19846))| Vikram V. Ramaswamy, Olga Russakovsky, tyleryzhu, serianni | This paper introduces Attention-IoU, a metric using attention maps to quantify biases within computer vision classification models by analyzing internal representations. The objective is to identify spurious correlations and understand how specific image features contribute to biased predictions, moving beyond performance disparities. The core methodology uses a generalized Intersection-over-Union (Attention-IoU) to compare GradCAM attention maps against ground-truth feature masks (mask score) or other attribute attention maps (heatmap score). Validation on Waterbirds shows the mask score accurately tracks induced bias (decreasing from 0.72±0.02 to 0.42±0.03 as bias increases from 50% to 100%), and analysis on CelebA reveals Attention-IoU uncovers correlations like that between `Blond_Hair` and `Male` (heatmap score 0.72±0.02) potentially linked to unlabeled confounders, unlike `Wavy_Hair` (0.65±0.03). For AI practitioners, Attention-IoU provides a tool to pinpoint spatial sources of bias within models, indicating that biases can stem from internal representations not solely reflected in dataset label correlations, thus informing more targeted debiasing interventions. |
| Self-Supervised Learning of Motion Concepts by Optimizing
  Counterfactuals (Read more on [arXiv](https://arxiv.org/abs/2503.19953) or [HuggingFace](https://huggingface.co/papers/2503.19953))| Kevin Feigelis, Rahul Venkatesh, Seungwoo Kim, Stefan Stojanov, kmeisthax | Opt-CWM introduces a self-supervised technique for optical flow and occlusion estimation by optimizing counterfactual probes on a pre-trained video prediction model without labeled data. The primary objective is to develop a method that extracts motion concepts from unlabeled videos by learning optimal input perturbations for a base Counterfactual World Model (CWM), avoiding fixed heuristics. Key methodology involves parameterizing perturbations with a learnable network trained jointly with a sparse flow-conditioned predictor using an asymmetric masking principle and RGB reconstruction loss. Results demonstrate state-of-the-art performance on real-world benchmarks compared to other self-supervised methods, achieving an Average Jaccard (AJ) of 47.53 and Average Distance (AD) of 8.73 on TAP-Vid First (DAVIS). For AI practitioners, this work provides a scalable, self-supervised approach to extract robust motion primitives from vast unlabeled video data, beneficial for applications requiring motion understanding without reliance on synthetic datasets or manual heuristics. |
| Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.16870) or [HuggingFace](https://huggingface.co/papers/2503.16870))| kw1jjang, Rock222, AndrewAhn, ya-mehdi, Anshumann | This paper introduces Random Sampling Knowledge Distillation (RS-KD), an importance-sampling method for accelerating LLM pre-training distillation using sparse teacher logits. The research aims to develop an efficient offline knowledge distillation strategy for LLM pre-training that requires storing only a sparse subset of teacher logits without compromising student model performance or calibration. The key methodology involves using importance sampling (specifically, sampling proportional to teacher probabilities) to create unbiased sparse target distributions, theoretically and empirically contrasting this with biased Top-K sampling approaches. Primary results show that RS-KD achieves performance comparable to full distillation using only 12 unique sampled tokens, maintains near-perfect calibration (ECE ~0.8%), preserves expected gradients (4° angular difference vs. FullKD), and offers significant training throughput gains (1.7x-2.6x faster than FullKD). For AI practitioners, RS-KD offers a computationally efficient method to pre-train smaller LLMs via offline distillation, drastically reducing the storage required for teacher logits (using ~0.01%) and accelerating training with marginal overhead compared to standard cross-entropy training. |
| DINeMo: Learning Neural Mesh Models with no 3D Annotations (Read more on [arXiv](https://arxiv.org/abs/2503.20220) or [HuggingFace](https://huggingface.co/papers/2503.20220))| Alan Yuille, Weijie Guo, wufeim, guofeng1123 | DINeMo presents a neural mesh model for category-level 3D pose estimation trained without 3D annotations. The main objective is to overcome the limitation of requiring extensive 3D annotations for training neural mesh models, enabling broader applicability and scalability. The key methodology involves leveraging pseudo-correspondence derived from large visual foundation models (SD-DINO) via a novel bidirectional generation process that integrates local features and global context, combined with Grounded-SAM for enhanced inference. DINeMo significantly outperforms previous zero- and few-shot methods on PASCAL3D+ car pose estimation (e.g., narrowing the gap with fully-supervised methods by 67.3% on Acc@pi/18, LO) and demonstrates effective scaling with additional unlabeled training data. For AI practitioners, this work offers a viable pathway to develop robust 3D object understanding models without relying on difficult-to-obtain 3D ground truth, utilizing unlabeled image data for training. |
| Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred
  Image (Read more on [arXiv](https://arxiv.org/abs/2503.17358) or [HuggingFace](https://huggingface.co/papers/2503.17358))| r0nn13, jerredchen | This paper introduces a method to estimate instantaneous camera rotational (ω) and translational (v) velocity directly from motion blur within a single image. The objective is to leverage motion blur, often considered an artifact, as the primary source of information for robust ego-motion estimation during fast camera movements, eliminating the need for IMUs or multi-frame analysis. The approach first predicts dense motion flow and monocular depth using a neural network, then recovers velocity by solving a differentiable linear least squares system derived from motion field equations, enabling end-to-end training on synthetic and real data. Evaluated on real-world data, the method yields state-of-the-art velocity estimates (e.g., average rotational RMSE 1.22/0.91/1.76 rad/s), significantly outperforming MASt3R and COLMAP, and achieves real-time performance (30 FPS). AI practitioners can apply this technique for real-time, drift-free, IMU-like velocity measurements in high-motion scenarios (e.g., robotics, AR/VR) using only a single blurred camera image, enhancing robustness where traditional VO/SLAM methods fail. |
| PathoHR: Breast Cancer Survival Prediction on High-Resolution
  Pathological Images (Read more on [arXiv](https://arxiv.org/abs/2503.17970) or [HuggingFace](https://huggingface.co/papers/2503.17970))| Rundong Xue, Jiaxuan Xiao, Jun Liu, Shiru Wang, Yang Luo | PathoHR is a novel pipeline for breast cancer survival prediction using enhanced high-resolution pathological image features and optimized similarity learning. The main objective is to improve survival prediction accuracy by effectively extracting representative features from high-resolution WSIs while managing computational costs and addressing tumor heterogeneity. The methodology involves patch-wise feature extraction using a pre-trained encoder, integrating a plug-and-play high-resolution Vision Transformer (ViTAR) for feature enhancement, and systematically evaluating various similarity metrics (e.g., Cosine, Euclidean, Attention Score) for adaptive token merging. Results demonstrate that using enhanced 16x16 patches with the PathoHR pipeline (specifically with cosine similarity) achieves superior performance (AUC 0.90741) compared to baseline methods using larger raw 24x24 patches (AUC 0.8), validating the approach's effectiveness and efficiency. For AI practitioners, this implies that integrating resolution enhancement techniques (like high-res ViTs) with optimized similarity-based feature learning can enable more accurate analysis of large medical images using smaller patches, reducing computational overhead without sacrificing predictive power. |
