

## Papers for 2025-03-03

| Title | Authors | Summary |
|-------|---------|---------|
| DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking (Read more on [arXiv](https://arxiv.org/abs/2502.20730) or [HuggingFace](https://huggingface.co/papers/2502.20730))| luyaojie, sanmusunrise, xuanang, yhycai, lzq2021 | The paper introduces a new benchmark and system for complex engineering solution design. The main research objective is to evaluate and improve systems' ability to generate complete and feasible solutions for engineering problems with multiple constraints. The key methodology is SolutionRAG, leveraging tree-based exploration and a bi-point thinking mechanism (alternating solution design and review) to generate solutions. SolutionRAG achieved a 66.4 analytical score and 67.9 technical score on the SolutionBench, outperforming baselines like Naive-RAG and Self-RAG. AI practitioners can use SolutionBench to benchmark and the SolutionRAG architecture to improve the generation of solutions for complex, multi-constraint engineering problems.  |
| Chain of Draft: Thinking Faster by Writing Less (Read more on [arXiv](https://arxiv.org/abs/2502.18600) or [HuggingFace](https://huggingface.co/papers/2502.18600))| Lingxiao Zhao, Wenhao Xie, DeBERTa, sileixu | Chain of Draft (CoD) is a new prompting strategy that improves the efficiency of large language models (LLMs) by generating concise reasoning steps. The research proposes and evaluates Chain of Draft (CoD), a prompting method that minimizes verbosity in LLM reasoning. CoD prompts LLMs to produce brief, information-dense intermediate steps, resembling human draft-thinking, during multi-step reasoning tasks. The results show that CoD matches or surpasses Chain-of-Thought (CoT) accuracy on GSM8K, date, sports, and coin flip tasks, while using up to 92.4% fewer tokens in a specific Sports Understanding case. AI practitioners can use CoD to reduce latency and computational costs in LLM applications without significantly sacrificing accuracy, especially in resource-constrained environments.  |
| ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents (Read more on [arXiv](https://arxiv.org/abs/2502.18017) or [HuggingFace](https://huggingface.co/papers/2502.18017))| xpjandy, shihang, vickywu, lovesnowbest, autumncc | ViDoRAG is a multi-agent RAG framework for visually-rich documents using dynamic retrieval and iterative reasoning. The main research objective is to address the limitations of existing RAG methods in handling visually rich documents, particularly the challenges of multi-modal retrieval and insufficient reasoning capabilities. The methodology employs a Gaussian Mixture Model (GMM)-based hybrid retrieval strategy (textual and visual) and a multi-agent framework (seeker, inspector, answer) for iterative reasoning. Primary results show ViDoRAG outperforms existing methods on the ViDoSeek benchmark by over 10% in overall accuracy. AI practitioners can leverage ViDoRAG's multi-agent framework and dynamic retrieval strategy to build more effective and robust RAG systems for applications dealing with visually rich documents.  |
| SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers (Read more on [arXiv](https://arxiv.org/abs/2502.20545) or [HuggingFace](https://huggingface.co/papers/2502.20545))| Coralia Cartis, Wenqi Zhu, Kechen Li, Shiweiliuiiiiiii, jitianbo | Large Language Models (LLMs) can be effectively used to solve sum-of-squares (SoS) polynomial problems with proper reasoning guidance. The main research question is whether LLMs can determine the nonnegativity of a given multivariate polynomial, a computationally intractable problem related to Hilbert's Seventeenth Problem. The researchers introduced a dataset (SoS-1K) of ~1,000 polynomials and evaluated various LLMs using plain questions, simple instructions, and expert-designed reasoning instructions based on five criteria. The results show that high-quality reasoning instructions significantly improve accuracy, with the best-performing model (DeepSeek-R1) reaching 81% accuracy with SoS Reasoning instructions, compared to around 60% with plain question. Supervised fine-tuning of a 7B model on SoS-1K achieved 70% accuracy outperforming the 671B Deepseek-V3. AI practitioners can leverage specialized datasets and reasoning-guided instructions to significantly enhance LLMs' ability to solve complex mathematical problems and tackle NP-hard problems.  |
| Optimal Brain Apoptosis (Read more on [arXiv](https://arxiv.org/abs/2502.17941) or [HuggingFace](https://huggingface.co/papers/2502.17941))| Delei Kong, Junjie Jiang, Jiaxu Wang, Zheng Fang, Mingyuan Sun | Optimal Brain Apoptosis (OBA) is a novel pruning method that calculates the Hessian-vector product to estimate parameter importance for neural network compression. The main research objective is to develop a more precise and efficient pruning method that avoids approximations of the Hessian matrix used in prior work. The key methodology involves decomposing the Hessian matrix across network layers, identifying conditions for non-zero inter-layer Hessian submatrices, and efficiently computing the second-order Taylor expansion of parameters using a Jacobian-vector product forward propagation technique. The primary results show that OBA achieves a 2x speedup on ImageNet with ResNet50 with only a 0.53% accuracy decrease, outperforming existing methods. The principal implication for AI practitioners is that OBA offers a more accurate and efficient way to prune both convolutional neural networks and Transformers, directly leading to computational savings in inference.  |
| Tell me why: Visual foundation models as self-explainable classifiers (Read more on [arXiv](https://arxiv.org/abs/2502.19577) or [HuggingFace](https://huggingface.co/papers/2502.19577))| Christian Lovis, Gianmarco Mengaldo, Mina Bjelogrlic, hturbe | Visual foundation models (VFMs) can be adapted into self-explainable classifiers through a novel prototypical architecture called ProtoFM. The main research objective is to develop a self-explainable model (SEM) leveraging VFMs that achieves competitive classification performance and improved interpretability. The methodology involves training a lightweight head (approximately 1 million parameters) on top of frozen VFMs, using a student-teacher approach and specialized training objectives, including assignment, alignment, contrastive, sparsity, and classification losses. The ProtoFM architecture achieved a mean explainability score (mX) of 0.92 on the FunnyBirds framework, outperforming existing prototypical models. AI practitioners can leverage frozen VFMs to create efficient and interpretable classifiers, improving transparency and trust, particularly in critical applications.  |
| Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids (Read more on [arXiv](https://arxiv.org/abs/2502.20396) or [HuggingFace](https://huggingface.co/papers/2502.20396))| Yuke Zhu, Linxi Fan, Kartik Sachdev, Toru Lin, jitendra1995 | This paper presents a sim-to-real reinforcement learning recipe for vision-based dexterous manipulation tasks on humanoid robots. The main research objective is to identify and address the key challenges in applying sim-to-real reinforcement learning to solve contact-rich dexterous manipulation tasks on humanoids. The key methodology includes an automated real-to-sim tuning module, a generalized reward design scheme, a divide-and-conquer distillation process, and a mixture of sparse and dense object representations. The primary results include a 62.3% success rate on the grasp-and-reach task, 80% on the box lift task, and 52.5% on bimanual handover, demonstrating generalization and robustness against force perturbations; also shown is the correlation that lower MSE measured by autotune module and higher sim-to-real transfer success rate. AI practitioners can utilize the proposed techniques to train humanoid robots for dexterous manipulation, achieving robust generalization and high performance without human demonstrations.  |
| LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation (Read more on [arXiv](https://arxiv.org/abs/2502.20583) or [HuggingFace](https://huggingface.co/papers/2502.20583))| kasikci, kojimano, jungok, kamahori | LITEASR is a compression scheme for ASR encoders that maintains transcription accuracy while reducing computational costs. The main research objective is to reduce the computational intensity of ASR encoders, which are a deployment bottleneck. The key methodology leverages low-rank properties in intermediate activations by applying PCA and optimizing self-attention in a reduced dimension, implemented using a specialized GPU kernel. Applying LITEASR to Whisper large-v3 reduces encoder size by over 50%, matching Whisper medium's size with better transcription accuracy. AI practitioners can deploy more efficient ASR systems by leveraging the compressed, and Pareto-optimal, models.  |
| HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.20811) or [HuggingFace](https://huggingface.co/papers/2502.20811))| Fuzheng Zhang, Yuanxing Zhang, Jingyun Hua, Xiao Wang, lwher1996 | This paper introduces HAIC, a two-stage data annotation pipeline and two datasets, to improve human action understanding and generation in multi-modal large language models (MLLMs). The main research objective is to address the lack of high-quality data for training MLLMs on videos involving human actions, especially multi-person interactions. The methodology involves a two-stage data annotation pipeline: accumulating videos with clear human actions, and annotating videos with a standardized caption format detailing individual attributes, actions, and interactions. Training with the curated HAICTrain dataset improves human action understanding, as evidenced by a 2.1% accuracy improvement on the HAICBench benchmark compared to the baseline LLaVA-Video-7B model. AI practitioners can use the released datasets and annotation pipeline to enhance MLLMs' performance in tasks requiring fine-grained understanding of human actions and interactions in videos.  |
