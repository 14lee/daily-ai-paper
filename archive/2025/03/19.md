

## Papers for 2025-03-19

| Title | Authors | Summary |
|-------|---------|---------|
| RWKV-7 "Goose" with Expressive Dynamic State Evolution (Read more on [arXiv](https://arxiv.org/abs/2503.14456) or [HuggingFace](https://huggingface.co/papers/2503.14456))| saitejautpala, Guangyu, SmerkyG, ZhangRC, BlinkDL | RWKV-7 "Goose" is a new sequence modeling architecture with pre-trained language models that introduces a generalized delta rule with vector-valued gating for improved performance. The main research objective is to develop a sequence modeling architecture that achieves state-of-the-art performance while maintaining efficiency in terms of memory usage and inference time. The key methodology involves a generalized formulation of the delta rule with vector-valued gating, in-context learning rates, and a relaxed value replacement rule, integrated into a modified RWKV-6 architecture. Primary results show that RWKV-7 models achieve state-of-the-art multilingual performance at the 3 billion parameter scale, matching current SoTA English language performance while requiring only constant memory usage and inference time per token; and on English-focused benchmarks the RWKV7-World3-2.9B achieved 71.5 average accuracy. AI practitioners can use RWKV-7 models as efficient alternatives to Transformers, benefiting from reduced inference costs and constant memory usage, particularly beneficial for long-sequence applications.  |
| Impossible Videos (Read more on [arXiv](https://arxiv.org/abs/2503.14378) or [HuggingFace](https://huggingface.co/papers/2503.14378))| Hai Ci, mikeshou, ZechenBai | This paper introduces IPV-BENCH, a benchmark for evaluating video generation and understanding models on impossible or counterfactual video content. The main research questions are whether current video generation models can create impossible videos from prompts and whether video understanding models can comprehend them. The key methodology involved creating a taxonomy of impossible video types, generating a dataset of text prompts (IPV-TXT) and videos (IPV-VID), and evaluating various models on tasks including video generation, judgment, multiple-choice question answering, and open-ended question answering. A key finding is that the top-performing video generation model, Mochi 1, generated high-quality impossible videos in only 37.3% of cases. This demonstrates the need for significant improvement in video models' ability to generate and understand non-real-world scenarios, providing AI practitioners a clear benchmark and identified limitations to guide the development of more robust and creative video models.  |
| Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM (Read more on [arXiv](https://arxiv.org/abs/2503.14478) or [HuggingFace](https://huggingface.co/papers/2503.14478))| Yingji Liang, Shengyuan Ding, Kai Lan, Zhijian Chen, Xinyu Fang | Creation-MMBench is a new benchmark for evaluating the visual creative capabilities of Multimodal Large Language Models (MLLMs) in real-world, image-based tasks.  i) Main research question or objective: To introduce and evaluate Creation-MMBench, a multimodal benchmark designed to assess the creative capabilities of MLLMs in real-world, image-based tasks.  ii) Key methodology used: Creation-MMBench comprises 765 test cases across 51 fine-grained tasks, with instance-specific evaluation criteria for assessing response quality and factual consistency with visual inputs, using MLLM-as-a-Judge (GPT-4o) methodology.  iii) Primary results: Current open-source MLLMs significantly underperform compared to proprietary models in creative tasks; for instance, Qwen2.5-VL-72B-Instruct achieved a reward of -5.82 and visual factuality score of 8.33 on the overall benchmark, while Gemini-2.0-pro-exp achieved a reward of 4.48 and visual factuality of 8.53.  iv) Principal implication for AI practitioners: AI practitioners should address the limitations of current MLLMs in context-aware creativity and visual-based language generation, and focus on developing more comprehensive and fine-grained evaluation criteria, recognizing that visual fine-tuning can negatively impact the base LLM's creative abilities.  |
| DAPO: An Open-Source LLM Reinforcement Learning System at Scale (Read more on [arXiv](https://arxiv.org/abs/2503.14476) or [HuggingFace](https://huggingface.co/papers/2503.14476))| Xiaochen Zuo, Yufeng Yuan, Ruofei Zhu, Zheng Zhang, Qiying Yu | DAPO is an open-source system for large-scale reinforcement learning (RL) with language models (LLMs), achieving state-of-the-art results on mathematical reasoning. The main research objective is to develop and open-source a scalable and reproducible RL system for LLMs that addresses limitations in existing approaches and reproduces industry-level RL results. The key methodology is the Decoupled Clip and Dynamic sampling Policy Optimization (DAPO) algorithm, incorporating techniques like Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping, built upon the `verl` framework. The primary result is that DAPO achieves 50 points on AIME 2024 using a Qwen2.5-32B base model, surpassing previous state-of-the-art results with 50% fewer training steps. Principal implications for AI practitioners include that this paper presents a fully open-sourced algorithm, training code and dataset, providing techniques to solve problems like reward noise and training instability for reinforcement learning.  |
| DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs
  for Knowledge-Intensive Visual Grounding (Read more on [arXiv](https://arxiv.org/abs/2503.12797) or [HuggingFace](https://huggingface.co/papers/2503.12797))| Zonghao Guo, Zhicong Luo, carboncoo, sdudzy, MaxyLee | DeepPerception enhances Multimodal Large Language Models (MLLMs) for knowledge-intensive visual grounding by integrating cognitive reasoning with visual perception. The research introduces and addresses the challenge of knowledge-intensive visual grounding (KVG), requiring fine-grained perception and domain knowledge integration in MLLMs. The methodology involves a two-stage training framework: supervised fine-tuning for cognitive reasoning and reinforcement learning to optimize perception-cognition synergy, using an automated data synthesis pipeline. DeepPerception achieved an 8.08% accuracy improvement on the new KVG-Bench compared to direct fine-tuning, also showcasing +4.60% superior cross-domain generalization. AI practitioners can leverage DeepPerception's training framework and the KVG-Bench dataset to develop MLLMs with improved cognitive visual perception, enabling more human-like visual understanding in AI systems.  |
| CapArena: Benchmarking and Analyzing Detailed Image Captioning in the
  LLM Era (Read more on [arXiv](https://arxiv.org/abs/2503.12329) or [HuggingFace](https://huggingface.co/papers/2503.12329))| Qiushi Sun, Zheng Ma, Jiaxin Fan, songwp, cckevinn | CapArena benchmarks detailed image captioning with large language models (LLMs) through human evaluations and analyzes automated metrics. The main research questions are how well current Vision-Language Models (VLMs) perform on detailed image captioning compared to humans, and how reliably automated metrics can assess detailed caption quality. The key methodology involved creating CapArena, a platform with over 6000 pairwise caption battles with human preference votes, and evaluating various traditional and recent captioning metrics against these human annotations. Primary results showed that top models like GPT-4o achieve or surpass human-level performance, and the VLM-as-a-Judge approach correlated with human rankings at 94.3% at $4 per test. AI practitioners should use VLM-as-a-Judge for efficient and reliable evaluation of detailed image captioning models, as it aligns better with human preference than traditional metrics.  |
| Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated
  Objects via Procedural Generation (Read more on [arXiv](https://arxiv.org/abs/2503.13424) or [HuggingFace](https://huggingface.co/papers/2503.13424))| Li Ray Luo, Yitong Wang, Ruiming Liang, Zichao Yu, Xinyu Lian | Infinite Mobility is a procedural pipeline for synthesizing large-scale, high-fidelity 3D articulated objects. The main research objective is to develop a method for generating high-quality articulated objects that overcomes the limitations of existing data-driven and simulation-based approaches. The key methodology utilizes a tree-growing strategy for articulation structure generation, combined with procedural mesh generation or dataset retrieval with refinement, and ensures physical plausibility through constraint rules. The primary results show that the method produces objects comparable to human-annotated datasets, with an average Tree Edit Distance of 78.62 compared to 3.88 of PartNet-Mobility, and outperforms existing generative models in both physical property and mesh quality evaluations. The principal implication for AI practitioners is that the proposed pipeline provides a scalable and high-fidelity data source for training embodied AI agents and generative models, facilitating tasks requiring interaction with articulated objects.  |
| Frac-Connections: Fractional Extension of Hyper-Connections (Read more on [arXiv](https://arxiv.org/abs/2503.14125) or [HuggingFace](https://huggingface.co/papers/2503.14125))| Jundong Zhou, Hongzhi Huang, Defa Zhu, Taoer, FetchFortune | Frac-Connections are introduced as a memory-efficient alternative to Hyper-Connections for deep learning models. The main research objective is to address the seesaw effect between gradient vanishing and representation collapse in residual connections without increasing memory access costs. The key methodology is to divide hidden states into multiple parts (fractional expansion), rather than expanding their width, and construct fractional connection strengths. Primary results show that OLMoE-7B-DFC×4 models achieve a training loss reduction of 0.012 and outperform the baseline by +0.95% on WinoGrande. The principal implication for AI practitioners is that Frac-Connections can improve training stability and downstream task performance in large language models with minimal parameter overhead.  |
| Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal
  Control (Read more on [arXiv](https://arxiv.org/abs/2503.14492) or [HuggingFace](https://huggingface.co/papers/2503.14492))| Tiffany Cai, Maciej Bala, Jose Alvarez, Hassan Abu Alhaija, NVIDIA | Cosmos-Transfer1 is a diffusion-based conditional world model that generates videos based on multiple spatial control inputs with an adaptive weighting scheme. The main research objective is to develop a highly controllable world generation model that can leverage multimodal inputs (segmentation, depth, edge) to produce high-quality and diverse simulations. The key methodology involves adding multiple ControlNet branches to a diffusion transformer-based world model (Cosmos-Predict1), training these branches separately, and fusing them with spatiotemporal control maps during inference. Primary results include a Blur SSIM of 0.87 and a Quality Score of 8.54 on the TransferBench evaluation when using uniform weights across all modalities, outperforming single-modality baselines. Principal implication for AI practitioners is that Cosmos-Transfer1 provides a framework for generating high-fidelity and controllable simulations useful in applications requiring diverse and controllable environments, such as robotics Sim2Real transfer and autonomous vehicle data enrichment, where it achieves a real-time generation of a 5-second video in 4.2 seconds.  |
| MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process
  Errors Identification (Read more on [arXiv](https://arxiv.org/abs/2503.12505) or [HuggingFace](https://huggingface.co/papers/2503.12505))| Kai Wang, Wangbo Zhao, Jiaxin Ai, Pengfei Zhou, Zhaopan Xu | MPBench is a new benchmark for evaluating multimodal process reward models (PRMs) across diverse reasoning tasks. The main research objective is to systematically assess the effectiveness of PRMs in diverse reasoning scenarios using multi-task, multimodal data. The key methodology involves three evaluation paradigms: Step Correctness, Answer Aggregation, and Reasoning Process Search, applied to a dataset of 9,745 instances across six sub-categories. A primary result is that the state-of-the-art model, GPT-4o, achieved an overall score of 71.2, while weaker models like Qwen2.5-VL-3B scored below random chance on some assessments. The principal implication for AI practitioners is that current multimodal PRMs, even advanced ones, struggle with complex reasoning tasks, indicating a need for improved model capacity and training strategies specifically for process-level supervision and multimodal understanding.  |
| Aligning Multimodal LLM with Human Preference: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.14504) or [HuggingFace](https://huggingface.co/papers/2503.14504))| Jinda Lu, Junkang Wu, Chaoyou Fu, Tao Yu, yifanzhang114 | This survey provides a comprehensive and systematic review of alignment algorithms for multimodal large language models (MLLMs). The main research question is how to categorize and understand the current advancements in aligning MLLMs with human preferences, focusing on application scenarios, dataset construction, and evaluation benchmarks. The key methodology involves a systematic literature review, categorizing existing methods based on application scenarios (general image understanding, complex modalities, extended applications), dataset construction factors (data sources, model responses, preference annotations), and evaluation benchmarks. The primary result found 13 benchmarks used in current MLLM alignment research, and no publicly available, fully human-annotated dataset over 200,000 samples. The principal implication for AI practitioners is the need for developing more efficient methods to balance dataset scalability with quality and find new methods that efficiently use visual information in alignment, moving beyond current limitations.  |
| Measuring AI Ability to Complete Long Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.14499) or [HuggingFace](https://huggingface.co/papers/2503.14499))| Katharyn Garcia, Amy Deng, Joel Becker, Ben West, Thomas Kwa | The paper introduces a metric to quantify AI capabilities on long tasks, finding exponential growth in AI task completion time horizon. The main research objective is to quantify AI capabilities in terms of human capabilities, and track the progress. The authors measured human and AI performance on a new dataset of 170 software engineering, cybersecurity, machine learning, and general reasoning tasks, and fit a logistic model to estimate the "50%-task-completion time horizon" for each AI model. Results show the 50% time horizon for frontier AI models like Claude 3.7 Sonnet is around 50 minutes, and has been doubling approximately every seven months since 2019. For AI practitioners, the time horizon metric and trend provide a quantitative framework to assess and forecast AI agent capabilities for performing complex, real-world, long-duration tasks.  |
| Concat-ID: Towards Universal Identity-Preserving Video Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.14151) or [HuggingFace](https://huggingface.co/papers/2503.14151))| Chongxuan Li, Xiaotao Gu, Jiayan Teng, Zhuoyi Yang, Yong Zhong | Concat-ID is a unified framework for identity-preserving video generation that scales to multiple identities and subjects. The main research objective is to develop a framework that achieves a balance between maintaining identity consistency and facial editability in generated videos, without needing extra modules or parameters. The key methodology uses Variational Autoencoders (VAEs) to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms, combined with a cross-video pairing strategy and a multi-stage training regimen. Primary results show that Concat-ID achieves an ArcSim score of 0.442 and a CLIPDist score of 0.325 for single-identity generation, superior to existing methods in both identity consistency and facial editiablity. Principal implication for AI practitioners is that a single and concise model is sufficient to achieve single-identity, multi-identity, and multi-subject preservation in video generation without additional modules.  |
| Temporal Consistency for LLM Reasoning Process Error Identification (Read more on [arXiv](https://arxiv.org/abs/2503.14495) or [HuggingFace](https://huggingface.co/papers/2503.14495))| Xinzhe Juan, Kaixuan Huang, Jiahao Qiu, Yue Wu, Jiacheng Guo | This paper introduces a temporal consistency method to improve large language models' (LLMs) ability to identify errors in mathematical reasoning processes. The main research question is whether leveraging consistency in a sequence of self-reflection actions can improve verification accuracy in identifying mathematical process errors. The key methodology involves iterative self-checking by LLMs, where each LLM reviews its own verification results based on previous assessments until a stable result is achieved. Applying the method to DeepSeek R1 distilled models, improvements of 46.6% on MathCheck*, 37.9% on ProcessBench, and 29.0% on PRM800K with the 8B model. AI practitioners can use this temporal consistency approach to enhance the reliability of LLM-based verification systems, particularly for mathematical reasoning, by incorporating iterative self-reflection to reduce errors.  |
| PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for
  Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12545) or [HuggingFace](https://huggingface.co/papers/2503.12545))| Wangbo Zhao, Jiaxin Ai, Weidong Tang, Pengfei Zhou, Zhaopan Xu | PEBench is a new benchmark for evaluating machine unlearning in multimodal large language models, focusing on personal entities and events. The main research objective is to develop a standardized framework to assess the efficacy of machine unlearning (MU) methods in removing specific visual concepts (identity and event) from Multimodal Large Language Models (MLLMs) while preserving performance on unrelated concepts. The key methodology involves creating a synthetic dataset, PEBench, with 200 fictitious individuals and 40 event scenes, coupled with six MU methods, to evaluate unlearning efficacy, generality, and scope using metrics like precision, ROUGE-L, and G-Eval. A primary result is that while most MU methods achieve nearly 100% efficacy for people unlearning, the ROUGE-L score for event descriptions drops from 0.99 to an average of 0.88, showing an impact from the unlearning process in people to events. AI practitioners can use PEBench to systematically evaluate and improve MU methods for MLLMs, ensuring effective removal of specific concepts without degrading performance on unrelated tasks, particularly in privacy-sensitive applications.  |
| MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.13111) or [HuggingFace](https://huggingface.co/papers/2503.13111))| Justin Lazarow, Haiming Gang, David Griffiths, Nina Wenzel, Erik Daxberger | MM-Spatial introduces a new dataset and benchmark, CA-VQA, to improve 3D spatial understanding in multimodal large language models (MLLMs). The main research objective is to develop an MLLM, MM-Spatial, that excels at 3D spatial reasoning tasks using large-scale 3D scene data. The key methodology involves generating a supervised fine-tuning dataset, CA-VQA, from high-quality 3D scene data, and training MM-Spatial with diverse spatial tasks, metric depth, and multi-view inputs. MM-Spatial achieves state-of-the-art performance on 3D spatial understanding benchmarks, with a 70.1 average score on the CA-VQA spatial category. The principal implication is that AI practitioners can leverage the CA-VQA dataset and MM-Spatial model to enhance MLLMs' 3D spatial reasoning capabilities, crucial for applications like robotics and AR/VR.  |
| Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion
  Transformers via In-Context Reflection (Read more on [arXiv](https://arxiv.org/abs/2503.12271) or [HuggingFace](https://huggingface.co/papers/2503.12271))| Yusuke Kato, Arsh Koneru, Akash Gokul, Konstantinos Kallidromitis, Shufan Li | Reflect-DiT improves text-to-image generation by enabling Diffusion Transformers to iteratively refine outputs using past generations and textual feedback. The main research objective is to develop an inference-time scaling method for text-to-image diffusion models that improves image quality and text alignment without extensive retraining. The methodology, Reflect-DiT, uses a vision-language model to critique generated images and provide textual feedback, which a Diffusion Transformer then uses along with previous generations as in-context examples to refine subsequent outputs. Reflect-DiT achieved a new state-of-the-art score of 0.81 on the GenEval benchmark using only 20 samples per prompt. AI practitioners can use Reflect-DiT to improve the quality and prompt alignment of text-to-image diffusion models during inference, achieving better results with fewer samples compared to best-of-N sampling.  |
| Florenz: Scaling Laws for Systematic Generalization in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.09443) or [HuggingFace](https://huggingface.co/papers/2503.09443))| Sven Behnke, Sebastian Houben, Spravil | Florenz investigates scaling laws for systematic generalization in vision-language models (VLMs) by training monolingual models on multilingual tasks with incomplete data coverage. The main research question is how model size and the number of seen training samples affect a monolingual VLM's ability to generalize to unseen task-language pairs in a multilingual setting. The key methodology involves training a novel encoder-decoder VLM, Florenz, on a synthetic dataset with intentionally missing language coverage for image captioning, using a combination of pre-trained VLM (Florence-2) and LLM (Gemma-2) components. A primary result is that a 30B parameter model could achieve a cross-entropy loss of 2.31 on unseen captioning, and increasing model size has more significant effect on generalizaton than quantity of training samples. This result implies that AI practitioners can potentially achieve cross-lingual transfer in VLMs even with monolingual models by focusing on scaling model size, mitigating the need for exhaustive multilingual data collection for every task.  |
| Pensez: Less Data, Better Reasoning -- Rethinking French LLM (Read more on [arXiv](https://arxiv.org/abs/2503.13661) or [HuggingFace](https://huggingface.co/papers/2503.13661))| HoangHa | Pensez 7B, a bilingual English-French language model, demonstrates competitive reasoning performance with significantly less training data than comparable models. The main research question is whether strategic fine-tuning on a small, high-quality, bilingual dataset can enhance both the reasoning capabilities and French language proficiency of a large language model. The key methodology involves supervised fine-tuning of a Qwen2.5 7B Instruct base model on a curated 2,000-example bilingual (English-French) dataset, emphasizing data quality, diversity, and explicit reasoning chains. Pensez 7B achieves a 12-point accuracy increase on a French MATH level 5 benchmark compared to the base model. The principal implication is that AI practitioners can achieve strong reasoning performance in LLMs with focused, high-quality datasets, reducing reliance on massive, resource-intensive training corpora.  |
| Hyperbolic Safety-Aware Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12127) or [HuggingFace](https://huggingface.co/papers/2503.12127))| Rita Cucchiara, Lorenzo Baraldi, Pascal Mettes, Tejaswi Kasarla, tobi1modna | HySAC introduces a novel approach to address unsafe content in vision-language models (VLMs) using hyperbolic space. The main research objective is to develop a VLM that can distinguish between safe and unsafe content without unlearning unsafe concepts, enabling controlled retrieval and classification. The key methodology involves encoding safe and unsafe image-text pairs in a hyperbolic space, employing entailment loss functions to model hierarchical relationships, and using a traversal mechanism to adjust query embeddings for safe or unsafe retrieval. Primary results show that HySAC achieves a recall of 49.8% at R@1 and 90.7% at R@20 for safe content retrieval on the ViSU test set, outperforming existing safety-unlearning CLIP and hyperbolic CLIP models. AI practitioners can use HySAC to build VLMs with enhanced safety awareness, allowing for dynamic control over content moderation and safer retrieval by design without removing information.  |
| KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for
  Open-Vocabulary Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2503.10546) or [HuggingFace](https://huggingface.co/papers/2503.10546))| Yunzhu Li, Mingtong Zhang, Zixian Liu | KUDA is an open-vocabulary robotic manipulation system that integrates visual prompting and dynamics learning through a unified keypoint representation. The main research objective is to develop a system that can perform complex manipulation tasks based on free-form language instructions while accounting for object dynamics. The key methodology involves using a vision-language model (VLM) to generate keypoint-based target specifications from language instructions and RGBD observations, and then employing model-based planning with a learned dynamics model to achieve the specified goals. The system achieved an 80.0% success rate across 60 trials on various manipulation tasks, significantly outperforming baseline methods. AI practitioners can leverage KUDA's unified keypoint representation to bridge vision-language models and dynamics models, enabling more flexible and robust robotic manipulation systems that can handle a wider variety of objects and tasks.  |
| RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2503.10410) or [HuggingFace](https://huggingface.co/papers/2503.10410))| Junhao Ge, Yifan Lu, Zichen Chao, Anning Hu, yuwendu | RoCo-Sim is a simulation framework for improving roadside collaborative perception by generating diverse, multi-view consistent simulated data. The main research objective is to address data limitations in roadside collaborative perception, such as calibration errors, sparse data, and multi-view inconsistency, by developing a simulation framework. The key methodology involves using dynamic foreground editing and full-scene style transfer of single images, Camera Extrinsic Optimization, a Multi-View Occlusion-Aware Sampler (MOAS), DepthSAM, and a Scalable Post-Processing Toolkit. RoCo-Sim outperforms state-of-the-art methods on the Rcooper-Intersection dataset by 83.74% for AP70. AI practitioners can use RoCo-Sim to generate realistic and diverse roadside perception datasets, substantially enhancing the performance of camera-only 3D detection models without needing extensive real-world data collection or model architecture changes.  |
