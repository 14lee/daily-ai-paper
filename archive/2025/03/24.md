

## Papers for 2025-03-24

| Title | Authors | Summary |
|-------|---------|---------|
| MAPS: A Multi-Agent Framework Based on Big Seven Personality and
  Socratic Guidance for Multimodal Scientific Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2503.16905) or [HuggingFace](https://huggingface.co/papers/2503.16905))| Xinyu Zhang, Zhangqi Wang, Zhiyuan Wang, Qika, VentureZJ | MAPS is a multi-agent framework for multimodal scientific problem-solving, leveraging the Big Seven Personality theory and Socratic questioning to improve reasoning and reflection in AI systems. The main research question is how to leverage and elicit off-the-shelf Multimodal Large Language Models (MLLMs) to address challenging Multimodal Scientific Problems (MSPs). The key methodology involves a multi-agent framework with seven distinct agents, each based on a Big Seven personality trait, using a progressive four-agent solving strategy and a Critic agent for Socratic feedback. The primary results show that MAPS outperforms the current state-of-the-art model by 15.84% across all tasks on the EMMA, Olympiad, and MathVista datasets, and slightly exceeds human expert by 3.58%. The principal implication is that AI practitioners can use this framework to enhance multi-model comprehensive reasoning and provide continous feedback mechanism to improve the accuracy in complex, multimodal scientific problem-solving scenarios.  |
| MARS: A Multi-Agent Framework Incorporating Socratic Guidance for
  Automated Prompt Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.16874) or [HuggingFace](https://huggingface.co/papers/2503.16874))| Jun Liu, Haiping Zhu, Zhangqi Wang, Qika, VentureZJ | MARS is a multi-agent framework for automated prompt optimization (APO) that uses Socratic guidance and autonomous planning. The main research objective is to address the limited flexibility of fixed templates and inefficient search in prompt spaces that are present in existing APO methods. The key methodology involves a multi-agent architecture with seven agents, including a Planner, and a Teacher-Critic-Student Socratic dialogue pattern for iterative prompt refinement. Primary results show that MARS outperforms the previous state-of-the-art by 6.04% on general tasks and achieves 85.11% accuracy on 12 general tasks. The use of MARS can help AI practitioners by enabling more efficient and precise prompt refinement, leading to better performance of LLMs across various tasks without needing to create complex meta prompts.  |
| RoboFactory: Exploring Embodied Agent Collaboration with Compositional
  Constraints (Read more on [arXiv](https://arxiv.org/abs/2503.16408) or [HuggingFace](https://huggingface.co/papers/2503.16408))| Xiaohong Liu, Zhenfei Yin, Xiufeng Song, FACEONG, IranQin | RoboFactory introduces a framework for generating safe and efficient collaborative data for multi-agent embodied systems using compositional constraints. The main research objective is to address the challenges of multi-agent collaboration in embodied systems by proposing and validating a compositional constraint-based approach. The key methodology involves using a large language model (RoboBrain) to generate sub-goals and textual constraints, constructing constraint interfaces (RoboChecker) to ensure adherence, and generating trajectories using predefined motion primitives. Primary results show that in tasks involving three agents, an average success rate of 20.5% was achieved using diffusion policy with 150 demonstrations, and the use of a "local view" with "separate policy" improves task success rates for the "Food Place" task from 0% to 20% in imitation learning when compared with a "shared policy". The principal implication for AI practitioners is that they can use RoboFactory's compositional constraints and automated data collection framework to develop and evaluate multi-agent manipulation systems more efficiently.  |
| When Less is Enough: Adaptive Token Reduction for Efficient Image
  Representation (Read more on [arXiv](https://arxiv.org/abs/2503.16660) or [HuggingFace](https://huggingface.co/papers/2503.16660))| Andrey Kuznetsov, Elizaveta Goncharova, Eduard Allakhverdov | This paper introduces an adaptive token reduction method for vision encoders to improve efficiency without compromising performance. The main research objective is to determine if all visual tokens generated by vision encoders are equally valuable, or if some can be discarded to reduce computational costs. The key methodology involves integrating an autoencoder with a Gumbel-Softmax selection mechanism to identify and retain only the most informative visual tokens, based on reconstructability. Primary results show that on OCR-based tasks, over 50% of the visual context can be removed with minimal performance loss using the LLaVA-NeXT model. Principal implication for AI practitioners is that multimodal pruning can be adaptively performed, facilitating scalable and low-overhead inference without requiring additional model fine-tuning.  |
| Bridging Continuous and Discrete Tokens for Autoregressive Visual
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.16430) or [HuggingFace](https://huggingface.co/papers/2503.16430))| Yuanzhi Zhu, Yao Teng, Zhijie Lin, ShuhuaiRen, Epiphqny | TokenBridge bridges continuous and discrete token representations for autoregressive visual generation, achieving high-quality image synthesis with simplified modeling. The main objective is to maintain the representational capacity of continuous tokens while preserving the modeling simplicity of discrete tokens in autoregressive visual generation. The key methodology is post-training quantization of pre-trained continuous VAE features using a dimension-wise quantization strategy, paired with a lightweight autoregressive prediction mechanism for large token spaces. The proposed method achieved an FID score of 1.55 and an IS of 313.3 on ImageNet 256x256, matching state-of-the-art continuous approaches while still using discrete token prediction. AI practitioners can leverage this approach to build high-quality autoregressive visual generation models using standard categorical prediction, bypassing the complexity of continuous distribution modeling, without compromising image quality.  |
| OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning
  via Iterative Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2503.17352) or [HuggingFace](https://huggingface.co/papers/2503.17352))| Wei Wang, Nanyun Peng, Fan Yin, Hritik Bansal, Yihe Deng | OpenVLThinker explores iteratively improving vision-language reasoning in large vision-language models (LVLMs) through a combination of supervised fine-tuning (SFT) and reinforcement learning (RL). The main research objective is to investigate whether complex reasoning capabilities, similar to those in large language models, can be integrated into LVLMs and improve performance on multimodal reasoning tasks. The key methodology involves iterative SFT and RL, with each iteration's RL-improved model generating refined SFT datasets for the next round, using distilled reasoning steps from text-only models. Primary results show that OpenVLThinker-7B achieved 70.2% accuracy on MathVista, surpassing the Qwen2.5-VL-7B baseline of 68.5%. Principal implication for AI practioners that is combining SFT with verifiable RL can enhance the multi-step reasoning in LVLMs.  |
| Modifying Large Language Model Post-Training for Diverse Creative
  Writing (Read more on [arXiv](https://arxiv.org/abs/2503.17126) or [HuggingFace](https://huggingface.co/papers/2503.17126))| Max Kreminski, Yuqian Sun, Melissa Roemmele, Vishakh Padmakumar, John Joon Young Chung | The paper introduces a post-training approach that modifies large language models (LLMs) to improve output diversity in creative writing while maintaining quality. The primary objective is to enhance LLM output diversity during creative writing tasks by incorporating "deviation" (difference from other outputs for the same prompt) into the training objective. The methodology involves adapting Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) by weighting training instances with the deviation of the winning response. Results showed that a Llama-3.1-8B-based diversified DPO model achieved on-par diversity with a human-created dataset and output quality similar to instruction-tuned models like GPT-4o. AI practitioners can leverage this approach to promote output diversity in creative writing LLMs, balancing diverse and high-quality outputs by incorporating the instance deviation during post-training.  |
| ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question
  Generation and Answering (Read more on [arXiv](https://arxiv.org/abs/2503.16867) or [HuggingFace](https://huggingface.co/papers/2503.16867))| Wei Liu, Peng Zhang, Yuchong Sun, Zhengfeng Lai, Guan123 | ETVA is a new method for evaluating text-to-video alignment using question generation and answering. The main research objective is to develop a more accurate and fine-grained evaluation metric for text-to-video (T2V) alignment than existing methods. The key methodology involves a multi-agent system for generating atomic questions from text prompts using scene graphs and a knowledge-augmented multi-stage reasoning framework for answering questions about generated videos. Primary results show that ETVA achieves a Spearman's correlation coefficient of 58.47 with human judgment, significantly outperforming existing metrics like VideoScore (31.0). Principal implication is that AI practitioners can use ETVA and its associated benchmark (ETVABench) for more reliable and human-aligned evaluation of text-to-video generation models, focusing improvements on fine-grained semantic alignment.  |
| Single Image Iterative Subject-driven Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.16025) or [HuggingFace](https://huggingface.co/papers/2503.16025))| Idan Schwartz, Gal Chechik, yairshp | SISO is a training-free method for personalizing image generation and editing using only a single subject image. The main objective is to develop a method for subject-driven image generation and editing from a single image without requiring encoder pre-training. SISO iteratively optimizes a similarity score between the generated image and the input subject image using pre-trained models like DINO and IR. The method achieved a CMMD score of 0.18 in image generation on a benchmark dataset, improving prompt adherence while maintaining image fidelity compared to baselines. AI practitioners can use SISO as a plug-and-play optimization technique for existing image generators, enabling efficient single-image personalization without extensive training.  |
| MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical
  Problems (Read more on [arXiv](https://arxiv.org/abs/2503.16549) or [HuggingFace](https://huggingface.co/papers/2503.16549))| Jun Cen, Tao Feng, Yunqiu Xu, Felix Chen, JacobYuan | MathFlow decouples visual mathematical problem-solving in Multimodal Large Language Models (MLLMs) into perception and inference stages, improving performance. The main research objective is to evaluate and enhance MLLMs' ability to accurately perceive and interpret diagrams in visual mathematical problems. The key methodology involves creating a new benchmark, FlowVerse, to categorize information components, and developing MathFlow, a modular pipeline with a dedicated perception model (MathFlow-P-7B) trained via multi-task pretraining and supervised fine-tuning. Primary results indicate that MathFlow*GPT-4V achieved a 56.7% accuracy on MathVerse's testmini set, and integrated MathFlow-P-7B yields substantial performance gains with various inference models. For AI practitioners, MathFlow offers a modular problem-solving pipeline that enhances the model's mathematical problem understanding and solving ability by decoupling the perception and inference process.  |
| Enabling Versatile Controls for Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.16983) or [HuggingFace](https://huggingface.co/papers/2503.16983))| Jiaxing Yan, Xiaobin Lu, Haoming Qin, Hao Zhou, Xu Zhang | VCtrl is a unified framework for fine-grained control over pre-trained video diffusion models using diverse control signals. The main research objective is to enable precise and flexible spatiotemporal control in text-to-video generation, addressing limitations of existing methods. The key methodology involves a unified control signal encoding pipeline and a sparse residual connection mechanism, integrated with a conditional module, to handle various control signals (Canny edges, segmentation masks, human keypoints) without modifying the base generator. Results demonstrate that, on the Canny-to-Video task, VCtrl-Canny achieves a Canny Matching score of 0.24 and an FVD score of 985.31. For AI practitioners, VCtrl provides a generalizable and efficient way to incorporate diverse user-specified controls into existing video diffusion models, improving controllability and generation quality.  |
| When Preferences Diverge: Aligning Diffusion Models with Minority-Aware
  Adaptive DPO (Read more on [arXiv](https://arxiv.org/abs/2503.16921) or [HuggingFace](https://huggingface.co/papers/2503.16921))| Donghao Luo, Kai Hu, Chengming Xu, Chen Liu, Lingfan Zhang | This paper proposes Adaptive-DPO, a novel approach to align diffusion models with human preferences, addressing the challenge of minority samples in preference datasets. The main research question is how to mitigate the detrimental effects of minority preference samples (erroneous annotations and subjective divergences) on diffusion model alignment. The key methodology is a minority-instance-aware metric incorporating intra-annotator confidence and inter-annotator stability, used to adaptively reweight and adjust the DPO loss function. Primary results show that Adaptive-DPO outperforms standard DPO; for example it is found that on SD1.5 with 20% flipped labels, Adaptive-DPO achieves an ImageReward of 0.34, while DPO achieves 0.00. The principal implication for AI practitioners is that incorporating Adaptive-DPO can improve the robustness and effectiveness of preference learning in text-to-image generation tasks, especially in the presence of noisy or subjective preference data.  |
| FastCuRL: Curriculum Reinforcement Learning with Progressive Context
  Extension for Efficient Training R1-like Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.17287) or [HuggingFace](https://huggingface.co/papers/2503.17287))| Xuan Luo, Wenjie Yang, Zheng Li, Mao Zheng, Mingyang Song | FASTCURL accelerates reinforcement learning for reasoning models by segmenting training data and progressively extending the context window. The main objective is to improve the training efficiency and performance of R1-like reasoning models, particularly with a 1.5B parameter language model, in tackling complex reasoning tasks. The key methodology, FASTCURL, involves length-aware training data segmentation based on input prompt length and curriculum reinforcement learning with a progressively increasing context window. FASTCURL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across five benchmark datasets while using only 50% of the training steps. For AI practitioners, FASTCURL demonstrates a practical and efficient strategy of segmenting training dataset, and applying curriculum reinforcement learning to reduce training resources (by 50% in training steps, the paper illustrates) for R1-like large language models.  |
| From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration (Read more on [arXiv](https://arxiv.org/abs/2503.12821) or [HuggingFace](https://huggingface.co/papers/2503.12821))| Yu Cheng, Jiawei Zhou, Xiaoye Qu, hitsmy | Here's a concise summary of the research paper, adhering to your guidelines:  The paper introduces an Adaptive Data Refinement (ADR) framework to address the long-tail data distribution problem in Large Vision-Language Models (LVLMs). The main research objective is to investigate and mitigate the impact of imbalanced training data on the performance of LVLMs. The key methodology involves a two-stage approach: Data Rebalancing (DR), which filters redundant head data, and Data Synthesis (DS), which uses diffusion models to generate scarce tail data. Primary results show that ADR improves the average performance of LLaVA 1.5 by 4.36% across eleven benchmarks without increasing training data volume. Principal implication for AI practitioners is, ADR can be integrated into existing LVLMs to improve their performance on tasks with long-tail data distributions, enhancing robustness and generalization capabilities.  |
| PVChat: Personalized Video Chat with One-Shot Learning (Read more on [arXiv](https://arxiv.org/abs/2503.17069) or [HuggingFace](https://huggingface.co/papers/2503.17069))| Yuchen Li, Yumeng Li, Gang Xu, Weilong Yan, Master-Shi | PVChat is a personalized video large language model capable of subject-aware question answering from a single reference video. The main research objective is to develop a ViLLM that can understand and answer questions about specific individuals in videos after learning from only one video of each individual. The key methodology involves a Mixture-of-Heads (MoH) enhanced ViLLM optimized on a synthetically augmented video-QA dataset, using a progressive image-to-video learning strategy, and a ReLU Routing MoH attention mechanism. The primary result is that PVChat achieved an accuracy of 0.901, a BLEU score of 0.562, and a BERTScore of 0.952, outperforming state-of-the-art ViLLMs in personalized feature understanding. For AI practitioners, PVChat offers a framework for building video understanding models that can learn individual-specific information from minimal data, enabling more personalized applications in areas such as smart healthcare and home environments.  |
| Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model (Read more on [arXiv](https://arxiv.org/abs/2503.16282) or [HuggingFace](https://huggingface.co/papers/2503.16282))| Junlin Han, Runjia Li, Yun Liu, Guolei Sun, Zhaochong An | GFS-VL enhances generalized few-shot 3D point cloud segmentation (GFS-PCS) by integrating 3D vision-language models (VLMs) and few-shot samples. The main research objective is to improve the performance of GFS-PCS models in segmenting both base and novel object classes, particularly when limited labeled data is available for novel classes. The key methodology involves using a 3D VLM to generate pseudo-labels for novel classes, filtering these pseudo-labels with few-shot samples for accuracy, adaptively infilling unlabeled regions using a combination of pseudo-label context and few-shot data, and employing a novel-base mix strategy for data augmentation. The primary results show that on the ScanNet200 benchmark, GFS-VL achieves a 28.57% increase in harmonic mean (HM) and a 23.37% increase in mIoU-N over the existing state-of-the-art GFS-PCS methods for the 5-shot setting. The principal implication is that AI practitioners can leverage the combined strengths of 3D VLMs' open-world knowledge and the precision of few-shot samples to achieve significantly improved segmentation in scenarios where acquiring large labeled datasets for new object classes is impractical.  |
| Implicit Bias-Like Patterns in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.11572) or [HuggingFace](https://huggingface.co/papers/2503.11572))| Calvin K. Lai, l048596 | Reasoning models exhibit processing differences for association-compatible versus incompatible information, similar to human implicit bias. The research examined whether reasoning models show implicit bias-like patterns by expending differential computational effort on association-compatible versus incompatible information. The researchers adapted the Implicit Association Test (IAT) for reasoning models, called RM-IAT, measuring the number of reasoning tokens generated via API calls to OpenAI's `03-mini` model for different association tasks. The model generated significantly more reasoning tokens in the association-incompatible condition than the association-compatible condition in nine of ten RM-IATs; for example, the Instruments/Weapons + Pleasant/Unpleasant RM-IAT generated, on average, 84.29 more tokens in the incompatiable vs. compatiable condition.. AI practitioners should consider that reasoning models may have implicit bias-like patterns that increase computational effort when processing association-incompatible information, impacting efficiency and potentially leading to subtle biases.  |
| FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields (Read more on [arXiv](https://arxiv.org/abs/2503.17095) or [HuggingFace](https://huggingface.co/papers/2503.17095))| Junyong Noh, Hangyeul Shin, Chaelin Kim, Kwan Yun | FFaceNeRF is a NeRF-based method for 3D-aware face editing that enables customization with few-shot training on desired mask layouts. The main research objective is to overcome the limitation of existing mask-based 3D face editing methods that rely on pre-trained segmentation masks with fixed layouts. The key methodology involves a geometry adapter with feature injection and latent mixing for tri-plane augmentation (LMTA) to enable adapting to various mask layouts using few training samples. The proposed method achieved an average mIoU of 85.33% for mask generation on a test set, outperforming NeRFFaceEditing's 81.37%. For AI practitioners, FFaceNeRF facilitates personalized and detailed 3D face editing with limited data, reducing the dependency on extensive, specifically segmented datasets.  |
| TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented
  Reality via 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2503.17032) or [HuggingFace](https://huggingface.co/papers/2503.17032))| Tiansong Zhou, Zhonghua Jiang, Gaige Wang, Jingchuan Hu, Jianchuan Chen | TaoAvatar generates photorealistic, full-body avatars from multi-view sequences for real-time AR applications. The research objective is to create high-fidelity, lightweight, and drivable full-body talking avatars that can run in real-time on mobile and AR devices. The key methodology combines 3D Gaussian Splatting (3DGS) with a personalized clothed human parametric template (SMPLX++), using a teacher-student framework with non-rigid deformation baking and blend shapes compensation. The primary result is that TaoAvatar achieves state-of-the-art rendering quality, maintaining 90 FPS on high-definition stereo devices like the Apple Vision Pro at 2K resolution. For AI practitioners, TaoAvatar provides a lightweight and efficient approach for representing and rendering lifelike full-body avatars directly deployable to resource-constrained AR environments and mobile devices.  |
