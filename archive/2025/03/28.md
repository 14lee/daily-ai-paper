

## Papers for 2025-03-28

| Title | Authors | Summary |
|-------|---------|---------|
| Video-R1: Reinforcing Video Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2503.21776) or [HuggingFace](https://huggingface.co/papers/2503.21776))| Potentialts, guozonghao96, BreakLee, kxgong, KaituoFeng | Video-R1 introduces a rule-based reinforcement learning framework to enhance video reasoning capabilities within Multimodal Large Language Models (MLLMs). The primary objective is to adapt the R1 reasoning paradigm for video by addressing the lack of explicit temporal modeling in standard RL algorithms and the scarcity of high-quality video reasoning data. The methodology involves proposing the Temporal Group Relative Policy Optimization (T-GRPO) algorithm, which contrasts performance on ordered versus shuffled video frames, and utilizing curated hybrid datasets (Video-R1-COT-165k, Video-R1-260k) combining image and video reasoning samples. Key results show significant improvements across video benchmarks, notably achieving 35.8% accuracy on VSI-Bench with the 7B model, surpassing the proprietary GPT-4o model. For AI practitioners, this research demonstrates that temporal-aware RL algorithms like T-GRPO, coupled with hybrid image-video data, offer an effective approach to improve complex temporal reasoning in MLLMs for video understanding applications. |
| UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.21620) or [HuggingFace](https://huggingface.co/papers/2503.21620))| Xi Yin, hsli-cuhk, guoyaxuan0106, Yuxiang007, LZXzju | This paper introduces UI-R1, leveraging rule-based reinforcement learning (RL) to enhance graphical user interface (GUI) action prediction for multimodal large language models (MLLMs). The main objective was to investigate if rule-based RL could improve MLLM reasoning capabilities for GUI action prediction using significantly less data than supervised fine-tuning (SFT). Key methodology involved curating a 136-sample mobile GUI task dataset, designing a unified rule-based reward function for action type and coordinate accuracy, and applying Group Relative Policy Optimization (GRPO) for reinforcement fine-tuning (RFT) on a Qwen2.5-VL-3B model. The primary result showed UI-R1-3B improved action type accuracy by 15% and grounding accuracy by 10.3% on the in-domain ANDROIDCONTROL benchmark compared to its base model, while using only 136 training samples, and achieved competitive out-of-domain performance against larger SFT models trained on 76K data. The principal implication for AI practitioners is that rule-based RFT presents a highly data-efficient method for improving GUI agent performance and generalization, offering a viable alternative to large-scale SFT, particularly in resource-constrained or OOD scenarios. |
| Challenging the Boundaries of Reasoning: An Olympiad-Level Math
  Benchmark for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.21380) or [HuggingFace](https://huggingface.co/papers/2503.21380))| Wayne Xin Zhao, jrwen, TimothyCzp, EliverQ, CoderBak | This paper introduces OlymMATH, a new bilingual Olympiad-level mathematical benchmark designed to rigorously evaluate the complex reasoning capabilities of large language models (LLMs). The primary objective is to address the saturation of existing math reasoning benchmarks by providing a more challenging test set derived from manually verified, non-digital sources. The methodology involved curating 200 problems (split into AIME-level easy and harder Olympiad-level tiers) across four mathematical fields, providing parallel English and Chinese versions with verifiable numerical answers. Empirical results show state-of-the-art models like DeepSeek-R1 achieve low accuracy (21.2% Pass@1) on the OlymMATH-EN-HARD subset, indicating significant limitations in current LLM reasoning. For AI practitioners, OlymMATH serves as a demanding benchmark to better differentiate advanced reasoning models and identify weaknesses, such as reliance on heuristics over rigorous derivation, guiding the development of more robust mathematical problem-solving capabilities. |
| VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic
  Faithfulness (Read more on [arXiv](https://arxiv.org/abs/2503.21755) or [HuggingFace](https://huggingface.co/papers/2503.21755))| mimihe, yinanhe, jackyhate, HongboLiu, Ziqi | VBench-2.0 introduces an automated benchmark suite designed to evaluate the intrinsic faithfulness of video generation models, moving beyond superficial quality assessments. Its primary objective is to systematically measure adherence to principles like physics, commonsense reasoning, human fidelity, controllability, and creativity across 18 fine-grained dimensions. The methodology integrates Vision-Language Models (VLMs) and Large Language Models (LLMs) through text description alignment and video-based multi-question answering, alongside specialist detectors and heuristics, validated via human preference annotations. Evaluations reveal current state-of-the-art models struggle significantly with complex plot generation (~10-12% scores) and dynamic attribute control (~8-24% scores), although VBench-2.0's automated metrics show strong alignment with human judgment (Spearman's ρ > 0.8 across most dimensions). For AI practitioners, VBench-2.0 provides a standardized framework to assess and guide the development of video generation models towards greater realism and adherence to world principles, crucial for applications requiring simulation and complex scene understanding. |
| LeX-Art: Rethinking Text Generation via Scalable High-Quality Data
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.21749) or [HuggingFace](https://huggingface.co/papers/2503.21749))| Dakerqi, afdsafas, Xxxy13, QJerry, stzhao | LeX-Art introduces a data-centric framework using scalable high-quality synthesis to improve visual text rendering in text-to-image (T2I) generation. The main objective is to bridge the gap between prompt expressiveness and text rendering fidelity by enhancing data quality and fine-tuning models, rather than relying solely on control-based architectural changes. The methodology involves using DeepSeek-R1 for prompt enrichment, generating the LeX-10K dataset (10K 1024x1024 images) via multi-stage filtering, developing the LeX-Enhancer prompt model, fine-tuning LeX-FLUX and LeX-Lumina T2I models, and introducing the LeX-Bench benchmark and PNED metric for evaluation. Primary results demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain (indicating better text accuracy) on CreateBench compared to its baseline. For AI practitioners, the principal implication is that this scalable, data-centric approach, leveraging high-quality synthetic data and prompt enhancement, offers an effective method to substantially improve text rendering fidelity and aesthetics in T2I models without requiring complex model architecture modifications. |
| Large Language Model Agent: A Survey on Methodology, Applications and
  Challenges (Read more on [arXiv](https://arxiv.org/abs/2503.21460) or [HuggingFace](https://huggingface.co/papers/2503.21460))| qqlong, joeyleo, evan-gyy, yszhao, luojunyu | This survey systematically reviews Large Language Model (LLM) agents, covering their methodologies, applications, and challenges. The primary objective is to deconstruct LLM agent systems through a methodology-centered taxonomy, linking architectural foundations (construction), interaction mechanisms (collaboration), and improvement pathways (evolution). It employs a tripartite framework analyzing agent construction (profile, memory, planning, action execution), collaboration paradigms (centralized, decentralized, hybrid), and evolution mechanisms (autonomous learning, co-evolution, external resources), complemented by analysis of evaluation, tools, real-world issues, and applications. The survey provides a unified architectural perspective, identifies significant challenges including scalability, memory constraints, reliability, and evaluation complexity, and offers a structured understanding distinct from prior works focusing on isolated aspects. For AI practitioners, this work delivers a comprehensive taxonomy and framework for understanding the design principles, lifecycle, and practical considerations crucial for developing and deploying robust LLM agent systems. |
| Lumina-Image 2.0: A Unified and Efficient Image Generative Framework (Read more on [arXiv](https://arxiv.org/abs/2503.21758) or [HuggingFace](https://huggingface.co/papers/2503.21758))| luyiting, Paper99, RuoyiDu, JackyZhuo, Dakerqi | Lumina-Image 2.0 introduces a unified and efficient text-to-image generation framework improving upon Lumina-Next. The main objective is to enhance image fidelity, prompt adherence, and generation efficiency through architectural unification and improved training data. Key methodologies include the Unified Next-DiT architecture for joint text-image token processing, the Unified Captioner (UniCap) for generating high-quality, multi-granularity captions, multi-stage progressive training, and inference optimizations like CFG-Renormalization and CFG-Truncation. Lumina-Image 2.0 achieves strong performance, scoring 87.20 on the DPG benchmark with only 2.6B parameters, demonstrating superior efficiency and scalability compared to prior models. For AI practitioners, this work presents an efficient (2.6B parameters) and unified transformer architecture applicable beyond T2I, alongside a specialized captioning system (UniCap) that significantly improves training data quality and model convergence, offering a practical approach to building performant generative models. |
| ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large
  Reasoning Models with Iterative Retrieval Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2503.21729) or [HuggingFace](https://huggingface.co/papers/2503.21729))| chenyn66, liuweichuan, NeoZ123, caoshulin, ZhiCheng0326 | ReaRAG enhances Large Reasoning Model (LRM) factuality for multi-hop QA using iterative, knowledge-guided Retrieval-Augmented Generation (RAG) with reflective reasoning. The objective is to improve LRM factual accuracy on complex QA tasks by mitigating reliance on parametric knowledge and issues like overthinking and error propagation found in prior iterative RAG and RL-based approaches. The methodology involves constructing a dataset with bounded reasoning chains, fine-tuning ReaRAG-9B (based on GLM-4-9B) using a Thought-Action-Observation paradigm, iteratively querying a RAG engine, and employing reflection to refine the reasoning trajectory. ReaRAG-9B significantly outperforms baselines on multi-hop QA benchmarks, achieving a 14.5% ACCL improvement over SearChain on MuSiQue (66.00 vs 51.50 ACCL). For AI practitioners, ReaRAG provides a fine-tuning framework and inference strategy to build more factually reliable QA systems by effectively integrating iterative external knowledge retrieval and explicit reasoning steps, reducing errors compared to solely prompt-based or single-retrieval RAG methods. |
| Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.21696) or [HuggingFace](https://huggingface.co/papers/2503.21696))| Guiyang1001, tricktreat, yijiang, Gangao, zwq2018 | This paper presents Embodied-Reasoner, a model extending `ol`-style reasoning to interactive embodied search tasks by generating and learning from coherent Observation-Thought-Action trajectories. The primary objective is to enhance reasoning capabilities for embodied agents facing challenges like continuous multimodal interaction, spatial understanding, temporal reasoning, and self-reflection based on interaction history. Key methodology involves synthesizing 9.3k trajectories featuring diverse thinking processes (e.g., analysis, spatial reasoning, reflection) and employing a three-stage training pipeline comprising imitation learning, self-exploration via rejection sampling, and self-correction via reflection tuning. Results demonstrate significant improvements over advanced visual reasoning models, with Embodied-Reasoner exceeding OpenAI GPT-o1 by +9% and GPT-o3-mini by +24% in success rate, showing fewer repeated searches and better consistency on long-horizon tasks. For AI practitioners, this work provides a data synthesis and training framework to develop embodied agents with enhanced planning, reasoning, and interaction capabilities, particularly for complex tasks requiring adaptive behavior based on visual feedback and interaction history. |
| ResearchBench: Benchmarking LLMs in Scientific Discovery via
  Inspiration-Based Task Decomposition (Read more on [arXiv](https://arxiv.org/abs/2503.21248) or [HuggingFace](https://huggingface.co/papers/2503.21248))| yuqiangli, bgao22182, jinjieni, ZonglinY, yujieliu | ResearchBench introduces a benchmark for evaluating Large Language Models (LLMs) in scientific discovery by decomposing the process into inspiration retrieval, hypothesis composition, and ranking. The objective is to assess LLM performance on these fundamental sub-tasks using recent, contamination-resistant scientific literature across 12 disciplines. An automated LLM-based agentic framework extracts research components (questions, background, inspirations, hypotheses) from 1386 papers published in 2024, forming the basis for evaluation, including carefully selected negative examples for retrieval tasks. Results show LLMs excel at the out-of-distribution inspiration retrieval task (GPT-4o hit ratio: 45.65% for top 4% candidates), while hypothesis composition and ranking show moderate capabilities with potential for improvement; ranking is notably affected by position bias. For AI practitioners, this indicates LLMs can serve as "research hypothesis mines" capable of surfacing novel knowledge associations for automated discovery, though the bottleneck in retrieval suggests a reliance on pretraining depth over post-training refinement. |
| Optimal Stepsize for Diffusion Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.21774) or [HuggingFace](https://huggingface.co/papers/2503.21774))| Han Hu, Jianning Pei, cientgu | This paper introduces Optimal Stepsize Distillation (OSS), a dynamic programming framework to derive theoretically optimal stepsize schedules for accelerating diffusion model sampling. The objective is to overcome suboptimal discretization in diffusion sampling by focusing on principled stepsize schedule design, rather than solely optimizing update directions. OSS treats stepsize optimization as knowledge distillation, using dynamic programming to recursively minimize the global discretization error between a few-step student sampler and a many-step teacher reference trajectory. Experiments demonstrate that OSS enables significant acceleration, achieving 10x speedup for text-to-image generation while maintaining 99.4% of the teacher model's performance on the GenEval benchmark. For AI practitioners, OSS provides a robust, architecture-agnostic method to drastically reduce diffusion model inference latency with minimal performance loss, enabling more efficient deployment. |
| Exploring the Evolution of Physics Cognition in Video Generation: A
  Survey (Read more on [arXiv](https://arxiv.org/abs/2503.21765) or [HuggingFace](https://huggingface.co/papers/2503.21765))| huangsiteng, wangcunxiang, huangsiteng, yishanwang, minnielin | This survey reviews the integration of physical cognition into video generation models, organizing advancements along an evolutionary path inspired by human cognitive development. The main objective is to systematically categorize methods for improving physical fidelity in generated videos, addressing the gap between visual realism and physical plausibility. The paper proposes a three-tier taxonomy (Basic Schematic Perception, Passive Cognition, Active Cognition) to classify techniques like motion-guided generation, physics-inspired regularization, simulation integration, and LLM-based reasoning. Despite progress, the survey highlights that even state-of-the-art models often violate fundamental physical laws, generating visually appealing but physically inconsistent results, as evidenced by evaluations on benchmarks like PhyGenBench [86] and Physics-IQ [84]. For AI practitioners, this implies that achieving physically plausible video generation, essential for applications like robotics and simulation, requires moving beyond visual mimicry towards integrating explicit physical knowledge and interaction mechanisms. |
| ChatAnyone: Stylized Real-time Portrait Video Generation with
  Hierarchical Motion Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2503.21144) or [HuggingFace](https://huggingface.co/papers/2503.21144))| Peng Zhang, Chaonan Ji, Jinwei Qi, Liefeng, shengxu97 | ChatAnyone introduces a novel framework for generating stylized, real-time upper-body portrait videos from audio using a hierarchical motion diffusion model and hybrid control fusion GAN. The primary objective is to create expressive digital humans with synchronized facial expressions, head poses, and upper-body movements including hands, enabling fine-grained style control. The methodology involves a two-stage process: first, hierarchical motion diffusion models predict explicit and implicit motion representations from audio and optional style references; second, a warping-based GAN synthesizes the video using these representations, injected hand controls, and a face refinement module. Key results demonstrate real-time performance (up to 30fps at 512x768 on a 4090 GPU) and improved quantitative metrics, such as achieving a PSNR of 24.88 in self-reenactment, significantly outperforming prior GAN-based methods. For AI practitioners, this provides an effective approach for developing highly expressive, controllable, and real-time digital avatars for interactive applications like video chat and virtual assistants, demonstrating the power of combining diffusion models for motion generation with GANs for efficient synthesis. |
| FinAudio: A Benchmark for Audio Large Language Models in Financial
  Applications (Read more on [arXiv](https://arxiv.org/abs/2503.20990) or [HuggingFace](https://huggingface.co/papers/2503.20990))| Yueru1, Shashidhar, ShirleyY, Acatsama, YupengCao | FinAudio introduces the first benchmark specifically designed to assess Audio Large Language Models (AudioLLMs) within the financial domain. The primary objective is to evaluate the capacity of current AudioLLMs on realistic financial audio tasks, revealing their strengths and limitations. The methodology involves defining three tasks (short-clip ASR, long-recording ASR, and summarization), curating five datasets (MDRM, SPGISpeech, Earnings-21, Earnings-22, FinAudioSum) totaling over 400 hours, and evaluating seven diverse AudioLLMs. Key results show significant performance variation, with Whisper-v3 achieving the lowest Word Error Rate (WER) on short-clip ASR (2-3%), but performance degrading across models for long audio ASR (Whisper-v3: 12-16% WER) and summarization being dependent on initial ASR quality. For AI practitioners, this benchmark reveals that while open-source models like Whisper-v3 provide a strong baseline, current AudioLLMs struggle with long financial recordings and specialized terminology/numerical data, highlighting the need for improved context handling and domain-specific adaptation. |
| Synthetic Video Enhances Physical Fidelity in Video Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.20822) or [HuggingFace](https://huggingface.co/papers/2503.20822))| Ziyan Yang, Ziyu Wang, Qi Zhao, fengcheng1, Univstar | This research demonstrates that integrating synthetic videos from CGI pipelines improves the physical fidelity of generative video synthesis models. The objective was to investigate whether synthetic videos, generated with physical consistency using computer graphics, can enhance the physical realism (e.g., 3D consistency, human pose integrity) of diffusion-based video generation models. The methodology involved generating synthetic videos using Blender/Unreal Engine, curating this data based on factors like asset/rendering quality and camera setups, employing a specific captioning strategy, and introducing a training technique called `SimDrop` to integrate synthetic data while mitigating visual artifacts using a reference model and classifier-free guidance. Primary results show significant improvement in physical fidelity across tasks like large human motion, camera rotation, and layer decomposition; for instance, on the camera spin shot task, the synthetically-enhanced model achieved an 80% success rate in user studies compared to 20% for the baseline and reduced the 3D reconstruction re-projection error (`ê_proj`) from 0.437 to 0.135. The principal implication for AI practitioners is that leveraging carefully curated synthetic video data, combined with techniques like `SimDrop`, offers a data-centric approach to enhance the physical consistency and reduce artifacts in video generation models without requiring modifications to the core model architecture. |
| ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging (Read more on [arXiv](https://arxiv.org/abs/2503.21088) or [HuggingFace](https://huggingface.co/papers/2503.21088))| Ziyan Jiang, Yi Zhong, Yanqiu Zhao, Saberlve, HaomingXu | ZJUKLAB employed TIES-Merging of two specialized models to address selective unlearning in Large Language Models for SemEval-2025 Task 4. The objective was to effectively erase sensitive content by balancing the trade-off between over-forgetting general knowledge and under-forgetting targeted data. Their methodology involved training two distinct LoRA models using Negative Preference Optimization (NPO), Gradient Descent on Retain set (GDR), and KL divergence minimization (KLR) to induce complementary biases, then merging them using TIES-Merging. The merged system ranked second online (Task Aggregate 0.944) and locally achieved an Aggregate Score of 0.806 and a near-optimal MIA AUC of 0.501, significantly outperforming the individual biased models. For AI practitioners, this demonstrates model merging as a practical technique to combine models with opposing unlearning biases for more effective and balanced sensitive data removal, though limitations in current evaluation metrics are noted. |
| Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile
  Gaussian Feature Fields (Read more on [arXiv](https://arxiv.org/abs/2503.20776) or [HuggingFace](https://huggingface.co/papers/2503.20776))| Hui Ren, Fanzhiwen, ir1d, ShuwangZhang00, shijiezhou | Feature4X provides a universal framework to lift arbitrary 2D vision foundation model functionalities into interactive 4D agentic AI systems using only monocular video input. Its main objective is to enable versatile 4D scene understanding and interaction (segmentation, editing, VQA) from readily available monocular videos, overcoming the limitations of 4D data scarcity. The key methodology involves distilling diverse 2D features into a compact, unified dynamic 4D Gaussian feature field represented using Gaussian Splatting and Motion Scaffolds, trained end-to-end and integrated with LLMs. Primary results include robust novel-view segmentation, language-guided 4D scene editing, and spatiotemporal VQA, with semantic segmentation achieving comparable accuracy to baselines while being approximately 6.2x more space-efficient (95.4MB vs 593.9MB). For AI practitioners, this offers a scalable method to extend existing 2D vision model capabilities to dynamic 4D environments, facilitating the development of interactive 4D agentic AI applications without requiring extensive annotated 4D datasets. |
| Unified Multimodal Discrete Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.20853) or [HuggingFace](https://huggingface.co/papers/2503.20853))| Katerina Fragkiadaki, Deepak765, Sid1275, mihirpd, aswerdlow | This paper introduces UniDisc, a unified multimodal discrete diffusion model for joint text and image generation. The objective is to explore discrete diffusion models as an alternative unified generative formulation for joint text and image domains, comparing their advantages over autoregressive (AR) models. UniDisc employs a transformer architecture trained using a discrete diffusion process involving masking tokens (text and image) with an absorbing state and learning to denoise via a weighted cross-entropy objective. Results show UniDisc outperforms AR models in conditional generation using classifier-free guidance (CFG), enables zero-shot joint text-image inpainting, and demonstrates superior joint retrieval accuracy (e.g., 0.64 vs 0.17 on DataComp1B). For AI practitioners, UniDisc offers enhanced controllability, editability, and a flexible inference time vs. quality trade-off for multimodal generation tasks compared to traditional AR approaches, although scaling analysis indicates it requires approximately 13.2x more training compute for equivalent loss levels. |
| LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized
  Text-Guided Image Editing (Read more on [arXiv](https://arxiv.org/abs/2503.21541) or [HuggingFace](https://huggingface.co/papers/2503.21541))| Sirisha Rambhatla, Meet Soni, Achint Soni | LOCATEdit introduces graph Laplacian optimization on cross- and self-attention maps (CASA graphs) for precise, localized text-guided image editing. The primary objective is to improve spatial consistency and confine edits to target regions, mitigating artifacts and distortions common in methods relying solely on cross-attention maps from diffusion models. Key methodology involves constructing CASA graphs from attention maps, applying graph Laplacian regularization to enforce smoothness and optimize attention values, integrating IP-Adapter guidance, and using selective pruning on text embedding differences. LOCATEdit significantly outperforms baselines on PIE-Bench, achieving, for example, a background preservation SSIM of 86.52 (x10^2) with DPM-Solver++(20), demonstrating superior localization and fidelity. For AI practitioners, this work provides a robust, training-free technique using graph-based optimization on attention mechanisms to achieve more controlled and spatially consistent results in text-guided generative image editing tasks. |
| LLPut: Investigating Large Language Models for Bug Report-Based Input
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.20578) or [HuggingFace](https://huggingface.co/papers/2503.20578))| Tarannum Shaila Zaman, imranraad, Subarna10, alifalhasan | This paper investigates the effectiveness of generative Large Language Models (LLMs) in extracting failure-inducing input commands from natural language bug reports. The primary research objective is to empirically evaluate how effectively three open-source generative LLMs (LLaMA, Qwen, Qwen-Coder) can extract these inputs compared to a fine-tuned BERT model. Using a dataset of 206 annotated Linux coreutils bug reports and a one-shot prompting strategy, the study evaluates extraction accuracy against human annotations using BLEU scores. The generative LLMs significantly outperformed the BERT baseline, with Qwen yielding the best results, achieving a BLEU-2 score of ≥ 0.5 for 62.62% of its extracted commands. For AI practitioners, this indicates that generative LLMs offer considerable potential for automating the extraction of executable commands from bug reports, aiding debugging workflows, though challenges in handling command variations and extraction failures persist. |
