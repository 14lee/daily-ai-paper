

## Papers for 2025-03-17

| Title | Authors | Summary |
|-------|---------|---------|
| ReCamMaster: Camera-Controlled Generative Rendering from A Single Video (Read more on [arXiv](https://arxiv.org/abs/2503.11647) or [HuggingFace](https://huggingface.co/papers/2503.11647))| Zuozhu, Mu437, Xintao, menghanxia, jianhongbai | ReCamMaster is a framework for re-rendering a given video with novel camera trajectories using a generative model. The main research objective is to develop a camera-controlled generative video re-rendering framework that can reproduce the dynamic scene of an input video at novel camera trajectories. The key methodology involves conditioning a pre-trained text-to-video diffusion model on both the source video and target camera poses using a frame-dimension concatenation technique, and training on a new multi-camera synchronized video dataset created with Unreal Engine 5. The method achieved a FID score of 57.10 and FVD of 122.74 on visual quality, outperforming existing state-of-the-art approaches. AI practitioners can use this framework for video editing tasks like stabilization, super-resolution, and outpainting, offering improved control over camera movements in generated videos.  |
| Adversarial Data Collection: Human-Collaborative Perturbations for
  Efficient and Robust Robotic Imitation Learning (Read more on [arXiv](https://arxiv.org/abs/2503.11646) or [HuggingFace](https://huggingface.co/papers/2503.11646))| AutobotZero, hsli-cuhk, Eralien, morninghaze, SiyuanH | Here's a concise summary of the paper:  i) Adversarial Data Collection (ADC) framework improves robotic imitation learning by introducing human-collaborative perturbations during data acquisition. ii) The main research objective is to maximize per-demonstration information density and improve the efficiency and robustness of robotic imitation learning. iii) Key methodology involves a "Two-Humans-in-the-Loop" approach where an adversarial operator dynamically introduces visual and linguistic perturbations during teleoperation by a primary operator. iv) Models trained with 20% of ADC-collected data volume achieved superior generalization and robustness compared to models trained with 100% of traditionally collected data. v) For AI practitioners, ADC provides a practical strategy for enhancing data quality over quantity, reducing the reliance on large datasets for training robust robotic policies in real-world, dynamic environments.  |
| Technologies on Effectiveness and Efficiency: A Survey of State Spaces
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.11224) or [HuggingFace](https://huggingface.co/papers/2503.11224))| yuchenFan, xuekai, iseesaw, Youbang, XingtaiHF | i) This survey provides a structured overview of State Space Models (SSMs), comparing their effectiveness and efficiency against transformers. ii) The main objective is to present a coherent and systematic analysis of SSMs, covering their theoretical underpinnings, mathematical formulations, and applications. iii) The survey categorizes SSMs into three main sections: original SSMs, structured SSMs (S4), and selective SSMs (Mamba), emphasizing the technical aspects and key techniques. iv) The paper highlights techniques such as Euler's method, ZOH, and bilinear transform discretization for enabling the transformation of SSMs from continuous-time to discrete-time, and references the Mamba model achieving a 20-40 time speedup by performing SSM parameter discretization and recurrence computation directly in the GPU SRAM rather than the GPU HBM. v) AI practitioners can use this survey to understand the trade-offs between different SSM architectures, enabling them to make informed decisions when selecting models for sequential data processing and long-context tasks where efficiency is critical.  |
| API Agents vs. GUI Agents: Divergence and Convergence (Read more on [arXiv](https://arxiv.org/abs/2503.11069) or [HuggingFace](https://huggingface.co/papers/2503.11069))| Eliblo1969, SiQin88, liqul, shilhe, vyokky | i) This paper comparatively analyzes API-based and GUI-based LLM agents for software automation, examining their divergence and potential convergence. ii) The main objective is to systematically analyze the architectural differences, development workflows, and user interaction models of API-based versus GUI-based LLM agents. iii) The methodology involves a comparative study across key dimensions such as modality, reliability, efficiency, availability, flexibility, security, transparency, human-like interaction, and maintainability, along with illustrative use cases. iv) The primary result shows API agents offer efficiency and security with stable endpoints while GUI agents provide broader applicability, with the finding being that hybrid approaches can combine UI-based steps where APIs are unavailable with direct calls for data-heavy tasks. v) The principal implication for AI practitioners is the need to consider hybrid agent architectures that leverage the strengths of both API- and GUI-based approaches to achieve comprehensive automation across diverse software ecosystems.  |
| Large-scale Pre-training for Grounded Video Caption Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10781) or [HuggingFace](https://huggingface.co/papers/2503.10781))| Josef Sivic, Cordelia Schmid, ekazakos | This paper introduces a method for generating video captions with objects grounded via temporally dense bounding boxes, including a new model, datasets, and pre-training approach. The main research objective is to generate video-level captions with corresponding bounding boxes that consistently localize key noun phrases across the video frames. The key methodology includes an automatic annotation method that aggregates frame-level grounded captions into temporally consistent video annotations, coupled with a Grounded Video Caption Generation model (GROVE) that uses spatio-temporal adapters and a temporal objectness head. The primary results show that GROVE, pre-trained on the new, automatically-annotated HowToGround1M dataset (1M videos) and fine-tuned on the manually-annotated iGround dataset, achieves a CIDEr score of 85.4 on the iGround test set. The principal implication is that AI practitioners can leverage large-scale automatic annotation and pre-training, followed by fine-tuning on smaller, high-quality datasets, to achieve state-of-the-art results in grounded video caption generation.  |
| FlowTok: Flowing Seamlessly Across Text and Image Tokens (Read more on [arXiv](https://arxiv.org/abs/2503.10772) or [HuggingFace](https://huggingface.co/papers/2503.10772))| Liang-Chieh Chen, QHL067, QihangYu, turkeyju | FlowTok is a framework that enables direct flow matching between text and images by encoding both into compact 1D tokens. The main research question is whether multimodal understanding and generation can be unified by enabling direct transitions within a shared, compact 1D latent space. The key methodology involves projecting both text and images into a unified 1D latent space using an enhanced image tokenizer and a text projector, then applying flow matching. FlowTok reduces the latent space size by 3.3x compared to prior methods at 256 resolution and achieves a COCO FID-30K score of 9.67 while completing training in 26.1 8-A100 days. For AI practitioners, FlowTok offers a more memory-efficient and faster approach to text-to-image and image-to-text generation, by leveraging a compact 1D token representation.  |
| Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision
  Transformers? (Read more on [arXiv](https://arxiv.org/abs/2503.10632) or [HuggingFace](https://huggingface.co/papers/2503.10632))| Xin Li, Killian Hitsman, aritradutta, maitysubhajit | This paper investigates learnable attention mechanisms based on Kolmogorov-Arnold Networks (KANs) for Vision Transformers (ViTs). The main research question is whether a learnable multi-head self-attention (MHSA) module, specifically a Kolmogorov-Arnold Attention (KArAt), can improve the performance of vanilla ViTs. The key methodology involves designing a general KArAt, and a specific variant, Fourier-KArAt, and evaluating them against vanilla ViTs on CIFAR-10, CIFAR-100, and ImageNet-1K datasets, analyzing loss landscapes, weight distributions, and attention maps. The primary result shows ViT-Tiny+Fourier KArAt outperforms ViT-Tiny on CIFAR-10 by 5.40% in Top-1 accuracy, but larger ViT models with KArAt show diminished gains or worse performance. The implication is that directly replacing softmax with learnable activations in ViT's attention mechanism does not guarantee improved performance, requiring careful design due to increased model complexity and optimization challenges, although in some instances, smaller models can improve their performance.  |
| Cockatiel: Ensembling Synthetic and Human Preferenced Training for
  Detailed Video Caption (Read more on [arXiv](https://arxiv.org/abs/2503.09279) or [HuggingFace](https://huggingface.co/papers/2503.09279))| Hao Li, Zhiyu Tan, xiaomengyang, Kobeshegu, Fr0zencr4nE | Cockatiel-13B is a video captioning model that ensembles synthetic and human-aligned training to generate detailed and human-preferred video descriptions. The main research objective is to address the imbalanced video-caption alignment and misalignment with human preferences in existing video detailed captioning (VDC) models. The key methodology involves a three-stage training pipeline that curates data using a human-aligned caption quality scorer, trains a 13B parameter model (Cockatiel-13B) on the curated data, and distills an 8B parameter model (Cockatiel-8B) from it. Primary results show Cockatiel-13B achieving a new state-of-the-art VDCSCORE average of 43.80, outperforming existing models. The principal implication is that AI practitioners can achieve more human-aligned and dimension-balanced video descriptions by utilizing a training procedure that selectively combines diverse model strengths, guided by structured human preferences.  |
| Neighboring Autoregressive Modeling for Efficient Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10696) or [HuggingFace](https://huggingface.co/papers/2503.10696))| Hong Zhou, Feng Chen, Shaoxuan He, Yuanyu He, Yefei He | Neighboring Autoregressive Modeling (NAR) is a new paradigm for efficient visual generation that formulates autoregressive visual generation as a progressive outpainting procedure. The main research objective is to develop an autoregressive visual generation method that improves efficiency and preserves spatial/temporal locality, unlike raster-order "next-token prediction" approaches. The key methodology is a near-to-far "next-neighbor prediction" mechanism, using dimension-oriented decoding heads to predict multiple adjacent tokens in parallel along orthogonal dimensions. Results show that on ImageNet 256x256, NAR-L achieves a lower FID (3.06) than LlamaGen-XXL (3.09) with 87.8% fewer steps and 13.8x higher throughput. AI practitioners can use NAR to achieve more efficient autoregressive visual generation with improved fidelity compared to traditional next-token prediction and existing parallel approaches, particularly beneficial for high-resolution image and video tasks.  |
| ProJudge: A Multi-Modal Multi-Discipline Benchmark and
  Instruction-Tuning Dataset for MLLM-based Process Judges (Read more on [arXiv](https://arxiv.org/abs/2503.06553) or [HuggingFace](https://huggingface.co/papers/2503.06553))| Fanrui Zhang, Ming Li, Zhaopan Xu, Pengfei Zhou, Jiaxin Ai | ProJudge is a benchmark and instruction-tuning dataset for evaluating multi-modal large language models (MLLMs) as automated process judges for scientific problem-solving. The main research objective is to assess and enhance the capability of MLLMs to perform fine-grained evaluation of step-by-step reasoning in scientific problems, including error detection, classification, and diagnosis. The key methodology involves creating ProJudgeBench, a benchmark of 2,400 multi-modal scientific problems with 50,118 step-level annotations, and ProJudge-173k, a large-scale instruction-tuning dataset, accompanied by a Dynamic Dual-Phase fine-tuning strategy. A key finding is that after fine-tuning on ProJudge-173k, InternVL2.5-8B showed a 58.92% increase in step correctness accuracy. Principal implication for AI practioners is that open-source models, through the ProJudge, can significantly enhance their performance to match that of many state-of-art closed-source, enabling more reliable and nuanced process evaluation in multi-modal reasoning tasks.  |
| ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model
  with Interleaved Multimodal Generation via Asymmetric Synergy (Read more on [arXiv](https://arxiv.org/abs/2503.06542) or [HuggingFace](https://huggingface.co/papers/2503.06542))| Zizhen Li, Fanrui Zhang, Chuanhao Li, Yukang Feng, Jianwen Sun | ARMOR v0.1 is a resource-efficient framework that upgrades existing multimodal large language models (MLLMs) to unified models (UniMs) capable of both understanding and interleaved text-image generation. The main research objective is to enable MLLMs to perform multimodal generation while preserving their understanding capabilities and minimizing computational overhead. The key methodology involves an asymmetric encoder-decoder architecture with a forward-switching mechanism, a curated interleaved dataset, and a three-stage "What or How to Generate" (WoHG) training algorithm. Experimental results show that ARMOR outperforms existing UniMs on multimodal understanding benchmarks (78.8 score on MMB versus 62.6 for Janus-pro) while achieving comparable generation performance. AI practitioners can leverage ARMOR to build UniMs by fine-tuning existing MLLMs, thereby reducing training costs and enabling natural text-image interleaved generation.  |
| Learning Few-Step Diffusion Models by Trajectory Distribution Matching (Read more on [arXiv](https://arxiv.org/abs/2503.06674) or [HuggingFace](https://huggingface.co/papers/2503.06674))| Yujun Cai, jingtang, JIACSUN96, whatlegequ, Luo-Yihong | Learning Few-Step Diffusion Models by Trajectory Distribution Matching (TDM) introduces a unified distillation paradigm for accelerating diffusion model sampling. The main research objective is to develop a few-step diffusion model distillation method that combines the strengths of distribution and trajectory matching, overcoming their individual limitations. The key methodology is a data-free score distillation objective that aligns the student's trajectory with the teacher's at the distribution level, coupled with a sampling-steps-aware objective for flexible multi-step adaptation. The method distills PixArt-α into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution, accomplishing this with only 500 iterations and 2 A800 hours. For AI practitioners, TDM offers a highly efficient way to train fast and high-quality few-step diffusion models, significantly reducing training cost while surpassing teacher model performance, as demonstrated on text-to-image tasks.  |
| ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant
  Tightness (Read more on [arXiv](https://arxiv.org/abs/2503.10624) or [HuggingFace](https://huggingface.co/papers/2503.10624))| Yuliang Xiu, Michael J. Black, Zeyu Cai, Haiwen Feng, Boqian-Li | ETCH is a novel framework for fitting a 3D body model to point clouds of clothed humans by modeling cloth-to-body mapping. The main research objective is to accurately estimate the underlying body shape and pose from 3D scans of clothed humans, generalizing across diverse poses, shapes, and garment types. The key methodology is Equivariant Tightness Fitting, which uses SE(3)-equivariant displacement vectors to represent "tightness" and leverages pose-invariant body correspondences for sparse marker regression. The method reduces directional errors by 67.2% ~ 89.8% in one-shot (out-of-distribution) settings with approximately 1% of training data. AI practitioners can use this method to obtain accurate body shape and pose estimations from 3D scans of clothed individuals, with robustness to variations in clothing and pose, even with limited training data.  |
| Open-World Skill Discovery from Unsegmented Demonstrations (Read more on [arXiv](https://arxiv.org/abs/2503.10684) or [HuggingFace](https://huggingface.co/papers/2503.10684))| Yitao Liang, Anji Liu, Shaofei Cai, Zihao Wang, Jingwen Deng | This paper introduces Skill Boundary Detection (SBD), a self-supervised algorithm for segmenting unsegmented demonstration videos into discrete skills for open-world learning. The main research question is how to automatically segment long, unsegmented demonstration videos into meaningful, skill-consistent segments without manual annotations. SBD leverages a pretrained unconditional action-prediction model and detects skill boundaries by identifying significant increases in prediction error, based on event segmentation theory. The method improved the average performance of conditioned policies in Minecraft by 63.7% and 52.1% on short-term atomic skill tasks. AI practitioners can leverage this method to train instruction-following agents from diverse, unlabeled video data, such as YouTube, without requiring manual segmentation or labeling.  |
| GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories
  Generation in End-to-End Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2503.05689) or [HuggingFace](https://huggingface.co/papers/2503.05689))| Bo Jiang, Yang Hu, Xingyu Zhang, WonderingWorld, XXXXing | GoalFlow is an end-to-end autonomous driving method that generates high-quality multimodal trajectories using goal-driven flow matching. The main research objective is to address trajectory selection complexity and reduced quality in existing multimodal trajectory generation methods for autonomous driving. The key methodology involves introducing GoalFlow, which constrains trajectory generation using a goal point selected via a novel scoring mechanism, employs Flow Matching for efficient generation, and uses a refined scoring mechanism for optimal trajectory selection. Primary results show GoalFlow achieved a PDMS of 90.3 on the Navsim benchmark, significantly outperforming other methods, and requires only a single denoising step for excellent performance. Principal implication for AI practitioners is that GoalFlow provides a method for generating high-quality, diverse, yet, safe candidate actions for autonomous driving systems enhancing robustness and real-world deployability.  |
| MaRI: Material Retrieval Integration across Domains (Read more on [arXiv](https://arxiv.org/abs/2503.08111) or [HuggingFace](https://huggingface.co/papers/2503.08111))| Yuxuan Chen, Huixiong Zhang, Yangfan He, Jianhui Wang, yangzhifei | MaRI is a framework for aligning visual and material properties in a shared embedding space for material retrieval. The main research objective is to bridge the feature space gap between synthetic and real-world materials to improve material retrieval accuracy. The key methodology involves using dual DINOv2-based encoders trained contrastively to map images and materials into a shared space, leveraging a new dataset combining synthetic and real-world material data. Primary results show that on a trained material dataset, MaRI achieves a top-1 instance accuracy of 26.0% and a top-5 instance accuracy of 90.0%. AI practitioners can use MaRI's framework and dataset to improve the accuracy and generalization of material retrieval, enhancing 3D asset creation and applications requiring realistic material representation.  |
| VGGT: Visual Geometry Grounded Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.11651) or [HuggingFace](https://huggingface.co/papers/2503.11651))| Christian Rupprecht, Andrea Vedaldi, Nikita Karaev, Minghao Chen, Jianyuan Wang | VGGT is a feed-forward transformer that directly infers 3D attributes of a scene from multiple images, achieving state-of-the-art results in several 3D tasks. The main research objective is to determine if 3D tasks can be solved directly by a neural network without visual geometry post-processing. The key methodology is a large transformer with alternating frame-wise and global self-attention, trained on multiple 3D-annotated datasets to predict camera parameters, depth maps, point maps, and 3D point tracks. The primary results show that VGGT outperforms state-of-the-art methods on RealEstate10K and CO3Dv2 datasets for camera pose estimation (AUC@30 of 93.5 and 91.8 respectively, with BA), and also achieves superior accuracy on the DTU and ETH3D datasets for multi-view depth and point map estimation, exceeding optimization-based and other feed-forward methods. Principal implication is that AI practitioners can leverage VGGT for fast and accurate 3D reconstruction, reducing or eliminating the reliance on costly iterative optimization techniques commonly used in computer vision, potentially simplifying and accelerating 3D vision pipelines.  |
| From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM (Read more on [arXiv](https://arxiv.org/abs/2503.10620) or [HuggingFace](https://huggingface.co/papers/2503.10620))| Tsz Kin Lam, Anil Keshwani, Sonal Sannigrahi, Kshitij Ambilduke, bpop | SPIRE extends the TOWER language model to process speech by incorporating discretized speech units and continued pre-training. The main research objective is to integrate English speech processing (transcription and translation) into an existing text-only multilingual LLM, TOWER, while maintaining its original text-task performance. The methodology involves two stages: continued pre-training (CPT) on a mixture of ASR data and TOWER's text data, and instruction tuning (IT) on MT, ASR, and ST datasets, employing HuBERT-based k-means clustering for speech discretization. SPIREFULL achieves a Word Error Rate (WER) of 4.2 on the LibriSpeech test-clean set, outperforming models like Spirit-LM and the Whisper-base, though not matching the performance of more heavily speech-trained models. AI practitioners can adapt a text-based LLM for speech tasks with preserved performance on text-based tasks by leveraging the recipe of speech discretization and CPT+IT.  |
| Group-robust Machine Unlearning (Read more on [arXiv](https://arxiv.org/abs/2503.09330) or [HuggingFace](https://huggingface.co/papers/2503.09330))| Massimiliano Mancini, Elisa Ricci, Stéphane Lathuilière, Subhankar Roy, Thomas De Min | This paper introduces group-robust machine unlearning to address performance degradation in specific demographic groups caused by non-uniformly distributed data removal requests. The main research question is how to unlearn data from a trained model while preserving performance for groups that are over-represented in the forget set. The key methodology involves sample distribution reweighting during retraining and a novel approximate unlearning method (MIU) that minimizes mutual information between model features and group information, alongside mutual information calibration with original model. Primary results show that MIU outperforms standard unlearning methods on CelebA, Waterbirds, and FairFace datasets; for example it achieves 69.0% group accuracy (GA) on CelebA compared with next best of 66.2%, preserving model robustness. The principle implication is that AI practitioners should use distribution reweighting and mutual information-based techniques to mitigate fairness issues in machine unlearning scenarios where data removal requests are not uniformly distributed across groups.  |
