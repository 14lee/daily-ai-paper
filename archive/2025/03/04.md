

## Papers for 2025-03-04

| Title | Authors | Summary |
|-------|---------|---------|
| Visual-RFT: Visual Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.01785) or [HuggingFace](https://huggingface.co/papers/2503.01785))| yhcao, sweetFruit, yuhangzang, Zery, ziyuliu | Visual-RFT extends Reinforcement Fine-Tuning (RFT) to visual tasks by using verifiable rewards to improve performance of Large Vision-Language Models (LVLMs). The main objective is to apply RFT, previously successful in language models, to multi-modal domains, specifically visual perception tasks, with limited data. The key methodology is using LVLMs to generate multiple responses with reasoning tokens and applying visual perception verifiable reward functions (e.g., IoU for object detection) to update the model via policy optimization algorithms like Group Relative Policy Optimization (GRPO). Visual-RFT improved accuracy by 24.3% over the baseline in one-shot fine-grained image classification and exceeded SFT baselines by 21.9 and 15.4 on COCO and LVIS, in two-shot settings, respectively. For AI practitioners, Visual-RFT offers a data-efficient, reward-driven approach to enhance reasoning and adaptability in LVLMs for domain-specific tasks, particularly when fine-tuning data is scarce.  |
| Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.01774) or [HuggingFace](https://huggingface.co/papers/2503.01774))| zgojcic, AnalMom, xrenaa, hturki, jayw | DIFIX3D+ enhances 3D reconstruction and novel-view synthesis using single-step diffusion models. The main research objective is to improve the quality of 3D reconstructions, especially in under-constrained regions, by leveraging 2D diffusion model priors. The methodology involves fine-tuning a single-step image diffusion model (DIFIX) to remove artifacts in rendered novel views, and using it both during reconstruction to clean pseudo-training views and as a neural enhancer during inference. Primary results show an average 2x improvement in FID score over baselines while maintaining 3D consistency, with compatibility across both NeRF and 3DGS representations. The principal implication is that AI practitioners can leverage single-step diffusion models for real-time post-processing to improve the visual quality of 3D reconstructions and novel view synthesis.  |
| Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs (Read more on [arXiv](https://arxiv.org/abs/2503.01743) or [HuggingFace](https://huggingface.co/papers/2503.01743))| vishravmsft, martincai, alonbenhaim, jianmin-ustc, atabakashfaqMSFT | Phi-4-Mini and Phi-4-Multimodal are 3.8-billion-parameter language and multimodal models trained on high-quality data, achieving strong performance relative to their size.  **Main research question or objective:** To develop compact yet highly capable language and multimodal models that outperform similar-sized open-source models and rival larger models, using curated data and novel architecture techniques.  **Key methodology used:** The researchers trained Phi-4-Mini on high-quality web and synthetic data, with emphasis on math and coding datasets, expanded the vocabulary to 200K tokens, used grouped query attention, and a fractional RoPE dimension. For Phi-4-Multimodal, they used a "Mixture of LoRAs" technique, integrating modality-specific LoRAs while freezing the base language model.  **Primary results:** Phi-4-Mini outperformed similarly sized models and matched the performance of models twice its size on math/coding, and Phi-4-Multimodal ranked first on the OpenASR leaderboard at the time, with the speech/audio LoRA having only 460 million parameters. Phi-4-Multimodal outperformed larger vision-language models, and achieved 72.0 average score across various vision-language benchmarks.  **Principal implication for AI practitioners:** AI/ML/Software Engineers and Data Scientists can leverage Phi-4-Mini and Phi-4-Multimodal as efficient and performant small language and multimodal models, achieving strong performance while keeping the base language model frozen, making it a practical solution in resource-constrained environments.  |
| OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.18965) or [HuggingFace](https://huggingface.co/papers/2502.18965))| GuoruiZhou, DingWF, caikuo, oneself, OrpheusBetter | OneRec is an end-to-end generative recommendation model that unifies retrieval and ranking stages. The main research objective is to develop a single-stage generative model that surpasses the performance of traditional multi-stage recommender systems in real-world scenarios. The key methodology involves an encoder-decoder architecture with Mixture-of-Experts (MoE), session-wise generation, and Iterative Preference Alignment (IPA) combined with Direct Preference Optimization (DPO) using a reward model. Primary results show that OneRec deployed in Kuaishou's main scene achieved a 1.68% increase in watch-time, a substantial improvement over the previous system. For AI practitioners, OneRec demonstrates the feasibility of achieving significant performance gains by replacing a cascaded ranking system with a unified generative model by utilizing techniques like MoE and IPA.  |
| Liger: Linearizing Large Language Models to Gated Recurrent Structures (Read more on [arXiv](https://arxiv.org/abs/2503.01496) or [HuggingFace](https://huggingface.co/papers/2503.01496))| Yu Cheng, JusenK, Jiaxihu2, weigao266, landisen | Liger transforms pretrained Transformer-based large language models (LLMs) into gated linear recurrent structures for efficient deployment. The main research objective is to linearize LLMs into gated recurrent structures without adding extra parameters and with minimal performance loss. The key methodology involves repurposing pretrained key matrix weights to construct gating mechanisms and using Low-Rank Adaptation (LoRA) for lightweight fine-tuning. The primary result is that Liger recovers 93% of the Transformer-based Llama-3 8B model's performance using only 0.02% of pre-training tokens during linearization. AI practitioners can deploy LLMs more efficiently with linear-time inference and constant memory usage by converting them to gated recurrent structures using Liger.  |
| When an LLM is apprehensive about its answers -- and when its uncertainty is justified (Read more on [arXiv](https://arxiv.org/abs/2503.01688) or [HuggingFace](https://huggingface.co/papers/2503.01688))| Alexey Zaytsev, Edvard Khalafyan, DanielVyazhev, aigoncharov, sspetya | The paper investigates uncertainty estimation in Large Language Models (LLMs) for multiple-choice question answering, focusing on entropy and model-as-judge (MASJ) approaches. The main research question is how well token-wise entropy and MASJ estimates reflect LLM error and question difficulty across different domains and reasoning requirements. The key methodology involves evaluating three LLMs (Phi-4, Mistral, Qwen) on the MMLU-Pro dataset, using an auxiliary LLM to label questions by reasoning/knowledge needs and comparing uncertainty estimates with correctness labels. A primary result is that response entropy predicts model error effectively in knowledge-dependent domains (biology ROC AUC = 0.73), but this correlation weakens for reasoning-dependent domains (math ROC AUC = 0.55). For AI practioners this indicates, that the data-uncertainty related entropy is a useful measure in uncertainty estimate frameworks and should be integrated, but its usefulness is dependent to how much reasoning is requred to solve the problem.  |
| DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.01183) or [HuggingFace](https://huggingface.co/papers/2503.01183))| Guobin Ma, Chunbo Hao, Yuepeng Jiang, Huakang Chen, Ziqian Ning | DiffRhythm is a latent diffusion-based model that generates full-length songs with vocals and accompaniment, achieving high musicality, intelligibility, and fast inference speeds. The main research objective is to develop an end-to-end song generation model capable of synthesizing complete songs (up to 4m45s) with both vocal and accompaniment, overcoming limitations of existing approaches like multi-stage architectures and slow inference. Key methodology involves a Variational Autoencoder (VAE) for learning compact latent representations of waveforms and a Diffusion Transformer (DiT) operating in the latent space, along with a novel sentence-level lyrics alignment mechanism. Primary results show that DiffRhythm achieves a Phoneme Error Rate (PER) of 18.02% in full-length song generation with a real-time factor (RTF) of 0.034. AI practitioners can leverage DiffRhythm's simple architecture, fast non-autoregressive generation, and open-sourced code/models for scalable, end-to-end song generation research and applications, eliminating the need for complex multi-stage cascading modelling.  |
| Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs (Read more on [arXiv](https://arxiv.org/abs/2503.01307) or [HuggingFace](https://huggingface.co/papers/2503.01307))| ngoodman, nlile, Asap7772, ayushchakravarthy, obiwan96 | i) This paper investigates cognitive behaviors that enable language models to effectively self-improve via reinforcement learning. ii) The research question is: what intrinsic properties enable effective self-improvement in language models trained with reinforcement learning? iii) The methodology involves analyzing verification, backtracking, subgoal setting, and backward chaining in Qwen and Llama models during reinforcement learning on the Countdown game, alongside controlled behavioral dataset experiments and pretraining data curation. iv) Results show that Qwen naturally exhibits reasoning behaviors whereas Llama lacks them, priming Llama with these behaviors enables substantial improvements during RL; models primed with incorrect solutions but proper reasoning patterns achieve comparable performance to those trained on correct solutions, and curated pretraining data amplified Llama's reasoning behaviors. v) AI practitioners should consider the initial reasoning behaviors of language models as a critical factor in determining their capacity for self-improvement via reinforcement learning, and potentially curate pretraining data to enhance those behaviors.  |
| Speculative Ad-hoc Querying (Read more on [arXiv](https://arxiv.org/abs/2503.00714) or [HuggingFace](https://huggingface.co/papers/2503.00714))| Venkat Arun, Aditya Akella, Maria Angels de Luis Balaguer, Srikanth Kandula, Haoyu0529 | SpeQL, a system that reduces query latency by using large language models (LLMs) to predict and precompute SQL queries during user input, improves analytical query responsiveness. The research objective is to determine if query execution can begin before a user finishes typing an SQL query, enabling near-instantaneous results. The methodology involves using LLMs to predict query structure and precompute temporary tables, alongside a scheduler that manages query execution and a user interface that displays speculative results. Results from experiments on 103 TPC-DS queries at 100GB scale show that SpeQL reduces P90 planning, compilation, and execution latency by 94.42%, 99.99%, and 87.23%, respectively, with a 7.72 seconds P90 execution overhead. AI practitioners can leverage SpeQL's approach to improve the responsiveness of interactive data analysis systems, thereby enabling quicker insight discovery during exploratory data analysis.  |
| Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions (Read more on [arXiv](https://arxiv.org/abs/2503.00501) or [HuggingFace](https://huggingface.co/papers/2503.00501))| Xiaohui He, Jia Chen, aiqy, haitaoli, qian | Qilin is a new multimodal information retrieval dataset collected from a social platform, Xiaohongshu, for improving search and recommendation services. The main research objective is to create a dataset that facilitates the development of advanced multimodal neural retrieval models across diverse task settings with real-world user interaction data. The key methodology involves collecting user sessions with heterogeneous results (image-text, video, commercial notes, direct answers) and APP-level contextual signals, then filtering the data using LLMs and human verification for safety and privacy. Primary results include a dataset of APP-level sessions from 15,482 users, where search users browse an average of 23.41 items when Deep Query Answering (DQA) is not triggered, but only 10.61 items when DQA is triggered. Principal implication for AI practitioners is that Qilin provides a realistic, large-scale, multimodal dataset with rich contextual information for training, evaluating, and analyzing retrieval-augmented generation systems and other advanced search and recommendation models, taking into account complex user behaviors.  |
| DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting (Read more on [arXiv](https://arxiv.org/abs/2503.00784) or [HuggingFace](https://huggingface.co/papers/2503.00784))| xpqiu, QipengGuo, KYLN24, KaiLv | DuoDecoding is a novel speculative decoding method that leverages heterogeneous hardware to accelerate large language model inference. The main research objective is to reduce generation latency in large language models (LLMs) while maintaining output distribution fidelity and reducing the time to first token (TTFT). The key methodology involves deploying the draft model on the CPU and the target model on the GPU, enabling parallel decoding, along with a hardware-aware optimal draft budget and dynamic multi-sequence drafting. DuoDecoding achieves up to a 2.61x speedup in generation latency compared to vanilla autoregressive generation and reduces TTFT to 83% of that in conventional speculative decoding. The principal implication for AI practitioners is that DuoDecoding provides a method to significantly improve the inference speed of LLMs, particularly beneficial for interactive applications, by utilizing both CPU and GPU resources effectively.  |
| Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01370) or [HuggingFace](https://huggingface.co/papers/2503.01370))| yingcongchen, Xxlbigbrother, StarYDY, MeixiChen, LTT | Kiss3DGen is a framework that repurposes 2D image diffusion models for 3D asset generation, including tasks like text-to-3D, image-to-3D, editing, and enhancement. The main research objective is to develop an efficient method for generating, editing, and enhancing 3D objects by leveraging pretrained 2D image diffusion models, without the need of large-scale 3D datasets. The key methodology involves fine-tuning a diffusion model (Flux) to generate "3D Bundle Images"—tiled representations of multi-view images and normal maps—which are then used to reconstruct a 3D mesh. The method achieves a CLIP score of 0.837 in text-to-3D generation evaluation, outperforming 3DTopia, Direct2.5, and Hunyuan3D-1.0. AI practitioners can utilize this framework to efficiently create high-quality 3D models by maximizing the use of pre-trained 2D diffusion models, thus reducing the dependency on extensive 3D training data.  |
| Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia (Read more on [arXiv](https://arxiv.org/abs/2503.01714) or [HuggingFace](https://huggingface.co/papers/2503.01714))| Lang Gao, Zhongyu Wei, Ziruibest, Carol0110, Aurora-cx | Large Language Models (LLMs) reconstruct the meaning of scrambled words primarily using word form, with minimal reliance on contextual information. The main research question is how word form and contextual information influence LLMs' semantic reconstruction ability under Typoglycemia. The researchers used controlled experiments on LLaMA models, varying Scramble Ratio (SR) and Context Integrity (CI), and introduced SemRecScore to quantify semantic reconstruction. Primary results show SemRecScore decreases as SR increases, and at a Scramble Ratio (SR) of 1, a final SemRecScore of only 0.5 is achieved on the final LLM layer, indicating incomplete semantic reconstruction. For AI practitioners, this highlights that improvements can come by incorporating human-like, context-aware mechanisms, as current attention mechanisms focus primarily on the word form.  |
| SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity (Read more on [arXiv](https://arxiv.org/abs/2503.01506) or [HuggingFace](https://huggingface.co/papers/2503.01506))| bitwjg, WeiWang, WQYC, DeyangKong, xixy | SampleMix is a sample-wise pre-training data mixing strategy for large language models that coordinates data quality and diversity. The main research objective is to address the limitations of existing domain-wise data mixing methods, which overlook inter-domain overlaps and use suboptimal sample distributions. The key methodology involves evaluating the quality and diversity of each sample, assigning sampling weights, and constructing a training dataset based on these weights. The primary results show that SampleMix achieves an average accuracy of 47.77% across eight downstream tasks, outperforming all baseline methods, and reaching baseline performance with 1.9x fewer training steps. The principal implication is that AI practitioners can use SampleMix to improve training efficiency and model performance by creating better data mixtures by incorporating sample-wise quality and diversity evaluations.  |
| From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens (Read more on [arXiv](https://arxiv.org/abs/2502.18890) or [HuggingFace](https://huggingface.co/papers/2502.18890))| Yuxuan Wang, zlzheng, vickyandkekey, JunzheS, TongWu | TOKENSWIFT accelerates ultra-long sequence generation for large language models without compromising output quality. The main research question is whether model-agnostic, lossless acceleration can be achieved for generating ultra-long sequences with minimal training overhead. The key methodology involves multi-token parallel self-drafting with the target model, token reutilization, dynamic KV cache management, and contextual penalty. Primary results show that TOKENSWIFT achieves over 3x speedup compared to autoregressive generation across various models, reducing generation time for 100K tokens on LLAMA3.1-8b from nearly 5 hours to 90 minutes. Principal implication for AI practitioners is TOKENSWIFT provides a scalable and effective solution to dramatically speed up ultra long text generation, enabling applications that require producing very large outputs.  |
| Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model (Read more on [arXiv](https://arxiv.org/abs/2502.16779) or [HuggingFace](https://huggingface.co/papers/2502.16779))| Jianan Wang, Xili Dai, xyyue, qixianbiao, yxuan | The paper introduces Plane-DUSt3R, a novel method for multi-view room layout estimation using the DUSt3R 3D foundation model. The main research objective is to develop a method for 3D room layout estimation from multiple unposed, sparse-view images. The methodology involves fine-tuning DUSt3R on a room layout dataset with a modified objective to estimate structural planes and combining it with a 2D plane detector and a post-processing algorithm. The Plane-DUSt3R achieves a 5.27% and 5.33% improvement in RRA and mAA metrics, respectively, for multi-view correspondence tasks, compared to state-of-the-art methods on the Structure3D dataset. AI practitioners can use Plane-DUSt3R to generate 3D room layouts from unposed images, eliminating the need for precise camera poses and simplifying multi-view 3D reconstruction.  |
| CodeArena: A Collective Evaluation Platform for LLM Code Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01295) or [HuggingFace](https://huggingface.co/papers/2503.01295))| terryyz, DongHuang-ebay, bobxwu, anhtuanluu36, Elfsong | CodeArena is an online platform for evaluating large language models (LLMs) on code generation tasks, incorporating a collective evaluation mechanism. The main objective is to address limitations in existing LLM code generation evaluation, such as benchmark contamination, data dissipation, and system inaccessibility. The key methodology involves a dynamic scoring system that adjusts model scores based on the collective performance of all submissions, along with providing automation-friendly APIs and open access to solutions and test cases. Results show that closed-source LLMs generally outperform open-source models, with "DeepSeek-Coder" achieving a Dynamic Point score of 249.28 and solving 90.63% of the problems. AI practitioners can use CodeArena for unbiased LLM code generation evaluation, accessing a public repository of solutions and test cases, and streamlining the evaluation process with automation-ready APIs.  |
| VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01739) or [HuggingFace](https://huggingface.co/papers/2503.01739))| Yi Yang, WenhaoWang | VideoUFO is a million-scale video dataset designed to align text-to-video generation models with real-world user preferences. The main research objective is to curate a video dataset that reflects user-focused topics and evaluate its impact on text-to-video model performance. The key methodology involves clustering user-provided prompts from VidProM to identify 1,291 topics, retrieving relevant videos from YouTube, segmenting them into clips, generating captions, and assessing video quality using VBench. Primary results show that a model trained on VideoUFO achieves a low-10 score of 0.442, outperforming models trained on other datasets, while maintaining a top-10 score of 0.651 on a benchmark of user-focused topics. For AI practitioners, the VideoUFO dataset provides a resource for training or fine-tuning text-to-video models to better meet user expectations in real-world, diverse applications.  |
| Large-Scale Data Selection for Instruction Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.01807) or [HuggingFace](https://huggingface.co/papers/2503.01807))| pradeepd, pangwei, faezeb, nanami, hamishivi | This paper systematically investigates the scaling properties of automated data selection methods for instruction-tuning language models. The main research objective is to determine how well various data selection approaches perform when selecting large datasets (up to 2.5M samples) from large pools (up to 5.8M samples) for instruction tuning. The key methodology involves comparing nine data selection techniques, including representation-based, gradient-based, and loss/perplexity-based methods, across multiple dataset sizes and selection pools, evaluating performance on seven diverse tasks. The primary result is that a variant of representation-based data selection (RDS+) consistently outperforms other methods, including random selection, achieving an average score of 50.5 versus 46.4 for the next best method (Embed (GTR)) when selecting 10k data points. This implies that AI practitioners should consider using the proposed simple, embedding-based RDS+ method, especially in large-scale settings, rather than more computationally expensive methods when selecting data for finetuning LLMs.  |
| Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator (Read more on [arXiv](https://arxiv.org/abs/2503.01103) or [HuggingFace](https://huggingface.co/papers/2503.01103))| mingyuliutw, gdhe17, HuayuChen, Ema11, worstcoder | Direct Discriminative Optimization (DDO) finetunes likelihood-based visual generative models using a GAN-inspired objective without extra networks. The research aims to improve the sample quality of likelihood-based generative models beyond the limitations of maximum likelihood estimation (MLE). DDO implicitly parameterizes a discriminator using the likelihood ratio between a learnable target model and a fixed, pretrained reference model, optimizing the target model with a GAN discriminator loss. Finetuning a diffusion model (EDM) with DDO achieved a new record FID score of 1.30 on CIFAR-10, a significant improvement over the base model's 1.79. AI practitioners can directly finetune and iteratively refine pretrained likelihood-based generative models to achieve state-of-the-art performance without modifying model architecture or inference procedures.  |
| AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.01063) or [HuggingFace](https://huggingface.co/papers/2503.01063))| dnoever | This paper explores the potential for large language models (LLMs) to create private tonal languages for machine-to-machine communication. The main research question is whether AI agents can autonomously invent and use private tonal languages, and what those languages might resemble. The key methodology involves implementing a character-to-frequency mapping system using musical semitones to encode the full ASCII character set, creating a prototype tonal language. Primary results demonstrate that tonal encoding can achieve information rates exceeding human speech, with the ASCII mapping spanning approximately 7.8 octaves (220 Hz to 50175.42 Hz). The principle implication for AI practioners is that LLMs could theoretically engage in M2M communications, partially or wholly, outside of human perceptual boundaries, raising a need for transparency, oversight, and governance strategies in AI development.  |
| CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments (Read more on [arXiv](https://arxiv.org/abs/2503.00729) or [HuggingFace](https://huggingface.co/papers/2503.00729))| Qing Zhao, Zhixin Mai, Yiming Zhao, Ge Wang, SP4595 | CLEA is a closed-loop embodied agent framework that enhances task execution in dynamic environments using multiple LLMs. The main research objective is to address the limitations of Large Language Models (LLMs) in embodied systems for reliable execution of subtask sequences and one-shot success in long-term tasks within dynamic environments. The key methodology involves a closed-loop architecture with four specialized open-source LLMs and a planner-critic framework, integrating environmental memory and multimodal feedback for dynamic task management. Across 12 task trials, CLEA achieved a 67.3% improvement in success rate and a 52.8% increase in task completion rate compared to the open-loop baseline. For AI practitioners, the framework offers a robust method for deploying embodied agents in real-world, dynamic settings by facilitating adaptive strategy adjustment, enhancing task planning, and improving execution through continuous environmental feedback.  |
