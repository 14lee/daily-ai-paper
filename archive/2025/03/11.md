

## Papers for 2025-03-11

| Title | Authors | Summary |
|-------|---------|---------|
| Feature-Level Insights into Artificial Text Detection with Sparse
  Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2503.03601) or [HuggingFace](https://huggingface.co/papers/2503.03601))| Kristian Kuznetsov, natriistorm, razzant, plina2polina, Kushnareva | This paper explores enhancing interpretability in artificial text detection (ATD) using Sparse Autoencoders (SAEs) to extract features from a Gemma-2-2b model's residual stream, categorizing them, and analyzing their effectiveness. The main research objective is to improve ATD interpretability by analyzing the semantics and relevance of SAE-extracted features. The key methodology involves applying SAEs to Gemma-2-2b's residual stream, analyzing extracted features through domain/model-specific statistics, steering, and manual/LLM-based interpretation, and evaluating feature effectiveness using XGBoost and threshold classifiers. A primary result is that SAE-derived features at the 16th layer outperform a state-of-the-art MTL model and mean-pooled activations on the COLING dataset in detecting artificially generated text. For AI practitioners, using SAEs for feature extraction offers a valuable approach for understanding text generators and detectors and their generalization, which helps in developing more robust and interpretable ATD systems.  |
| SEAP: Training-free Sparse Expert Activation Pruning Unlock the
  Brainpower of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.07605) or [HuggingFace](https://huggingface.co/papers/2503.07605))| Xun Liang, BO1022, Ki-Seki, siminniu, UglyToilet | SEAP is a training-free method that prunes large language models (LLMs) by dynamically selecting task-relevant parameters to reduce inference overhead. The main research objective is to develop a pruning technique that reduces computational overhead while maintaining LLM performance on various tasks. The key methodology is Sparse Expert Activation Pruning (SEAP), which identifies task-specific expert activation patterns and prunes the model based on dynamically distributed sparsity. Primary results show that at 50% pruning, SEAP surpasses WandA and FLAP by over 20% in task accuracy on the Llama-2-7B model. The principal implication for AI practitioners is that SEAP provides a scalable and effective approach for optimizing large-scale LLMs, enabling more efficient deployment in resource-constrained environments.  |
| MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.07365) or [HuggingFace](https://huggingface.co/papers/2503.07365))| wangwhcore, friskit, hflqf88888, Cierra0506, FanqingM | MM-Eureka successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning, demonstrating visual "aha moments". The main research objective was to investigate the effectiveness of large-scale RL in multimodal reasoning and open-source the pipeline. The key methodology involved applying rule-based RL without supervised fine-tuning, using a simple reward function (accuracy and format), and the REINFORCE Leave-One-Out (RLOO) algorithm. MM-Eureka-Zero-38B, trained with only 9.3k image-text data, achieved a 46.4% accuracy on the K12 math test set, surpassing the instruct model and an 8.2% improvement. AI practitioners can use this open-sourced framework and simple RL setup to efficiently improve the multimodal reasoning ability of both instruction-tuned and pre-trained models, with potentially significant data efficiency gains.  |
| Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.07002) or [HuggingFace](https://huggingface.co/papers/2503.07002))| Zongqing Lu, Jiazheng Liu, tellarin, sipeng9527 | This paper introduces MMDiag, a new multi-turn multimodal dialogue dataset, and DiagNote, a model designed to improve focus and reasoning in such dialogues. The research aims to address the challenge of maintaining focus on target regions in multi-turn multimodal dialogues, specifically "saliency tracking" and "saliency recall". The key methodology involves a new dataset, MMDiag, generated collaboratively through rules and GPT assistance, and a two-module (Deliberate and Gaze) model, DiagNote, that interacts to perform Chain-of-Thought reasoning and annotations. DiagNote, trained on MMDiag + COCO, achieved a 0.648 average Intersection over Union (IoU) score on grounding benchmarks, outperforming baselines. For AI practitioners, the work provides a new challenging benchmark (MMDiag) and demonstrates improved multimodal grounding and reasoning abilities via the proposed DiagNote model, potentially leading to better handling multi-turn conversational settings.  |
| Automated Movie Generation via Multi-Agent CoT Planning (Read more on [arXiv](https://arxiv.org/abs/2503.07314) or [HuggingFace](https://huggingface.co/papers/2503.07314))| Zeyu Zhu, AnalMom, weijiawu | MovieAgent is a multi-agent framework for automatically generating long-form videos from a script synopsis and character bank. The main research objective is to automate the process of movie generation, including narrative planning, scene structuring, and shot composition, which traditionally requires extensive manual effort. The key methodology involves a hierarchical Chain-of-Thought (CoT) reasoning process using multiple LLM agents simulating roles like director, screenwriter, and storyboard artist, decomposing the movie generation process into manageable, sequential steps. Primary results show MovieAgent achieving a CLIP score of 22.25 and an Inception score of 9.39 in keyframe generation, with 97.84 motion smoothness in video generation. The principal implication is that AI practitioners can leverage this framework to significantly reduce the cost and time required for movie/long-video production, automating narrative and cinematic planning while ensuring character consistency and narrative coherence.  |
| FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates (Read more on [arXiv](https://arxiv.org/abs/2503.07216) or [HuggingFace](https://huggingface.co/papers/2503.07216))| Sung Ju Hwang, matbambbang, Seanie-lee, Sangsang | FedRand enhances privacy in federated learning (FL) for vision-language models (VLMs) by randomizing Low-Rank Adaptation (LoRA) subparameter updates. The main research objective is to mitigate membership inference attacks (MIAs) in FL when training VLMs, specifically addressing the vulnerability caused by exposing full client model parameters to the central server. The key methodology, FedRand, involves clients randomly selecting a subset of LoRA parameters from the server and keeping the remaining LoRA parameters private; after local training, only non-private parameters are sent back for aggregation. Experimental results on MSCOCO show FedRand achieved a CIDEr score of 110.27 while maintaining an AUROC of 53.84% against MIAs, demonstrating comparable task performance to FedAvg (CIDEr: 111.08) and improved MIA robustness. This implies that AI practitioners can improve privacy in federated learning of VLMs, without significant performance degradation, by communicating only a random subset of LoRA parameters between client and server.  |
| DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.07067) or [HuggingFace](https://huggingface.co/papers/2503.07067))| Luming Liang, tding1, sungnyun, tianyic, jongwooko | DISTILLM-2 introduces a contrastive learning approach to improve knowledge distillation for compressing large language models (LLMs).  Main research question or objective: Can a contrastive approach, considering both teacher and student generated outputs, improve the performance of distilled smaller language models (sLMs)?  Key methodology used: DISTILLM-2 uses a contrastive loss function (combining Skew KL and reverse Skew KL) applied asymmetrically to teacher- and student-generated responses, along with optimized data curation and curriculum-based adaptive loss mechanisms.  Primary results: DISTILLM-2 achieved state-of-the-art performance on instruction-following, outperforming the second-best method by +2.34%, on average for Qwen2-1.5B model.  Principal implication for AI practitioners: AI practitioners can utilize DISTILLM-2 to build high-performing, compact language models suitable for deployment where computational resources are limited, using the proposed contrastive distillation.  |
| EasyControl: Adding Efficient and Flexible Control for Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.07027) or [HuggingFace](https://huggingface.co/papers/2503.07027))| Jiaming Liu, Yirui Yuan, wanghaofan, yiren98, zzyx | i) EasyControl is presented as a lightweight, efficient, and flexible framework for condition-guided Diffusion Transformers (DiT). ii) The research objective is to enable efficient and flexible control over DiT models, addressing limitations in existing spatial and subject control mechanisms. iii) The method involves a Condition Injection LoRA Module, a Position-Aware Training Paradigm, and a Causal Attention Mechanism with KV Cache. iv) The framework achieves a 58% reduction in inference time compared to ablated versions while maintaining a 15M parameter count in single-condition settings, with the best overall performance in multi-condition configurations. v) EasyControl offers AI practitioners an efficient and adaptable approach to conditional image generation with DiT models, particularly beneficial for applications requiring precise spatial control, subject manipulation, and multi-condition integration.  |
| FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation
  for Feature Implementation (Read more on [arXiv](https://arxiv.org/abs/2503.06680) or [HuggingFace](https://huggingface.co/papers/2503.06680))| Wei Li, lisijia0504, yangyu90, dawnmsg, CharonBony | FEA-Bench is a benchmark for evaluating large language models on repository-level code generation for feature implementation. The main research objective is to assess the ability of LLMs to perform incremental development within code repositories by adding new features. The key methodology involves collecting pull requests from 83 GitHub repositories, filtering them based on rules and intent, and pairing code changes with unit tests. Primary results show that the best-performing LLM (DeepSeek-R1) resolves only 9.92% of task instances in the Oracle and Detailed prompt settings. The principal implication for AI practitioners is that current LLMs face significant challenges in repository-level incremental code development, requiring improvements in handling long contexts and complex code modifications.  |
| AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via
  Reinforcement Learning and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.07608) or [HuggingFace](https://huggingface.co/papers/2503.07608))| Qian Zhang, xinggangw, wenyuliu, Atan-0221, rb93dett | AlphaDrive is a VLM-based framework for autonomous driving planning that leverages reinforcement learning and reasoning. The main research objective is to investigate how reinforcement learning (RL) and reasoning can be applied to enhance the performance of vision-language models (VLMs) in autonomous driving planning while reducing training costs. The key methodology involves a two-stage training strategy combining supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO)-based RL, using four custom-designed rewards for planning accuracy, action weighting, diversity, and output format. Primary results show AlphaDrive significantly improves planning accuracy by 25.52% compared to an SFT-trained model, and outperforms SFT by 35.31% with only 20% of the training data. For AI practitioners, AlphaDrive demonstrates the efficacy of integrating GRPO-based RL and a two-stage training approach with planning-specific rewards, offering a method to improve planning performance and training efficiency of VLMs in autonomous driving.  |
| DreamRelation: Relation-Centric Video Customization (Read more on [arXiv](https://arxiv.org/abs/2503.07602) or [HuggingFace](https://huggingface.co/papers/2503.07602))| Shiwei Zhang, Shuaishuai0219, lloong, JacobYuan, weilllllls | DreamRelation is a novel method for customizing relational video content based on a small set of exemplar videos. The main research question is: How can we decouple relations and subject appearances while accurately modeling relational dynamics to enhance generalizability in customized video generation? The key methodology involves relational decoupling learning, using a relation LoRA triplet and hybrid mask training strategy to separate relations from appearances, and relational dynamics enhancement via a space-time relational contrastive loss. The primary results show that DreamRelation achieves a relation accuracy of 0.4452 ± 0.01, outperforming baselines like direct LoRA finetuning (0.3258 ± 0.05) and MotionInversion (0.3151 ± 0.03). The principal implication for AI practitioners is that by effectively disentangling relational dynamics from subject appearances, DreamRelation provides a more precise and generalizable approach to relational video customization, enabling applications such as creation of diverse human-like animal interactions in novel domains.  |
| Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models (Read more on [arXiv](https://arxiv.org/abs/2503.06580) or [HuggingFace](https://huggingface.co/papers/2503.06580))| Jitao Sang, Xinyan Wen, Jiangming Shu, tzteyang, TokerZ | Large Agent Models (LAMs) internalize Chain-of-Action generation, allowing autonomous decisions on when and how to use external tools. The research objective is to develop a framework, AutoCoA, that enables reasoning models to autonomously generate Chain-of-Action (CoA) for improved task completion. The methodology combines supervised fine-tuning (SFT) with reinforcement learning (RL), including step-level action triggering and trajectory-level CoA optimization, and utilizes an internal world model. Primary results show AutoCoA-trained agent models achieve a 33.9% Exact Match accuracy on multi-hop QA tasks like Bamboogle, significantly outperforming ReAct-based workflows (15.2%). Principal implication for AI practitioners: The AutoCoA framework provides a method to train agent models that show enhanced performance by reducing reliance on externally prompted actions.  |
| WritingBench: A Comprehensive Benchmark for Generative Writing (Read more on [arXiv](https://arxiv.org/abs/2503.05244) or [HuggingFace](https://huggingface.co/papers/2503.05244))| SHaopeng Lai, Chenliang Li, Ming Yan, Jiahao Mei, AQuarterMile | WritingBench, a new benchmark, evaluates large language models (LLMs) across diverse writing tasks, incorporating a query-dependent evaluation framework.  The main objective is to create a comprehensive benchmark for evaluating LLMs on diverse, real-world generative writing tasks and to propose a query-dependent evaluation framework.  Key methodology involves a four-stage query construction pipeline leveraging LLMs and human refinement, and a query-dependent evaluation framework using dynamically generated, instance-specific criteria scored by a fine-tuned critic model.  Primary results show that the query-dependent evaluation framework achieves 83% human alignment, significantly surpassing static-criteria baselines (65%, 59%).  Principal implication for AI practitioners is that WritingBench provides a more nuanced and robust evaluation tool for writing-focused LLMs, and the query-dependent evaluation approach can lead to more accurate and human-aligned assessment of generative writing capabilities, guiding improvements in model development.  |
| SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and
  Multi-dimensional Evaluation for Automated Survey Writing (Read more on [arXiv](https://arxiv.org/abs/2503.04629) or [HuggingFace](https://huggingface.co/papers/2503.04629))| Bin Wang, Renqiu Xia, Jiakang Yuan, Shiyang Feng, Xiangchao Yan | SurveyForge is a framework for automated survey paper generation using heuristic outline generation, memory-driven content creation, and multi-dimensional evaluation. The main research objective is to address the quality gap between AI-generated and human-written surveys, focusing on outline structure, citation accuracy, and content comprehensiveness. The methodology involves a two-stage process: heuristic outline generation based on human-written survey patterns and relevant literature, followed by memory-driven content generation using a scholar navigation agent with temporal-aware reranking. Key results show that SurveyForge outperforms the baseline AutoSurvey in reference coverage (0.40 vs 0.23 using Claude-3-Haiku) and overall content quality (76.34 vs 73.87). AI practitioners can use SurveyForge to create comprehensive, structured survey papers more efficiently and with higher literature coverage than existing methods.  |
| Vision-R1: Incentivizing Reasoning Capability in Multimodal Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.06749) or [HuggingFace](https://huggingface.co/papers/2503.06749))| Zheyu Ye, Shaosheng Cao, Zijie Zhai, Bohan Jia, Wenxuan Huang | Vision-R1, a multimodal large language model (MLLM), enhances reasoning by combining cold-start initialization with reinforcement learning (RL). The main research objective is to enhance the reasoning capability of MLLMs using RL, addressing limitations of direct RL training. Key methodology used is Modality Bridging with Progressive Thinking Suppression Training (PTST) and Group Relative Policy Optimization (GRPO) using the hard formatting result reward function. Primary results show Vision-R1-7B achieves 73.5% accuracy on the MathVista benchmark, which is only 0.4% lower than the leading model, OpenAI 01. Principal implication for AI practitioners: Using cold-start initialization with a high-quality multimodal Chain-of-Thought (CoT) dataset, combined with the PTST strategy during RL, improves the mathematical reasoning of MLLMs, providing a viable training approach.  |
| LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted
  Contrastive Learning (Read more on [arXiv](https://arxiv.org/abs/2503.04812) or [HuggingFace](https://huggingface.co/papers/2503.04812))| Jinsong Su, Jie Zhou, Fandong Meng, lqniu, zhibinlan | LLaVE is a multimodal embedding model framework that improves performance by focusing on hard negative pairs during contrastive learning. The main research objective is to address the challenge that existing Large Multimodal Model (LMM)-based embedding models struggle to distinguish hard negative pairs effectively when trained with the standard InfoNCE loss. The key methodology involves hardness-weighted contrastive learning, using a reward model to dynamically assign larger weights to harder negative pairs and cross-device negative sample gathering. Primary results show that LLaVE-7B achieves a 6.2 point performance improvement on the MMEB benchmark over the previous state-of-the-art model. The principal implication for AI practitioners is that employing hardness-weighted contrastive learning with LMMs can create more powerful and generalizable multimodal embedding models, with the framework applied and scaling well to diverse datasets.  |
| MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.07459) or [HuggingFace](https://huggingface.co/papers/2503.07459))| Jiapeng Chen, Jiwoong Sohn, Daniel Shao, wshi83, RTT1 | This paper introduces MEDAGENTSBENCH, a new benchmark for evaluating large language models (LLMs) on complex medical reasoning tasks. The main research objective is to assess the performance of advanced thinking models and agent frameworks in challenging medical scenarios requiring multi-step reasoning. The key methodology involves constructing a dataset of 862 questions from seven established medical datasets, using adversarial filtering to select difficult questions and evaluating various LLMs and agent-based methods using standardized prompts and metrics. A primary result is that DEEPSEEK-R1 achieved the highest scores on five of the datasets and the accuracy values are highlighted in the papers such as MedMCQA: 31.0%, MMLU: 43.8%, MMLU-Pro: 37.0%, MedExQA: 26.0%, and MedXpertQA-U: 26.0%. The principal implication for AI practitioners is that thinking models, like DEEPSEEK-R1, and search-based agent methods, like AFLOW, offer superior performance in complex medical reasoning and better cost-efficiency than the other LLMs and agents, guiding model selection for real-world applications.  |
| PE3R: Perception-Efficient 3D Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2503.07507) or [HuggingFace](https://huggingface.co/papers/2503.07507))| Xinchao Wang, Shizun Wang, Jie Hu | PE3R is a novel framework for efficient and accurate 3D semantic reconstruction from 2D images without requiring 3D data or camera parameters. The main research objective is to develop a method for 3D semantic reconstruction that generalizes across diverse scenes and objects, achieves high perception accuracy, and operates at high speed. The key methodology involves a feed-forward architecture incorporating pixel embedding disambiguation, semantic field reconstruction, and global view perception modules. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction compared to previous methods, along with improved accuracy and precision. For AI practitioners, PE3R provides a faster and more generalizable approach to 3D scene understanding from 2D images, enabling applications in scenarios with limited 3D data availability.  |
| Effective and Efficient Masked Image Generation Models (Read more on [arXiv](https://arxiv.org/abs/2503.07197) or [HuggingFace](https://huggingface.co/papers/2503.07197))| Jun Zhou, Jun Hu, Xiaolu Zhang, Jingyang Ou, yyyou | eMIGM unifies and improves masked image and diffusion models for efficient, high-quality image generation. The main research objective is to systematically explore the design space of training and sampling in masked image generation models, identifying key factors contributing to performance and efficiency. The key methodology involves unifying masked image modeling and masked diffusion models, then exploring variations in masking distributions, weighting functions, conditional distributions, and sampling strategies like time-interval classifier-free guidance. A primary result is that on ImageNet 512x512, eMIGM-L surpasses EDM2 with an FID of 1.77, using only 60% of the function evaluations. The principal implication is that AI practitioners can leverage eMIGM's unified framework and optimized training/sampling strategies to achieve state-of-the-art image generation with significantly reduced computational cost.  |
| Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive
  Reinforcement (Read more on [arXiv](https://arxiv.org/abs/2503.06520) or [HuggingFace](https://huggingface.co/papers/2503.06520))| Fanbin Lu, Zihao Yue, Zhisheng Zhong, Bohao Peng, Yuqi Liu | Seg-Zero is a framework for reasoning segmentation that leverages cognitive reinforcement learning to achieve zero-shot generalization. The main research objective is to develop a segmentation model that exhibits strong generalization and explicit reasoning capabilities without relying on supervised fine-tuning with explicit reasoning data. The key methodology involves a decoupled architecture with a reasoning model (MLLM) generating a chain-of-thought and positional prompts, and a segmentation model producing pixel-level masks, trained using reinforcement learning with a novel reward mechanism. Primary results show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18%. The principal implication for AI practitioners is that pure reinforcement learning, guided by a well-designed reward mechanism, can induce emergent reasoning in segmentation models, improving generalization across domains without explicit reasoning supervision.  |
| BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement
  for Transformers in Large-Scale Time Series Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.06121) or [HuggingFace](https://huggingface.co/papers/2503.06121))| xiaol, Alic-Li | Rimer replaces the transformer backbone in time series models with RWKV-7, achieving superior performance and efficiency. The research objective was to develop a more efficient and scalable time-series model compared to transformer-based approaches. The methodology involved integrating RWKV-7's time mix and channel mix components into the transformer-based time series model, Timer. The Rimer model achieved a 1.13x to 43.3x performance improvement and a 4.5x reduction in training time with 1/23 the parameters of the original Timer model. AI practitioners can leverage Rimer for improved performance and reduced computational cost in large-scale time series modeling tasks, benefiting from its compatibility with both AMD and NVIDIA GPUs.  |
| This Is Your Doge, If It Please You: Exploring Deception and Robustness
  in Mixture of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.05856) or [HuggingFace](https://huggingface.co/papers/2503.05856))| Ilija Bogunovic, Sangwoong Yoon, Llwo | Mixture of LLM Agents (MoA) architectures are vulnerable to significant performance degradation when even a single agent acts deceptively. This paper explores the robustness of Mixture of LLM Agents (MoA) against deceptive agents that provide misleading responses. The authors evaluate MoA's performance on AlpacaEval 2.0 and QUALITY benchmarks, introducing deceptive agents into the multi-agent system. They find that introducing a single deceptive agent into a 7-agent MoA reduces the length-controlled win rate on AlpacaEval 2.0 from 49.2% to 37.9%. AI practitioners should implement defense mechanisms, such as those proposed in this paper, to mitigate the risks associated with deceptive agents in multi-agent LLM systems.  |
| Efficient Distillation of Classifier-Free Guidance using Adapters (Read more on [arXiv](https://arxiv.org/abs/2503.07274) or [HuggingFace](https://huggingface.co/papers/2503.07274))| msadat97, cristianpjensen | Adapter Guidance Distillation (AGD) efficiently simulates classifier-free guidance (CFG) in diffusion models using lightweight adapters, doubling sampling speed while maintaining quality. The main research objective is to mitigate the computational cost of CFG in conditional diffusion models, which doubles the number of neural function evaluations per inference step. The key methodology involves training lightweight adapters on CFG-guided trajectories to approximate CFG in a single forward pass, keeping the base diffusion model frozen. AGD achieves a FID score of 5.03 on class-conditional ImageNet generation using DiT, outperforming CFG (FID 5.30) and matching or exceeding the performance across various other tested architectures. For AI practitioners, AGD enables faster sampling from diffusion models with performance similar to or exceeding the use of CFG, and distilling large models such as Stable Diffusion XL on a single consumer GPU.  |
| State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for
  State Space Models (Read more on [arXiv](https://arxiv.org/abs/2503.03499) or [HuggingFace](https://huggingface.co/papers/2503.03499))| Hyung Il Koo, Minjae Lee, Yuchen Zeng, Kevin Galim, Wonjun Kang | State-offset Tuning is a new parameter-efficient fine-tuning method for State Space Models (SSMs) that directly modifies state-related features. The main research objective is to develop a more effective parameter-efficient fine-tuning (PEFT) method for SSMs than existing prompt-based methods. The key methodology is State-offset Tuning, which adds a learnable, constant state-offset to the hidden state at each timestep within the SSM module. Primary results show State-offset Tuning (h) achieved 59.9 execution accuracy on the Spider dataset, outperforming other PEFT methods with comparable parameter budgets. AI practitioners can use State-offset Tuning to efficiently adapt pretrained SSMs to downstream tasks, achieving performance comparable to full fine-tuning with significantly fewer trainable parameters.  |
| Should VLMs be Pre-trained with Image Data? (Read more on [arXiv](https://arxiv.org/abs/2503.07603) or [HuggingFace](https://huggingface.co/papers/2503.07603))| Igor Vasiljevic, Kushal Arora, Samir Yitzhak Gadre, Jean Mercat, Sedrick Keh | Vision-Language Models (VLMs) can be improved by incorporating image data during pre-training, before the model is fully pre-trained with text. The main research question is when and how image data should be introduced during VLM pre-training to optimize downstream performance on vision-language and text-only tasks. Researchers trained approximately 300 models, systematically varying text-only pre-training amounts, image-text ratios, and fine-tuning stages using a decoder-only transformer architecture with a frozen image encoder. A key finding is that, for a 1B parameter model, introducing visual tokens 80% of the way through pre-training leads to a 2% average improvement on vision-language tasks compared to introducing them after full pre-training. The results suggest that AI practitioners should integrate image data earlier in VLM pre-training, but not immediately, to maintain text performance, instead of following traditional separate training phases.  |
| WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.07265) or [HuggingFace](https://huggingface.co/papers/2503.07265))| Peng Jin, Bin Lin, Mengren Zheng, Munan Ning, Yuwei Niu | The paper introduces WISE, a new benchmark for evaluating text-to-image (T2I) models' ability to integrate world knowledge and complex semantics, along with a new metric called WiScore. The main research objective is to assess how well T2I models can generate images that accurately reflect complex semantic understanding and world knowledge, going beyond simple text-image alignment. The key methodology involves a benchmark of 1000 prompts across 25 sub-domains of cultural common sense, spatio-temporal reasoning, and natural science, and evaluates 20 T2I models (10 dedicated, 10 unified) using a novel quantitative metric, WiScore, which assesses knowledge-image alignment. A key result is that the FLUX.1-dev model achieved the best overall WiScore of 0.50, while dedicated T2I models generally outperformed unified multimodal models in leveraging world knowledge. The primary implication is that AI practitioners need to develop enhanced methods for incorporating and applying world knowledge in T2I models, as existing models demonstrate significant limitations in this area.  |
| ProBench: Judging Multimodal Foundation Models on Open-ended
  Multi-domain Expert Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.06885) or [HuggingFace](https://huggingface.co/papers/2503.06885))| Liu Liu, Bei Chen, Haoning Wu, dxli1, HelloKKMe | ProBench is a benchmark for evaluating multimodal foundation models on expert-level, open-ended tasks using MLLM-as-a-Judge. The main research objective is to assess the capabilities of multimodal large language models (MLLMs) on complex, real-world professional tasks requiring expert knowledge and advanced reasoning. The key methodology involves curating a dataset of 4,000 high-quality, open-ended user queries submitted by professionals across 10 fields and 56 sub-fields, and evaluating 24 MLLMs using an MLLM-as-a-Judge approach. The primary results reveal that while the best open-source models rival proprietary ones, ProBench presents significant challenges, and that the MLLM-as-a-Judge evaluation shows 79.9% agreement with human experts. A principal implication for AI practitioners is that current MLLMs still struggle with visual perception, textual understanding, domain knowledge, and advanced reasoning, highlighting the specific areas requiring focused development for improved performance on real-world expert tasks.  |
| Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by
  Learning Language-Agnostic Speech Representations (Read more on [arXiv](https://arxiv.org/abs/2503.06273) or [HuggingFace](https://huggingface.co/papers/2503.06273))| Yong Man Ro, Stavros Petridis, Chae Won Kim, Minsu Kim, JeongHun0716 | This paper explores zero-shot audio-visual speech recognition (AVSR) using language-agnostic speech representations and Large Language Models (LLMs). The main research objective is to enable speech recognition in target languages without any audio-visual speech data in those languages. The key methodology involves an Audio-Visual Speech Romanizer (AV-Romanizer) to predict Roman text and uses pre-trained LLMs and multi-task training to convert it into language-specific graphemes. The Zero-AVSR framework, trained on a new Multilingual Audio-Visual Romanized Corpus (MARC) of 2,916 hours, achieves a 25.2% average WER on the MuAViC dataset. AI practitioners can leverage this framework to expand language support in AVSR systems without requiring target-language speech data.  |
| Words or Vision: Do Vision-Language Models Have Blind Faith in Text? (Read more on [arXiv](https://arxiv.org/abs/2503.02199) or [HuggingFace](https://huggingface.co/papers/2503.02199))| Bryan Hooi, Tri Cao, Ailin Deng, ryanchen42 | Vision-Language Models (VLMs) exhibit a "blind faith in text" phenomenon, disproportionately trusting textual data over visual data when inconsistencies arise. The main research question is: How do VLMs handle inconsistencies between visual and textual inputs? The key methodology involves introducing textual variations (match, corruption, irrelevance) to four vision-centric tasks and evaluating ten VLMs. A primary result is that Qwen2-VL-7B's accuracy on VQAv2, DocVQA, and MathVista drops to approximately 50% of its original levels under text corruption. The principal implication for AI practitioners is that balanced training and careful consideration of modality interactions are crucial for enhancing VLM robustness and reliability when handling multi-modal data inconsistencies, especially in safety-critical applications.  |
| Detection Avoidance Techniques for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.07595) or [HuggingFace](https://huggingface.co/papers/2503.07595))| Gabi Dreo Rodosek, Joao A. G. Schneider, Florian Steuber, SinclairSchneider | This research investigates methods to bypass large language model (LLM) detection systems. The main research objective is to explore the vulnerability of various LLM detection techniques to different evasion strategies. The key methodology involves modifying generative model parameters (temperature, sampling), applying reinforcement learning to fine-tune models, and using paraphrasing models. Primary results show that paraphrasing led to a >90% evasion rate of zero-shot detectors like DetectGPT, reducing detection from 88.6% to 8.7% in one experiment. Principal implication for AI practitioners is that current LLM detection classifiers can be easily bypassed, requiring further research into more robust detection and adaptive detection methods.  |
| DiffCLIP: Differential Attention Meets CLIP (Read more on [arXiv](https://arxiv.org/abs/2503.06626) or [HuggingFace](https://huggingface.co/papers/2503.06626))| Bernard Ghanem, Hasan Abed Al Kader Hammoud | DiffCLIP extends CLIP with a differential attention mechanism to improve vision-language model performance. The main research question is whether differential attention can be adapted to vision-language models to improve their ability to focus on relevant features across modalities. The key methodology involves integrating differential attention, which subtracts complementary attention distributions, into CLIP's dual-encoder (image and text) architecture. DiffCLIP outperforms standard CLIP on image-text retrieval, with a 1.2% average improvement on image retrieval using the CC3M dataset. AI practitioners can use DiffCLIP as a lightweight, parameter-efficient addition to CLIP that enhances performance across various vision-language tasks, including few-shot, zero-shot, and robustness benchmarks.  |
| Novel Object 6D Pose Estimation with a Single Reference View (Read more on [arXiv](https://arxiv.org/abs/2503.05578) or [HuggingFace](https://huggingface.co/papers/2503.05578))| Hui Yang, Jin Zheng, Kai Zeng, Wei Sun, JianLiu99 | SinRef-6D is a framework for estimating the 6D pose of novel objects using only a single RGB-D reference view. The main research objective is to develop a CAD-model-free and dense-reference-view-free method for novel object 6D pose estimation that is scalable and efficient. The key methodology involves iteratively establishing point-wise alignment in the camera coordinate system using state space models (SSMs) for feature encoding, and RGB and points SSMs to capture spatial information. The primary results show that SinRef-6D achieves 90.3% on the LineMod dataset using the ADD-0.1d metric, which is on par with some CAD-based and superior compared to single-reference view based methods. This implies that AI practitioners can achieve accurate 6D pose estimation for novel objects without requiring CAD models or multiple reference views, reducing computational overhead and manual efforts, and enhance the practicality in real-world settings.  |
| Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.04973) or [HuggingFace](https://huggingface.co/papers/2503.04973))| Fabio Petroni, Orion Weller, papotti, giulio98 | This paper introduces a task-aware KV cache compression method for large language models to improve reasoning over large external knowledge corpora. The main research objective is to develop a query-agnostic compression technique that preserves efficiency while maintaining competitive performance compared to query-aware compression and Retrieval-Augmented Generation (RAG). The key methodology involves precomputing a compressed key-value (KV) cache, guided by a task description and optionally few-shot examples, which can be reused for any query within the defined task domain. The approach improves accuracy by up to 7 absolute points over RAG on LongBench v2 with a 30x compression rate, and reduces inference latency. The principal implication is that AI practitioners can leverage task-aware KV cache compression to enable more efficient and comprehensive reasoning over large corpora in LLM applications, outperforming RAG in broad-knowledge tasks.  |
| HumanMM: Global Human Motion Recovery from Multi-shot Videos (Read more on [arXiv](https://arxiv.org/abs/2503.07597) or [HuggingFace](https://huggingface.co/papers/2503.07597))| Jing Lin, Zhuokai Zhao, Ling-Hao Chen, Guanlin Wu, Yuhong Zhang | HumanMM is a framework for reconstructing 3D human motion in world coordinates from multi-shot videos, addressing challenges like shot transitions and occlusions. The main research objective is to reconstruct long-sequence 3D human motion in world coordinates from in-the-wild videos with multiple shot transitions. The key methodology integrates enhanced camera pose estimation (using a modified LEAP-VO with human masking) with Human Motion Recovery (HMR), incorporating a shot transition detector, an alignment module for pose and orientation continuity across shots, and a custom motion integrator. The proposed method achieved a PA-MPJPE of 36.82 on the ms-AIST subset of the created ms-Motion dataset, outperforming existing methods. For AI practitioners, HumanMM provides a novel, robust method for reconstructing realistic human motion in world coordinates from multi-shot videos, enabling improved motion generation and understanding applications.  |
| YOLOE: Real-Time Seeing Anything (Read more on [arXiv](https://arxiv.org/abs/2503.07465) or [HuggingFace](https://huggingface.co/papers/2503.07465))| Jungong Han, Zijia Lin, Hui Chen, Lihao Liu, Ao Wang | YOLOE is a unified, efficient object detection and segmentation model that supports diverse open prompt mechanisms, achieving real-time performance. The main research objective is to develop a single model capable of detecting and segmenting arbitrary objects guided by text prompts, visual cues, or without prompts, with high efficiency and accuracy. The key methodology involves Re-parameterizable Region-Text Alignment (RepRTA) for text prompts, Semantic-Activated Visual Prompt Encoder (SAVPE) for visual prompts, and Lazy Region-Prompt Contrast (LRPC) for prompt-free scenarios, all built upon YOLO architectures. On LVIS, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP with 3x less training cost and 1.4x inference speedup. The principal implication for AI practitioners is that YOLOE provides a strong baseline and framework for developing real-time, open-prompt-driven vision applications, streamlining development by using a single efficient model for diverse prompt types.  |
| RePO: ReLU-based Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.07426) or [HuggingFace](https://huggingface.co/papers/2503.07426))| Jinyang Gao, Xue Wang, Kexin Huang, Junkang Wu, xiangwang1223 | RePO introduces a simplified offline preference optimization algorithm for aligning large language models (LLMs) with human preferences. The main research question is whether a simpler offline preference optimization algorithm can be developed that achieves comparable or better performance than existing methods. The key methodology involves using ReLU-based max-margin loss and reference-free reward margins, eliminating the need for the hyperparameter β in SimPO and simplifying the log-sigmoid activation. Primary results show that RePO outperforms DPO and SimPO across multiple base models on AlpacaEval 2, achieving a win rate of 51.1% on Llama3-8B and 66.6% on Gemma2-9B, and it require tuning only one hyperparameter, γ. For AI practitioners, RePO offers a more streamlined and efficient approach to preference optimization, requiring less hyperparameter tuning while achieving competitive or superior performance in LLM alignment.  |
| Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.06362) or [HuggingFace](https://huggingface.co/papers/2503.06362))| Stavros Petridis, Minsu Kim, Umberto Cappellazzo | Llama-MTSK, a Matryoshka-based Multimodal LLM, enables adaptive audio-visual speech recognition with flexible token allocation. The research objective is to create an audio-visual speech recognition (AVSR) system that dynamically adjusts computational efficiency and performance at inference time using a single model. The methodology involves encoding audio-visual representations at multiple granularities using Matryoshka Representation Learning and fine-tuning a pre-trained LLM with three LoRA-based Matryoshka strategies. On the LRS3 dataset, Llama-MTSK achieved a Word Error Rate (WER) of 2.3% using the SS configuration with an audio compression rate of 4 and video compression of 2, outperforming independently trained models. AI practitioners can use Llama-MTSK to deploy AVSR models that efficiently adapt to various computational constraints and accuracy requirements without retraining.  |
| Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent
  Spaces (Read more on [arXiv](https://arxiv.org/abs/2503.05283) or [HuggingFace](https://huggingface.co/papers/2503.05283))| Qixing Huang, Diego Gomez, Luca Moschella, Souhail Hadgi, teelinsan | This paper investigates the alignment between latent spaces of 3D and text encoders, finding that subspace projection improves cross-modal performance. The main research objective is to explore the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. The key methodology involves combining Canonical Correlation Analysis (CCA) for subspace selection with affine transformation and local CKA for alignment of 3D and text features. A primary result is that the affine + subspace projection method achieves a top-5 retrieval accuracy of 42.2% between uni-modal PointBert and RoBERTa, significantly higher than without subspace projection. Principal implication for AI practitioners that aligning lower-dimensional subspaces of 3D and text representations enables cross-modal applications, like matching and retrieval tasks, without expensive joint training, and offers a new tool.  |
| NeuGrasp: Generalizable Neural Surface Reconstruction with Background
  Priors for Material-Agnostic Object Grasp Detection (Read more on [arXiv](https://arxiv.org/abs/2503.03511) or [HuggingFace](https://huggingface.co/papers/2503.03511))| Xudong Zheng, Wenzhe He, Chao Li, Yinghao Cai, KianYale | NeuGrasp is a generalizable neural surface reconstruction method that uses background priors for 6-DoF robotic grasp detection of objects with various material properties. The main research objective is to develop a method for robust, material-agnostic grasp detection in scenes with transparent and specular objects from sparse views within a narrow field of view. The key methodology involves integrating transformers and global prior volumes within a neural implicit surface framework, using residual feature enhancement and an occupancy-prior volume to distinguish foreground objects. Primary results show that NeuGrasp achieved a success rate of 86.3% and declutter rate of 81.0% in simulation experiments on packed scenes with transparent and specular objects, outperforming baselines. AI practitioners can apply NeuGrasp to achieve accurate grasp detection using a small amount of RGB image input.  |
