

## Papers for 2025-07-16

| Title | Authors | Summary |
|-------|---------|---------|
| Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation
  from Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.07104) or [HuggingFace](https://huggingface.co/papers/2507.07104))| Jieneng Chen, Yu-cheng Chou, Yitong Li, lambertxiao, PatZhang11 | The Vision-Language-Vision (VLV) auto-encoder is a framework for distilling knowledge from frozen text-to-image (T2I) diffusion models to create a high-quality captioner with minimal cost. The primary objective is to develop a state-of-the-art captioning model that avoids the need for massive paired image-text datasets by leveraging existing pretrained models. The methodology uses a two-stage process: an encoder is first trained using only images to produce continuous embeddings that allow a frozen T2I diffusion model to reconstruct the image, then a pretrained LLM is fine-tuned to decode these embeddings into natural language. VLV achieves captioning performance comparable to proprietary models, with its captions yielding a text-to-image reconstruction FID score of 6.64, which is competitive with GPT-4o's score of 6.20, for a total training cost under $1,000. The principal implication for AI practitioners is that state-of-the-art multimodal systems can be built in a data- and cost-efficient manner by distilling knowledge from existing open-source models, drastically lowering entry barriers for developing advanced captioning capabilities. |
| EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and
  Reasoning Modes (Read more on [arXiv](https://arxiv.org/abs/2507.11407) or [HuggingFace](https://huggingface.co/papers/2507.11407))| Stanley Jungkyu Choi, Kibong Choi, Eunbi Choi, Kyunghoon Bae, LG AI Research | This paper introduces EXAONE 4.0, a series of unified language models that integrate distinct NON-REASONING and REASONING modes into a single architecture with agentic tool-use capabilities. The main objective is to unify the instruction-following usability of EXAONE 3.5 and the advanced reasoning of EXAONE Deep into a single model, while expanding context length to 128K and adding Spanish language support. The methodology involves pre-training on up to 14T tokens, employing a hybrid attention architecture (3:1 local-to-global ratio), and a multi-stage post-training pipeline featuring a novel reinforcement learning algorithm, AGAPO (Asymmetric Sampling and Global Advantage Policy Optimization). The EXAONE 4.0 32B model, in its REASONING mode, achieves a score of 85.3 on the AIME 2025 math benchmark, outperforming the larger Qwen 3 235B model. For AI practitioners, the key implication is the availability of an open-weight model that provides a switchable trade-off between fast, efficient responses and computationally intensive, high-accuracy reasoning within a single deployment, enabling flexible application development. |
| Scaling Laws for Optimal Data Mixtures (Read more on [arXiv](https://arxiv.org/abs/2507.09404) or [HuggingFace](https://huggingface.co/papers/2507.09404))| Enrico Fini, David Grangier, Dan Busbridge, Louis Bethune, Mustafa Shukor | This research proposes and validates scaling laws that predict foundation model loss as a function of model size (N), training tokens (D), and data domain mixture weights (h). The primary objective is to create a systematic method for determining the optimal data mixture for any target domain under a given training budget (N,D), replacing ad-hoc, trial-and-error approaches. The methodology extends Chinchilla-style power laws by modeling the law's coefficients as parametric functions of the domain mixture weights, with parameters estimated from a few small-scale training runs across different mixtures. The scaling laws accurately extrapolate from small-scale fits (e.g., models <1B parameters) to predict the loss of large-scale models (e.g., 8B parameters) on new, unseen domain mixtures; a 7B LLM trained with an optimized mixture achieved a CORE score of 58, outperforming the base (52) and uniform (53) mixtures. The principal implication for AI practitioners is the ability to use a few small-scale, low-cost experiments to computationally derive a near-optimal data mixture for a large-scale training budget, providing a principled alternative to costly, ad-hoc mixture selection. |
| Can Multimodal Foundation Models Understand Schematic Diagrams? An
  Empirical Study on Information-Seeking QA over Scientific Papers (Read more on [arXiv](https://arxiv.org/abs/2507.10787) or [HuggingFace](https://huggingface.co/papers/2507.10787))| Arman Cohan, Chuhan Li, Chengye Wang, yilunzhao | This paper introduces MISS-QA, a benchmark with 1,500 expert-annotated examples for evaluating multimodal models' ability to answer questions by interpreting schematic diagrams in scientific papers. The primary objective is to assess how well frontier foundation models can interpret these diagrams and synthesize information from the surrounding paper context, and to identify their key failure modes. The methodology involved constructing the benchmark from 465 AI papers, with questions requiring grounding in highlighted visual elements, and evaluating 18 models using an automated system with GPT-4.1 as the judge. The primary result shows a significant performance gap, with human experts achieving 89.0% accuracy while the top open-source model (Qwen2.5-VL-72B) scored only 61.6%, and most models exhibited overconfidence on unanswerable questions. The principal implication for AI practitioners is that current multimodal models are not yet reliable for scientific document analysis that requires contextual understanding of schematic diagrams, frequently failing to interpret visual structures or retrieve relevant text, and practitioners can use MISS-QA to test and mitigate these weaknesses. |
| LLMalMorph: On The Feasibility of Generating Variant Malware using
  Large-Language-Models (Read more on [arXiv](https://arxiv.org/abs/2507.09411) or [HuggingFace](https://huggingface.co/papers/2507.09411))| Ashish Kundu, Arun Iyengar, Imtiaz Karim, Adrian Shuai Li, Ajwad | The paper introduces LLMalMorph, a semi-automated, source-code-level framework that uses a pre-trained Large-Language-Model with engineered prompts to generate functional, evasive malware variants. The primary objective is to determine the feasibility of using pre-trained LLMs, without additional fine-tuning, to develop a semi-automated framework for generating evasive malware variants from C/C++ source code that preserve semantics and can bypass antivirus engines and ML-based classifiers. The framework, LLMalMorph, systematically extracts functions from malware source code using an AST parser, generates tailored prompts incorporating one of six transformation strategies (e.g., Code Optimization, Security), and uses the Codestral-22B model to produce modified code, which is then reintegrated and compiled with a human-in-the-loop process for debugging. Primary results demonstrate that LLMalMorph successfully reduced antivirus detection rates, with the "Windows" transformation strategy achieving a 37% detection rate reduction for the RansomWar sample. Furthermore, against an ML-based classifier (Malgraph), the "Security" transformation strategy achieved an attack success rate of 90.9% for the Babuk ransomware sample, despite not being explicitly optimized for ML evasion. The principal implication for AI practitioners is that general-purpose, pre-trained code-generating LLMs can be effectively repurposed for sophisticated offensive security tasks, demonstrating a critical dual-use concern and underscoring the need for robust, semantically-aware malware detectors resilient to LLM-driven code transformations. |
| OpenCodeReasoning-II: A Simple Test Time Scaling Approach via
  Self-Critique (Read more on [arXiv](https://arxiv.org/abs/2507.09075) or [HuggingFace](https://huggingface.co/papers/2507.09075))| Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, smajumdar94 | The paper introduces OPENCODEREASONING-II, a 2.5 million sample dataset for code generation and critique, and uses it to demonstrate a test-time scaling method that improves code generation performance via self-critique. The main objective is to determine if fine-tuning models on a large-scale dataset containing code solutions and corresponding critiques can enable effective test-time performance improvement through a self-selection mechanism. The methodology involves a two-stage supervised fine-tuning process on Qwen2.5-Instruct models, first for code generation and then jointly for generation and critique, followed by an inference strategy that generates multiple solutions and selects the best one using a self-critique heuristic. The primary result shows that this self-critique method improves the `pass@1` score of their flagship OCR-2-32B model on the LiveCodeBench Python benchmark by 6.1 percentage points, from 61.3% to 67.4%. The principal implication for AI practitioners is that they can enhance the single-attempt accuracy of code generation models by fine-tuning on critique data and applying a simple self-critique selection strategy at inference time, obviating the need for external verifiers or complex reinforcement learning pipelines. |
| AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.08616) or [HuggingFace](https://huggingface.co/papers/2507.08616))| Bryan Perozzi, Mikhail Galkin, Jan Tönshoff, Luis Müller, Florian Grötschla | The paper introduces AGENTSNET, a new benchmark for evaluating the coordination and collaborative reasoning capabilities of multi-agent LLM systems. The primary objective is to assess whether complex networks of LLM agents can effectively self-organize, communicate, and form collaborative strategies given a specific network topology. The methodology challenges multi-agent systems with five problems from distributed computing—consensus, leader election, coloring, matching, and vertex cover—on graph networks of varying sizes (up to 100 agents) and topologies, using a synchronous message-passing protocol. Results show that while frontier models like Gemini 2.5 Pro achieve high performance (0.80 mean score) on small networks of up to 16 agents, performance degrades significantly as network size scales, dropping to near-zero for 100-agent networks. For AI practitioners, this implies that current LLMs exhibit emergent coordination in small groups, but developing scalable multi-agent systems requires significant improvements in the models' ability to maintain coherent global strategies under increasing communication complexity. |
| Planted in Pretraining, Swayed by Finetuning: A Case Study on the
  Origins of Cognitive Biases in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07186) or [HuggingFace](https://huggingface.co/papers/2507.07186))| Gabriel Stanovsky, Yonatan Belinkov, itay1itzhak | This research investigates the origins of cognitive biases in LLMs and concludes they are predominantly established during pretraining. The study's objective is to disentangle whether these biases are planted during pretraining or shaped by instruction data and randomness during the finetuning phase. A two-step causal framework is used, featuring multi-seed finetuning to measure randomness and a "cross-tuning" methodology where different pretrained models (OLMo-7B, T5-11B) are finetuned on swapped instruction datasets. Results show pretraining is the dominant factor; clustering models by their bias vectors (across 32 biases) reveals that grouping by pretrained identity is significantly more coherent than by finetuning data, achieving a Silhouette score of 0.104 versus 0.028 for instruction-based clustering. The principal implication for AI practitioners is that mitigating cognitive biases requires interventions at the pretraining stage, as post-hoc finetuning has a limited ability to alter these foundational patterns. |
