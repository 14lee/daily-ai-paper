

## Papers for 2025-07-07

| Title | Authors | Summary |
|-------|---------|---------|
| Eka-Eval : A Comprehensive Evaluation Framework for Large Language
  Models in Indian Languages (Read more on [arXiv](https://arxiv.org/abs/2507.01853) or [HuggingFace](https://huggingface.co/papers/2507.01853))| Mayank Singh, Abhishek Upperwal, Samridhi Raj Sinha, RajveeSheth | The paper presents EKA-EVAL, a unified, open-source framework for evaluating Large Language Models across over 35 global and 10 Indic language benchmarks. The primary objective is to create a comprehensive and accessible evaluation tool that overcomes the English-centric bias of existing frameworks by integrating diverse tasks, including reasoning, long-context understanding, and tool use, with specific support for Indian languages. The methodology involves a modular four-component architecture—Evaluation Engine, Benchmark Registry, Model Interface Layer, and Results Processing System—that supports distributed inference, quantization, and both local and API-based models via an interactive CLI. The framework successfully integrates these capabilities, and a sample evaluation of the `google/gemma-2b` model on Reading Comprehension tasks showed scores of approximately 77.6 on BoolQ and 46.8 on SQuAD, although the specific metric for these scores is not explicitly stated in the provided figure. The principal implication for AI practitioners is the availability of a production-ready, extensible toolkit that significantly lowers the barrier for conducting comprehensive, reproducible, and multilingual LLM evaluations, facilitating more rigorous model assessment, especially for Indic languages. |
| How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation
  Models on Standard Computer Vision Tasks (Read more on [arXiv](https://arxiv.org/abs/2507.01955) or [HuggingFace](https://huggingface.co/papers/2507.01955))| Oğuzhan Fatih Kar, Andrei Atanov, Roman Bachmann, Ali Garjani, Rahul Ramachandran | This paper benchmarks the performance of popular multimodal foundation models (MFMs) like GPT-4o on standard computer vision tasks using a novel evaluation framework. The primary objective is to quantitatively assess the visual understanding capabilities of leading MFMs (including GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet) on tasks such as object detection, segmentation, and depth prediction, for which they are not natively designed. To overcome API limitations and text-only outputs, the authors developed a "prompt chaining" framework that decomposes standard vision tasks into a sequence of text-promptable, classification-style sub-tasks. The results show that while MFMs are respectable generalists, they do not match state-of-the-art specialist models; for instance, GPT-4o achieved a 60.62 AP50 in object detection, significantly behind specialist models but leading other tested MFMs in 4 of 6 tasks, with a notable performance gap between semantic and geometric tasks. For AI practitioners, this indicates that current general-purpose MFMs are not yet suitable as direct replacements for specialized vision models in high-precision applications, and the proposed prompt-chaining benchmark offers a standardized method for evaluating the pure vision capabilities of future text-out MFMs. |
