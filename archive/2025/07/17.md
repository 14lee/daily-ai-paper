

## Papers for 2025-07-17

| Title | Authors | Summary |
|-------|---------|---------|
| Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning
  Systems in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.09477) or [HuggingFace](https://huggingface.co/papers/2507.09477))| Wei-Chieh Huang, Yuyao Yang, Yangning Li, TreeForest, WZDavid | This survey provides a unified taxonomy for systems integrating Retrieval-Augmented Generation (RAG) and deep reasoning in LLMs, charting an evolution from one-way enhancements to synergized, agentic frameworks. The primary objective is to systematically categorize and analyze the convergence of retrieval and reasoning methodologies in LLMs, moving beyond static Retrieval-Then-Reasoning to describe iterative, agentic systems that dynamically interleave both processes. The paper conducts a comprehensive literature review, structuring its analysis around a proposed three-part taxonomy: 1) Reasoning-Enhanced RAG, where reasoning improves RAG stages; 2) RAG-Enhanced Reasoning, where retrieval grounds reasoning; and 3) Synergized RAG-Reasoning, characterized by iterative, agentic interplay. The survey synthesizes findings from over 200 research papers, identifying a paradigm shift towards Synergized RAG-Reasoning systems that employ complex reasoning workflows (chain, tree, graph-based) and agentic orchestrations (single- and multi-agent) to solve knowledge-intensive tasks. AI practitioners can use this survey's taxonomy and benchmark analysis (covering 46 benchmarks across 13 tasks) to select appropriate architectural patterns—such as tree-based workflows for ambiguous tasks or multi-agent systems for heterogeneous data—and evaluation methods for building and validating more robust, factually-grounded, and adaptable reasoning systems. |
| PhysX: Physical-Grounded 3D Asset Generation (Read more on [arXiv](https://arxiv.org/abs/2507.12465) or [HuggingFace](https://huggingface.co/papers/2507.12465))| Linag Pan, liuziwei7, FrozenBurning, Caoza | The paper introduces PhysX, an end-to-end framework for generating 3D assets with grounded physical properties, supported by a new richly annotated dataset, PhysXNet. The primary objective is to create a methodology and dataset for generating 3D models with comprehensive physical attributes, such as material, kinematics, and absolute scale, to enhance their utility in physical simulations. The methodology utilizes a dual-branch VAE to encode structural and physical properties into separate latent spaces, followed by a conditional diffusion transformer that jointly generates these latents by fine-tuning a pre-trained geometric model on the new PhysXNet dataset. Compared to a strong baseline using Trellis, PartField, and GPT-4o, PhysXGen achieves significant relative improvements, including a 64% enhancement in material property prediction and a 72% improvement in kinematics parameter generation. For AI practitioners, this work provides a model and dataset to generate physically coherent 3D assets, enabling more realistic development and testing of agents for robotics and embodied AI in simulated environments. |
| MOSPA: Human Motion Generation Driven by Spatial Audio (Read more on [arXiv](https://arxiv.org/abs/2507.11949) or [HuggingFace](https://huggingface.co/papers/2507.11949))| Leo Ho, Liang Pan, Mingyi Shi, frankzydou, JimSYXu | The paper introduces MOSPA, a diffusion-based generative model for synthesizing human motion from spatial audio, and presents SAM, the first large-scale dataset for this task. The primary objective is to generate realistic and responsive 3D human motion conditioned on spatial audio signals by modeling the complex interplay between auditory spatial cues and human movement. The methodology involves MOSPA, a diffusion-based probabilistic model with an encoder-only transformer, trained on the novel 9-hour SAM dataset to denoise motion sequences conditioned on extracted audio features, sound source location, and motion genre. MOSPA achieves state-of-the-art performance, attaining a Fréchet Inception Distance (FID) of 7.981, significantly outperforming the next best baseline (EDGE at 13.993) and closely approaching real motion data. For AI practitioners, this work provides a framework and dataset for creating more immersive virtual agents that can react dynamically to the location and semantics of sound, moving beyond traditional audio-to-motion generation that ignores spatial information. |
| MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.12463) or [HuggingFace](https://huggingface.co/papers/2507.12463))| Mingyang Wu, Renjie Li, vztu, waynefan, jerryye0110 | This paper introduces MMHU, a large-scale multimodal benchmark with 57k human instances for comprehensive human behavior understanding in autonomous driving scenarios. The objective is to provide a unified dataset to evaluate and advance algorithms for human behavior analysis, which is critical for safety but lacks a comprehensive benchmark. The methodology involves collecting video data from diverse sources (Waymo, YouTube, self-recorded) and using a human-in-the-loop pipeline to generate rich annotations, including 3D SMPL motion, trajectories, hierarchical text descriptions, and labels for 13 critical behaviors. Experiments show that fine-tuning models on MMHU yields significant performance gains; for example, fine-tuning the Qwen2.5-VL model on the behavior VQA task improved its F1-score from 44.72% to 68.54%. For AI practitioners, MMHU serves as a crucial resource to benchmark and improve models for nuanced human-centric tasks in autonomous driving, demonstrating a direct path to enhancing the performance and safety of perception systems. |
| SWE-Perf: Can Language Models Optimize Code Performance on Real-World
  Repositories? (Read more on [arXiv](https://arxiv.org/abs/2507.12415) or [HuggingFace](https://huggingface.co/papers/2507.12415))| Zhijie Fan, Lin Yan, Xinyi He, Elfsong, SivilTaram | This paper introduces SWE-Perf, the first benchmark designed to systematically evaluate the ability of Large Language Models to optimize code performance in real-world repositories. The research objective is to quantify the gap between current LLM capabilities and human expert performance on complex, repository-level optimization tasks. The authors constructed the benchmark by curating 140 instances from performance-improving pull requests on popular GitHub projects, creating executable environments to measure runtime changes, and evaluating models under file-level (Oracle) and repo-level (Realistic) agentic settings. Results demonstrate a significant performance deficit: the best autonomous agent (OpenHands with Claude-3.7-sonnet) achieved a 2.26% performance improvement, far below the 10.85% achieved by the original human-expert patches. For AI practitioners, this highlights that while LLMs show potential, they currently lack the sophisticated reasoning to perform meaningful, cross-file performance optimizations, indicating that relying on them for this task is premature and further research is needed to bridge the gap with human expertise. |
| DrafterBench: Benchmarking Large Language Models for Tasks Automation in
  Civil Engineering (Read more on [arXiv](https://arxiv.org/abs/2507.11527) or [HuggingFace](https://huggingface.co/papers/2507.11527))| Yi Shao, zhendongucb, Eason666 | This paper introduces DrafterBench, a benchmark for evaluating LLM agents on technical drawing revision tasks in civil engineering. The objective is to systematically evaluate an LLM agent's proficiency in interpreting intricate instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The methodology uses 1,920 tasks from real-world files and 46 customized "dual functions" which record the agent's operation path for comparison against a ground truth path, rather than assessing the final output drawing. The primary results show that even the leading model, OpenAI o1, achieved an average task score of only 81.92, and all models showed significant performance degradation (up to 18%) when faced with incomplete instructions. The principal implication for AI practitioners is that current LLMs lack the required robustness for detailed industrial automation, specifically struggling with vague instructions and the implementation of new, overriding policies, which are critical areas for future development. |
| AnyI2V: Animating Any Conditional Image with Motion Control (Read more on [arXiv](https://arxiv.org/abs/2507.02857) or [HuggingFace](https://huggingface.co/papers/2507.02857))| Hao Luo, HenghuiDing, XinchengShuai, TribeRinb | The paper introduces AnyI2V, a training-free framework for animating images from diverse conditional modalities with user-defined motion trajectories. The objective is to create a method for image-to-video generation that enables spatial control from any conditional input (e.g., mesh, depth) and explicit motion control via trajectories, without the need for model retraining. The methodology injects debiased residual hidden and query features from an initial conditional image into a pretrained video diffusion model, then performs zero-shot trajectory control by optimizing latents to align these query features across frames, guided by an adaptive semantic mask. The proposed method achieves high motion control accuracy with an ObjMC score of 16.39, significantly outperforming the baseline (38.26) and demonstrating competitive performance against other state-of-the-art models. The principal implication for AI practitioners is that this training-free approach allows them to add controllable animation to various existing video diffusion backbones without computationally expensive fine-tuning, enabling flexible video generation from diverse structural inputs. |
| SpatialTrackerV2: 3D Point Tracking Made Easy (Read more on [arXiv](https://arxiv.org/abs/2507.12462) or [HuggingFace](https://huggingface.co/papers/2507.12462))| Yuxi Xiao, bykang, nikkar, cherubicxn, JianyuanWang | SpatialTrackerV2 is a feed-forward method for 3D point tracking from monocular videos that unifies the estimation of scene geometry, camera ego-motion, and object motion in a single end-to-end architecture. The primary objective is to develop a scalable 3D point tracking model that overcomes the limitations of modular pipelines by jointly reasoning about motion components, enabling training across diverse and weakly-supervised datasets. The methodology uses a dual-stage architecture where a front-end temporal encoder provides initial depth and camera poses, which are then refined by a novel back-end transformer, "SyncFormer," that iteratively optimizes 2D/3D trajectories and camera poses using a dual-branch design and in-loop bundle adjustment. The model establishes a new state-of-the-art on the TAPVid-3D benchmark, achieving an Average Jaccard (AJ) of 21.2, and matches the accuracy of leading dynamic 3D reconstruction methods while running 50x faster. For AI practitioners, the principal implication is that a unified, feed-forward model trained on heterogeneous data can surpass modular, optimization-based pipelines in complex 3D tracking tasks, offering a scalable path to building robust 3D perception systems without computationally expensive per-scene optimization. |
| Lizard: An Efficient Linearization Framework for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.09025) or [HuggingFace](https://huggingface.co/papers/2507.09025))| Franck-Dernoncourt, Nikosapa, TrungBui1111, jasubram, haniehds | The paper introduces Lizard, a framework that converts pretrained Transformers into subquadratic models for efficient infinite-context generation by replacing softmax attention with a hybrid of gated linear and sliding window attention. The objective is to linearize pretrained Large Language Models (LLMs) to overcome the quadratic complexity of softmax attention and the linear growth of the KV cache, thereby enabling efficient long-context processing while minimizing performance degradation compared to the original model. Lizard employs a two-stage process: first, a hybrid attention module—combining a data-dependent gated linear attention with a sliding window attention enhanced by meta-memory—is trained to approximate the original model's softmax outputs; second, the new module replaces the original, and the model is fine-tuned on a language modeling objective. On the 5-shot MMLU benchmark, the Lizard-linearized LLaMA-3-8B model achieves a score of 61.2, an 18-point improvement over the prior Mamba2-LLaMA-3-8B method, and demonstrates perfect recall on tasks requiring generalization beyond its training context length. For AI practitioners, this provides a method to adapt existing pretrained LLMs for long-context applications with constant-memory inference, avoiding the prohibitive computational costs of standard Transformers without a significant loss in performance. |
| Replacing thinking with tool usage enables reasoning in small language
  models (Read more on [arXiv](https://arxiv.org/abs/2507.05065) or [HuggingFace](https://huggingface.co/papers/2507.05065))| Roland Memisevic, Tim Bakker, crainone | This paper introduces Chain-of-Edits (CoE), a method that replaces natural language reasoning with structured tool interactions to enable small language models (≤3B parameters) to perform complex code repair tasks. The main objective is to determine if parameterizing "thinking" as a trace of interactions with a tool, rather than as natural language, allows smaller models to effectively perform multi-step reasoning via reinforcement learning. The methodology is a two-stage pipeline consisting of Supervised Fine-Tuning (SFT) on synthetic CoE demonstrations using a domain-specific language (DSL), followed by Reinforcement Learning with Verifiable Rewards (RLVR) on a code repair benchmark. The CoE method significantly improved performance for smaller models; for a 1B parameter model, it achieved a 7.82% pass@1 rate, substantially outperforming both direct 3-shot prompting (1.3%) and a Chain-of-Thought baseline (0.15%), though this advantage reversed for an 8B model. The principal implication for AI practitioners is that structuring a problem as an interaction with a tool via a constrained DSL can enable smaller, more efficient models to solve complex, stateful tasks, providing a viable path for deploying reasoning capabilities in resource-constrained environments. |
| RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.07451) or [HuggingFace](https://huggingface.co/papers/2507.07451))| Jingyuan Zhang, Jia Fu, GuoruiZhou, Edrex, hongzhizhang | RLEP is a two-phase Reinforcement Learning framework that improves LLM reasoning by replaying verified successful trajectories from a prior training run. The main objective is to mitigate RL training instability and policy drift in LLMs by using previously discovered high-quality reasoning paths to accelerate training and achieve a higher final performance. The methodology first collects a pool of verified correct trajectories from a converged baseline RL model, then restarts training, optimizing the policy at each step on a mixed batch of newly generated rollouts and replayed successes using a token-mean, asymmetrically clipped GRPO objective. On the Qwen2.5-Math-7B base model, RLEP improved accuracy on the AIME-2024 dataset from a 38.2% baseline peak to 39.9% and on the unseen AMC-2023 dataset from 77.0% to 82.2%. The principal implication for AI practitioners is that incorporating an experience replay mechanism with curated, successful past trajectories into the RL fine-tuning process can accelerate convergence and achieve a higher performance ceiling, providing a more stable and sample-efficient training paradigm. |
