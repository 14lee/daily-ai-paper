

## Papers for 2025-07-15

| Title | Authors | Summary |
|-------|---------|---------|
| SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual
  Dyadic Interactive Human Generation (Read more on [arXiv](https://arxiv.org/abs/2507.09862) or [HuggingFace](https://huggingface.co/papers/2507.09862))| Deyu Zhou, Jiahe Zhang, Duomin Wang, Zhaoyang Li, Youliang Zhang | This paper introduces SpeakerVid-5M, a large-scale, high-quality public dataset designed for audio-visual dyadic interactive human generation. The primary objective is to facilitate research into interactive virtual humans by providing a richly annotated dataset that addresses the scarcity of public resources for this task. The methodology involves a multi-stage pipeline for data curation from public videos, including scene detection, speaker diarization, lip-sync analysis, and extensive multi-modal annotation (e.g., ASR, pose, blur scores), followed by rigorous quality filtering. The primary result is the dataset itself, containing over 8.7K hours of video, 770K dyadic dialogue pairs, and a demonstration baseline model which, on the dyadic setting, achieves an FVD of 28.82 on the paper's VidChatBench benchmark. The principal implication for AI practitioners is the availability of a large-scale, tiered (pre-training and SFT) dataset with multiple interaction branches (dialogue, single-speaker, listening, multi-turn), enabling the development and standardized evaluation of models for more complex, coherent, and interactive audio-visual agents. |
| EmbRACE-3K: Embodied Reasoning and Action in Complex Environments (Read more on [arXiv](https://arxiv.org/abs/2507.10548) or [HuggingFace](https://huggingface.co/papers/2507.10548))| Kui Wu, Chengjie Jiang, Yitang Li, Wei Huang, Mingxian Lin | This paper introduces EmbRACE-3K, a benchmark dataset with over 3,000 language-guided tasks designed to address the poor performance of vision-language models (VLMs) in interactive, embodied environments. The primary objective is to create a challenging benchmark that captures the closed-loop perception-action cycle and enables training for long-horizon, instruction-guided tasks. The authors' methodology involves constructing the dataset with step-wise natural language reasoning annotations and then training a Qwen2.5-VL-7B model using a two-stage approach of supervised fine-tuning (SFT) followed by reinforcement learning (RL). The results demonstrate that while state-of-the-art models perform poorly in zero-shot settings (e.g., GPT-4o SR < 20%), the fine-tuned model's success rate on out-of-domain multi-stage tasks improves from 0.0% to 27.0%. For AI practitioners, this work provides a high-quality dataset and a validated SFT+RL training recipe to significantly enhance a VLM's embodied reasoning and planning abilities for agentic applications. |
| Reasoning or Memorization? Unreliable Results of Reinforcement Learning
  Due to Data Contamination (Read more on [arXiv](https://arxiv.org/abs/2507.10532) or [HuggingFace](https://huggingface.co/papers/2507.10532))| Jun Zhao, Zhiheng Xi, Qiaole Dong, Zhihao Zhang, Mingqi Wu | This research investigates the anomalous performance improvements of Qwen2.5 models from reinforcement learning on math benchmarks, attributing the gains to memorization from data contamination rather than genuine reasoning. The primary objective was to determine whether spurious rewards in RL genuinely enhance the Qwen2.5 model family's reasoning capabilities or merely trigger the recall of memorized answers from contaminated evaluation sets like MATH-500. The methodology involved a leakage audit using partial-prompt completion tests on existing benchmarks and controlled RL experiments on a newly created, leakage-free synthetic dataset called `RandomCalculation`. The study found that when prompted with 60% of a MATH-500 problem, Qwen2.5-Math-7B achieved a 54.60% exact match completion rate, and on the clean `RandomCalculation` dataset, performance only improved with accurate reward signals, while random or incorrect rewards provided no benefit. The principal implication for AI practitioners is the critical need to evaluate new methods on verifiably uncontaminated benchmarks to ensure that reported performance gains reflect true capability improvements and not test set leakage. |
| REST: Stress Testing Large Reasoning Models by Asking Multiple Problems
  at Once (Read more on [arXiv](https://arxiv.org/abs/2507.10541) or [HuggingFace](https://huggingface.co/papers/2507.10541))| Zinan Tang, Qiyao Sun, Yu Li, Qizhi Pei, Zhuoshi Pan | This paper introduces REST, a stress-testing framework that evaluates Large Reasoning Models (LRMs) by concurrently presenting multiple problems in a single prompt. The main objective is to evaluate how well LRMs handle multiple simultaneous reasoning tasks and to identify factors contributing to performance degradation under such multi-context stress, addressing the limitations of saturated single-question benchmarks. The key methodology, REST (Reasoning Evaluation through Simultaneous Testing), transforms existing benchmarks by concatenating a set number of questions (the stress level) into a single prompt, with performance evaluated by extracting and scoring individual answers from the model's unified response. Primary results show that even state-of-the-art models exhibit substantial performance degradation; for example, DeepSeek-R1's accuracy on AIME24 drops by 29.17% under REST compared to single-question evaluation. The framework reveals significant performance differences between models that score similarly on standard benchmarks, with "Question Omission" and positional bias (earlier questions are answered more accurately) being dominant failure modes. The principal implication for AI practitioners is that high performance on isolated, single-problem benchmarks does not guarantee robustness in multi-context applications. The finding that models trained with "long2short" techniques show greater resilience under REST offers a concrete architectural direction for developing more robust LRMs capable of managing dynamic cognitive loads. |
| Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive
  Token-Level Computation (Read more on [arXiv](https://arxiv.org/abs/2507.10524) or [HuggingFace](https://huggingface.co/papers/2507.10524))| Jiyoun Ha, Sungnyun Kim, Reza Bayat, Yujin Kim, Sangmin Bae | Mixture-of-Recursions (MoR) is a unified Transformer framework that combines parameter sharing with token-level adaptive computation to improve efficiency without sacrificing performance. The primary objective is to develop a single architecture that simultaneously achieves the benefits of both parameter sharing (via recursion) and adaptive computation (via dynamic routing) for language models. The key methodology is a "recursion block" of shared Transformer layers, where lightweight, learnable routers (either "expert-choice" or "token-choice") dynamically assign a specific number of recursion steps to each token. This is paired with specialized Key-Value (KV) caching strategies, such as "recursion-wise caching" which only caches active tokens at each recursion step. The primary result is a new Pareto frontier for model efficiency; under an equal training budget of 16.5e18 FLOPs, a 167M parameter MoR model achieves 43.1% average few-shot accuracy, surpassing a 315M parameter vanilla baseline (42.3% accuracy). Additionally, MoR demonstrates up to a 2.06x inference throughput speedup compared to a vanilla baseline. The principal implication for AI practitioners is that MoR provides an architectural path to attain the capabilities of larger models using significantly fewer parameters and less computational cost. This allows for training more powerful models under a fixed compute budget and deploying models with higher throughput and a smaller memory footprint, as the architecture inherently supports efficient techniques like continuous depth-wise batching. |
| LayerCake: Token-Aware Contrastive Decoding within Large Language Model
  Layers (Read more on [arXiv](https://arxiv.org/abs/2507.04404) or [HuggingFace](https://huggingface.co/papers/2507.04404))| Yanqiang Zheng, Jiawang Cao, Wenbo Zhu, Yongliang Wu, Jingze Zhu | The paper introduces LayerCake, a training-free decoding method that improves LLM factuality by selectively suppressing attention to specific token types at distinct layer depths. The primary objective is to improve the factual accuracy of LLM generations without retraining by leveraging the distinct functional roles that different token types (e.g., punctuation, conceptual) and transformer layers play in the model's reasoning process. The LayerCake methodology first identifies that punctuation tokens dominate attention in early layers while conceptual tokens are key in middle layers; it then induces controlled factual degradation by suppressing attention to these tokens at their respective stages and uses the resulting contrastive signal between original and perturbed outputs to guide decoding. The method demonstrates consistent factuality improvements across multiple LLMs, for instance, increasing the score on the FACTOR benchmark by 8.05 percentage points for LLaMA3-8B compared to the greedy decoding baseline. For AI practitioners, this provides a training-free decoding strategy to enhance the reliability of off-the-shelf LLMs in knowledge-intensive tasks by intervening directly on the attention mechanism at inference time, avoiding costly fine-tuning. |
| CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards (Read more on [arXiv](https://arxiv.org/abs/2507.09104) or [HuggingFace](https://huggingface.co/papers/2507.09104))| Kai Chen, Songyang Zhang, Alexander Lam, Maosong Cao, Taolin Zhang | This work introduces CompassJudger-2, a generalist judge model trained with a multi-domain data strategy and a refined margin policy gradient loss objective that utilizes verifiable rewards to improve evaluation robustness and accuracy. The primary objective is to develop a generalist LLM-as-judge model that overcomes the narrow specialization and limited robustness of existing evaluators, enabling comprehensive cross-domain judgment. The key methodology combines a task-driven data curation and synthesis pipeline with a novel training paradigm that uses a Chain-of-Thought (CoT) methodology to generate structured judgments, followed by rejection sampling to filter for high-quality examples, and finally, optimization via a margin policy gradient loss that directly incorporates verifiable binary reward signals. The 7B parameter CompassJudger-2 model demonstrates superior performance across multiple benchmarks, notably outperforming the comparable RISE-Judge-Qwen2.5-7B model by 22.58% on the JudgeBench dataset and achieving competitive accuracy with significantly larger models. The principal implication for AI practitioners is the provision of a validated framework for creating smaller, more cost-effective yet highly accurate generalist judge models, demonstrating that targeted data curation and a policy-gradient-based training strategy can significantly enhance evaluation capabilities without scaling model size, enabling more efficient automated assessment in model development cycles. |
| MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second (Read more on [arXiv](https://arxiv.org/abs/2507.10065) or [HuggingFace](https://huggingface.co/papers/2507.10065))| Honglei Yan, Yifan Yu, Panwang Pan, Yuchen Lin, Chenguo Lin | MoVieS is a feed-forward framework that unifies the modeling of appearance, geometry, and motion to perform dynamic 4D view synthesis from a single monocular video in approximately one second. The objective is to develop a single, efficient, feed-forward model that can reconstruct a dynamic 4D scene from a monocular video, enabling tasks like novel view synthesis and 3D point tracking without per-scene optimization. The key methodology involves representing dynamic scenes using "dynamic splatter pixels"—static 3D Gaussian primitives augmented with a learned, time-dependent deformation field. A transformer backbone extracts features from video frames, camera poses, and timestamps, which are fed into dedicated heads to predict depth, appearance attributes, and motion vectors for arbitrary query times. MoVieS achieves competitive performance while being orders of magnitude faster than prior methods; on the DyCheck benchmark for dynamic novel view synthesis, it achieves an mPSNR of 18.46 in 0.93 seconds, compared to optimization-based methods that take minutes. For 3D point tracking on the TAPVid-3D Panoptic Studio dataset, it achieves an End-Point Error (EPE3D) of 0.0352, outperforming methods like CoTracker3 (0.0617). The principal implication for AI practitioners is the availability of a general-purpose, high-speed foundation model for dynamic 3D perception that directly outputs geometry, appearance, and explicit motion from video, eliminating the need for per-scene optimization and enabling zero-shot applications like scene flow estimation and moving object segmentation for time-sensitive systems in robotics and AR/VR. |
| A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy
  with SFT and Efficiency with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2507.08267) or [HuggingFace](https://huggingface.co/papers/2507.08267))| Yuichi Inoue, Taiki Yamaguchi, Hiroshi Yoshihara | This paper presents a two-stage training recipe that first uses extended Supervised Fine-Tuning (SFT) to maximize the mathematical reasoning accuracy of LLMs, followed by Group Relative Policy Optimization (GRPO) to enhance token efficiency. The research objective is to establish a systematic methodology for combining SFT and Reinforcement Learning (RL), positing them as complementary rather than competing paradigms. The methodology involves an initial, prolonged SFT phase for 10 epochs on a high-difficulty dataset to push model accuracy, followed by a GRPO phase with a composite reward function (combining format, cosine similarity, and length penalty) to reduce solution length. On the MATH-500 benchmark, this recipe increased a 14B model's accuracy from 86.4% to 91.2% while reducing the mean output tokens from 2,556 to 2,084. The principal implication for AI practitioners is that they can use this sequential strategy—SFT for peak accuracy, then RL for efficiency—as a proven blueprint to develop highly effective and practical specialized models, particularly for complex reasoning tasks. |
| From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for
  LLM Evaluation (Read more on [arXiv](https://arxiv.org/abs/2507.08924) or [HuggingFace](https://huggingface.co/papers/2507.08924))| Yeonjung Hong, Soyeon Kim, Guijin Son, Sunkyoung Kim, Seokhee Hong | This paper introduces KMMLU-REDUX and KMMLU-PRO, two expert-level Korean benchmarks designed to evaluate LLM performance on industrial and professional knowledge by addressing noise and contamination in existing datasets. The primary objective is to create a reliable, contamination-free evaluation suite for assessing LLM capabilities on Korea-specific professional qualifications, moving beyond general academic knowledge. The methodology involves manually denoising and filtering the existing KMMLU for high-difficulty technical exams (KMMLU-REDUX) and constructing a new benchmark from official, annually updated professional licensure exams (KMMLU-PRO). Experiments show that while OpenAI's o1 model achieved the highest average accuracy of 79.55%, Anthropic's Claude 3.7 Sonnet (w/ thinking) passed more professional exams (12 out of 14), with all models performing significantly worse in law compared to medicine, highlighting the difficulty of acquiring region-specific expertise. For AI practitioners, this research demonstrates that evaluating LLMs for professional applications requires specialized, locally-adapted benchmarks, as standard accuracy on generic or translated datasets is insufficient for assessing practical readiness in regulated fields. |
| DreamPoster: A Unified Framework for Image-Conditioned Generative Poster
  Design (Read more on [arXiv](https://arxiv.org/abs/2507.04218) or [HuggingFace](https://huggingface.co/papers/2507.04218))| Dexiang Hong, Hui Zhang, Zhongqi Qi, Haokun Chen, Xiwei Hu | DreamPoster is a unified framework for image-conditioned generative poster design that synthesizes high-quality posters from user images and text prompts. The primary objective is to create a model that integrates visual and textual content into a coherent, aesthetically pleasing poster while maintaining content fidelity and design coherence. The methodology involves a transformer-based diffusion architecture trained on a novel dataset of deconstructed poster pairs using a three-stage progressive training strategy that incrementally adds text addition, multi-task editing, and aesthetic alignment capabilities. In human evaluations, DreamPoster achieved an 88.55% usability rate, significantly outperforming GPT-4o (47.56%) and SeedEdit3.0 (25.96%). For AI practitioners, this work provides a robust framework and a targeted training methodology for specializing foundation models for complex, domain-specific content generation tasks like advertising and graphic design, demonstrating a path to production-level quality. |
| Favicon Trojans: Executable Steganography Via Ico Alpha Channel
  Exploitation (Read more on [arXiv](https://arxiv.org/abs/2507.09074) or [HuggingFace](https://huggingface.co/papers/2507.09074))| Forrest McKee, David Noever | The paper introduces a steganographic method for embedding and executing compressed JavaScript payloads within the alpha channel of ICO favicon files to bypass web security measures. The main research objective is to demonstrate the feasibility of a novel, two-stage covert channel that uses the least significant bit (LSB) of an ICO file's alpha transparency layer to conceal and deliver self-decompressing, executable JavaScript within a web browser. The key methodology involves compressing a JavaScript payload, embedding its bits into the LSB of non-transparent alpha channel pixels of a base ICO image, and using a client-side decoder script to fetch the image, extract the bits via a canvas element, decompress the data, and execute the resulting code. The primary result is a successful proof-of-concept implementation that concealed and executed arbitrary script in modern browsers without visual artifacts, demonstrating that a 64×64 icon could hide a compressed payload of approximately 1.2 KB, bypassing standard browser security that treats the file as a static image. The principal implication for AI practitioners is that security and threat detection models must be updated to consider static image files as vectors for executable code, necessitating the development of specialized steganalysis models capable of detecting statistical anomalies like non-natural LSB distributions or high entropy within image alpha channels. |
