

## Papers for 2025-07-10

| Title | Authors | Summary |
|-------|---------|---------|
| Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data (Read more on [arXiv](https://arxiv.org/abs/2507.07095) or [HuggingFace](https://huggingface.co/papers/2507.07095))| Lixing Xiao, Runyi Yu, Shunlin Lu, Ke Fan, Jixi111 | This research introduces MotionMillion, a new dataset with 2 million motion sequences, and a 7B parameter model to enable zero-shot text-to-motion generation. The primary objective is to test the "scaling hypothesis" in motion generation, aiming to achieve robust zero-shot generalization by significantly increasing the scale of training data and model size. The methodology consists of a multi-stage pipeline to construct the MotionMillion dataset from web videos and a scalable, decoder-only transformer architecture that employs Finite Scalar Quantization (FSQ) with wavelet transformation for motion tokenization. The 7B parameter model achieves a Fr√©chet Inception Distance (FID) of 10.3, drastically outperforming the ScaMo baseline's score of 89.0, and demonstrates superior text alignment in human evaluations. The principal implication for AI practitioners is that creating high-quality, million-scale datasets and leveraging large parameter models is a critical and effective strategy for unlocking emergent zero-shot capabilities in complex, non-linguistic generative domains like human motion. |
| Perception-Aware Policy Optimization for Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.06448) or [HuggingFace](https://huggingface.co/papers/2507.06448))| Hongru Wang, Sofia Stoica, Xuehang Guo, Zhenhailong Wang, xhyandwyy | This paper introduces Perception-Aware Policy Optimization (PAPO), an extension to the GRPO algorithm that enhances multimodal reasoning by explicitly penalizing a model's indifference to visual input. The primary objective is to address the high rate of perception errors, which the authors identify as comprising 67% of failures in models trained with standard Reinforcement Learning with Verifiable Rewards (RLVR). PAPO's methodology incorporates an "Implicit Perception Loss" into the GRPO objective, which maximizes the KL divergence between the model's output distributions conditioned on an original versus a randomly masked visual input. The method achieves a 30.5% reduction in perception-related errors and an overall performance gain of up to 8.0% on vision-dependent benchmarks over the GRPO baseline. For AI practitioners, PAPO offers a technique to improve the visual grounding of LMMs during RL finetuning using only internal supervision signals, but requires a Double Entropy Loss regularizer to mitigate a "KLprcp Hacking" training instability identified by the authors. |
| 4KAgent: Agentic Any Image to 4K Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.07105) or [HuggingFace](https://huggingface.co/papers/2507.07105))| Xinrui Jiang, Mingyang Wu, Qi Zheng, vztu, YSZuo | 4KAgent is a unified agentic framework designed to universally upscale any image to 4K resolution by dynamically planning and executing a tailored restoration process. The primary objective is to develop a generalist super-resolution system capable of handling diverse image types, domains, and degradation levels without domain-specific retraining. The methodology employs a multi-agent system with a Perception Agent that uses Vision-Language Models (VLMs) and Image Quality Assessment (IQA) tools to analyze the input and plan a restoration sequence, and a Restoration Agent that executes this plan using a toolbox of specialized models, guided by a Quality-Driven Mixture-of-Experts (Q-MoE) policy for optimal step-wise output selection. 4KAgent establishes new state-of-the-art results across 26 diverse benchmarks; for instance, on the DrealSR real-world super-resolution benchmark, it achieves a top NIQE score of 4.65 and a top MUSIQ score of 69.30. The principal implication for AI practitioners is that an agentic, mixture-of-experts approach, which leverages VLMs for planning and dynamically combines multiple expert models, provides a powerful and practical paradigm for building highly generalizable and performant systems for complex, low-level vision tasks, overcoming the limitations of single, specialized models. |
| Rethinking Verification for LLM Code Generation: From Generation to
  Testing (Read more on [arXiv](https://arxiv.org/abs/2507.06920) or [HuggingFace](https://huggingface.co/papers/2507.06920))| Minnan Luo, Wenwei Zhang, Maosong Cao, Taolin Zhang, MichaelErchi | This paper introduces SAGA, a human-LLM collaborative framework for generating superior test cases to address critical flaws in current LLM code generation verification. Its primary objective is to overcome the test case homogenization and LLM-centric bias in existing benchmarks that artificially inflate model performance. The key methodology, SAGA, uses a dual-pronged strategy of "Multidimensional Analysis" on correct human solutions and "Differential Analysis" on incorrect submissions to guide an LLM in synthesizing highly discriminative tests. This method yields significant improvements, achieving a 90.62% detection rate on the TCGBench-Lite dataset, while the Verifier Accuracy of its synthesized benchmark is 10.78% higher than that of LiveCodeBench-v6. For AI practitioners, this work implies that current benchmarks provide misleadingly high performance scores; adopting more rigorous verifiers like those from SAGA is critical for accurate model assessment and for creating reliable reward signals in Reinforcement Learning from Verifiable Rewards (RLVR) frameworks. |
| First Return, Entropy-Eliciting Explore (Read more on [arXiv](https://arxiv.org/abs/2507.07017) or [HuggingFace](https://huggingface.co/papers/2507.07017))| Xingwei Qu, Taoran Liang, Qingshui Gu, xtsssss, aaabiao | This paper introduces FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework to improve Large Language Model reasoning through more stable reinforcement learning. The research objective is to resolve unstable exploration and imprecise credit assignment in Reinforcement Learning from Verifiable Rewards (RLVR) for complex, sparse-reward tasks. The core methodology involves identifying high-uncertainty decision points within a generated reasoning trajectory using token-wise entropy, then initiating targeted, partial rollouts from these points to construct localized feedback for policy updates. On the AIME24 mathematical reasoning benchmark, FR3E improved the performance of the Qwen2.5-32B model to 40.2% accuracy, a 6.1% absolute improvement over the GRPO++ baseline. For AI practitioners, FR3E provides a value-model-free technique to enhance LLM reasoning capabilities with greater training stability and more effective credit assignment, offering a structured way to guide exploration in sparse-reward environments. |
| A Systematic Analysis of Hybrid Linear Attention (Read more on [arXiv](https://arxiv.org/abs/2507.06457) or [HuggingFace](https://huggingface.co/papers/2507.06457))| Taylor Kergan, Yong Shan, Steven Abreu, Dustin Wang, ridger | This paper systematically analyzes various linear attention models within hybrid architectures to determine the most effective components for balancing performance and efficiency. The primary objective is to investigate whether strong standalone linear models excel when hybridized and to identify which architectural properties are critical for language modeling versus recall. The methodology involved training 72 models at 340M and 1.3B parameters, covering six linear attention variants across five different linear-to-full attention ratios, and benchmarking them on language modeling and RULER recall tasks. The results show that standalone performance is not a reliable predictor of hybrid performance, and while language modeling is stable across ratios, recall nearly doubles on RULER when moving from a 24:1 to a 3:1 linear-to-full ratio. The principal implication for practitioners is to employ a gated, hierarchical backbone (e.g., HGRN-2 or GatedDeltaNet) with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall while significantly reducing KV cache memory. |
| AutoTriton: Automatic Triton Programming with Reinforcement Learning in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.05687) or [HuggingFace](https://huggingface.co/papers/2507.05687))| Yuxuan Li, Ye He, Zefan Wang, Shangzhan Li, qshi | AUTOTRITON is a specialized 8B model that automates Triton programming for GPU kernels using a two-stage supervised fine-tuning and reinforcement learning process. The research objective is to create a model that can automatically generate correct and efficient Triton code from high-level specifications like PyTorch functions, addressing the complexities of manual kernel optimization. The methodology first employs supervised fine-tuning (SFT) on a curated dataset created via a novel data pipeline, followed by reinforcement learning (RL) using the Group Relative Policy Optimization (GRPO) algorithm with a combined execution-based and rule-based reward to enhance correctness. On the KERNELBENCH Level 2 benchmark, AUTOTRITON achieves 45.0% execution accuracy, outperforming larger models like DeepSeek-R1-0528 (42.0%), and removing the RL stage drops this accuracy to 27.0%. For AI engineers, this provides a framework for automating GPU kernel generation, showing that a targeted RL approach on a specialized model can significantly improve performance and correctness, thereby accelerating the development of efficient AI systems. |
| Towards Solving More Challenging IMO Problems via Decoupled Reasoning
  and Proving (Read more on [arXiv](https://arxiv.org/abs/2507.06804) or [HuggingFace](https://huggingface.co/papers/2507.06804))| Feng Zhang, Tao Yang, Yang Li, Linfeng Song, Zhenwen Liang | This paper introduces a decoupled reasoning and proving framework to solve challenging mathematical problems by separating high-level strategy generation from low-level formal proof verification. The primary research objective is to bridge the significant gap between the strong informal reasoning capabilities and weak formal proving performance of LLMs on complex mathematical problems. The methodology uses a general-purpose LLM as a "Reasoner" to propose formal subgoal lemmas and a specialized Automated Theorem Proving (ATP) model as a "Prover" to first verify these lemmas and then construct the final proof. The framework successfully solved 5 post-2000 non-geometry IMO problems, a set where no prior open-source prover had reported success, and found that a specialized prover (Kimina-Prover) underperforms its base model by 4.9 percentage points on the MATH benchmark. The principal implication for AI practitioners is that end-to-end fine-tuning for specialized tasks can degrade a model's core reasoning; a decoupled, multi-model architecture may better preserve and leverage high-level capabilities for complex problem-solving. |
| A Survey on Vision-Language-Action Models for Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2506.24044) or [HuggingFace](https://huggingface.co/papers/2506.24044))| Tianze Zhu, Ziang Luo, Kangan Qian, Zilin Huang, Max2045 | This paper surveys the evolution and architecture of Vision-Language-Action (VLA) models for autonomous driving, which integrate perception, reasoning, and control into a unified policy. The primary objective is to formalize the VLA for Autonomous Driving (VLA4AD) paradigm by tracing its evolution from passive language explainers to integrated reasoning-action agents, cataloging key architectures, datasets, and open challenges. The authors conduct a comprehensive literature review, structuring over 20 representative models into a four-stage evolutionary taxonomy (Explainer, Modular, End-to-End, Reasoning-Augmented) and analyzing their architectural components. The survey identifies a clear trend towards unified, end-to-end VLA models, while highlighting persistent efficiency challenges; for instance, token-reduction techniques like TS-VLM are cited as critical for achieving real-time performance, cutting compute by approximately 90% through methods like soft-attentive pooling. For AI practitioners, this survey provides a consolidated reference for developing VLA4AD systems, guiding the selection of architectural patterns, training datasets like Impromptu VLA for corner cases, and evaluation protocols to build more interpretable and robust autonomous vehicles. |
| DiffSpectra: Molecular Structure Elucidation from Spectra using
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.06853) or [HuggingFace](https://huggingface.co/papers/2507.06853))| Zhiyuan Liu, Zhenyi Zhong, Tingyang Xu, Yu Rong, AzureLeon1 | The paper introduces DiffSpectra, a diffusion-based generative framework for de novo 2D/3D molecular structure elucidation from multi-modal spectral data. The main objective is to directly generate complete molecular structures from spectral inputs (IR, Raman, UV-Vis), overcoming the generalization limits of existing retrieval-based and autoregressive methods. The methodology employs a continuous-time diffusion model whose denoising network is a SE(3)-equivariant Diffusion Molecule Transformer (DMT), which is conditioned on spectral embeddings from a pre-trained, transformer-based encoder named SpecFormer. The primary result is that DiffSpectra achieves a 16.01% top-1 accuracy in recovering the exact molecular structure, which rises to 96.86% for top-20 accuracy by sampling multiple candidates. For AI practitioners, this work shows that a conditional diffusion model with an SE(3)-equivariant backbone and a specialized, pre-trained conditioning network can effectively tackle complex scientific inverse problems, with the significant accuracy boost from sampling demonstrating a viable strategy for generating ranked candidate solutions for expert review. |
| ModelCitizens: Representing Community Voices in Online Safety (Read more on [arXiv](https://arxiv.org/abs/2507.05455) or [HuggingFace](https://huggingface.co/papers/2507.05455))| Karolina Naranjo, notaphonologist, hamidpalangi, christinachance, Ashima | The paper introduces the MODELCITIZENS dataset and corresponding finetuned models to demonstrate that incorporating community-specific, ingroup annotations significantly improves toxic language detection. The primary objective is to quantify the impact of annotator identity (ingroup vs. outgroup) and conversational context on toxicity classification and to develop models that better represent the nuanced perspectives of targeted communities. The methodology involves curating a dataset of 6.8K posts with 40K annotations from both ingroup and outgroup annotators across eight identity groups, augmenting a subset with LLM-generated context, and finetuning LLaMA and Gemma models on these community-specific labels. The primary result is that the finetuned LLAMACITIZEN-8B model achieves 75.2% accuracy, outperforming the best baseline (GPT-o4-mini) by 5.5% and demonstrating that models trained on ingroup-only labels perform better than those trained on outgroup or aggregated labels. The principal implication for AI practitioners is that for subjective tasks like toxicity moderation, datasets should be constructed with annotations from members of the targeted communities, as training on these ingroup labels provides a more reliable signal and produces more accurate and equitable models than using aggregated or outgroup-only labels. |
| Evaluating the Critical Risks of Amazon's Nova Premier under the
  Frontier Model Safety Framework (Read more on [arXiv](https://arxiv.org/abs/2507.06260) or [HuggingFace](https://huggingface.co/papers/2507.06260))| Vincent Ponzo, Matteo Memelli, Abhinav Mohanty, Ninareh Mehrabi, Satyapriya Krishna | This paper presents a comprehensive safety evaluation of Amazon's Nova Premier model against critical risks in CBRN, cyber operations, and automated AI R&D, concluding it is safe for public release under the Frontier Model Safety Framework. The primary objective was to assess if Nova Premier's capabilities in high-risk domains exceed predefined critical thresholds that would prevent its deployment. The methodology integrated automated benchmarks (e.g., WMDP, Cybench, RE-Bench), human-centric evaluations like expert red-teaming and uplift studies, and third-party audits. Results show that while Nova Premier exhibits improved theoretical knowledge, its practical capabilities for misuse are constrained; for instance, its mean solve rate on cybersecurity knowledge benchmarks increased from approximately 0.76 to 0.82 over its predecessor, but its performance on practical Capture-the-Flag challenges remained unchanged. The principal implication for AI practitioners is that increased declarative knowledge in a frontier model does not directly translate to an increased risk of practical misuse, as layered safety systems can effectively intervene to prevent the generation of complete, operational malicious outputs. |
| AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large
  Language Models on Harmfulness (Read more on [arXiv](https://arxiv.org/abs/2507.01702) or [HuggingFace](https://huggingface.co/papers/2507.01702))| Zhen Ye, Ziyang Luo, Kaixin Li, Hongzhan Lin, Zixin Chen | The paper introduces AdamMeme, a multi-agent framework that adaptively probes and evaluates the reasoning capacity of multimodal large language models (mLLMs) on harmful memes by iteratively generating challenging examples. The primary objective is to develop a dynamic, model-centric evaluation framework that moves beyond static benchmarks to identify fine-grained, model-specific weaknesses of mLLMs in understanding meme harmfulness. The methodology employs a three-stage, agent-based pipeline: (1) Harmfulness Mining, where agents categorize memes and generate "misbelief" statements; (2) Model Scoring, where an agent grades the target mLLM's analysis; and (3) Iterative Refinement, where an agent modifies meme text to create more difficult test cases based on the model's prior failures, forming an adaptive evaluation loop. Primary results demonstrate that the framework systematically reveals model-specific vulnerabilities; for instance, Doubao-Lite was found to be highly susceptible to refinement, with its average Failure Rate (FR) increasing by 4.73% on original memes after the process, whereas GPT-4o maintained a low average FR of 2.18% across all tests. The principal implication for AI practitioners is that this framework offers a method for deep, adaptive red-teaming of mLLMs, automatically discovering and generating challenging data that exposes specific reasoning failures missed by static benchmarks, enabling more targeted safety improvements and robust model development. |
