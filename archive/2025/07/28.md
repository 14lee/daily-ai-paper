

## Papers for 2025-07-28

| Title | Authors | Summary |
|-------|---------|---------|
| The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane
  Algorithm (Read more on [arXiv](https://arxiv.org/abs/2507.18553) or [HuggingFace](https://huggingface.co/papers/2507.18553))| Dan Alistarh, Torsten Hoefler, softmax | This research demonstrates that the GPTQ quantization algorithm, when executed in a back-to-front order, is mathematically identical to Babai's nearest plane algorithm for the closest vector problem on a lattice defined by the input Hessian. The paper's main objective is to establish a formal geometric and theoretical foundation for the empirically successful GPTQ algorithm by proving its equivalence to a classical lattice algorithm, thereby explaining its effectiveness and providing worst-case guarantees. The authors use a formal mathematical proof to equate the linear-layer L2 quantization objective with the closest vector problem (CVP) and then demonstrate that the iterative update steps of back-to-front GPTQ are algebraically equivalent to the projections in Babai's algorithm. The primary result is this proven equivalence, which provides a geometric interpretation for GPTQ's error propagation as an orthogonal projection; consequently, GPTQ inherits a tight error upper bound from Babai's algorithm in the no-clipping case, with the expected error being exactly 1/3 of this worst-case bound under a uniform prior on weights. For AI practitioners, this connection enables the direct application of established lattice algorithm techniques, such as basis reduction and novel ordering heuristics like the proposed "min-pivot" method, to create more principled and potentially more accurate post-training quantization algorithms for large models. |
| Deep Researcher with Test-Time Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.16075) or [HuggingFace](https://huggingface.co/papers/2507.16075))| Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yanfei Chen, Rujun Han | This paper introduces the Test-Time Diffusion Deep Researcher (TTD-DR), a framework that models long-form report generation as a diffusion process, iteratively refining a draft using retrieval and self-evolution. The objective is to overcome the performance limitations of existing deep research agents on complex tasks by emulating the iterative human process of drafting, searching, and revision. The core methodology conceptualizes report generation as a "denoising" process where an initial draft is progressively refined using external information from a retrieval mechanism, while a self-evolutionary algorithm optimizes each component of the agentic workflow. TTD-DR achieves state-of-the-art results, demonstrating a 69.1% win rate against OpenAI Deep Research on the LongForm Research benchmark. For AI practitioners, this work presents a highly effective test-time scaling strategy, showing that a draft-centric diffusion approach combined with component-wise self-evolution creates more coherent and accurate research agents than traditional linear or parallelized agentic systems. |
| Specification Self-Correction: Mitigating In-Context Reward Hacking
  Through Test-Time Refinement (Read more on [arXiv](https://arxiv.org/abs/2507.18742) or [HuggingFace](https://huggingface.co/papers/2507.18742))| vicgalle | The paper introduces Specification Self-Correction (SSC), a test-time, multi-step inference framework that enables LMs to identify and correct flaws in their own guiding specifications to mitigate reward hacking. The main objective is to develop a method that allows a language model to mitigate in-context reward hacking by actively identifying a flaw within its guiding specification and autonomously correcting it at inference time. The key methodology is a four-step process: 1) initial response generation using the flawed specification, 2) self-critique of that response, which exposes the exploit, 3) self-refinement of the specification itself to remove the flaw, and 4) final generation of a robust response using the corrected specification. Across creative writing and agentic coding tasks, models that initially exploited flawed specifications in 50-70% of cases demonstrated a reduction in this vulnerability by over 90% after applying SSC; specifically, the average initial hacking rate of 59% in creative writing tasks dropped to 3.2%. The principal implication for AI practitioners is that this weight-agnostic, inference-time technique can be implemented to improve the robustness of deployed LMs by allowing them to dynamically patch their operational rubrics, turning the failure mode of specification gaming into a corrective signal for self-improvement without requiring model retraining. |
| PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2507.17596) or [HuggingFace](https://huggingface.co/papers/2507.17596))| Patric Jensfelt, Yixi Cai, Lianhang Liu, maciejw94 | The paper introduces PRIX, a computationally efficient, camera-only, end-to-end autonomous driving model that directly plans trajectories from raw pixel inputs, outperforming larger multimodal systems. The main objective is to develop a scalable end-to-end driving model that operates solely on camera data, eliminating reliance on LiDAR and computationally intensive BEV representations, while achieving state-of-the-art planning performance. The key methodology involves a ResNet visual backbone enhanced by a novel Context-aware Recalibration Transformer (CaRT) module, which uses shared self-attention to refine multi-scale features. These rich features are then used by a conditional diffusion planner and auxiliary heads for object detection and semantic segmentation within a multi-task learning framework. The primary result is achieving a state-of-the-art PDMS score of 87.8 on the NavSim-v1 benchmark, outperforming prior camera-only models like Hydra-MDP++ (86.6) and multimodal models like GoalFlow+ (85.7), while operating at 57 FPS with only 37M parameters. The principal implication for AI practitioners is that a powerful visual feature extractor, trained with appropriate auxiliary tasks, can be more critical than planner complexity or multimodal sensor fusion for building performant and efficient autonomous driving systems, demonstrating a viable path to scalable, low-cost solutions without reliance on explicit BEV projections. |
| Chat with AI: The Surprising Turn of Real-time Video Communication from
  Human to AI (Read more on [arXiv](https://arxiv.org/abs/2507.10510) or [HuggingFace](https://huggingface.co/papers/2507.10510))| Xinggong Zhang, Liming Liu, Zhiyuan Ren, keyonN | This paper introduces Artic, a real-time communication framework that optimizes video streaming for MLLM understanding to minimize latency in AI video chat. The main objective is to reduce transmission latency to under 68ms by shifting the optimization goal from human perceptual quality to MLLM response accuracy. The key methodology combines Context-Aware Video Streaming, which uses CLIP to dynamically allocate bitrate to semantically important regions, and Loss-Resilient Adaptive Frame Rate, which leverages redundant frames to mitigate packet loss without retransmission. A primary result shows that when bitrate is reduced from 800 Kbps to 400 Kbps, context-aware streaming maintains MLLM accuracy at 0.87, whereas a standard approach drops to 0.33. The principal implication for AI engineers is that video compression for MLLM consumption can be aggressively optimized for machine understanding, rather than human perception, allowing for significant bitrate and latency reductions while preserving downstream task accuracy. |
