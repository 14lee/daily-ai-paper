

## Papers for 2025-07-18

| Title | Authors | Summary |
|-------|---------|---------|
| A Survey of Context Engineering for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.13334) or [HuggingFace](https://huggingface.co/papers/2507.13334))| ShowerMaker, LImax72, YuyaoGe, Theodyy, Chevalier | This survey introduces "Context Engineering" as a formal discipline for optimizing information payloads for LLMs and presents a comprehensive taxonomy of its components and implementations. The objective is to systematically review and organize the field of context manipulation for LLMs by proposing a structured taxonomy that distinguishes between foundational components (retrieval, processing, management) and their system-level implementations (RAG, Memory Systems, Tool Use, Multi-Agent Systems). The methodology consists of a systematic literature review and analysis of over 1400 research papers, which are synthesized into a hierarchical taxonomy. The survey establishes a technical roadmap for Context Engineering, organizing techniques like Tree-of-Thoughts (ToT) which increases Game of 24 success rates from 4% to 74%, and identifies a critical asymmetry where models' advanced context comprehension capabilities significantly outpace their limited ability to generate sophisticated, long-form outputs. AI practitioners are provided with a unified framework to navigate and implement sophisticated context-aware systems, shifting the focus from ad-hoc prompt design to a systematic, engineering-driven approach for managing information payloads in RAG, agentic, and multi-agent architectures. |
| VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2507.13348) or [HuggingFace](https://huggingface.co/papers/2507.13348))| Hengshuang Zhao, Bei Yu, Xin Lai, Junyi Li, Senqiao Yang | VisionThink is a novel framework that uses reinforcement learning to enable a Vision-Language Model (VLM) to dynamically decide whether to process a low-resolution image or request a higher-resolution one for more efficient inference. The primary objective is to overcome the inefficiency of static visual token counts by creating a model that autonomously determines on a per-sample basis if a compressed, low-resolution image is sufficient or if a high-resolution input is necessary to solve a given task. The methodology employs reinforcement learning with the Group Relative Policy Optimization (GRPO) algorithm, using an "LLM-as-Judge" strategy to generate reward signals for open-ended VQA and a penalty-controlled reward function to manage the high-resolution image request ratio. Experiments show VisionThink achieves 101.4% of the baseline model's performance on average across nine benchmarks while retaining only approximately 51.3% of visual tokens, thereby maintaining high performance on OCR-heavy tasks where fixed-compression methods degrade. For AI practitioners, this research introduces a practical paradigm for sample-level dynamic efficiency where models adapt their computational load to input complexity, and validates the "LLM-as-Judge" strategy as a viable method for applying RL to complex generative vision-language tasks without complex reward engineering. |
| π^3: Scalable Permutation-Equivariant Visual Geometry Learning (Read more on [arXiv](https://arxiv.org/abs/2507.13347) or [HuggingFace](https://huggingface.co/papers/2507.13347))| Yang Zhou, Wenzheng Chang, Haoyi Zhu, Jianjun Zhou, Yifan Wang | π³ is a scalable, permutation-equivariant neural network for visual geometry reconstruction that eliminates the need for a fixed reference view. The main objective is to develop a robust and scalable visual geometry reconstruction model that is invariant to the order of input images, overcoming the instability caused by the traditional reliance on a fixed reference view. The key methodology is a fully permutation-equivariant transformer architecture that discards order-dependent components and predicts affine-invariant camera poses and scale-invariant local point maps for each view, supervised through relative poses and a globally consistent scale factor. The model demonstrates state-of-the-art performance and superior robustness; on the Sintel benchmark for camera pose estimation, π³ reduces the Absolute Trajectory Error (ATE) to 0.074 from the previous state-of-the-art of 0.167. The principal implication for AI practitioners is that this reference-free, permutation-equivariant design provides a more robust and scalable foundation for multi-view 3D reconstruction systems, enabling stable performance on unordered image sets and showing consistent performance gains with increased model size. |
| The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner (Read more on [arXiv](https://arxiv.org/abs/2507.13332) or [HuggingFace](https://huggingface.co/papers/2507.13332))| Songyang Gao, Chengqi Lyu, Wenwei Zhang, vanilla1116, ZhouqiHUA | This paper introduces Turing MAchine Imitation Learning (TAIL), a data-driven framework that enhances LLM length generalization by fine-tuning on synthetic Chain-of-Thought data that mimics the execution process of a Turing Machine. The objective is to develop a universal reasoning structure that enables LLMs to solve a broad class of "computable problems" with inputs longer than those seen during training. The core methodology involves synthesizing CoT data with three key properties: Linear Transition (unrolling all steps sequentially), Atomic States (decomposing steps into minimal read/write/logic operations), and a Memory Fetcher (explicitly retrieving operands before use). Experiments show that a Qwen2.5-7B model fine-tuned with TAIL achieves 86.5% accuracy on long-sequence large number addition, significantly outperforming prior methods like Index Hint (24.0%), and surpasses DeepSeek-R1 on the majority of 18 algorithmic tasks. The principal implication for AI practitioners is that structuring synthetic training data to explicitly model the fundamental, atomic steps of an algorithm is a highly effective, data-centric strategy for teaching LLMs to generalize their reasoning capabilities to longer problem instances without requiring model architecture changes. |
| AnyCap Project: A Unified Framework, Dataset, and Benchmark for
  Controllable Omni-modal Captioning (Read more on [arXiv](https://arxiv.org/abs/2507.12841) or [HuggingFace](https://huggingface.co/papers/2507.12841))| Gao Meng, Yu Li, Zhiqiang Lin, Yiming Ren, Ruihang | The AnyCap project introduces a unified framework (ACM), dataset (ACD), and benchmark (AnyCapEval) for controllable omni-modal captioning across images, video, and audio. The primary objective is to address the lack of fine-grained control, dedicated datasets, and reliable evaluation protocols for generating captions that precisely follow user instructions. The key methodology is the AnyCapModel (ACM), a lightweight, plug-and-play module that refines initial captions from frozen base models by incorporating user instructions and modality features, trained on the new 300k-entry preference-based AnyCapDataset. The paper reports that the ACM framework significantly improves caption quality, with the 8B parameter version (ACM-8B) boosting GPT-4o's content scores by 45% and style scores by 12% on the AnyCapEval benchmark. For AI practitioners, the ACM framework offers a practical, low-cost method to enhance the controllability of existing foundation models for multimodal tasks, enabling precise, instruction-aligned outputs without requiring expensive retraining of the base models. |
| Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos
  with Spatio-Temporal Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.13344) or [HuggingFace](https://huggingface.co/papers/2507.13344))| Zhen Xu, Tao Xie, Xuan Wang, Sida Peng, krahets | Diffuman4D is a spatio-temporal diffusion model that synthesizes high-fidelity, 4D-consistent human performances from sparse-view videos. The primary objective is to resolve spatio-temporal inconsistencies in generative models for novel view synthesis by introducing a novel sliding iterative denoising process on a 4D latent grid, guided by a mixed conditioning scheme of 3D human skeletons and Plücker coordinates. The key methodology involves alternately denoising this latent grid along spatial and temporal dimensions, allowing information to propagate across the entire sequence to enforce consistency. The model significantly outperforms existing approaches, achieving a PSNR of 25.393 on the DNA-Rendering dataset with 4-view inputs, compared to 21.445 from the next-best generative baseline. The principal implication for AI practitioners is that the sliding iterative denoising technique offers a memory-efficient strategy to enforce long-range consistency in video generation, enabling diffusion models to handle large spatio-temporal domains more effectively. |
| FantasyPortrait: Enhancing Multi-Character Portrait Animation with
  Expression-Augmented Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2507.12956) or [HuggingFace](https://huggingface.co/papers/2507.12956))| Yonggang Qi, Yaqi Fan, Fan Jiang, Mengchao Wang, wangqiang9 | FantasyPortrait is a Diffusion Transformer-based framework for generating high-fidelity, emotionally expressive single and multi-character portrait animations. The research aims to overcome the limitations of geometry-based methods in cross-identity reenactment and multi-character animation by avoiding explicit priors and preventing feature interference. The key methodology combines an expression-augmented learning strategy using implicit facial representations to capture fine-grained emotions with a novel masked cross-attention mechanism that spatially isolates driving signals for each character within the model's latent space. On the proposed ExprBench benchmark for cross-reenactment, FantasyPortrait achieved a state-of-the-art Average Expression Distance (AED) of 33.45 (x10⁻²), outperforming comparable methods. For AI practitioners, the principal implication is the masked cross-attention technique, which offers a robust method for achieving independent, region-specific control in multi-subject generative diffusion models by directly manipulating attention scores, a concept applicable to various compositional generation tasks. |
| MindJourney: Test-Time Scaling with World Models for Spatial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.12508) or [HuggingFace](https://huggingface.co/papers/2507.12508))| Reuben Tan, Siyuan Zhou, Zheyuan Zhang, Jiageng Liu, yyuncong | The paper introduces MindJourney, a test-time framework that enhances a VLM's 3D spatial reasoning by coupling it with a controllable video diffusion world model to explore imagined viewpoints. The primary objective is to grant Vision-Language Models (VLMs) the ability to reason about the visual consequences of egocentric motion in a 3D scene from a single image, without any model fine-tuning. The methodology involves an iterative Spatial Beam Search where a VLM proposes camera trajectories, a world model generates the corresponding egocentric video rollouts, and the VLM then scores and selects the most informative generated views to answer a spatial query. The framework achieves a significant performance boost across various VLMs on the SAT benchmark, increasing the accuracy of GPT-4.1 on SAT-Real from 67.3% to 82.6% (+15.3%). The principal implication for AI practitioners is that this plug-and-play approach provides a direct, training-free method to improve the spatial intelligence of existing VLMs for embodied AI tasks by integrating them with world models to create a "mental workspace" for the agent at inference time. |
| AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research (Read more on [arXiv](https://arxiv.org/abs/2507.13300) or [HuggingFace](https://huggingface.co/papers/2507.13300))| Yixin Liu, Manasi Patwardhan, Zhijian Xu, Weiyuan Chen, Yilun Zhao | The paper introduces ABGEN, a benchmark of 1,500 expert-annotated examples from 807 NLP papers, to evaluate the ability of Large Language Models to design scientific ablation studies. The primary objective is to assess how well frontier LLMs perform in generating detailed and sound ablation study designs for a specified research module, given a comprehensive research context. The methodology involves having NLP experts create reference designs from published papers and then rate LLM-generated outputs on a 1-5 scale across importance, faithfulness, and soundness. The evaluation reveals a significant performance gap, with the top-performing model, DeepSeek-R1-0528, achieving an average human evaluation score of 4.11, considerably lower than the 4.80 achieved by human experts. The principal implication for AI practitioners is that current LLMs are not yet reliable for autonomously designing valid scientific experiments and that LLM-based evaluation systems for such complex, domain-specific tasks require significant improvement to align with expert assessment. |
| Teach Old SAEs New Domain Tricks with Boosting (Read more on [arXiv](https://arxiv.org/abs/2507.12990) or [HuggingFace](https://huggingface.co/papers/2507.12990))| Yaroslav Aksenov, Nikita Koriagin, kefirski, elephantmipt, dlaptev | The paper introduces SAE Boost, a residual learning method to enhance pretrained Sparse Autoencoders (SAEs) with domain-specific features by training a secondary SAE on the reconstruction error of the primary one. The main objective is to adapt a general-purpose SAE to capture features from a specialized domain without full retraining or catastrophic forgetting of general capabilities. The key methodology involves training a secondary "residual" SAE on domain-specific data, using the reconstruction error (x - x̂) from a frozen, pretrained SAE as its target; during inference, the outputs of both SAEs are summed. Primary results show significant improvements on specialized domains, for instance, increasing explained variance on chemistry data from 0.571 to 0.716 (+25.39%) while maintaining performance on general tasks. The principal implication for AI practitioners is that they can use SAE Boost to efficiently and modularly extend existing interpretability tools to new domains, enabling targeted analysis of LLM behavior without rebuilding models from scratch. |
| FLEXITOKENS: Flexible Tokenization for Evolving Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.12720) or [HuggingFace](https://huggingface.co/papers/2507.12720))| Sachin Kumar, Orevaoghene Ahia, Abraham Toluase Owodunni | This paper introduces FLEXITOKENS, a method for training byte-level language models with a flexible, gradient-based tokenizer that adapts its segmentation strategy during finetuning. The research objective is to overcome the rigidity of static subword and fixed-compression-rate tokenizers, which limits model performance when adapting to new data distributions. The methodology involves a byte-level hourglass architecture trained with a novel hinge-like loss that enforces a flexible lower bound on the compression rate, rather than a fixed target, allowing segmentation to vary based on the input. Evaluation across multiple benchmarks demonstrates that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvement on downstream task performance compared to baselines. The principal implication for AI practitioners is that this method allows for more effective domain and language adaptation, as the tokenizer co-adapts with the model during finetuning, improving performance and efficiency without requiring complex tokenizer retraining or replacement. |
| TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame
  Interpolation (Read more on [arXiv](https://arxiv.org/abs/2507.04984) or [HuggingFace](https://huggingface.co/papers/2507.04984))| Chen Chen, ucfzl | The paper introduces TLB-VFI, a temporal-aware latent diffusion model using a Brownian Bridge process for efficient and high-quality video frame interpolation. The primary objective is to create a video frame interpolation model that extracts rich temporal information while overcoming the high computational cost, large model size, and extensive data requirements of previous video-based diffusion methods. The methodology employs a temporal-aware autoencoder that operates in both pixel space (using 3D-wavelet gating) and latent space (using temporal blocks with 3D convolution), and applies a Brownian Bridge Diffusion Model between the latent codes of the original video clip and a version with the intermediate frame zeroed-out, ensuring a significant distributional shift for effective generation. The model achieves state-of-the-art performance, including a 20% FID improvement on the SNU-FILM extreme dataset over prior image-based diffusion methods, while having 3x fewer parameters and requiring up to 9000x less training data than other video-based diffusion approaches. For AI practitioners, this work provides a framework for building highly efficient generative video models that do not require massive-scale training, demonstrating how to effectively apply Brownian Bridge diffusion for conditional tasks by engineering a meaningful latent space gap between the condition and the target. |
| Automating Steering for Safe Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.13255) or [HuggingFace](https://huggingface.co/papers/2507.13255))| Nay Oo, Tri Cao, Ziwen Xu, Mengru Wang, Lyucheng Wu | i) A 1-line summary The paper introduces AutoSteer, a modular and adaptive inference-time framework to automatically enhance the safety of Multimodal Large Language Models by detecting and steering away from harmful content without model retraining.  ii) Main research question or objective The primary objective is to develop a fully automated technique to improve MLLM safety during inference against textual, visual, and cross-modal threats while preserving the model's general-purpose capabilities.  iii) Key methodology used AutoSteer employs a three-part methodology: a novel Safety Awareness Score (SAS) to automatically identify the most safety-relevant internal layer, an adaptive safety prober trained on that layer's activations to estimate toxicity, and a lightweight refusal head that conditionally steers generation towards a safe refusal.  iv) Primary results (include at least one specific quantitative finding) Experiments show AutoSteer significantly reduces the Attack Success Rate (ASR); for the LLaVA-OV model on the VLSafe benchmark, ASR was reduced from 60.0% to 4.2% while its accuracy on the RealWorldQA benchmark was fully preserved (61.8% vs. 61.8% original).  v) Principal implication for AI practitioners AI practitioners can use AutoSteer as a practical, plug-and-play safety solution for existing MLLMs that automates intervention without requiring costly fine-tuning, minimizing the trade-off between safety and utility for safer real-world deployment. |
| Voxtral (Read more on [arXiv](https://arxiv.org/abs/2507.13264) or [HuggingFace](https://huggingface.co/papers/2507.13264))| Corentin Barreau, Clément Denoix, Andy Lo, Andy Ehrenberg, Alexander H. Liu | The paper introduces Voxtral Mini and Voxtral Small, two open-weight multimodal audio chat models designed for advanced speech and text understanding. The primary objective is to develop and evaluate open-source audio language models that can process long-form audio (up to 40 minutes) and achieve state-of-the-art performance across transcription, translation, and audio reasoning tasks. The methodology involves a Transformer-based architecture comprising a Whisper large-v3 audio encoder, an MLP adapter layer that downsamples audio embeddings by a factor of 4x to a 12.5Hz frame rate, and a Mistral language decoder, trained via pretraining, supervised finetuning, and preference alignment. The models demonstrate superior performance on various benchmarks; notably, Voxtral Small achieves state-of-the-art speech translation scores on the FLEURS benchmark across all tested language pairs, such as a 57.3 BLEU score for English-to-French translation. The principal implication for AI practitioners is the availability of Apache 2.0 licensed, high-performance models that serve as a strong open-source foundation for building applications requiring long-context audio understanding, providing a competitive alternative to closed-source systems. |
| Einstein Fields: A Neural Perspective To Computational General
  Relativity (Read more on [arXiv](https://arxiv.org/abs/2507.11589) or [HuggingFace](https://huggingface.co/papers/2507.11589))| Johannes Brandstetter, Arturs Berzins, Sandeep Suresh Cranganore, AndreiB137 | This paper introduces Einstein Fields (EinFields), a neural tensor field representation that compresses computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. The main objective is to create a memory-efficient, continuous, and differentiable representation of the spacetime metric tensor, allowing for the accurate derivation of physical quantities via automatic differentiation without relying on domain discretization. The methodology involves parameterizing the metric tensor with a multi-layer perceptron (MLP) trained using a Sobolev loss that supervises the network's output as well as its Jacobian and Hessian derivatives. The primary result shows that EinFields can achieve a Mean Absolute Error as low as 6.89E-8 on the Schwarzschild metric components, compressing the data by a factor of up to 4035 compared to explicit grids. The principal implication for AI practitioners is that implicit neural representations, combined with Sobolev training, can act as highly efficient and differentiable compressors for complex scientific tensor fields, providing a framework for modeling physical systems where accurate derivatives are crucial. |
