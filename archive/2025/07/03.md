

## Papers for 2025-07-03

| Title | Authors | Summary |
|-------|---------|---------|
| Kwai Keye-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.01949) or [HuggingFace](https://huggingface.co/papers/2507.01949))| huxiao09, yw95, TinaGao, hjy, caojiangxia | This paper introduces Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for state-of-the-art performance in short-video understanding. The objective is to develop a model that can comprehend dynamic, information-dense video content, a key limitation in existing MLLMs. The methodology rests on a massive 600-billion-token video-centric dataset and an innovative training recipe featuring a four-stage pre-training process followed by a two-phase post-training process that utilizes a five-mode "cold-start" data mixture and reinforcement learning to elicit reasoning. Keye-VL achieves state-of-the-art results on video benchmarks, including an 8.7% absolute improvement on Video-MMMU, and remains competitive on general image tasks. The principal implication for AI practitioners is that a targeted training strategy combining high-quality video data, a mixed-mode reasoning approach (e.g., CoT, Auto-Think), and iterative RL alignment can significantly advance MLLM capabilities for complex temporal video analysis. |
| LongAnimation: Long Animation Generation with Dynamic Global-Local
  Memory (Read more on [arXiv](https://arxiv.org/abs/2507.01945) or [HuggingFace](https://huggingface.co/papers/2507.01945))| Zhendong Mao, Yihao Meng, Mengqi Huang, CNcreator0331 | This paper presents LongAnimation, a novel framework for automated, long-term animation colorization that maintains consistent color over extended sequences. The research objective is to solve the problem of color inconsistency in long animations, which existing local-paradigm methods fail to address. The core methodology is a dynamic global-local paradigm, which features a SketchDiT for hybrid reference feature extraction and a Dynamic Global-Local Memory (DGLM) module that uses a long video understanding model to compress historical features and fuse them with the current generation context. Quantitatively, LongAnimation improves long-term video quality by 49.1% in Frechet Video Distance (FVD) over prior methods on sequences averaging 500 frames. The principal implication for AI practitioners is that the DGLM's approach of using a long-context understanding model to dynamically inject compressed global information into a generative process offers a powerful, transferable technique for maintaining consistency in other long-sequence generation tasks. |
| Depth Anything at Any Condition (Read more on [arXiv](https://arxiv.org/abs/2507.01634) or [HuggingFace](https://huggingface.co/papers/2507.01634))| Qibin Hou, Bowen Yin, Modi Jin, BBBBCHAN | The paper presents DepthAnything-AC, a foundation monocular depth estimation model finetuned for robustness against diverse and adverse environmental conditions.  i) The primary research objective is to adapt a general-purpose foundation MDE model to perform reliably under challenging conditions like poor lighting, adverse weather, and sensor-induced distortions, without compromising its original capabilities on standard scenes.  ii) The core methodology involves an unsupervised consistency regularization paradigm that finetunes a base model on a small set of unlabeled images with applied perturbations (e.g., lighting, blur, weather). This is coupled with a knowledge distillation loss from a frozen teacher model and a novel Spatial Distance Constraint that explicitly enforces patch-level relative geometric relationships to preserve object boundaries.  iii) Experimental results show that DepthAnything-AC improves zero-shot performance on corrupted data benchmarks; specifically, on the DA-2K blur benchmark, the model achieves a pairwise comparison accuracy of 0.880, improving upon the 0.862 of the baseline DepthAnything V2, while maintaining comparable performance on general benchmarks like NYU-D.  iv) The principal implication for AI practitioners is that this work offers a data-efficient strategy to enhance the robustness of large foundation models for specific, challenging domains by using only a small corpus of unlabeled data and a perturbation-based consistency framework, avoiding the need for extensive data collection and labeling for each target condition. |
| A Survey on Vision-Language-Action Models: An Action Tokenization
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2507.01925) or [HuggingFace](https://huggingface.co/papers/2507.01925))| Zhang Chen, Fengshuo Bai, Yifan Zhong, Feernnn, phython96 | This paper surveys Vision-Language-Action (VLA) models, proposing a unified framework that classifies them based on their method of "action tokenization". The primary objective is to systematically analyze VLA research by framing it as a process where a series of modules generate a chain of intermediate action tokens—such as language, code, affordance, or raw actions—to translate multimodal inputs into physical execution. The key methodology is a literature review structured around a novel taxonomy of eight distinct action token types, analyzing the advantages and limitations of each. The survey finds that different token types offer unique trade-offs; for example, latent representations enable high training efficiency, with UniVLA achieving performance comparable to a baseline using only 4.45% of the training time, but lack the interpretability of explicit tokens like code. For AI practitioners, the principal implication is that designing robust embodied agents requires a hierarchical architecture that strategically combines different action token types, leveraging their complementary strengths for different levels of task planning and execution. |
| FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2507.01953) or [HuggingFace](https://huggingface.co/papers/2507.01953))| Ziwei Liu, Jinghao Wang, Chenyang Si, Yukang Cao | FreeMorph is a novel, tuning-free image morphing framework that leverages a pre-trained diffusion model to generate high-fidelity, smooth transitions between semantically or structurally diverse images without per-instance optimization. The primary objective is to develop a generalized image morphing method that operates without fine-tuning pre-trained diffusion models, enabling fast and high-quality transitions between any two input images, even those with significant semantic and layout differences. The methodology involves modifying the self-attention modules within a pre-trained latent diffusion model during a scheduled DDIM inversion and denoising process by introducing a "guidance-aware spherical interpolation" that aggregates key/value features from both inputs to preserve identity, and a "step-oriented variation trend" that parametrically blends attention outputs to ensure a gradual transformation. The method demonstrates superior performance over existing techniques, achieving an overall mean Frechet Inception Distance (FID) of 152.88, significantly outperforming prior methods like DiffMorpher (209.10), while being 10x-50x faster. The principal implication for AI practitioners is a highly efficient, tuning-free pipeline for image morphing that reduces generation time from minutes to under 30 seconds, making advanced generative transitions practical for interactive applications and showcasing that complex control can be achieved by manipulating the internal mechanics of foundational models instead of through costly fine-tuning. |
| Locality-aware Parallel Decoding for Efficient Autoregressive Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2507.01957) or [HuggingFace](https://huggingface.co/papers/2507.01957))| Kelly Peng, Shang Yang, Chengyue Wu, Luke J. Huang, Zhuoyang Zhang | This paper introduces Locality-aware Parallel Decoding (LPD), a framework to significantly accelerate autoregressive image generation through high-degree parallelization. The primary objective is to reduce the high latency of next-patch prediction in autoregressive models while maintaining generation quality and compatibility with universal flat token representations. The methodology combines two key techniques: a "Flexible Parallelized Autoregressive Modeling" architecture that uses learnable position query tokens to enable arbitrary-order parallel generation, and a "Locality-aware Generation Ordering" schedule that strategically groups tokens to maximize contextual support and minimize intra-group dependencies. The proposed method reduces the number of generation steps for 256x256 ImageNet generation from 256 to 20, achieving at least 3.4x lower latency than previous parallelized autoregressive models without compromising quality. For AI practitioners, this work provides a method to build significantly faster autoregressive visual generation systems that retain compatibility with standard vision backbones and can perform zero-shot editing tasks like inpainting and outpainting. |
| JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2506.23552) or [HuggingFace](https://huggingface.co/papers/2506.23552))| Youngjung Uh, Jaesik Park, Jaeseok Jung, Mingi Kwon, alex4727 | JAM-Flow introduces a unified framework for the joint synthesis of facial motion and speech using flow matching and a Multi-Modal Diffusion Transformer (MM-DiT). The objective is to create the first single architecture that simultaneously models, generates, and conditions on both audio and motion, overcoming the traditional separation of text-to-speech and talking head synthesis. The methodology leverages two specialized, yet coupled, transformer modules (Audio-DiT and Motion-DiT) with selective joint attention, scaled rotary positional embeddings for temporal alignment, and an inpainting-style training objective. In talking head generation experiments on the HDTF dataset, the model significantly outperforms prior art, achieving a Fréchet Video Distance (FVD) of 25.07, compared to scores over 160 for competing methods. For AI practitioners, this work provides a practical, unified architecture that eliminates the need for separate pipelines, enabling more coherent and flexible audio-visual synthesis for applications like virtual avatars and automated dubbing from diverse conditioning inputs. |
| STR-Match: Matching SpatioTemporal Relevance Score for Training-Free
  Video Editing (Read more on [arXiv](https://arxiv.org/abs/2506.22868) or [HuggingFace](https://huggingface.co/papers/2506.22868))| Bohyung Han, Junoh Kang, jslee525 | This paper presents STR-Match, a training-free algorithm that performs high-fidelity, text-guided video editing by matching a novel SpatioTemporal Relevance score. The objective is to resolve temporal inconsistencies, motion distortions, and limited domain transformations in existing video editing methods by better modeling spatiotemporal pixel relevance. The core methodology is a latent optimization process guided by the proposed STR score, which is calculated from the multiplicative combination of 2D spatial self-attention and 1D temporal-attention maps from a pretrained text-to-video (T2V) model, eliminating the need for 3D attention. Quantitatively, STR-Match with a mask achieves a Motion Error of 1.932, significantly outperforming the T2V-based method DMT (5.741), while also achieving superior background preservation (BL of 0.103 vs. 0.499). The principal implication for AI practitioners is that STR-Match offers a computationally efficient, zero-shot framework to significantly improve the consistency and flexibility of video editing on top of existing T2V models without any retraining, proving especially effective for challenging domain shifts. |
| MARVIS: Modality Adaptive Reasoning over VISualizations (Read more on [arXiv](https://arxiv.org/abs/2507.01544) or [HuggingFace](https://huggingface.co/papers/2507.01544))| Chinmay Hegde, Oussama Elachqar, Lennart Purucker, Benjamin Feuer | MARVIS is a training-free method that enables vision-language models to perform prediction tasks on any data modality by reasoning over visual representations of that modality's embedding space. The research objective is to combine the reasoning capabilities of foundation models with the representational power of specialist models without requiring modality-specific fine-tuning or exposing personally identifiable information (P.I.I.). The core methodology involves using a domain-specific model to generate vector embeddings, applying t-SNE to create a 2D visualization of the embedding space, and prompting a VLM (Qwen 2.5 VL 3B) with this visualization to predict the class of a query point. Using a single 3B parameter model, MARVIS achieved competitive performance across vision, audio, biological, and tabular domains, improving upon foundation model baselines like Gemini by an average of 16.7 percentage points. For AI practitioners, this method offers a universal, training-free interface to apply pre-trained VLMs to specialized, non-traditional, or privacy-sensitive data modalities by converting them into a visual format, thus avoiding costly, domain-specific model training and direct data serialization. |
