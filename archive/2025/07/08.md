

## Papers for 2025-07-08

| Title | Authors | Summary |
|-------|---------|---------|
| MemOS: A Memory OS for AI System (Read more on [arXiv](https://arxiv.org/abs/2507.03724) or [HuggingFace](https://huggingface.co/papers/2507.03724))| Hanyu Wang, Chenyang Xi, Shichao Song, Zhiyu Li, Wentao-PKU | This paper introduces MEMOS, a memory operating system that provides a unified framework for managing heterogeneous memory types in LLMs to enable persistent, long-term intelligence. The primary objective is to overcome the limitations of static models and transient retrieval by treating memory as a first-class, schedulable resource, implemented via an OS-inspired, three-layer architecture (Interface, Operation, Infrastructure) and a standardized `MemCube` unit for dynamic lifecycle management. In evaluations, MEMOS achieved a top overall LLM-Judge score of 73.31 on the LOCOMO benchmark, outperforming all baselines, and its KV-based memory injection demonstrated up to a 91.4% reduction in Time-to-First-Token (TTFT) without altering output semantics. For AI practitioners, MEMOS provides a standardized API and abstraction layer to manage LLM memory as a controllable resource, simplifying the development of stateful agents with long-term consistency and enabling significant inference latency reduction in production systems. |
| 4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous
  Capture (Read more on [arXiv](https://arxiv.org/abs/2507.05163) or [HuggingFace](https://huggingface.co/papers/2507.05163))| Xiuyuan Yu, Lihe Ding, Tianshuo Yang, Shi Guo, Yutian Chen | 4DSloMo presents a joint hardware-software solution for high-speed 4D scene reconstruction using low-FPS cameras by combining an asynchronous capture scheme with a video-diffusion-based artifact-fix model. The objective is to reconstruct high-speed dynamic scenes from multi-view videos captured by low frame-rate cameras, which traditionally fail to capture sufficient intermediate motion. The methodology involves staggering the start times of standard cameras to increase the effective capture frame rate, then using a 4D Gaussian Splatting model for initial reconstruction, and finally refining the result with a fine-tuned video diffusion model to correct artifacts caused by the induced viewpoint sparsity. On the DNA-Rendering dataset, the method achieves a PSNR of 26.76, significantly outperforming the baseline GS4D's 24.75. For AI practitioners, this work demonstrates a practical application of fine-tuned video diffusion models as effective priors for 4D reconstruction, enabling the correction of complex artifacts from spatially sparse data while maintaining temporal consistency, thereby facilitating high-fidelity motion capture without specialized high-speed hardware. |
| DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive
  World Knowledge (Read more on [arXiv](https://arxiv.org/abs/2507.04447) or [HuggingFace](https://huggingface.co/papers/2507.04447))| Yunnan Wang, Hongsi Liu, Wenyao Zhang, RunpeiDong, qizekun | DreamVLA is a Vision-Language-Action (VLA) framework that improves robot manipulation by forecasting a compact set of world knowledge (dynamics, depth, semantics) before predicting actions. The main research objective is to enhance VLA models by incorporating an efficient, future-state forecasting capability that moves beyond redundant pixel-level prediction to include comprehensive world knowledge, establishing a perception-prediction-action loop. The key methodology involves using a GPT-2-based transformer with specialized `<dream>` queries to generate a "world embedding" that encapsulates predicted future dynamic regions, depth, and semantics, which in turn conditions an action-generating diffusion transformer; a block-wise structured attention mechanism is used to prevent information leakage between the different knowledge types during forecasting. DreamVLA achieves a state-of-the-art 4.44 average task length on the CALVIN ABC-D benchmark and a 76.7% success rate on real-world robot tasks, with ablations revealing that forecasting dynamic regions is the most critical component. For AI practitioners, the principal implication is that robot policy performance can be significantly improved by adding an intermediate step that explicitly forecasts a compact, disentangled representation of future world states—particularly motion dynamics—rather than directly mapping observations to actions or predicting full future frames. |
| Should We Still Pretrain Encoders with Masked Language Modeling? (Read more on [arXiv](https://arxiv.org/abs/2507.00994) or [HuggingFace](https://huggingface.co/papers/2507.00994))| Emmanuel Malherbe, Duarte M. Alves, Manuel Faysse, Nicolas-BZRD, hgissbkh | This paper investigates the relative efficacy of Masked Language Modeling (MLM) and Causal Language Modeling (CLM) for pretraining text encoders, finding that a sequential CLM-then-MLM strategy is optimal. The main objective is to determine whether the performance gains of recent CLM-repurposed encoders stem from the CLM objective itself or from confounding factors like model and data scale. The methodology involves a large-scale controlled study training 38 models (210M to 1B parameters) on 100B tokens, comparing MLM-only, CLM-only, and sequential CLM+MLM pretraining, evaluated via over 15,000 fine-tuning runs on various NLP tasks. The primary results show that while MLM generally yields better final performance, CLM is more data-efficient and stable, and a biphasic strategy combining both objectives is superior; for continued pretraining (CPT), adapting a CLM model with 22,000 steps of MLM significantly outperforms continuing to train an MLM-only model on sequence classification. The principal implication for AI practitioners is that adapting readily available, large pretrained CLM decoders with an MLM objective is a more compute-efficient path to creating state-of-the-art encoder models than training them from scratch. |
| Pre-Trained Policy Discriminators are General Reward Models (Read more on [arXiv](https://arxiv.org/abs/2507.05197) or [HuggingFace](https://huggingface.co/papers/2507.05197))| Yunhua Zhou, Yicheng Zou, Shichun Liu, Shihan Dou, Umean | This paper introduces POLicy DiscriminAtive LeaRning (POLAR), a novel paradigm that pre-trains reward models as policy discriminators to improve their generality and scalability. The objective is to establish a scalable, criterion-agnostic pre-training framework for reward models (RMs) to overcome the generalization and data scarcity limitations of traditional preference-based training. The methodology involves pre-training an RM on a large synthetic corpus to distinguish between trajectories from the same versus different policies using a contrastive objective, followed by fine-tuning on human-ranked data to align with desired criteria. The primary result shows that POLAR-7B, when used in RLHF, improves the performance of the LLaMa3.1-8B policy model from an average of 47.36% to 56.33% on 20 benchmarks. The principal implication for AI practitioners is a highly effective method for developing robust RMs that provide more reliable reward signals for policy alignment, applied through a process called Reinforcement Fine-Tuning (RFT) where the RM scores candidate trajectories relative to a reference. |
| BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning
  Dataset (Read more on [arXiv](https://arxiv.org/abs/2507.03483) or [HuggingFace](https://huggingface.co/papers/2507.03483))| Yufang Liu, Honglin Guo, Yutao Fan, Guanyu Li, Zhiheng Xi | This paper introduces BMMR, a large-scale (110k instances) bilingual, multimodal, and multi-disciplinary reasoning dataset designed to benchmark and enhance the capabilities of large multimodal models (LMMs). The primary objective is to develop a comprehensive, college-level dataset spanning 300 subjects to rigorously evaluate LMMs' knowledge and reasoning, and to provide a high-quality training set (BMMR-Train) to advance open-source model development. Data was curated from print and digital sources using a human-in-the-loop framework, resulting in two subsets: BMMR-Eval for evaluation and BMMR-Train for fine-tuning, with each instance containing a high-quality reasoning path. A process-based "BMMR-Verifier" was also proposed for fine-grained evaluation of reasoning steps. Primary results show that even state-of-the-art models like Gemini-2.5-Pro achieve only 50.15% accuracy on BMMR-Eval, indicating substantial room for improvement. Fine-tuning with BMMR-Train significantly boosts performance, with the finetuned BMMR-InternVL2.5-78B model showing a 19.07% improvement in overall performance. The principal implication for AI practitioners is that the BMMR-Train dataset provides a valuable, high-quality, multi-disciplinary resource for fine-tuning open-source LMMs to improve their reasoning capabilities, while the BMMR-Eval benchmark allows for rigorous assessment of model weaknesses across a broad range of academic subjects. |
| RoboBrain 2.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.02029) or [HuggingFace](https://huggingface.co/papers/2507.02029))| Zhoues, Caozhou1995, MinglanLin, yuheng2000, cmyopu | The paper introduces RoboBrain 2.0, a series of embodied vision-language foundation models (7B and 32B) designed to unify perception, reasoning, and planning for complex physical tasks. The primary objective is to develop a foundation model that overcomes key limitations in existing VLMs, specifically their limited spatial understanding, weak temporal modeling, and insufficient reasoning, to enable more effective interaction in real-world embodied scenarios. The methodology combines a heterogeneous architecture (vision encoder + Qwen2.5-VL language model) with a progressive three-stage training curriculum that includes foundational learning, embodied enhancement, and chain-of-thought fine-tuning using both supervised and reinforcement learning on synthesized interaction data. The 32B variant achieves state-of-the-art performance on multiple embodied AI benchmarks, outperforming prior models; for instance, it scored 72.43 on the RoboSpatial benchmark, significantly surpassing Gemini-2.5-Pro's score of 59.87. For AI practitioners, RoboBrain 2.0 provides an open-source, high-performance foundation model and a detailed training recipe for building agents capable of complex spatial-temporal reasoning, with direct applications in robotics for long-horizon planning, multi-agent coordination, and affordance prediction. |
| Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM
  Fine-Tuning Data from Unstructured Documents (Read more on [arXiv](https://arxiv.org/abs/2507.04009) or [HuggingFace](https://huggingface.co/papers/2507.04009))| Jingyuan Wang, Qiyu Sun, Ziyang Miao, hiyouga, oGYCo | The paper introduces Easy Dataset, a unified framework with a GUI for synthesizing high-quality, persona-driven fine-tuning data from unstructured documents. The primary objective is to automate the generation of diverse and factually consistent fine-tuning datasets from heterogeneous documents to overcome the scarcity of domain-specific data for LLM adaptation. Its methodology combines adaptive document processing using VLMs and a hybrid chunking strategy with a two-stage, persona-driven data synthesis pipeline that leverages (Genre, Audience) pairs to guide QA generation, all within a human-in-the-loop interface. Experiments demonstrate that fine-tuning a Qwen2.5-7B-Instruct model on the synthesized financial data improved its domain-specific knowledge score from a baseline of 3.2 to 59.6, while maintaining general capabilities. For practitioners, this open-source tool provides an end-to-end solution to rapidly create custom fine-tuning datasets for domain adaptation, significantly reducing manual effort and integrating directly with training frameworks like LlamaFactory. |
| RefineX: Learning to Refine Pre-training Data at Scale from
  Expert-Guided Programs (Read more on [arXiv](https://arxiv.org/abs/2507.03253) or [HuggingFace](https://huggingface.co/papers/2507.03253))| Dayiheng Liu, Xingzhang Ren, Shenghua Liu, Baolong Bi, Chevalier | REFINEX is a framework that refines LLM pretraining data by distilling expert-generated text improvements into minimal, deletion-only programmatic edits. The objective is to create a scalable and reliable data refinement method that improves data quality without the high costs of end-to-end generation or the unreliability of directly generating complex edit programs. The core methodology is a two-stage distillation pipeline: an expert model first generates a high-quality, clean version of a text, then a minimal edit distance algorithm extracts only the deletion operations required for this transformation, which are used to train a small, efficient "refine model." Primarily, models pretrained on REFINEX-processed data show superior performance; a 750M parameter model achieves 2.6%-7.2% average gains on downstream LightEval tasks over baselines and introduces zero new words, ensuring refinement does not add hallucinations. For AI practitioners, this provides a scalable blueprint to create a custom data-cleaning model that systematically removes noise from corpora, enhancing downstream performance and data efficiency while preserving the authenticity of the original text. |
| Reviving Cultural Heritage: A Novel Approach for Comprehensive
  Historical Document Restoration (Read more on [arXiv](https://arxiv.org/abs/2507.05108) or [HuggingFace](https://huggingface.co/papers/2507.05108))| Yongxin Shi, Pengyu Yan, Zhenhua Yang, Peirong Zhang, Yuyi Zhang | i) This paper introduces AutoHDR, a modular, three-stage framework for the comprehensive restoration of historical documents, supported by a new full-page dataset named FPHDR.  ii) The primary research objective is to develop a fully automated system capable of restoring both the textual content and visual appearance of full-page historical documents, addressing the limitations of prior single-modality or patch-level methods.  iii) The methodology consists of a sequential pipeline: 1) OCR-Assisted Damage Localization identifies damaged regions; 2) a Vision-Language Context Prediction (VLCP) algorithm synergizes OCR and LLM outputs to predict missing text; and 3) a patch-autoregressive diffusion model performs pixel-level visual reconstruction.  iv) On severely damaged documents, AutoHDR improves character recognition accuracy from a 46.83% baseline to 84.05%, with human-in-the-loop collaboration further increasing accuracy to 94.25%.  v) The principal implication for AI practitioners is the demonstration of a practical architecture for building complex, cascaded AI systems where specialized models (detection, language, generative) are integrated, and the modular design explicitly enables human-in-the-loop validation at each stage to enhance final output reliability. |
| StreamDiT: Real-Time Streaming Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2507.03745) or [HuggingFace](https://huggingface.co/papers/2507.03745))| Yue Zhao, Masayoshi Tomizuka, Ji Hou, Tingbo Hou, AkiCumulo | StreamDiT is a novel framework for real-time, streaming text-to-video generation using a specialized training, modeling, and distillation pipeline. The research objective is to develop a system for generating continuous, high-quality video streams in real-time, addressing the offline, short-clip limitations of existing models. The methodology combines a buffered flow matching training process using a moving frame buffer, a modified adaLN Diffusion Transformer (DiT) with time-varying embeddings and window attention, and a tailored multistep distillation technique to reduce inference steps. The primary result is a distilled 4B parameter model that achieves real-time generation of 512p video streams at 16 FPS on a single GPU. For AI practitioners, this framework enables the development of interactive video applications, such as dynamic video-to-video editing or generative game engines, by providing a method for continuous video output that can be modified by user prompts on the fly. |
| ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code
  Generation Evaluation (Read more on [arXiv](https://arxiv.org/abs/2507.04952) or [HuggingFace](https://huggingface.co/papers/2507.04952))| Ao Liu, Jiaheng Liu, Can Xu, Yuhang Li, Chenchen Zhang | This paper introduces ArtifactsBench, a new benchmark and automated evaluation paradigm for assessing the generation of dynamic, interactive visual artifacts by Large Language Models (LLMs). The central objective is to develop a framework that can automatically and holistically evaluate an LLM's ability to transform multimodal instructions into high-quality, interactive visual artifacts, moving beyond static code analysis. The methodology involves programmatically rendering the generated artifact, capturing its dynamic behavior via temporal screenshots, and then using a Multimodal LLM (MLLM) as a judge, guided by a fine-grained, per-task checklist, to assess both the visual evidence and the source code. The primary result is that the automated evaluation achieves a 94.4% ranking consistency with WebDev Arena, a human-preference gold standard, and over 90% pairwise agreement with human experts. The principal implication for AI practitioners is that ArtifactsBench provides a scalable, automated tool that reliably proxies human-perceived quality, enabling more accurate benchmarking and targeted development of LLMs for complex, user-centric visual generation tasks. |
| VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and
  Visual Documents (Read more on [arXiv](https://arxiv.org/abs/2507.04590) or [HuggingFace](https://huggingface.co/papers/2507.04590))| Xinyi Yang, Mingyi Su, Ye Liu, Rui Meng, ziyjiang | i) This paper introduces VLM2Vec-V2, a unified embedding model for text, images, videos, and visual documents, alongside MMEB-V2, a new comprehensive benchmark for its evaluation.  ii) The main objective is to develop and evaluate a single, general-purpose embedding model that can robustly represent and generalize across diverse visual modalities beyond natural images, including videos and structured visual documents, to support a wider range of downstream applications.  iii) The methodology involves fine-tuning a Qwen2-VL vision-language model using instruction-guided contrastive learning (InfoNCE loss) on a curated training dataset combining image-text, video-language, and visual document retrieval tasks. The training strategy utilizes interleaved sub-batching to balance cross-task diversity and improve optimization stability.  iv) The primary result is that VLM2Vec-V2 achieves state-of-the-art performance, with an overall average score of 58.0 across 78 tasks on the MMEB-V2 benchmark, outperforming prior baselines like GME (57.8) and VLM2Vec (52.3). It shows significant improvement on newly introduced video and visual document tasks while maintaining strong performance on image benchmarks.  v) The principal implication for AI practitioners is that a single, unified embedding model can effectively handle heterogeneous multimodal data, enabling the development of more versatile AI systems for tasks like multi-modal search, recommendation, and retrieval-augmented generation (RAG) that must process and align representations from images, videos, and documents simultaneously. |
| VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity
  Classification (Read more on [arXiv](https://arxiv.org/abs/2507.03607) or [HuggingFace](https://huggingface.co/papers/2507.03607))| adulau, cedricbonhomme | This paper presents VLAI, a fine-tuned RoBERTa-base model for automated classification of software vulnerability severity from text descriptions. The objective is to predict a vulnerability's severity category before an official CVSS score is available, thereby accelerating the triage process for security analysts. The methodology involves fine-tuning a RoBERTa-base model with a softmax classification head on a custom dataset of 610k vulnerabilities, which is updated and used for daily model retraining. VLAI achieves 82.8% classification accuracy on a held-out test set and, in a separate evaluation, its predictions matched the eventual expert-assigned severity approximately 85% of the time. For AI practitioners, this work provides a complete blueprint for an MLOps pipeline that continuously ingests data, retrains a large language model, and deploys it into a live, public-facing service (Vulnerability-Lookup) for real-time inference. |
| PresentAgent: Multimodal Agent for Presentation Video Generation (Read more on [arXiv](https://arxiv.org/abs/2507.04036) or [HuggingFace](https://huggingface.co/papers/2507.04036))| Meng Fang, Yanjie Liang, Biao Wu, Jingwei Shi, SteveZeyuZhang | The paper introduces PresentAgent, a multimodal agent that automatically generates narrated presentation videos from long-form documents, and a VLM-powered framework, PresentEval, for their evaluation. The primary objective is to automate the task of Document-to-Presentation Video Generation by creating a system that can process a source document and produce a fully synchronized video with slide-style visuals and spoken narration, mimicking a human-style presentation. PresentAgent employs a four-stage modular pipeline: (1) an LLM-based parser segments the document into a structured outline, (2) a slide composition module generates layout-aware visual frames, (3) a separate LLM pass generates oral-style narration which is converted to audio via a Text-to-Speech system, and (4) a video assembly module composes the visuals and audio into a temporally aligned video. On a curated benchmark, PresentAgent variants achieved factual comprehension scores that surpass human performance; specifically, the Claude-3.7-Sonnet and GPT-4o-Mini backends both achieved a quiz accuracy of 0.64, higher than the human-created video reference score of 0.56. The principal implication for AI practitioners is that this modular, agent-based pipeline provides a blueprint for systems that transform static, text-heavy information into dynamic, accessible multimodal content. The most impactful finding—that these agents can exceed human-level performance in preserving factual accuracy during content transformation—demonstrates a viable path for automating complex professional communication workflows. |
| Beyond Simple Edits: X-Planner for Complex Instruction-Based Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2507.05259) or [HuggingFace](https://huggingface.co/papers/2507.05259))| Yuheng Li, Richard Zhang, Nanxuan Zhao, Yilin Wang, danielchyeh | The paper introduces X-Planner, an MLLM-based planning system that decomposes complex image editing instructions into simpler, actionable sub-tasks with automatically generated masks and bounding boxes. The objective is to develop a system that can robustly interpret and execute complex, indirect, and multi-part image editing instructions while preserving object identity and localizing edits, overcoming the limitations of models that require manual guidance. X-Planner employs a GLaMM-based MLLM architecture, trained on a new 260K-pair dataset (COMPIE), which uses chain-of-thought to break down a user prompt into a sequence of sub-instructions, each with a predicted edit type, an object anchor for mask generation, and a predicted bounding box for insertion tasks. On the newly introduced COMPIE benchmark for complex instructions, integrating X-Planner with an InstructPix2Pix* model improved the MLLM-based text-image alignment score (MLLM_ti) from 0.6727 to 0.7408. The principal implication for AI practitioners is that X-Planner can serve as a planning module to enhance existing generative editing models, enabling them to handle sophisticated, natural language requests by translating high-level intent into precise, machine-executable steps with spatial guidance, significantly improving instruction-following capabilities for complex tasks without retraining the core editor. |
| Evaluating LLMs on Real-World Forecasting Against Human Superforecasters (Read more on [arXiv](https://arxiv.org/abs/2507.04562) or [HuggingFace](https://huggingface.co/papers/2507.04562))| Janna Lu | This paper evaluates the forecasting accuracy of state-of-the-art large language models (LLMs) against human superforecasters on 464 real-world questions from the Metaculus platform. The research objective is to quantify how well frontier LLMs forecast future events by using the Brier score as the primary metric, feeding models summarized news articles and testing both direct and narrative prompting strategies. The primary result shows that the top-performing model, `o3`, achieved a mean Brier score of 0.1362, which is better than the general human crowd's score of 0.149 but significantly worse than the 0.0225 median Brier score of human superforecasters on a subset of the same questions. Furthermore, the models performed substantially worse when using a narrative prompt compared to a direct prediction prompt, indicating that fictional framing can degrade accuracy. The principal implication for AI practitioners is that while current LLMs can surpass general human crowd forecasting abilities, they are not yet a substitute for specialized human expertise and their reasoning accuracy is sensitive to prompting style, with fictionalized scenarios compromising performance. |
| MOD-X: A Modular Open Decentralized eXchange Framework proposal for
  Heterogeneous Interoperable Artificial Agents (Read more on [arXiv](https://arxiv.org/abs/2507.04376) or [HuggingFace](https://huggingface.co/papers/2507.04376))| Aaron Elkins, Vinija Jain, Christos Constantinou, Georgios Ioannides, amanchadha | The paper proposes MOD-X, a conceptual architectural framework designed for creating decentralized, interoperable ecosystems of heterogeneous AI agents. The objective is to design a framework that overcomes the limitations of existing agent communication protocols by addressing semantic fragmentation, state management conflicts, and security-interoperability tensions. The proposed methodology is a layered architecture featuring a Universal Message Bus (UMB) for publish-subscribe communication, a Translation Layer for semantic interoperability, contextual state management, and a tiered, blockchain-based security model. As a conceptual proposal, the paper presents no empirical results but illustrates its capability discovery mechanism through a worked example where a multimodal synthesis of ontological matching and vector similarity (cosine score of 0.97) produces a final agent capability relevance score of 0.92. The principal implication for AI practitioners is a proposed blueprint for integrating diverse AI systems—from legacy rule-based systems to modern LLMs—into a coherent, scalable ecosystem without requiring centralized coordination, facilitated by semantic discovery and automated translation. |
| Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs
  More Realistic and Less Risky (Read more on [arXiv](https://arxiv.org/abs/2507.03336) or [HuggingFace](https://huggingface.co/papers/2507.03336))| Sebastian Schreiber, Julien Yu, ashutosh1919 | This paper introduces DIAFORGE, a disambiguation-centric fine-tuning pipeline that improves LLM reliability for enterprise tool-calling by training them to handle near-duplicate APIs and underspecified arguments. The research objective is to enhance LLMs' multi-turn dialogue capabilities to iteratively elicit missing information and select the correct tool from a dense, overlapping API surface. The methodology consists of a three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues with distractor tools, (ii) performs supervised fine-tuning on open-source models, and (iii) evaluates them using a dynamic, interactive benchmark called DIABENCH. The primary result is that models fine-tuned with DIAFORGE increased tool-invocation success by 49 percentage points over a prompted Claude-3.5-Sonnet on the dynamic benchmark. For AI practitioners, this provides a concrete methodology and an open-source dataset of ~5,000 APIs and dialogues to build more reliable and less risky tool-calling agents for enterprise environments where API ambiguity is common. |
| SeqTex: Generate Mesh Textures in Video Sequence (Read more on [arXiv](https://arxiv.org/abs/2507.04285) or [HuggingFace](https://huggingface.co/papers/2507.04285))| Yan-Pei Cao, Yuan-Chen Guo, Yangtian Sun, Xin Yu, Ze Yuan | SeqTex is an end-to-end framework that leverages pretrained video foundation models to directly generate high-fidelity UV texture maps for 3D meshes by treating the task as a video sequence generation problem. The primary objective is to overcome the data scarcity and error accumulation issues of existing 3D texturing methods by developing a single-stage model that directly generates complete UV maps by adapting priors from video models. The methodology reformulates texture synthesis as a sequence generation task, where a video diffusion model is fine-tuned to jointly predict a sequence of multi-view renderings and the final UV texture map. Key architectural components include decoupled multi-view and UV processing branches, a geometry-informed attention mechanism to align features between the view and UV domains, and an adaptive token resolution strategy that processes UV maps at a higher resolution. The model achieves state-of-the-art performance, demonstrating a Fréchet Inception Distance (FID) of 30.27 on the image-conditioned texturing task, significantly outperforming the previous best method's FID of 34.53. The principal implication for AI practitioners is that large-scale pretrained video models can be effectively adapted for native 3D generation tasks beyond simple view synthesis. The technique of structuring a hybrid output (multi-view images + UV map) as a video sequence provides a robust framework for transferring powerful 2D priors to structured 3D asset generation, improving consistency and reducing reliance on multi-stage pipelines. |
| OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding (Read more on [arXiv](https://arxiv.org/abs/2507.02659) or [HuggingFace](https://huggingface.co/papers/2507.02659))| Yicheng Lin, Chen Feng, Shaojie Zhuo, Ramchalam Kinattinkara Ramakrishnan, justinyyy | The paper introduces OmniDraft, a framework for a universal draft model that performs speculative decoding for any target LLM, even across different vocabularies. The main objective is to overcome the tight coupling between draft and target models in speculative decoding, enabling a single, small drafter to work with various large models and adapt online to user data. The key methodology combines an online n-gram cache to map token sequences between mismatched vocabularies and a hybrid distillation loss to continuously align the draft model with the target model's outputs during inference. Primary results demonstrate that a single Llama-68M draft model can pair with diverse target models like Vicuna-7B, Qwen2-7B, and Llama3-8B, achieving up to a 1.70x speedup on the GSM8K reasoning task with the Llama3-8B target. The principal implication for AI practitioners is the significant reduction in overhead for deploying speculative decoding at scale; a single, optimized on-device drafter can be used universally across different and evolving target models, eliminating the need to train and maintain a specific drafter for each target model family. |
