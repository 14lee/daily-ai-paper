

## Papers for 2025-07-21

| Title | Authors | Summary |
|-------|---------|---------|
| A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges
  in Russian Speech Generative Models (Read more on [arXiv](https://arxiv.org/abs/2507.13563) or [HuggingFace](https://huggingface.co/papers/2507.13563))| Mikhail Gorodnichev, Maxim Maslov, Vasiliy Kudryavtsev, Nikita Vasiliev, Kirill Borodin | This paper introduces Balalaika, a new 2,000+ hour Russian speech dataset with comprehensive linguistic annotations, to improve generative speech models. The primary objective is to address specific phonetic and prosodic challenges in Russian text-to-speech (TTS), such as variable stress, vowel reduction, and homograph ambiguity. The methodology consists of an automated pipeline that collects studio-quality speech and applies state-of-the-art models for quality filtering (NISQA-S), transcription (GigaAMv2-RNNT), speaker clustering, and importantly, adding explicit stress and punctuation annotations. In experiments, a VITS model trained on the highest-quality partition of Balalaika achieved a manual Mean Opinion Score (MOS) of 3.618 ± 0.083, outperforming models trained on all 11 other compared datasets. The principal implication for AI practitioners is that for morphologically complex languages, a data-centric approach focused on high-quality audio combined with rich linguistic annotations (especially stress) is more effective for developing state-of-the-art generative models than using larger but less-curated corpora. |
| The Devil behind the mask: An emergent safety vulnerability of Diffusion
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.11097) or [HuggingFace](https://huggingface.co/papers/2507.11097))| Ruixi Wu, Zhiyuan Liu, Dongrui Liu, Zichen Wen, Joshua999 | This paper introduces DIJA, a novel jailbreak attack that exploits the inherent architectural properties of diffusion LLMs—bidirectional context modeling and parallel decoding—to bypass safety alignments. The objective is to systematically investigate the emergent safety vulnerabilities of diffusion-based large language models (dLLMs) and develop an automated attack framework, DIJA, that exploits these unique weaknesses. The DIJA framework automatically transforms standard harmful prompts into adversarial interleaved mask-text prompts using in-context learning, forcing the target dLLM to generate harmful content within masked spans to maintain contextual consistency while its parallel decoding architecture prevents dynamic refusal. DIJA significantly outperforms existing jailbreak methods, achieving an evaluator-based Attack Success Rate (ASR) on JailbreakBench that surpasses the strongest prior baseline by up to 78.5% and achieving a 37.7 point higher StrongREJECT score against the Dream-Instruct model. The principal implication for AI practitioners is that the core mechanisms of dLLMs create a new attack surface not addressed by safety alignments designed for autoregressive models, necessitating the development of novel, dLLM-specific alignment techniques. |
| Franca: Nested Matryoshka Clustering for Scalable Visual Representation
  Learning (Read more on [arXiv](https://arxiv.org/abs/2507.14137) or [HuggingFace](https://huggingface.co/papers/2507.14137))| Spyros Gidaris, Lukas Knobel, Mohammadreza Salehi, Valentinos Pariza, Shashanka Venkataramanan | The paper introduces Franca, a fully open-source vision foundation model for scalable representation learning using nested Matryoshka clustering on public internet-scale data. The primary objective is to create a transparent and reproducible model that matches or surpasses the performance of leading proprietary models like DINOv2 and SigLIPv2. The methodology employs a multi-head clustering projection head on a Vision Transformer, where features are sliced into progressively smaller dimensional subsets (e.g., d, d/8, d/16) to learn a coarse-to-fine semantic hierarchy. Franca demonstrates strong performance on diverse downstream tasks, achieving a 76.7 score on In-Context Learning (VOC) which is a +3.0 improvement over DINOv2. For AI practitioners, Franca offers a high-performance, open-weight vision backbone that can be used for various applications without reliance on proprietary data or models, enhancing reproducibility and accessibility. |
| Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.12566) or [HuggingFace](https://huggingface.co/papers/2507.12566))| Xue Yang, Wenhao Li, Wenhan Dou, Gen Luo, wzk1015 | The paper introduces Mono-InternVL-1.5, an efficient monolithic Multimodal Large Language Model (MLLM) that integrates visual encoding and language decoding into a single architecture. The primary objective is to overcome catastrophic forgetting and high computational costs associated with training monolithic MLLMs by enabling stable visual knowledge acquisition within a pre-trained LLM. The methodology involves embedding visual experts into a frozen LLM using a multimodal mixture-of-experts (MMoE) architecture, trained via a data-efficient progressive strategy called EViP++, and accelerated in inference with a custom fused CUDA kernel. The resulting model achieves performance comparable to strong modular MLLMs while reducing first-token latency by up to 69.3% compared to its modular counterpart, InternVL-1.5. For AI practitioners, this research provides a blueprint for developing high-performance, deployment-friendly monolithic MLLMs by adapting pre-trained LLMs with specialized visual experts, significantly reducing training and inference overhead. |
| CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models (Read more on [arXiv](https://arxiv.org/abs/2507.13984) or [HuggingFace](https://huggingface.co/papers/2507.13984))| Khoi Nguyen, Anh Tran, Quang Nguyen, Minh Luu, nqbinh | i) This paper introduces CSD-VAR, a novel framework for disentangling content and style from a single image using Visual Autoregressive (VAR) models by exploiting their inherent multi-scale generation architecture.  ii) The main objective is to perform effective content-style decomposition (CSD) within VAR models, which have been underexplored for this task, to enable high-fidelity content recontextualization and style transfer from a single reference image.  iii) The key methodology combines a scale-aware alternating optimization strategy that aligns content and style losses with their respective generation scales, an SVD-based rectification method to purify style embeddings by removing content-related components, and augmented Key-Value (K-V) memories to better preserve subject identity.  iv) On the newly proposed CSD-100 benchmark, CSD-VAR significantly outperforms existing methods, with its Infinity-based variant achieving a content alignment CSD-C score of 0.660 and a CLIP-I score of 0.795, demonstrating superior simultaneous content preservation and stylization fidelity compared to baselines like DreamBooth.  v) The principal implication for AI practitioners is that the scale-wise generation process of VAR models offers a structured and effective mechanism for attribute disentanglement, presenting a viable and powerful alternative to diffusion models for building controllable and personalized image generation systems. |
| RedOne: Revealing Domain-specific LLM Post-Training in Social Networking
  Services (Read more on [arXiv](https://arxiv.org/abs/2507.10605) or [HuggingFace](https://huggingface.co/papers/2507.10605))| Ziyan Liu, Zheyong Xie, Yue Wang, Chonggang Lu, Hiiamein | The paper introduces RedOne, a domain-specific Large Language Model developed via a three-stage post-training strategy to improve performance on tasks within Social Networking Services (SNS). The primary objective is to develop a foundational LLM for the SNS domain that overcomes the performance limitations of single-task models and can generalize across diverse social media applications. The methodology involves a three-stage post-training pipeline applied to a general foundation model: 1) Continued Pretraining (CPT) on a large-scale corpus of general and SNS-specific data; 2) Supervised Fine-Tuning (SFT) on a variety of defined SNS tasks; and 3) Preference Optimization (PO) using Direct Preference Optimization (DPO) to align model outputs with human preferences. The resulting RedOne models demonstrate significant improvements, with RedOne-7B achieving a 14.02% average score increase on the SNS-Bench and a 7.56% increase on SNS-TransBench compared to its base model. In online A/B testing, RedOne reduced harmful content exposure by 11.23% and improved the post-view search click-through rate by 14.95%. The principal implication for AI practitioners is that a structured, multi-stage post-training approach—specifically CPT followed by SFT and PO—is a highly effective strategy for adapting a general-purpose LLM to a specialized domain with unique linguistic characteristics like social media, providing a clear blueprint for creating robust, domain-aware models. |
| Mitigating Object Hallucinations via Sentence-Level Early Intervention (Read more on [arXiv](https://arxiv.org/abs/2507.12455) or [HuggingFace](https://huggingface.co/papers/2507.12455))| Zhuotao Tian, Li Jiang, Senqiao Yang, Shangpin Peng | This paper introduces SENTINEL, a framework that mitigates object hallucinations in Multimodal Large Language Models (MLLMs) by intervening at the sentence level where they first emerge, using automatically generated, in-domain preference data. The research objective is to develop an efficient method to suppress hallucinations without human annotation by bootstrapping preference pairs from model outputs, cross-checking object existence with two open-vocabulary detectors, and then fine-tuning with a novel context-aware Direct Preference Optimization (C-DPO) loss. SENTINEL demonstrates a significant reduction in hallucinations, lowering the response-level hallucination rate on the Object HalBench benchmark by over 90% (from 52.7% to 4.3%) compared to the baseline LLaVA-v1.5-7B model. For AI practitioners, this provides a scalable and model-agnostic methodology to enhance MLLM factuality in a resource-efficient manner, enabling the development of more trustworthy applications by automatically creating high-quality, in-domain training data. |
| The Generative Energy Arena (GEA): Incorporating Energy Awareness in
  Large Language Model (LLM) Human Evaluations (Read more on [arXiv](https://arxiv.org/abs/2507.13302) or [HuggingFace](https://huggingface.co/papers/2507.13302))| Pedro Reviriego, Javier Conde, Eneko Sendin, Gonzalo Martínez, Carlos Arriaga | This paper introduces the Generative Energy Arena (GEA) to measure the impact of energy awareness on human evaluation of LLMs. The study's primary objective is to quantify how providing information on relative energy consumption influences human evaluators' model preferences in a head-to-head comparison. The methodology involves a two-step human evaluation where users first select the better of two anonymized model responses and are then asked if they would change their vote after being informed that their initial choice was the higher-energy model. A key result shows that users changed their vote to favor the more energy-efficient model in approximately 46% of these cases, leading to a final preference for smaller models over 75% of the time. The principal implication for AI practitioners is that smaller, more energy-efficient models are often sufficient and preferred by users for many tasks when energy cost is a factor, suggesting that energy metrics should be a critical component of LLM evaluation and deployment decisions. |
| Inverse Reinforcement Learning Meets Large Language Model Post-Training:
  Basics, Advances, and Opportunities (Read more on [arXiv](https://arxiv.org/abs/2507.13158) or [HuggingFace](https://huggingface.co/papers/2507.13158))| Mihaela van der Schaar, Hao Sun | This paper provides a comprehensive review of Large Language Model (LLM) alignment through the lens of inverse reinforcement learning (IRL), focusing on the necessity of learning neural reward models from human feedback.  The paper's main objective is to survey, structure, and analyze the foundations, recent advances, and practical challenges of applying IRL and reinforcement learning (RL) techniques to LLM post-training, contrasting them with conventional RL tasks.  The key methodologies analyzed are framing LLM generation as a Markov Decision Process without a reward function (MDP\R) and reviewing IRL approaches to solve it, including Reinforcement Learning from Human Feedback (RLHF) via PPO, Direct Preference Optimization (DPO), and Alignment from Demonstration (AfD) analyzed through f-divergence minimization.  As a review paper, it synthesizes existing findings, highlighting reward overoptimization as a primary result; it cites research (Gao et al., 2023) showing that as an LLM policy is optimized, the gap between its score on the learned reward model and a held-out "gold" reward model widens, quantifying reward hacking.  The principal implication for AI practitioners is that moving beyond simple imitation (SFT) to an IRL paradigm with explicit reward models is crucial for robust alignment; this involves a practical trade-off between stable methods like DPO and potentially higher-performing but complex methods like PPO, with a critical need to monitor for reward overoptimization. |
| Quantitative Risk Management in Volatile Markets with an Expectile-Based
  Framework for the FTSE Index (Read more on [arXiv](https://arxiv.org/abs/2507.13391) or [HuggingFace](https://huggingface.co/papers/2507.13391))| 0xnu | This research develops and validates an expectile-based framework for quantitative risk management that outperforms traditional Value-at-Risk (VaR) models for the FTSE 100 index. The objective was to create an advanced risk framework that addresses the shortcomings of conventional quantile-based approaches by providing greater sensitivity to tail losses, especially in volatile market conditions. The methodology utilizes expectile regression on two decades of FTSE 100 daily returns, incorporating GARCH-type dynamics for heteroscedasticity and novel mathematical formulations for time-varying parameters and adaptive thresholds. The primary result from out-of-sample backtesting shows the Expectile-based VaR (EVaR) model achieved a 5.0% violation rate for a 95% confidence level, passing the Conditional Coverage test (p=0.756), whereas traditional methods like Historical Simulation failed with a 12.1% violation rate. The principal implication for AI practitioners is that implementing systems with expectile regression models, which inherently capture tail risk magnitude and asymmetry, offers superior predictive accuracy and robustness over standard quantile-based methods, though it may necessitate infrastructure upgrades to support more complex, real-time computations. |
