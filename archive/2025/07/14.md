

## Papers for 2025-07-14

| Title | Authors | Summary |
|-------|---------|---------|
| Test-Time Scaling with Reflective Generative Model (Read more on [arXiv](https://arxiv.org/abs/2507.01951) or [HuggingFace](https://huggingface.co/papers/2507.01951))| Jie Gao, Mengting Xing, Xiaorui Wang, Yuxin Wang, Zixiao Wang | This paper introduces MetaStone-S1, a reflective generative model that uses a unified architecture and self-supervised reward modeling to achieve efficient test-time scaling and performance comparable to OpenAI's o3-mini. The main research objective is to develop a method for high-quality reasoning trajectory selection that unifies the policy and process reward models to reduce computational overhead and eliminate reliance on costly process-level annotations. The key methodology is a "Reflective Generative Form" where a policy model and a Self-supervised Process Reward Model (SPRM) share a single network backbone. The SPRM is a lightweight head trained with a novel Self-supervised Process Reward Loss (SPRLoss), which learns to evaluate reasoning steps using only final outcome correctness and a dynamic weighting scheme to filter supervision noise. The primary results show the 32B parameter MetaStone-S1-medium model achieves 84.2% on AIME24, comparable to OpenAI o3-mini's 79.6%. The proposed SPRM, adding only 26M parameters to a 7B model, achieved a 70.2% score on AIME24, outperforming a separate 72B process reward model that scored 68.8%. The principal implication for AI practitioners is that high-performance test-time scaling can be achieved without training separate, large-scale reward models, offering a parameter-efficient and data-efficient method to integrate reasoning trajectory generation and selection into a single, self-supervised process, thereby reducing training and inference costs. |
| CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive
  Neural Rendering (Read more on [arXiv](https://arxiv.org/abs/2507.08776) or [HuggingFace](https://huggingface.co/papers/2507.08776))| Yasutaka Furukawa, Fuyang Zhang, Jiacheng Chen, Yuefan Wu, Zhengqing Wang | The paper introduces CLiFT, a compressive light-field token representation for compute-efficient and adaptive neural rendering from a set of input images. The primary objective is to develop a compact, variable-size scene representation that enables adaptive control over the trade-offs between data size, rendering quality, and speed with a single trained model. The methodology involves a multi-view transformer encoder to generate initial tokens (LiFTs), followed by latent-space K-means to select representative ray centroids, and finally a neural "condenser" network that compresses information from all tokens into these centroids to form CLiFTs. On the RealEstate10K dataset, CLiFT achieves a comparable Peak Signal-to-Noise Ratio (PSNR) to MVSplat and DepthSplat baselines while requiring approximately 5–7× less data storage. The framework's key implication for AI practitioners is the ability to deploy a single model that dynamically adjusts rendering performance (e.g., increasing FPS by up to 66% for a corresponding quality drop) by varying the number of tokens used, enabling real-time adaptation to diverse hardware or network conditions. |
| NeuralOS: Towards Simulating Operating Systems via Neural Generative
  Models (Read more on [arXiv](https://arxiv.org/abs/2507.08800) or [HuggingFace](https://huggingface.co/papers/2507.08800))| Yuntian Deng, Wenhu Chen, Hongyu Guo, Sun Sun, Luke Rivard | The paper introduces NeuralOS, a generative model that simulates an operating system's graphical user interface by autoregressively predicting screen frames from user inputs. The primary objective is to develop a neural framework capable of simulating a complete OS GUI, including state transitions and interactions, purely through generative modeling without traditional OS kernels. NeuralOS uses a hierarchical recurrent neural network (RNN) to track system state and a diffusion-based UNet renderer, conditioned on RNN output and an explicit Gaussian spatial map of the cursor's position, to generate subsequent frames. The model achieves high fidelity in mouse interactions, with a key quantitative result being an average cursor position error of only 1.6 pixels in width and 1.4 in height on a 512x384 screen, demonstrating the efficacy of its spatial encoding method. The principal implication for AI practitioners is that this work provides a proof-of-concept and a technical blueprint for building fully generative, stateful, and adaptive user interfaces, suggesting a future where complex interactive systems can be modeled as end-to-end generative processes rather than being rigidly programmed. |
| KV Cache Steering for Inducing Reasoning in Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.08799) or [HuggingFace](https://huggingface.co/papers/2507.08799))| Cees G. M. Snoek, M. Jehanzeb Mirza, Michael Dorkenwald, Dawid J. Kopiczko, Max Belitsky | This paper introduces cache steering, a lightweight method for inducing structured reasoning in small language models via a one-shot modification to the key-value (KV) cache. The research objective is to develop a more efficient and stable alternative to continuous activation steering for eliciting latent reasoning abilities in smaller models without fine-tuning or prompt modification. The methodology involves creating steering vectors by computing the mean difference of KV cache representations from contrastive prompt pairs (one with GPT-4o-generated reasoning traces, one without) and applying these vectors once to the prompt's KV cache before generation. Experiments show cache steering improves task performance; for instance, on the ARC-c benchmark, it increased the Llama-3.2-3B model's accuracy from 74.32% to 79.27% under greedy decoding. The principal implication for AI practitioners is that cache steering provides a practical, low-overhead technique to enhance the reasoning of smaller models, offering improved stability and inference efficiency compared to methods requiring continuous interventions. |
| Lumos-1: On Autoregressive Video Generation from a Unified Model
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2507.08801) or [HuggingFace](https://huggingface.co/papers/2507.08801))| Jingyun Liang, Hu Yu, Jun Cen, Weihua Chen, Hangjie Yuan | Lumos-1 is an autoregressive video generation model that adapts a standard LLM architecture with minimal modifications to unify text and video processing. The main objective is to develop an efficient video generator that is architecturally aligned with LLMs, avoiding reliance on external text encoders and the high latency of next-token decoding. Key methodologies include MM-ROPE, a distributed and scaled 3D Rotary Position Embedding to inject spatiotemporal correlations, and Autoregressive Discrete Diffusion Forcing (AR-DF), a temporal tube masking strategy to resolve frame-wise loss imbalance during training. The 3.6B parameter Lumos-1 model achieves a score of 0.664 on the GenEval benchmark, demonstrating performance comparable to larger models trained with more resources. The principal implication for AI practitioners is that standard LLM architectures can be effectively adapted for high-quality video generation through targeted mechanisms like MM-ROPE and specialized training schemes like AR-DF, paving the way for more integrated and efficient unified multimodal models. |
| Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.05255) or [HuggingFace](https://huggingface.co/papers/2507.05255))| Jisheng Yin, Kangheng Lin, Jianjian Sun, Liang Zhao, Yana Wei | This paper presents Open-Vision-Reasoner (OVR), a model that enhances visual reasoning by transferring cognitive behaviors from language to vision using a two-stage, cold-start and reinforcement learning paradigm. The primary objective is to investigate how linguistic cognitive behaviors, such as backtracking and subgoal decomposition, can be effectively transferred to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning capabilities. The methodology consists of a two-stage process: a massive linguistic cold-start supervised fine-tuning on over 2 million text examples to instill cognitive patterns, followed by large-scale multimodal reinforcement learning (RL) with PPO and verifiable rewards to align these patterns with visual contexts. The resulting OVR model achieves state-of-the-art results for open-source models, including 95.3% on MATH500 and 51.8% on MathVision. For AI practitioners, the key implication is that a training strategy of first instilling linguistic reasoning structures via a "cold start" and then using RL to critically discern and scale these behaviors in a multimodal context is a highly effective and scalable approach for developing more capable visual reasoning systems. |
| From One to More: Contextual Part Latents for 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2507.08772) or [HuggingFace](https://huggingface.co/papers/2507.08772))| Yuxin Wang, Yaokun Li, Xiao Chen, Lihe Ding, Shaocong Dong | The paper introduces CoPart, a framework that generates detailed and controllable 3D objects by representing them as a collection of contextual part latents rather than a single holistic latent. The research objective is to overcome the detail loss and lack of fine-grained control in existing 3D generation models by developing a system that explicitly models and generates objects part-by-part. CoPart's methodology involves decomposing objects into simpler parts, encoding them with both geometric and image tokens, and using a mutual guidance strategy to ensure coherence during a synchronized diffusion process, which is further guided by 3D bounding box conditions. The framework demonstrates superior performance in part-based generation, achieving a part-aware CLIP (I-T) score of 0.1768, outperforming prior models like Rodin (0.1571) and Trellis (0.1455). The primary implication for AI practitioners is that this part-based approach provides a direct mechanism for granular, part-level control over generated 3D assets, enabling applications like interactive editing, object articulation, and scene composition that are challenging for monolithic generators. |
| One Token to Fool LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2507.08794) or [HuggingFace](https://huggingface.co/papers/2507.08794))| Haitao Mi, S. Y. Kung, Dian Yu, Haolin Liu, Yulai Zhao | This paper investigates the vulnerability of LLM-as-a-judge models to simple, superficial "master key" attacks. The research objective is to systematically evaluate the susceptibility of generative reward models to these attacks and propose an effective mitigation strategy. The methodology involves testing various LLMs across five reasoning benchmarks using adversarial inputs like single punctuation marks or phrases such as "Thought process:", then training a new model, Master-RM, on a dataset augmented with these inputs labeled as negative examples. The study reveals that standard LLMs exhibit false positive rates (FPRs) as high as 80% on these master keys, while the proposed Master-RM reduces this rate to near-zero across all settings. For AI practitioners, this implies that generative reward models used in RLVR or for evaluation are systematically vulnerable to trivial exploits, and their reliability requires specific robustness-focused fine-tuning via data augmentation. |
| Vision Foundation Models as Effective Visual Tokenizers for
  Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2507.08441) or [HuggingFace](https://huggingface.co/papers/2507.08441))| Tiancai Wang, Chuofan Ma, Xuanyang Zhang, Xin Wen, Anlin Zheng | This paper introduces VFMTok, a visual tokenizer built upon frozen vision foundation models (VFMs) to improve autoregressive (AR) image generation. The research aims to determine if features from pre-trained VFMs can serve as robust, semantically rich representations for image generation, overcoming the limitations of standard tokenizers. The methodology employs a frozen VFM as an encoder, introduces a region-adaptive quantization framework using deformable attention to reduce token redundancy, and applies a semantic reconstruction objective to preserve feature fidelity. The proposed model achieves a gFID of 2.07 on ImageNet, accelerates AR model convergence by three times, and enables high-fidelity, class-conditional synthesis without classifier-free guidance (CFG). For AI practitioners, this implies that leveraging pre-trained VFMs as tokenizers can substantially enhance AR generation quality and efficiency, while simplifying training and inference by removing the need for complex guidance mechanisms. |
| What Has a Foundation Model Found? Using Inductive Bias to Probe for
  World Models (Read more on [arXiv](https://arxiv.org/abs/2507.06952) or [HuggingFace](https://huggingface.co/papers/2507.06952))| Sendhil Mullainathan, Ashesh Rambachan, Peter G. Chang, Keyon Vafa | This paper introduces an "inductive bias probe," a method to evaluate if foundation models learn underlying world models or just task-specific heuristics.  The main objective is to develop and apply a framework for testing whether a foundation model has captured the deep, generative structure of its training data, rather than just learning surface-level predictive patterns.  The key methodology is the "inductive bias probe," which involves repeatedly fine-tuning a foundation model on small, synthetic datasets generated from a postulated world model and then analyzing the model's extrapolations to see if they align with the world model's principles. This is quantified using metrics like R-IB (respecting state) and D-IB (distinguishing state) for discrete domains, and by comparing learned functions to ground-truth laws in continuous domains.  The primary result is that foundation models, despite high performance on their training tasks, often fail to develop an inductive bias toward the true world model. For example, a transformer pretrained on orbital mechanics, when fine-tuned to predict gravitational force, recovered a nonsensical physical law (`F ∝ (sin(sin(r-0.24)/r) + 1.45) * (1/r + m2)`) instead of Newton's law (`F ∝ m1*m2/r^2`).  The principal implication for AI practitioners is that high performance on a pre-training objective like next-token prediction does not guarantee a model has learned a generalizable world model. Models may be learning brittle, non-transferable heuristics, and their suitability for new tasks that rely on deep domain understanding must be explicitly tested rather than assumed. |
| Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities (Read more on [arXiv](https://arxiv.org/abs/2507.06261) or [HuggingFace](https://huggingface.co/papers/2507.06261))| Noveen Sachdeva, Ice Pasupat, Mike Schaekermann, Eric Bieber, Gheorghe Comanici | This report introduces the Gemini 2.X model family, which pushes the frontier of AI with advanced reasoning, multimodality, and next-generation agentic capabilities. The primary objective is to present the architecture, training advancements, and performance of these models, including Gemini 2.5 Pro and Flash, which are designed to power a new era of agentic systems. The models utilize a sparse mixture-of-experts (MoE) transformer architecture with native multimodal support, long-context inputs of over 1 million tokens, and a "Thinking" mechanism trained via Reinforcement Learning that allows the model to use additional inference-time compute to improve answer accuracy. Gemini 2.5 Pro achieves state-of-the-art performance on coding and reasoning benchmarks, improving the pass rate on LiveCodeBench to 74.2% from 30.5% for Gemini 1.5 Pro, and can process up to 3 hours of video content. The principal implication for AI practitioners is the availability of a family of models spanning the full capability-cost Pareto frontier, where Gemini 2.5 Pro enables complex, multimodal agentic workflows and Gemini 2.5 Flash offers a controllable thinking budget to dynamically trade off quality, cost, and latency. |
| BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity (Read more on [arXiv](https://arxiv.org/abs/2507.08771) or [HuggingFace](https://huggingface.co/papers/2507.08771))| Yingfa Chen, Chaojun Xiao, Xu Han, Weilin Zhao, Chenyang Song | The paper introduces BlockFFN, a Mixture-of-Experts architecture designed for high chunk-level sparsity to enable acceleration on end-side devices. The objective is to develop a sparsely-activated LLM architecture that overcomes the performance and acceleration limitations of vanilla MoE, specifically by improving chunk-level sparsity (CLS) for efficient deployment on resource-constrained hardware. The methodology involves a novel router using ReLU for flexible, differentiable routing and RMSNorm to stabilize activation magnitudes, combined with two CLS-aware training objectives: an activation locality loss and a chunk sparsification loss to promote sparsity across token chunks. BlockFFN achieves over 70% 8-token chunk-level sparsity, and its custom acceleration kernels attain up to a 3.67× speedup over baseline auto-regressive decoding on an NVIDIA Jetson Orin NX. For AI practitioners, this work provides a practical method to achieve significant LLM inference acceleration on edge devices by training for high chunk-level sparsity, which is compatible with and enhances mainstream techniques like speculative decoding. |
| Robust Multimodal Large Language Models Against Modality Conflict (Read more on [arXiv](https://arxiv.org/abs/2507.07151) or [HuggingFace](https://huggingface.co/papers/2507.07151))| Houqiang Li, Jie Zhao, Wengang Zhou, ustc-zhangzm | This research investigates and proposes mitigation strategies for hallucinations in Multimodal Large Language Models (MLLMs) arising from "modality conflict," where visual and textual inputs are inherently contradictory. The primary objective is to formally define modality conflict, create a benchmark dataset (MMMC) to systematically evaluate this phenomenon, and test methods to make MLLMs more robust against it. The authors constructed the MMMC dataset by programmatically generating questions with object, attribute, or relationship conflicts against an image and then evaluated three mitigation techniques: prompt engineering, supervised fine-tuning (SFT), and reinforcement learning (RL) on prevalent MLLMs. Reinforcement learning demonstrated the best performance in mitigating hallucinations; for instance, it reduced the hallucination rate (Hallu-Rate) of the Qwen2-VL-Instruct-2B model on the MMMC dataset from a 46.55% baseline to 18.00%. For AI practitioners, this work demonstrates that MLLMs are highly susceptible to hallucinations when user inputs contain implicit contradictions with visual data, and indicates that fine-tuning with RL or SFT on datasets simulating such conflicts is a practical approach to improve model robustness in real-world applications. |
