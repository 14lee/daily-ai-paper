

## Papers for 2025-07-09

| Title | Authors | Summary |
|-------|---------|---------|
| A Survey on Latent Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.06203) or [HuggingFace](https://huggingface.co/papers/2507.06203))| Tianhao Peng, jeshragh, chujiezheng, Jinfa, ridger | This survey provides a comprehensive overview of latent reasoning, an emerging paradigm where multi-step inference is performed within a model's continuous hidden state, bypassing the expressive bandwidth limitations of explicit Chain-of-Thought (CoT). The paper's objective is to unify and categorize diverse latent reasoning methodologies, including their architectural foundations, training strategies, and mechanistic interpretability. The authors conduct a systematic review, classifying techniques into vertical recurrence (activation-based methods like looped transformers) and horizontal recurrence (hidden-state-based methods), and also explore infinite-depth reasoning via text diffusion models. The survey's primary result highlights that latent reasoning, by exchanging full hidden states (~40,960 bits), provides a ~2.7 × 10³-fold greater expressive bandwidth than explicit reasoning with discrete tokens (~15 bits). For AI practitioners, the principal implication is that shifting reasoning from discrete tokens to the continuous latent space offers a pathway to build models with more powerful and efficient reasoning capabilities, moving beyond the constraints of explicit verbalization for complex problem-solving. |
| SingLoRA: Low Rank Adaptation Using a Single Matrix (Read more on [arXiv](https://arxiv.org/abs/2507.05566) or [HuggingFace](https://huggingface.co/papers/2507.05566))| Ron Kimmel, Daniel Bensaïd, David Bensaïd, royve, noamrot | SingLoRA is a parameter-efficient fine-tuning method that resolves LoRA's training instability by using a single matrix and its transpose for the low-rank update. The primary objective is to address the unstable training dynamics in LoRA, which arise from scale disparities between its two adapter matrices, and to develop a more stable and parameter-efficient alternative. The key methodology involves reformulating the low-rank update from LoRA's `W₀ + BA` to `W₀ + AAT`, thereby using only a single learnable matrix `A`. The paper provides theoretical analysis demonstrating that this formulation is transformation-invariant and guarantees stable feature learning by construction. Primary results show significant improvements in both performance and efficiency: fine-tuning LLaMA-7B on MNLI with SINGLORA achieved 91.3% accuracy with 12M parameters, outperforming LoRA which reached 89.1% with 20M parameters. The principal implication for AI practitioners is that SINGLORA can serve as a more robust and efficient alternative to LoRA, enabling them to achieve superior fine-tuning performance with approximately half the parameter budget and reduced sensitivity to hyperparameter choices like the learning rate. |
| OmniPart: Part-Aware 3D Generation with Semantic Decoupling and
  Structural Cohesion (Read more on [arXiv](https://arxiv.org/abs/2507.06165) or [HuggingFace](https://huggingface.co/papers/2507.06165))| Yukun Huang, Zi-Xin Zou, Yuan-Chen Guo, Yufan Zhou, Yunhan Yang | The paper introduces OmniPart, a two-stage framework for generating controllable, part-based 3D objects from 2D images and masks. The primary objective is to generate structured 3D assets with high semantic decoupling between parts and robust overall structural cohesion, overcoming the utility limitations of monolithic generation methods. The methodology first employs an autoregressive transformer to plan a 3D part layout as a sequence of bounding boxes guided by 2D masks, then a spatially-conditioned synthesis module, fine-tuned from a pre-trained generator, synthesizes all parts simultaneously within this layout. Quantitatively, OmniPart achieves a part-level F1-score of 0.74 (at a Chamfer Distance threshold of < 0.1), significantly outperforming existing part-aware generation baselines. For AI practitioners, this framework provides a direct pipeline to create editable, structured 3D assets from simple 2D inputs, enabling downstream applications like part-specific editing, animation, and material assignment in interactive systems. |
| StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2507.05240) or [HuggingFace](https://huggingface.co/papers/2507.05240))| Yuqiang Yang, Tai Wang, Xiqian Yu, Meng Wei, cywan | StreamVLN is a streaming vision-language navigation framework using a hybrid slow-fast context model with 3D-aware token pruning to achieve efficient, low-latency navigation on long video streams.  The research objective is to develop a vision-language navigation (VLN) framework that can process continuous visual streams for long-horizon tasks while maintaining low inference latency, bounded memory usage, and high navigational performance, addressing the limitations of existing Video-LLM methods.  The key methodology is a hybrid slow-fast context modeling strategy: a "fast" sliding-window KV cache retains a fixed number of recent dialogue turns for responsive action generation, while a "slow-updating" memory context compresses historical visual history using a voxel-based spatial pruning algorithm that discards spatially redundant tokens based on their 3D projections.  The framework achieves state-of-the-art performance for RGB-only methods on VLN-CE benchmarks, attaining a Success Rate (SR) of 56.9% and a Success weighted by Path Length (SPL) of 51.9% on the R2R Val-Unseen split; its voxel-based pruning reduces input tokens by approximately 20% while concurrently improving SR.  For AI practitioners, the slow-fast context management with voxel-based pruning provides a practical method for deploying large multimodal models in real-time, resource-constrained embodied AI applications, enabling models trained on short clips to operate on long, continuous data streams with bounded computational cost and stable latency. |
| CriticLean: Critic-Guided Reinforcement Learning for Mathematical
  Formalization (Read more on [arXiv](https://arxiv.org/abs/2507.06181) or [HuggingFace](https://huggingface.co/papers/2507.06181))| Yifan Yao, Zhongyuan Peng, zhangysk, zhouliang, yifAI | This paper introduces CriticLean, a framework using a reinforcement learning-trained critic model to guide and validate the translation of natural language mathematics into formal Lean 4 code. The research objective is to improve mathematical autoformalization by systematically optimizing the "critic phase," where the semantic correctness of generated formalizations is evaluated, going beyond mere compilation success. The methodology involves developing `CriticLeanGPT`, a critic model trained with supervised fine-tuning and RL, and integrating it into an iterative generation pipeline that refines outputs based on feedback from both the Lean compiler and the critic. The primary result shows that this pipeline significantly improves autoformalization accuracy, raising it from 38.0% (single pass) to 84.0% on a human-evaluated set of 50 problems from Omni-MATH. For AI practitioners, this demonstrates that integrating a dedicated, trained semantic critic into a generation loop is a highly effective strategy for improving the reliability and semantic fidelity of domain-specific code generation systems. |
| RLVER: Reinforcement Learning with Verifiable Emotion Rewards for
  Empathetic Agents (Read more on [arXiv](https://arxiv.org/abs/2507.03112) or [HuggingFace](https://huggingface.co/papers/2507.03112))| Zhiwei He, Xingyu Chen, Bang Zhang, vvibt, CedarWang | This paper introduces RLVER, a framework that uses reinforcement learning with verifiable, deterministic emotion rewards from a simulated user to enhance the empathetic capabilities of LLMs. The primary objective is to cultivate higher-order emotional intelligence in LLMs by optimizing for simulated user satisfaction, without relying on human-annotated data. The methodology involves fine-tuning a Qwen2.5-7B model using Proximal Policy Optimization (PPO), where rewards are verifiable emotion scores generated turn-by-turn from a psychologically-grounded user simulator, and includes an analysis of an explicit "think-then-say" reasoning scaffold. The proposed RLVER framework increased the model's Sentient-Benchmark score from 13.3 to 79.2, a nearly six-fold improvement, while largely preserving its general reasoning capabilities. For AI practitioners, this research provides a practical and scalable methodology for improving subjective, human-centric LLM capabilities by replacing expensive human feedback loops (RLHF) with verifiable rewards from a well-designed, deterministic simulator. |
| MedGen: Unlocking Medical Video Generation by Scaling
  Granularly-annotated Medical Videos (Read more on [arXiv](https://arxiv.org/abs/2507.05675) or [HuggingFace](https://huggingface.co/papers/2507.05675))| Shunian Chen, Zhenyang Cai, Ke Ji, Junying Chen, wangrongsheng | This paper introduces MedGen, a specialized medical video generation model, and MedVideoCap-55K, the dataset it was trained on, to address the failure of general-purpose models in producing medically accurate content. The primary objective is to create a foundational dataset and model for high-fidelity, domain-specific medical video generation. The methodology involves constructing the MedVideoCap-55K dataset by curating over 55,000 captioned video clips from public sources through a rigorous filtering pipeline, and then fine-tuning the open-source HunyuanVideo model on this dataset to create MedGen. In experiments, MedGen achieved a total score of 70.93 on the Med-VBench benchmark, outperforming all other evaluated open-source models and performing competitively against proprietary models like Pika. The principal implication for AI practitioners is that domain-specific fine-tuning on a large-scale, high-quality, and granularly-annotated dataset is a crucial strategy for adapting foundational models to high-stakes fields, significantly enhancing both domain-specific accuracy and overall output quality. |
| Is Diversity All You Need for Scalable Robotic Manipulation? (Read more on [arXiv](https://arxiv.org/abs/2507.06219) or [HuggingFace](https://huggingface.co/papers/2507.06219))| Jin Chen, Li Chen, sundrops, yxlu0, ModiShi | This paper systematically investigates the effects of task, embodiment, and expert diversity on scalable robotic manipulation, challenging the "more diverse is better" paradigm. The research objective is to evaluate how these three dimensions of data diversity impact the performance and scalability of robotic manipulation policies learned through imitation. The methodology involves extensive experiments using models like GO-1 and RDT on large-scale datasets, comparing different pre-training strategies (e.g., task-based vs. episode-based sampling, single- vs. multi-embodiment data) and introducing a distribution debiasing method, GO-1-Pro, to mitigate velocity multimodality in expert demonstrations. The primary results demonstrate that task diversity is more critical than demonstration quantity, multi-embodiment pre-training is optional for cross-embodiment transfer, and expert diversity can be confounding; the proposed GO-1-Pro method achieved a 15% performance gain, equivalent to using 2.5 times the pre-training data. The principal implication for AI practitioners is that for scalable robotics, curating datasets with high task diversity and implementing techniques to debias confounding expert-level variations like velocity is more data-efficient than merely increasing dataset size or embodiment diversity. |
| Coding Triangle: How Does Large Language Model Understand Code? (Read more on [arXiv](https://arxiv.org/abs/2507.06138) or [HuggingFace](https://huggingface.co/papers/2507.06138))| Songyang Zhang, Maosong Cao, Taolin Zhang, jnanliu, MichaelErchi | This paper introduces the "Coding Triangle" framework to systematically evaluate large language models' (LLMs) programming abilities across editorial analysis, code implementation, and test case generation. The main objective is to define and assess LLM coding capability by analyzing performance and interactions across these three dimensions. The methodology involves evaluating various LLMs on 200 AtCoder problems, using metrics like Pass@1 for code, LLM-as-a-judge for editorials, and the discriminative power of generated test cases. The study reveals a strong self-consistency bias, where models' self-generated solutions achieve pass rates up to 40% higher on their own generated test cases than on ground-truth cases, while also showing that model-generated solutions exhibit high error similarity (cosine similarity > 0.8), unlike diverse human solutions. The principal implication for AI practitioners is that relying on an LLM's self-verification is insufficient; enhancing robustness requires incorporating diverse human-generated data or using model mixtures to overcome the cognitive biases and limited error diversity inherent in single models. |
| GTA1: GUI Test-time Scaling Agent (Read more on [arXiv](https://arxiv.org/abs/2507.05791) or [HuggingFace](https://huggingface.co/papers/2507.05791))| Yuhao Yang, Yutong Dai, Dongxu Li, Yan Yang, Ziyang | The paper introduces GTA1, a GUI agent that improves task planning through a test-time scaling strategy and enhances visual grounding with a reinforcement learning model. The research objective is to address two primary challenges in GUI agent autonomy: 1) resolving ambiguity in task planning by selecting the most robust action from multiple plausible options without requiring multi-step lookahead, and 2) improving the accuracy of visually grounding actions on complex, high-resolution interfaces. The methodology comprises a two-stage approach: a test-time scaling strategy for planning that samples multiple candidate action proposals and uses a judge model to select the most suitable one, and a grounding model trained with Reinforcement Learning (RL) to directly predict interaction coordinates, using a binary reward for successful clicks within the target UI element. The primary result is that the GTA1-7B agent achieves a 45.2% task success rate on the OSWorld benchmark when paired with an o3 planner, outperforming all compared state-of-the-art methods. The principal implication for AI practitioners is that for GUI agent development, a simple binary click reward for RL-based grounding is more effective than complex objectives like enforcing "thinking" or bounding box prediction. Additionally, implementing a test-time sampling and judging strategy is a practical method to significantly improve planning robustness and overall task success rates by mitigating cascading failures. |
| Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts (Read more on [arXiv](https://arxiv.org/abs/2507.04569) or [HuggingFace](https://huggingface.co/papers/2507.04569))| Mohamed Anwar, Amr Mohamed, Ahmad Chamma, Hadi Abdine, guokan-shang | The paper introduces Nile-Chat, a family of large language models specifically designed to understand and generate Egyptian Arabic in both its native Arabic and Latin-based scripts. The primary objective is to develop high-performing LLMs for the dual-script Egyptian dialect, addressing the failure of existing models to adequately support this widespread language setting. The authors employ a comprehensive training pipeline including continual pre-training, instruction-tuning, and Direct Preference Optimization (DPO), and notably use the Branch-Train-MiX (BTX) strategy to merge a base model with script-specialized experts into a unified Mixture-of-Experts (MoE) model. The Nile-Chat models significantly outperform baselines, with the 12B model yielding a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. For AI practitioners, this work provides a replicable methodology for adapting LLMs to dual-script languages, showing that merging specialized experts via BTX into an MoE architecture is an effective strategy for improving capability in underrepresented linguistic contexts. |
| Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers (Read more on [arXiv](https://arxiv.org/abs/2507.06223) or [HuggingFace](https://huggingface.co/papers/2507.06223))| Yi Fang, Ting-ruen Wei, Zhiyuan Peng, yilunzhao, songtingyu | This research introduces E²R-FLOPs, a hardware-agnostic framework using floating-point operations (FLOPs) to evaluate the efficiency-effectiveness trade-off of LLM-based rerankers. The main objective is to establish a standardized method for comparing reranker performance that is independent of specific hardware and runtime configurations by addressing the limitations of proxy metrics like latency or token counts. The methodology involves deriving a closed-form FLOPs estimator for decoder-only and encoder-decoder architectures and proposing two metrics: Ranking metrics per PetaFLOP (RPP) and Queries per PetaFLOP (QPP), which are then used to evaluate various reranking methods on the TREC-DL datasets. Primary results demonstrate that simpler, pointwise methods are vastly more efficient; a Flan-T5-large `pointwise.yes_no` model achieved the highest RPP of 72.67 on DL19, while more effective methods like pairwise sorting caused RPP to drop to approximately 0.1, highlighting a severe efficiency cost for marginal quality gains. The principal implication for AI practitioners is that scaling up model size for reranking offers diminishing returns, and the provided FLOPs estimator enables the selection of more computationally efficient architectures, like pointwise methods, for practical, large-scale deployment. |
| PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to
  Graphs (Read more on [arXiv](https://arxiv.org/abs/2507.05101) or [HuggingFace](https://huggingface.co/papers/2507.05101))| Zhiyuan Liu, Fanding Xu, Hao Du, JinzheFudan, piaolaidangqu | PRING is a comprehensive benchmark that evaluates protein-protein interaction (PPI) prediction models on their ability to reconstruct biologically coherent networks, moving beyond simple pairwise classification. The primary objective is to assess how well current PPI prediction models can recapitulate the topological and functional properties of real-world PPI networks, a capability overlooked by existing pairwise-focused benchmarks. The authors constructed PRING, a multi-species PPI dataset, and used it to benchmark sequence-based, PLM-based, and structure-based models on topology-oriented (network construction) and function-oriented (pathway analysis) tasks using graph-level metrics. The results show that even the best models struggle to reconstruct accurate network topologies, with the top-performing model achieving a maximum Graph Similarity score of only 0.491, and they perform poorly on functional tasks with functional alignment scores below 0.4. For AI practitioners, this research critically implies that standard classification metrics are insufficient for evaluating models in network reconstruction contexts; high pairwise accuracy does not guarantee the generation of structurally or functionally valid graphs, highlighting the need to adopt graph-centric evaluation protocols. |
| SAMed-2: Selective Memory Enhanced Medical Segment Anything Model (Read more on [arXiv](https://arxiv.org/abs/2507.03698) or [HuggingFace](https://huggingface.co/papers/2507.03698))| Rong Zhou, Yiwei Li, Sifan Song, Zhiling Yan, songdj | SAMed-2 is a medical image segmentation foundation model that enhances the SAM-2 architecture with a temporal adapter and a confidence-driven memory mechanism to improve performance on diverse and noisy medical data. The main objective is to adapt a general "segment anything" model for the medical domain by addressing its specific challenges, including handling volumetric/temporal data, mitigating the effects of noisy annotations, and preventing catastrophic forgetting during continual learning across multiple modalities. The key methodology involves integrating a temporal adapter with a 3D convolution into the image encoder to capture inter-slice correlations and implementing a confidence-driven memory that selectively stores high-certainty feature embeddings (based on predicted IoU) and retrieves them based on both feature similarity and confidence. The model achieved a mean Dice Similarity Coefficient (DSC) of 0.6938 on 10 external, unseen segmentation tasks, outperforming MedSAM by 10.53%, and in a human user study, SAMed-2-assisted annotation reduced the time required per frame by 87.61% compared to manual annotation. For AI practitioners, the principal implication is that adapting large vision models to specialized domains like medical imaging requires more than fine-tuning; domain-specific architectural modifications like temporal adapters and explicit, quality-aware memory mechanisms are critical for handling data-specific characteristics like volumetric structure and label noise to achieve robust performance. |
| Tora2: Motion and Appearance Customized Diffusion Transformer for
  Multi-Entity Video Generation (Read more on [arXiv](https://arxiv.org/abs/2507.05963) or [HuggingFace](https://huggingface.co/papers/2507.05963))| Weizhi Wang, Long Qin, Xiangyu Meng, Junchao Liao, Zhenghao Zhang | Tora2 is a diffusion transformer framework for generating videos with customized appearance and motion for multiple entities simultaneously. The main objective is to overcome the limitations of existing methods by enabling high-fidelity, simultaneous control over both the appearance (from reference images) and motion (from trajectories) for multiple distinct entities in a single generated video. The methodology involves a decoupled personalization extractor (DPE) that fuses low-frequency semantic features with high-frequency identity features using a Q-Former, a gated self-attention mechanism to bind entity, motion, and text embeddings, and a contrastive loss to enforce cross-modal alignment. The proposed method demonstrates superior control; for instance, the inclusion of a contrastive loss function reduced the Trajectory Error from 17.31 to 14.16 while simultaneously improving identity preservation scores. The principal implication for AI practitioners is that this paper provides a concrete architecture for integrating fine-grained, multi-modal controls (identity, trajectory, text) into large-scale diffusion transformer models, offering a robust pattern for building more precise and complex controllable video generation systems. |
| LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation
  framework (Read more on [arXiv](https://arxiv.org/abs/2507.04723) or [HuggingFace](https://huggingface.co/papers/2507.04723))| Ruoxi Sun, Baibei Ji, Haitian Wang, Zecheng Tang, QQTang1223 | The paper introduces LOOM-Scope, a comprehensive and efficient framework for the standardized evaluation of long-context language models (LCLMs) that integrates benchmarks, models, and inference acceleration techniques. The primary objective is to resolve inconsistencies and high computational costs in existing LCLM evaluation by creating a unified framework that standardizes assessment across diverse benchmarks, model architectures, and efficiency-improving augmentation methods like RAG and inference acceleration. The methodology involves a modular framework with three core components: a BENCHMARK module supporting 22 benchmarks, a DEPLOYMENT module handling various model architectures (e.g., Transformer, Mamba) and optimization techniques (e.g., KV Cache optimization, Sparse Attention), and an EVALUATOR module using diverse metrics. The authors also created LOOMBENCH, a lightweight composite benchmark derived from 12 existing datasets for holistic evaluation. The framework demonstrates significant efficiency gains, with integrated acceleration methods achieving up to a 12x speedup in testing time on 128K-length context tasks compared to a native Transformer implementation (reducing evaluation time from over 200 minutes to under 15 minutes on a 40GB A100 GPU for specific tasks). For AI practitioners, LOOM-Scope provides a tool to conduct fair, reproducible, and computationally efficient evaluations of LCLMs. Its most impactful feature is the direct integration of inference acceleration methods, which enables rigorous testing of models with very long contexts on more accessible hardware and allows for direct comparison of different optimization strategies within a single, standardized environment. |
| Differential Mamba (Read more on [arXiv](https://arxiv.org/abs/2507.06204) or [HuggingFace](https://huggingface.co/papers/2507.06204))| Eliya Nachmani, Itamar Zimerman, Nadav Schneider | This paper introduces Differential Mamba (Diff-Mamba), an architecture applying differential design to Mamba models to reduce overallocation to irrelevant context and enhance performance. The primary objective is to adapt the differential design from Transformers to the Mamba architecture to mitigate noisy representations and improve model robustness. The key methodology involves applying a differential operation across the entire Mamba block, where the output of one parameterized path is subtracted from another (`Mamba₁(X) - λMamba₂(X)`), and using a parallelized implementation to maintain computational efficiency. In experiments, a 12-layer Diff-Mamba model achieved a perplexity of 20.012 on Wikitext-103, outperforming the standard Mamba's 20.413. For AI practitioners, Diff-Mamba offers a more robust alternative to vanilla Mamba for tasks requiring strong long-context performance, as it demonstrably reduces noise in intermediate representations without increasing computational complexity. |
| How to Train Your LLM Web Agent: A Statistical Diagnosis (Read more on [arXiv](https://arxiv.org/abs/2507.04103) or [HuggingFace](https://huggingface.co/papers/2507.04103))| Megh Thakkar, Hadi Nekoei, Emiliano Penaloza, Santhoshi Ravichandran, Dheeraj Vattikonda | This paper presents a statistical analysis to determine the optimal compute allocation between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for post-training open-source LLM web agents. The primary objective is to find a compute-efficient and reproducible training strategy that improves the performance of smaller student models (Llama 3.1 8B) on multi-step web tasks by leveraging demonstrations from a larger teacher model (Llama 3.3 70B). The methodology involves a two-stage pipeline: first, SFT on teacher demonstrations, followed by on-policy RL (GRPO) initiated from various SFT checkpoints, with bootstrap analysis over 1,370 configurations to identify optimal hyperparameters. The key result is that a hybrid SFT+RL approach consistently outperforms pure SFT or RL, matching the peak performance of pure SFT on MiniWob++ while requiring only 55% of the compute (a 45% FLOPs reduction). The principal implication for AI practitioners is that initiating on-policy RL after a moderate SFT warm-up is a more compute-efficient strategy for developing capable open-source web agents than relying solely on extensive SFT. |
| any4: Learned 4-bit Numeric Representation for LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.04610) or [HuggingFace](https://huggingface.co/papers/2507.04610))| Jeff Johnson, melhoushi | This paper introduces any4, a learned 4-bit weight quantization method that creates an optimal, per-row numeric representation for LLMs without requiring weight or activation preprocessing. The primary objective is to develop a 4-bit weight-only quantization scheme that surpasses the accuracy of existing numeric formats (int4, fp4, nf4) and is competitive with orthogonal preprocessing techniques like AWQ and GPTQ. The method applies group-wise scaling to weights and then uses a weighted K-means clustering algorithm to learn a unique 16-value lookup table (LUT) for each matrix row, minimizing output activation error using statistics from a single, curated text sample for calibration. Across Llama, Mistral, and Mixtral models, any4 consistently achieves lower perplexity than other 4-bit numeric formats; for Llama3 70B, any4 achieves a C4 perplexity of 7.01, outperforming nf4 (7.67), fp4 (7.76), and int4 (7.97), and approaching the FP16 baseline of 6.77. For AI practitioners, this means they can quantize LLMs to 4-bits with higher accuracy and a simplified workflow, as it eliminates the need for complex weight/activation preprocessing and large calibration datasets, while the provided `tinygemm` library facilitates low-latency deployment. |
| Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion (Read more on [arXiv](https://arxiv.org/abs/2507.06230) or [HuggingFace](https://huggingface.co/papers/2507.06230))| Christian Rupprecht, Oliver Hahn, Felix Wimbauer, Christoph Reich, jev-aleks | This paper presents SceneDINO, a feed-forward framework for performing unsupervised semantic scene completion (SSC) from a single input image. The primary objective is to infer both the complete 3D geometry and semantic labels of a scene without relying on any manual geometric or semantic ground-truth annotations. SceneDINO's methodology involves training an encoder-decoder via multi-view self-supervision to lift 2D self-supervised learning (SSL) features into a continuous 3D feature field, from which unsupervised semantics are derived through a novel 3D feature distillation approach. On the SSCBench-KITTI-360 benchmark, SceneDINO achieves a semantic mIoU of 8.0% at a 51.2m range, and linear probing of its learned 3D features achieves a 10.57% mIoU, which slightly surpasses a supervised baseline trained with 2D labels (10.19% mIoU). The key implication for AI practitioners is the ability to generate high-quality 3D scene representations from unlabeled monocular videos, bypassing the need for expensive 3D data annotation and providing a strong foundation for various downstream 3D understanding tasks in robotics and autonomous systems. |
| High-Resolution Visual Reasoning via Multi-Turn Grounding-Based
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2507.05920) or [HuggingFace](https://huggingface.co/papers/2507.05920))| Rui Feng, Bo Li, Weiwei Tian, Yuhao Dong, Xinyu Huang | The paper introduces Multi-turn Grounding-based Policy Optimization (MGPO), a reinforcement learning framework to improve high-resolution visual reasoning in Large Multi-modal Models (LMMs). The primary objective is to enable LMMs to overcome challenges with high-resolution images by learning to iteratively identify and focus on relevant sub-regions. The key methodology involves a multi-turn conversational framework where the model first predicts grounding coordinates for a key area, then receives a cropped sub-image based on those coordinates, and finally provides an answer, with the entire process trained via a binary reward signal on the final answer's correctness, eliminating the need for grounding annotations. The primary result is that MGPO post-training on Qwen2.5-VL-7B achieves a 5.2% absolute improvement on the out-of-distribution V* Bench over the GRPO baseline. The principal implication for AI practitioners is that complex, interpretable visual reasoning skills like grounding can be effectively taught to LMMs using only standard visual question-answering data, significantly reducing the cost and effort of data annotation for high-resolution tasks. |
| The Landscape of Memorization in LLMs: Mechanisms, Measurement, and
  Mitigation (Read more on [arXiv](https://arxiv.org/abs/2507.05578) or [HuggingFace](https://huggingface.co/papers/2507.05578))| Dawn Song, Aneesh Pappu, Xuandong Zhao, Alexander Xiong | This paper provides a comprehensive survey on large language model (LLM) memorization, systematically reviewing its underlying mechanisms, detection methodologies, and mitigation strategies. The primary objective is to synthesize the current state of research on LLM memorization by exploring the factors that drive it, the techniques to measure it, and the resulting privacy and legal implications. The methodology is a literature review that organizes the field into a taxonomy covering definitions of memorization, influencing factors (e.g., model size, data duplication), detection attacks (e.g., prefix-based extraction, membership inference), and mitigation approaches (e.g., data cleaning, differential privacy, machine unlearning). Primary results confirm that memorization scales log-linearly with model size and is exacerbated by data duplication, while detection methods like divergence attacks can increase the extraction of verbatim sequences by up to 150x. The principal implication for AI practitioners is that managing the trade-off between model utility and privacy risk is critical, requiring the active integration of mitigation strategies like rigorous data de-duplication and differential privacy into the development lifecycle to prevent unintended leakage of sensitive or copyrighted data. |
| FAROS: Fair Graph Generation via Attribute Switching Mechanisms (Read more on [arXiv](https://arxiv.org/abs/2507.03728) or [HuggingFace](https://huggingface.co/papers/2507.03728))| Fragkiskos D. Malliaros, Daniele Malitesta, Hatim Mrabet, Oussama Kharouiche, badaoui | FAROS is a framework that improves fairness in graphs generated by pre-trained Graph Diffusion Models (GDMs) by applying an attribute switching mechanism during the generation process. The primary objective is to mitigate fairness discrepancies in generated graph data for downstream tasks like link prediction, without needing to re-train the GDM. The core methodology involves intervening in the GDM's generation process by calculating an optimal fraction of nodes and an optimal diffusion timestep to switch their sensitive attributes, using a multi-criteria optimization that balances node-topology preservation (via Fused Gromov-Wasserstein distance) and edge-attribute independence (via entropy). On the CORA dataset, FAROS-Prior reduced the fairness discrepancy ΔEO from 14.45±0.77 to 4.30±4.03 while maintaining comparable accuracy (AUC of 89.08±2.72 vs. 89.39±0.92), achieving a better accuracy-fairness trade-off under Pareto optimality. AI practitioners can use FAROS as a post-hoc module to generate fairer synthetic graph data from existing GDMs without the computational cost of re-training, making it valuable for fairness-critical applications. |
| AXLearn: Modular Large Model Training on Heterogeneous Infrastructure (Read more on [arXiv](https://arxiv.org/abs/2507.05411) or [HuggingFace](https://huggingface.co/papers/2507.05411))| Hanzhi Zhou, John Peebles, Chang Lan, Tom Gunter, Mark Lee | The paper presents AXLearn, a deep learning system for training large models that prioritizes modularity and support for heterogeneous hardware through strict component encapsulation. The primary objective is to design a production-grade training framework that enables rapid experimentation on diverse model architectures and can be deployed across various hardware backends (e.g., GPU, TPU, AWS Trainium) with minimal code changes. Methodologically, AXLearn is built on JAX/XLA and uses a hierarchical configuration system based on composition rather than inheritance, with system extensibility formally analyzed using a proposed "Lines-of-Code (LoC)-complexity" metric. The framework achieves constant (O(1)) LoC-complexity, allowing a feature like Rotary Position Embeddings (RoPE) to be integrated across hundreds of modules with just 10 lines of code, versus hundreds required in other systems, while maintaining state-of-the-art training performance (e.g., 54.2% MFU for Llama2-7B on 32 H100 GPUs). For AI practitioners, AXLearn's design significantly reduces engineering overhead by decoupling model logic from system-level concerns like parallelism and hardware-specific optimizations, allowing for faster development and easier migration of training workloads across different infrastructures. |
