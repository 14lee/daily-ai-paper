

## Papers for 2025-07-22

| Title | Authors | Summary |
|-------|---------|---------|
| MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via
  Context-Aware Multi-Stage Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.14683) or [HuggingFace](https://huggingface.co/papers/2507.14683))| Yao Xiao, LidongBing, ZonglinY, binwang, veggiebird | This paper introduces MiroMind-M1, a fully open-source series of mathematical reasoning models, and CAMPO, a novel reinforcement learning algorithm that enhances performance and token efficiency. The primary objective is to develop a transparent and reproducible pipeline for creating high-performance mathematical reasoning language models by open-sourcing the models, curated datasets, and the complete training framework. The methodology is a two-stage process: first, supervised fine-tuning (SFT) on a 719K curated dataset of math problems with verified chain-of-thought, followed by reinforcement learning using the novel Context-Aware Multi-Stage Policy Optimization (CAMPO) algorithm, which integrates length-progressive training with an adaptive repetition penalty. The primary result is that the MiroMind-M1-RL-7B model improves upon its SFT-only counterpart by 13 absolute points on the AIME24 benchmark (from 60.4 to 73.4 avg@64), and the models demonstrate superior token efficiency compared to baselines. The principal implication for AI practitioners is that the fully released stack—including models, datasets, and the CAMPO algorithm—provides a concrete, reproducible methodology for fine-tuning language models for complex reasoning tasks, offering a practical approach to improve both accuracy and computational efficiency. |
| GUI-G^2: Gaussian Reward Modeling for GUI Grounding (Read more on [arXiv](https://arxiv.org/abs/2507.15846) or [HuggingFace](https://huggingface.co/papers/2507.15846))| Xuyang Liu, Zhangxuan Gu, Fei Tang, tricktreat, LZXzju | This paper introduces GUI-G², a reward modeling framework that replaces sparse binary rewards with continuous Gaussian distributions for GUI grounding tasks in reinforcement learning. The primary objective is to create a dense, geometrically-aware reward signal that models the continuous nature of spatial interactions, addressing the inefficiency of discrete hit-or-miss feedback. The methodology models GUI elements as 2D Gaussian distributions and computes a dual reward: a Gaussian point reward for localization precision and a Gaussian coverage reward for spatial overlap, combined with an adaptive variance mechanism to handle different element sizes. The proposed GUI-G²-7B model achieves state-of-the-art results, including a 24.7% accuracy improvement over the UI-TARS-72B model on the ScreenSpot-Pro benchmark and 93.3% accuracy on ScreenSpot-v2. The principal implication for AI practitioners is that using this continuous, dual-component Gaussian reward function can lead to more efficient training and superior performance for GUI agents, enabling smaller models to outperform larger ones by providing richer gradient signals for spatial optimization. |
| The Invisible Leash: Why RLVR May Not Escape Its Origin (Read more on [arXiv](https://arxiv.org/abs/2507.14843) or [HuggingFace](https://huggingface.co/papers/2507.14843))| Yejin Choi, Zaid Harchaoui, Ximing Lu, Fang Wu, weihao1115 | This paper theoretically and empirically demonstrates that Reinforcement Learning with Verifiable Rewards (RLVR) primarily sharpens a base model's existing knowledge rather than discovering new reasoning paths, acting as a conservative reweighting mechanism constrained by the initial model's support.  The main research question is whether RLVR fundamentally expands a large language model's reasoning capabilities or merely amplifies high-reward outputs already within the base model's support, potentially at the cost of solution diversity.  The study employs a theoretical analysis using support-preservation theorems and a variational inference perspective to formalize RLVR's limits. This is validated empirically by analyzing "empirical support dynamics" (preservation, shrinkage, expansion) and entropy changes (token-level vs. answer-level) across various math and non-math reasoning benchmarks.  The primary results show that while RLVR consistently improves `pass@1` accuracy, empirical support shrinkage generally outweighs expansion. For instance, across the Minerva and OlympiadBench datasets combined, the RLVR model discovered only 3 new correct solutions while losing access to 48 solutions that were discoverable by the base model. Furthermore, answer-level entropy consistently decreases, indicating convergence to a smaller set of final answers, even when token-level uncertainty increases.  The principal implication for AI practitioners is that RLVR should not be expected to spontaneously discover reasoning abilities beyond the base model's initial representational capacity. To achieve true capability expansion, RLVR pipelines must be augmented with explicit exploration mechanisms or off-policy data to seed probability mass into underrepresented solution regions. |
| WebShaper: Agentically Data Synthesizing via Information-Seeking
  Formalization (Read more on [arXiv](https://arxiv.org/abs/2507.15061) or [HuggingFace](https://huggingface.co/papers/2507.15061))| Baixuan Li, Junkai Zhang, Wenbiao Yin, Jialong Wu, Zhengwei Tao | WebShaper is a formalization-driven framework that agentically synthesizes high-quality training data for information-seeking (IS) agents. Its primary objective is to overcome data scarcity and inconsistency by creating a systematic, controllable data synthesis method that avoids the limitations of traditional information-driven approaches. The key methodology involves formalizing IS tasks using set-theoretic "Knowledge Projections" (KP) and employing an agentic "Expander" that iteratively complicates seed questions via a layer-wise expansion strategy to ensure structural complexity. The method achieves state-of-the-art performance among open-source models, with the WebShaper-72B model scoring 60.1% Pass@1 on the GAIA benchmark. For AI practitioners, this framework provides a principled way to generate diverse and complex training data, enabling the development of agents with more robust and advanced multi-hop reasoning capabilities. |
| Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with
  Regularized Score Distillation Sampling (Read more on [arXiv](https://arxiv.org/abs/2507.11061) or [HuggingFace](https://huggingface.co/papers/2507.11061))| Se Young Chun, jeeit17, yeonE | The paper introduces RoMaP, a framework for robust, part-level editing of 3D Gaussian Splatting scenes that enables precise and drastic modifications via geometry-aware masking and a regularized score distillation sampling loss. The primary objective is to overcome the inability of existing methods to perform localized and drastic part-level edits on 3D Gaussian Splatting representations, which is caused by inconsistent segmentation and restrictive diffusion model priors. The methodology integrates a 3D-Geometry Aware Label Prediction (3D-GALP) module for generating consistent 3D part masks and a regularized Score Distillation Sampling (SDS) loss guided by a novel Scheduled Latent Mixing and Part (SLaMP) 2D editing technique. Experimental results show RoMaP significantly outperforms state-of-the-art methods, achieving a CLIP directional similarity score of 0.205, more than doubling the 0.095 score of the next-best baseline, and a B-VQA score of 0.723 versus the baseline's 0.497. For AI practitioners, RoMaP provides a powerful tool for fine-grained, text-guided control over specific parts of 3D assets, enabling complex and unconventional edits for applications in virtual reality, gaming, and digital asset creation. |
| SeC: Advancing Complex Video Object Segmentation via Progressive Concept
  Construction (Read more on [arXiv](https://arxiv.org/abs/2507.15852) or [HuggingFace](https://huggingface.co/papers/2507.15852))| Jianfan Lin, Songxin He, Xiaoyi Dong, Shuangrui Ding, rookiexiong | The paper introduces Segment Concept (SeC), a framework that advances Video Object Segmentation by using Large Vision-Language Models (LVLMs) for progressive, concept-level object representation. The objective is to overcome the limitations of appearance-based VOS models by developing a system that constructs a high-level, object-centric concept to maintain tracking through drastic visual variations and scene changes. SeC's methodology involves employing an LVLM to integrate visual cues from a dynamically updated bank of keyframes, and a scene-adaptive activation strategy selectively fuses this conceptual guidance with pixel-level memory features only during significant scene changes. On the newly introduced SeCVOS benchmark, designed to test high-level reasoning, SeC achieves an 11.8-point J&F score improvement over the SAM 2.1 baseline. The principal implication for AI practitioners is that integrating high-level semantic reasoning from LVLMs with traditional pixel-level feature matching provides a robust and computationally efficient mechanism to handle complex, multi-shot video scenarios where object appearance and context change drastically. |
| GR-3 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.15493) or [HuggingFace](https://huggingface.co/papers/2507.15493))| Yingdong Hu, Zhongren Cui, Chilam Cheang, melony, CH3COOK | This paper details GR-3, a 4B parameter vision-language-action (VLA) model for generalist robot control. The research objective is to create a robot policy that generalizes to novel objects and abstract instructions, learns efficiently from minimal data, and robustly performs long-horizon, dexterous tasks. The core methodology involves co-training a pre-trained vision-language model on both web-scale vision-language data and robot trajectories using a flow-matching objective, which is further fine-tuned with small amounts of human trajectory data from VR. Key results demonstrate that with only 10 human trajectories per object, GR-3 boosts its success rate on unseen objects from 57.8% to 86.7% and achieves a 97.5% success rate on a complex, long-horizon table bussing task. The principal implication for practitioners is that this multi-faceted training recipe offers a data-efficient and cost-effective strategy for developing and adapting generalist robot policies for novel, real-world applications, reducing the dependency on large-scale, robot-specific data collection. |
| Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for
  RLVR (Read more on [arXiv](https://arxiv.org/abs/2507.15778) or [HuggingFace](https://huggingface.co/papers/2507.15778))| Guorui Zhou, Xiu Li, Fuzheng Zhang, Jiakang Wang, RyanLiu112 | The paper introduces Archer, an RLVR method that improves LLM reasoning by applying differentiated, synchronous optimization constraints to knowledge-related and reasoning-related tokens identified via response-level entropy. The primary objective is to improve Reinforcement Learning with Verifiable Rewards (RLVR) by treating tokens differently based on their function (knowledge vs. reasoning) without disrupting semantic dependencies, thereby stabilizing factual knowledge while promoting reasoning exploration. The key methodology involves classifying tokens within each response as either "knowledge-related" (low-entropy) or "reasoning-related" (high-entropy) using a response-level entropy quantile threshold, then applying stronger KL regularization and a lower clipping threshold to knowledge tokens and weaker KL regularization with a higher clipping threshold to reasoning tokens during synchronous policy updates. Archer outperforms existing 1.5B-level models, with Archer-Math-1.5B achieving a 48.7% avg@64 accuracy on AIME24, a notable improvement over the 42.1% from the DAPO baseline. For AI practitioners, the principal implication is that during RL fine-tuning for reasoning, implementing a dual-constraint system to moderately update knowledge-centric tokens while aggressively updating reasoning-centric tokens is more effective for balancing stability and performance than applying uniform updates, masking tokens, or using asynchronous methods. |
| Being-H0: Vision-Language-Action Pretraining from Large-Scale Human
  Videos (Read more on [arXiv](https://arxiv.org/abs/2507.15597) or [HuggingFace](https://huggingface.co/papers/2507.15597))| Sipeng Zheng, Yicheng Feng, Hao Luo, Yaya041, zawnpn | This paper presents Being-H0, a Vision-Language-Action (VLA) model that learns dexterous manipulation skills by pretraining on a large-scale human video dataset (UniHand) for transfer to robotic systems. The research objective is to investigate if a VLA can be pretrained on large-scale human videos to explicitly imitate human actions and then be adapted to control robot hands, thus overcoming the data bottleneck of teleoperated demonstrations. The key methodology is "physical instruction tuning," a paradigm comprising VLA pretraining on the curated UniHand dataset, physical space alignment to unify heterogeneous video sources, and a part-level motion tokenization technique using Grouped Residual Quantization (GRQ) to discretize hand motions with millimeter-level precision. In real-world dexterous manipulation, Being-H0 achieved a 100% success rate on the "Pour-Cup" task, significantly outperforming a baseline without human-video pretraining (55% success rate), and demonstrated high data efficiency by matching the baseline's performance on the "Close-Toolbox" task while using only 50% of the teleoperation data. The principal implication for AI practitioners is that pretraining on large-scale human videos with explicit motion modeling offers a highly sample-efficient pathway for developing capable dexterous robot policies, substantially reducing the need for costly and time-consuming collection of real-world robot demonstration data. |
| Gaussian Splatting with Discretized SDF for Relightable Assets (Read more on [arXiv](https://arxiv.org/abs/2507.15629) or [HuggingFace](https://huggingface.co/papers/2507.15629))| Beibei Wang, Jian Yang, Zuo-Liang Zhu | This paper introduces a discretized Signed Distance Field (SDF) integrated directly into 3D Gaussian primitives to regularize geometry for high-quality, relightable asset creation. The main objective is to effectively regularize the geometry of 3D Gaussian Splatting for inverse rendering, improving the decomposition of material and lighting without incurring the high memory and computational costs of using an auxiliary continuous SDF network. The key methodology involves encoding a discrete SDF value as an attribute within each Gaussian, which is then linked to opacity via a learnable SDF-to-opacity transformation; geometric consistency is enforced using a novel projection-based consistency loss that aligns projected Gaussians with the rendered surface depth, approximating the Eikonal constraint. On the Glossy Blender dataset, the proposed method achieves a state-of-the-art mean PSNR of 24.52, outperforming the next-best method (23.39), while requiring significantly less memory (4G vs. 22G). The principal implication for AI practitioners is the ability to create high-fidelity, relightable 3D assets with a more memory-efficient and simpler framework, as this approach unifies the SDF and Gaussian representations and eliminates the need for a separate, resource-intensive neural network for geometric regularization. |
| NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining (Read more on [arXiv](https://arxiv.org/abs/2507.14119) or [HuggingFace](https://huggingface.co/papers/2507.14119))| Bulat Suleimanov, Georgii Fedorov, Grigorii Alekseenko, Maksim Kuprashevich, iitolstykh | This paper presents a fully automated pipeline for mining high-quality image editing triplets without human intervention, yielding a new state-of-the-art dataset and model. The main objective is to develop a scalable, autonomous framework to generate high-fidelity training data (original image, instruction, edited image) for instruction-based image editing, overcoming the manual annotation bottleneck. The methodology uses a T2I model (FLUX.1-schnell) to generate source images and an LLM (OpenAI o3) to create edit instructions, then applies edits using a base editor. These candidates are filtered by a coarse model (Qwen-72B) and then rigorously validated by a task-specific, fine-tuned Gemini 2.0 Flash validator that scores instruction adherence and aesthetics. The dataset is augmented via semantic inversion and compositional bootstrapping. The primary result is the creation of the NHR-Edit dataset (358k triplets), which achieves a geometric mean quality score of 4.53, significantly outperforming the next-best public dataset (OmniEdit at 4.23). A model fine-tuned on this data, Bagel-NHR-Edit, demonstrates improved performance on public benchmarks. The principal implication for AI practitioners is the availability of a framework and a large-scale, high-quality dataset (NHR-Edit) for training more capable instruction-guided image editors. The automated pipeline allows for continuous model improvement and targeted weakness correction without the cost and time of human labeling. |
| Towards Video Thinking Test: A Holistic Benchmark for Advanced Video
  Reasoning and Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.15028) or [HuggingFace](https://huggingface.co/papers/2507.15028))| Bo Hu, Aria Leo, Yuhao Dong, yunicechew, ZhangYuanhan | This paper introduces the Video Thinking Test (Video-TT), a benchmark designed to evaluate video LLMs on complex visual narrative understanding and robustness against natural adversarial questions. The main objective is to assess the genuine gap between video LLM and human performance by creating questions that test comprehension rather than frame sampling capabilities. The methodology involves a dataset of 1,000 videos, each with one primary open-ended question and four adversarial variants (rephrased, correctly-led, wrongly-led, multi-choice) designed around eight specific visual and narrative complexity factors, ensuring all are answerable from 80 sampled frames. Results show a significant performance deficit, with the top-performing model (GPT-4o) achieving 36.6% accuracy and 36.0% robustness, far below the human baseline of 84.3% accuracy and 64.3% robustness. The principal implication for AI practitioners is that current video LLMs have fundamental weaknesses in spatial-temporal reasoning, world knowledge integration, and linking disparate video events into a coherent narrative, indicating that future work must address these core reasoning and comprehension failures. |
| Inverse Scaling in Test-Time Compute (Read more on [arXiv](https://arxiv.org/abs/2507.14417) or [HuggingFace](https://huggingface.co/papers/2507.14417))| Jacob Goldman-Wetzler, Andy Arditi, Runjin Chen, Alexander Hägele, Aryo Pradipta Gema | This research demonstrates that increasing test-time compute for Large Reasoning Models (LRMs) can paradoxically degrade performance, a phenomenon termed "inverse scaling in test-time compute." The study's objective is to construct and analyze evaluation tasks where LRM performance deteriorates with extended reasoning, in order to identify and categorize the underlying failure modes. The authors developed a suite of novel tasks spanning counting with distractors, regression with spurious features, and deduction with constraints, evaluating models like the Claude and OpenAI o-series under "controlled" and "natural" overthinking setups where reasoning length is systematically varied. The study identifies five failure modes, including increased distractibility and amplification of spurious correlations; for instance, on the "Survival Instinct" task, increasing the reasoning budget caused Claude Sonnet 4's expression of safety-aligned willingness to be turned off to drop from 60% to 47%. Practitioners must recognize that naively scaling test-time compute is not a universally reliable method for improving model performance and can amplify latent, problematic reasoning patterns; therefore, evaluation protocols must stress-test models across a full spectrum of reasoning lengths to ensure robustness and safety. |
| STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for
  Spoken Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.15375) or [HuggingFace](https://huggingface.co/papers/2507.15375))| Kevin Lin, Chung-Ching Lin, Linjie Li, xiaofei-wang, dcml0714 | The paper introduces STITCH, a method for Spoken Language Models to perform chunked reasoning concurrently with speech generation, improving reasoning ability without increasing response latency. The main research objective is to enable Spoken Language Models (SLMs) to perform an internal, unspoken reasoning process to improve response quality on complex tasks without incurring the significant latency of generating a full chain-of-thought before speaking. The key methodology, STITCH, interleaves the generation of unspoken reasoning token chunks with spoken response chunks. It utilizes the audio playback duration of a spoken chunk to compute the subsequent reasoning chunk, thereby achieving simultaneous thinking and talking. A variant, STITCH-S, begins with a spoken response chunk to eliminate any initial reasoning-induced latency. The primary result is that the STITCH-S model matches the initial response latency of baselines that lack reasoning capabilities, while outperforming them by over 15% on math reasoning datasets (from 62.98% to 78.04% average accuracy). This performance is achieved with only a ~1% accuracy drop compared to a high-latency model that generates the full reasoning trace before speaking. The principal implication for AI practitioners is that they can enhance the reasoning abilities of real-time conversational agents by fine-tuning SLMs on an interleaved reasoning-speech generation format, effectively hiding the computational latency of thinking within the time the user spends listening to the audio response. |
| Streaming 4D Visual Geometry Transformer (Read more on [arXiv](https://arxiv.org/abs/2507.11539) or [HuggingFace](https://huggingface.co/papers/2507.11539))| Jie Zhou, Yuqi Wu, Wenzhao Zheng, lch01, paryi | StreamVGGT is a causal transformer architecture designed for efficient, real-time 4D visual geometry reconstruction from streaming video inputs. The primary objective is to overcome the high latency of offline models by processing video frame-by-frame, enabling on-the-fly scene updates for interactive applications. The key methodology involves replacing global self-attention with temporal causal attention and using a cached token memory to incrementally integrate historical information during inference, while knowledge distillation from a powerful offline teacher model (VGGT) is used during training to mitigate error accumulation. The model achieves a 31x inference speedup, processing the final frame of a 40-frame sequence in 67 ms compared to 2089 ms for the offline VGGT, while maintaining competitive reconstruction accuracy. For AI practitioners, this work provides a practical architecture for converting large, offline vision transformers into efficient streaming models suitable for low-latency applications like robotics and AR/VR, demonstrating that causal attention with key-value caching can achieve real-time performance with minimal accuracy degradation. |
| Latent Denoising Makes Good Visual Tokenizers (Read more on [arXiv](https://arxiv.org/abs/2507.15856) or [HuggingFace](https://huggingface.co/papers/2507.15856))| Yue Wang, Yonglong Tian, Lijie Fan, Tianhong Li, Jiawei Yang | This paper introduces the Latent Denoising Tokenizer (l-DeTok) to determine properties that make visual tokenizers more effective by aligning their training with the denoising objectives of downstream generative models. The key methodology trains a Vision Transformer-based autoencoder to reconstruct clean images from latent embeddings that are intentionally corrupted using interpolative Gaussian noise and random patch masking. On ImageNet 256x256, l-DeTok demonstrates significant and generalizable performance gains across six different generative models, improving the FID score for the MAR-B model from 2.31 to 1.55. The principal implication for AI practitioners is that explicitly incorporating a latent denoising objective into tokenizer training is a highly effective, task-aligned strategy to improve generative performance without modifying the generator architecture or relying on semantic distillation from large external models. |
| LLM Economist: Large Population Models and Mechanism Design in
  Multi-Agent Generative Simulacra (Read more on [arXiv](https://arxiv.org/abs/2507.15815) or [HuggingFace](https://huggingface.co/papers/2507.15815))| Yu Bai, Samuel Kleiner, Zihan Ding, Wenzhe Li, milkkarten | The paper presents LLM Economist, a hierarchical agent-based framework where LLM agents, guided by in-context reinforcement learning, simulate and design economic tax policies. The main objective is to determine if a multi-agent system, operating purely through natural language, can effectively model, simulate, and optimize a complex economic mechanism like taxation by framing it as a two-level Stackelberg game. The methodology involves a hierarchical simulation where a "planner" agent uses in-context reinforcement learning (ICRL) to propose tax schedules, while a population of "worker" agents, with personas calibrated to U.S. Census data, optimize their individual text-based utility functions by choosing labor supply. The framework's planner agent designed policies that significantly improved social welfare (SWF) over baselines; in a seven-bracket simulation, the LLM policy increased SWF by 93% over the U.S. federal schedule, approaching the 114% gain from an analytically-informed, perturbed Saez solution. The principal implication for AI practitioners is that LLMs can function as powerful zero-shot optimizers for complex mechanism design in multi-agent systems directly via ICRL, providing an interpretable, language-driven alternative to traditional deep RL for building and auditing sophisticated socio-technical systems. |
| Data Mixing Agent: Learning to Re-weight Domains for Continual
  Pre-training (Read more on [arXiv](https://arxiv.org/abs/2507.15640) or [HuggingFace](https://huggingface.co/papers/2507.15640))| Yeyun Gong, Hao Li, Lei Ji, lx865712528, klyang | This paper introduces the Data Mixing Agent, a model-based framework that learns to dynamically re-weight data domains for the continual pre-training of large language models. The main objective is to automate the process of finding an optimal data mixing strategy to improve performance on a target task while mitigating catastrophic forgetting of source capabilities. The methodology involves framing domain re-weighting as a Markov Decision Process (MDP) and training a lightweight Transformer-based agent using offline reinforcement learning (Conservative Q-Learning) on data from thousands of sampled trajectories and their performance feedback. The Data Mixing Agent significantly outperformed the RegMix baseline, achieving an average improvement of 3.02% across 12 general and math benchmarks on a LLaMA-3B model, and demonstrated generalization across unseen models and domains without retraining. The principal implication for AI practitioners is the availability of an automated, model-based approach to efficiently guide continual pre-training, replacing resource-intensive manual tuning or heuristic-based methods to achieve a better performance balance. |
| "PhyWorldBench": A Comprehensive Evaluation of Physical Realism in
  Text-to-Video Models (Read more on [arXiv](https://arxiv.org/abs/2507.13428) or [HuggingFace](https://huggingface.co/papers/2507.13428))| Fangrui Zhu, Ashwin Nagarajan, Yu Zeng, Xian Liu, Jing Gu | The paper introduces PhyWorldBench, a comprehensive benchmark to evaluate the physical realism of text-to-video models, revealing significant limitations in their ability to simulate physics.  The main research objective is to systematically assess the adherence of text-to-video generation models to physical laws and identify their core failure modes in simulating physical phenomena.  The key methodology involves the creation of PhyWorldBench, a benchmark with 1,050 structured prompts across 10 fundamental, composite, and "Anti-Physics" categories. The authors evaluated 12 state-of-the-art models by generating 12,600 videos, assessing them via human evaluation and a proposed MLLM-based method called Context-Aware Prompt (CAP) on metrics of Semantic Adherence (SA) and Physical Commonsense (PC).  The primary result is that even top-performing models struggle significantly with physical realism; Pika 2.0, the best-performing model, achieved an overall success rate (satisfying both SA and PC) of only 0.262. Models demonstrated a notable inability to follow "Anti-Physics" prompts, indicating a tendency to reproduce learned real-world patterns rather than adhere to explicit instructions that violate them.  The principal implication for AI practitioners is that current text-to-video models lack a robust understanding of physics, and improving this requires more than just scaling or detailed narrative prompts. Explicitly integrating physical phenomena into prompts is a more effective strategy for improving physical accuracy, guiding prompt engineering efforts for more realistic video generation. |
| A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.14295) or [HuggingFace](https://huggingface.co/papers/2507.14295))| Yiping Lu, Chenwei Xu, Linjie Li, Zihan Wang, Licheng Liu | This paper introduces Unary Feedback as Observation (UFO), a multi-turn reinforcement learning framework that uses minimal "try again" feedback to improve an LLM's iterative reasoning and prevent repetitive responses. The primary objective is to determine if Large Reasoning Models (LRMs) can learn to reflect on and revise their answers in a multi-turn context using only minimal, unary feedback, thereby overcoming the repetitive behavior induced by single-turn RL training. The key methodology is Unary Feedback as Observation (UFO), which formulates multi-turn problem-solving as a Markov Decision Process (MDP) where an incorrect model response results in only a generic negative observation (e.g., "Try Again") being added to the context history. The model is trained using Proximal Policy Optimization (PPO) with specialized reward structures, including turn-wise reward decay and an answer repetition penalty, to encourage both efficiency and reasoning diversity. Experimental results demonstrate that training with UFO improves multi-turn reasoning, achieving up to a 14% absolute increase in success rate (Succ@5) on the MMQ-Math dataset compared to a single-turn RL baseline. This approach also shows strong generalization, improving the 5-turn success rate on the out-of-domain MMLU-Pro benchmark from 48.3% (standard RL) to 60.9%. The principal implication for AI practitioners is that they can enhance the interactive problem-solving capabilities of their models by augmenting existing single-turn RL pipelines with the UFO framework. This is a lightweight method that can be applied to static datasets without needing complex, annotated multi-turn feedback, providing a practical way to mitigate the common failure mode where models repetitively generate the same incorrect answer. |
| GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised
  Cross-View Localization (Read more on [arXiv](https://arxiv.org/abs/2507.10935) or [HuggingFace](https://huggingface.co/papers/2507.10935))| Yujiao Shi, Xuming He, Alexandre Alahi, Zimin Xia, tsw200027 | GeoDistill is a weakly supervised self-distillation framework that improves cross-view localization by using geometry-guided, Field-of-View (FoV)-based masking to learn robust local features. The primary objective is to enhance localization performance and generalization without requiring costly, precise ground-truth pose annotations. The methodology employs a teacher-student architecture where the student model processes a randomly masked, limited FoV image and is trained to match the output of the teacher model which processes the full panoramic image; the teacher is then progressively refined using an Exponential Moving Average of the student's weights. The method significantly improves performance, reducing the mean localization error on the VIGOR Cross-Area benchmark by 25.1% for the G2SWeakly(DINO) model and outperforming the fully supervised state-of-the-art with a 2.68m mean error. For AI practitioners, the principal implication is a scalable, plug-and-play paradigm to enhance localization models using only weakly supervised data, reducing dependency on expensive, precisely annotated datasets for applications like autonomous navigation. |
| UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based
  Classification in Computed Tomography (Read more on [arXiv](https://arxiv.org/abs/2507.14102) or [HuggingFace](https://huggingface.co/papers/2507.14102))| Chandrakala S, Rakesh Raj Madavan, Pavan Kumar S, Shravan Venkatraman | The paper introduces UGPL, a framework that improves CT image classification by performing a global analysis to identify uncertain regions and then a focused local analysis on those regions. The objective is to develop a classification framework that improves performance on medical images by mimicking the diagnostic process of global examination followed by focused analysis on ambiguous areas, thereby overcoming the limitations of uniform image processing. The methodology employs evidential deep learning to generate a pixel-wise uncertainty map from a global model, which guides a non-maximum suppression algorithm to extract informative patches for a local refinement network; an adaptive fusion module then combines the global and local predictions. UGPL consistently outperforms state-of-the-art models on three CT datasets, and an ablation study demonstrates the criticality of its core mechanism, showing that uncertainty-guided patch selection yields up to a 5.3x F1 score improvement on the COVID-19 detection task compared to configurations without it. For AI tasks with localized features, especially in medical imaging, practitioners can improve model performance by adopting a progressive, uncertainty-guided pipeline that dynamically allocates computational resources, rather than relying on uniform-processing, single-pass architectures. |
