

## Papers for 2025-07-04

| Title | Authors | Summary |
|-------|---------|---------|
| WebSailor: Navigating Super-human Reasoning for Web Agent (Read more on [arXiv](https://arxiv.org/abs/2507.02592) or [HuggingFace](https://huggingface.co/papers/2507.02592))| Liwen Zhang, Huifeng Yin, Zhongwang Zhang, Kuan Li, xxwu | This paper presents WebSailor, a post-training methodology for LLMs to create web agents with superhuman reasoning for complex information-seeking tasks. The research objective is to instill in open-source models the ability to systematically reduce extreme uncertainty, closing the capability gap with proprietary agents. The core methodology involves generating high-uncertainty tasks (SailorFog-QA) through structured sampling and information obfuscation, followed by a two-stage training process: a Rejection Sampling Fine-Tuning (RFT) cold start and an efficient agentic RL algorithm, Duplicating Sampling Policy Optimization (DUPO). The primary result shows WebSailor-72B achieving 12.0% on BrowseComp-en and 30.1% on BrowseComp-zh, significantly outperforming all open-source counterparts and matching proprietary agent performance. The principal implication for AI practitioners is that sophisticated agentic reasoning can be instilled in open-source models not just through scale, but via a targeted pipeline of synthetic high-uncertainty data generation and a combined RFT-RL training strategy, providing a clear path to developing highly capable web agents. |
| LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with
  TriMap Video Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.02813) or [HuggingFace](https://huggingface.co/papers/2507.02813))| Minghui Yang, Jiawei Chi, Hao Li, Fangfu Liu, hanyang-21 | LangScene-X is a generative framework that reconstructs generalizable, language-queriable 3D scenes from sparse 2D views using a novel video diffusion model. The research objective is to overcome the dense-view requirements of existing methods by developing a system that generates high-fidelity, open-vocabulary 3D scenes from as few as two input images. Its methodology combines a TriMap video diffusion model, trained with progressive knowledge integration to generate consistent RGB, normal, and semantic maps, with a generalizable Language Quantized Compressor (LQC) that efficiently encodes language features without per-scene retraining. The system demonstrates state-of-the-art performance, achieving a 50.52% mean Intersection over Union (mIoU) for 2D semantic segmentation on the LERF-OVS dataset, significantly outperforming the next-best method's 39.94% mIoU. The principal implication for AI practitioners is the validation of a generative paradigm where a video diffusion model serves as a powerful prior to synthesize consistent, multi-modal 3D data from sparse inputs, enabling more scalable and robust 3D understanding systems. |
| IntFold: A Controllable Foundation Model for General and Specialized
  Biomolecular Structure Prediction (Read more on [arXiv](https://arxiv.org/abs/2507.02025) or [HuggingFace](https://huggingface.co/papers/2507.02025))| He Yan, Wayne Bai, Leon Qiao, The IntFold Team, FuxuLiu | This paper introduces IntFold, a controllable foundation model for general and specialized biomolecular structure prediction that achieves accuracy comparable to state-of-the-art methods. The research objective is to create a highly accurate structure prediction model that is also adaptable for specialized tasks, such as modeling allosteric states or applying user-defined constraints, through user-driven control. The methodology utilizes a diffusion-based architecture with a custom `FlashAttentionPairBias` kernel, while achieving controllability by inserting lightweight, trainable LoRA adapters into a frozen base model. IntFold demonstrates performance comparable to AlphaFold 3 on the FoldBench benchmark, and its guided folding capability for antibody-antigen interfaces improves prediction success rate from 37.6% to 69.0% when structural constraints are provided. For AI practitioners, the principal implication is the demonstration that modular adapters can efficiently specialize a large-scale foundation model for domain-specific tasks without full retraining, while also providing practical insights on mitigating training instabilities like activation explosion in deep transformer architectures. |
| Heeding the Inner Voice: Aligning ControlNet Training via Intermediate
  Features Feedback (Read more on [arXiv](https://arxiv.org/abs/2507.02321) or [HuggingFace](https://huggingface.co/papers/2507.02321))| Aibek Alanov, Andrey Kuznetsov, Maxim Nikolaev, Nina Konovalova | This paper introduces InnerControl, a training strategy to improve spatial control in diffusion models by enforcing consistency between control signals and intermediate U-Net features throughout the entire denoising process. The objective is to overcome the limitations of prior methods like ControlNet++, which only apply consistency losses during the final denoising steps, leading to misalignment when structure is formed early in generation. The core methodology involves training lightweight, timestep-conditioned convolutional probes to predict control signals (e.g., depth, edges) from intermediate U-Net decoder features at every denoising step, enabling a persistent alignment loss. The primary result shows significant improvement in control fidelity, reducing the RMSE for depth map generation by 7.87% (from 28.32 to 26.09) compared to ControlNet++ at a 7.5 guidance scale. The principal implication for AI practitioners is that integrating this intermediate feature feedback mechanism provides a more robust method to train controllable generation models, yielding higher alignment and better image quality, particularly for tasks requiring precise spatial conditioning. |
| Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy (Read more on [arXiv](https://arxiv.org/abs/2507.01352) or [HuggingFace](https://huggingface.co/papers/2507.01352))| Jiacai Liu, Jujie He, RickyShaw999, zengliangcs, chrisliu298 | This paper introduces Skywork-Reward-V2, a series of state-of-the-art reward models trained on a new 40M-pair preference dataset, SynPref-40M, curated via a human-AI synergy pipeline. The objective is to overcome the performance limitations of existing open reward models by developing a scalable methodology for creating high-quality, large-scale preference data. The key methodology is a two-stage pipeline that combines small-scale, iterative human-in-the-loop verification with large-scale, automated data filtering based on reward model prediction consistency. The resulting Skywork-Reward-V2-Llama-3.1-8B-40M model achieves state-of-the-art performance, with an average score of 88.6% across seven major benchmarks, outperforming all previous open models. The principal implication for AI practitioners is that meticulous, human-guided data curation is more critical for building high-performing reward models than simply increasing data scale or model size alone. |
| Thinking with Images for Multimodal Reasoning: Foundations, Methods, and
  Future Frontiers (Read more on [arXiv](https://arxiv.org/abs/2506.23918) or [HuggingFace](https://huggingface.co/papers/2506.23918))| Zhenhua Liu, Hangyu Guo, Peng Xia, Zhaochen Su, Xiaoye08 | This survey introduces the "Thinking with Images" paradigm, where Large Multimodal Models (LMMs) leverage visual information as a dynamic, intermediate step in their cognitive process rather than as a static input. The paper's objective is to chart the evolution from models that 'think about' images to those that can 'think with' them, defining the foundational methods, evaluations, and challenges of this new paradigm. The paper proposes a conceptual framework that charts this evolution through three stages of increasing cognitive autonomy: Tool-Driven Visual Exploration (using a fixed toolkit), Programmatic Visual Manipulation (generating code for custom operations), and Intrinsic Visual Imagination (internally generating visual thoughts). The primary contribution is a comprehensive taxonomy organizing methods across these stages; a key challenge identified is the "explosive token economy of visual thought," where the computational cost of processing intermediate visual steps is orders of magnitude higher than textual reasoning, creating a ceiling on the depth of visual deliberation. The principal implication for AI practitioners is a structured roadmap for designing more capable multimodal systems, allowing them to select the appropriate cognitive mechanism—from external tool use to internal imagination—based on specific application requirements for complexity, efficiency, and interpretability. |
| Decoupled Planning and Execution: A Hierarchical Reasoning Framework for
  Deep Search (Read more on [arXiv](https://arxiv.org/abs/2507.02652) or [HuggingFace](https://huggingface.co/papers/2507.02652))| Yutao Zhu, Yuyao Zhang, Guanting Dong, Xiaoxi Li, Jiajie Jin | The paper introduces HiRA, a hierarchical reasoning framework that decouples strategic planning from specialized execution to improve deep search task performance. The research objective is to address the inefficiency and limited scalability of monolithic models that handle both high-level planning and detailed execution by proposing a new architectural paradigm. The core methodology involves a three-tiered system: a Meta Reasoning Planner decomposes complex queries into subtasks, an Adaptive Reasoning Coordinator assigns these subtasks to appropriate Domain-Specialized Executors, and these executors leverage specific tools (e.g., search, code interpreters) to complete their assigned functions. On the complex GAIA benchmark, HiRA achieved an average accuracy of 42.5%, significantly outperforming the state-of-the-art WebThinker agent's 36.2%. For AI practitioners, the principal implication is that designing agentic systems with a modular, hierarchical architecture that separates planning from execution allows for more effective, scalable, and "plug-and-play" integration of diverse reasoning capabilities. |
| Fast and Simplex: 2-Simplicial Attention in Triton (Read more on [arXiv](https://arxiv.org/abs/2507.02754) or [HuggingFace](https://huggingface.co/papers/2507.02754))| Jiecao Yu, Sijia Chen, Sai Surya Duvvuri, Timothy Chou, Aurko Roy | This paper introduces an efficient Triton kernel for 2-simplicial attention, demonstrating it improves the parameter-scaling exponent and achieves better token efficiency on reasoning tasks compared to standard dot-product attention.  The primary objective is to investigate whether 2-simplicial attention, which generalizes standard attention to trilinear forms, can offer a more favorable scaling law exponent and thus better performance than standard Transformers under fixed token budget constraints.  The authors implement 2-simplicial attention within a sliding window using a custom Triton kernel optimized for efficiency, inspired by FlashAttention. They train a series of interleaved Mixture-of-Experts (MoE) models ranging from 1B to 3.5B active parameters on a fixed token budget, comparing their negative log-likelihood on reasoning, math, and coding benchmarks against standard Transformer baselines.  The primary result is that 2-simplicial attention models outperform identically-sized standard Transformers on reasoning-heavy tasks, with the performance gap widening at larger scales. Specifically, the paper demonstrates that 2-simplicial attention increases the parameter scaling exponent `α` in the neural scaling law; for the MMLU-pro benchmark, `α` increased by 20.2% (from 0.0901 to 0.1083) compared to the dot-product attention baseline.  The principal implication for AI practitioners is that 2-simplicial attention presents a viable architectural alternative to standard attention, particularly in token-constrained environments. The improved scaling exponent suggests that for a given limited dataset, a 2-simplicial model can achieve superior performance on complex reasoning tasks compared to a standard Transformer of the same parameter count, making it a promising direction for building more token-efficient models. |
| Can LLMs Identify Critical Limitations within Scientific Research? A
  Systematic Evaluation on AI Research Papers (Read more on [arXiv](https://arxiv.org/abs/2507.02694) or [HuggingFace](https://huggingface.co/papers/2507.02694))| Arman Cohan, Lovekesh Vig, Manasi Patwardhan, Yilun Zhao, Zhijian Xu | This research introduces LIMITGEN, a benchmark for systematically evaluating the capability of LLMs to identify critical limitations in AI research papers, and assesses the impact of Retrieval-Augmented Generation (RAG). The primary objective is to quantify how effectively LLMs can identify different types of limitations in scientific papers and to determine if RAG can enhance this capability to a level that assists human peer reviewers. The authors created the LIMITGEN benchmark, comprising a synthetic dataset (LIMITGEN-Syn) with controlled, introduced limitations and a human-derived dataset (LIMITGEN-Human) from ICLR 2025 reviews. They evaluated proprietary and open-source LLMs, as well as a multi-agent system (MARG), with and without a RAG pipeline that retrieves relevant literature from Semantic Scholar. Current LLMs demonstrate limited capability in identifying research limitations; on the LIMITGEN-Syn dataset, GPT-4o's coarse-grained accuracy for identifying introduced limitations was 52.0%, significantly lower than human performance (86.0%). However, incorporating a RAG pipeline substantially improved GPT-4o's accuracy by 12.2 percentage points. LLMs, in their current state, are not suitable for autonomous peer review or critical analysis of scientific work. AI engineers should focus on developing RAG-enhanced systems as tools to assist human experts by grounding model outputs in relevant literature, rather than attempting to replace human-in-the-loop processes for tasks requiring deep, contextualized domain expertise. |
| Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving (Read more on [arXiv](https://arxiv.org/abs/2507.02726) or [HuggingFace](https://huggingface.co/papers/2507.02726))| Jun Wang, Anthony Bordg, Rasul Tutunov, Xiaotong Ji, Matthieu Zimmer | The paper introduces Bourbaki, a theorem-proving system using a novel framework called self-generated goal-conditioned Markov Decision Processes (sG-MDPs) to navigate complex proof searches. The research objective is to overcome the sparse reward problem in automated theorem proving by enabling an agent to dynamically generate and pursue its own intermediate subgoals. The key methodology involves formulating the proof search as an sG-MDP, where large language models propose conjectures (subgoals), and a Monte Carlo Tree Search (MCTS) algorithm explores the resulting proof space, with rewards given for solving these intermediate steps. The primary result shows that the Bourbaki (7B) system solves 26 problems on the PutnamBench benchmark at a pass@512 sample budget, establishing a new state-of-the-art for 7B-scale models. For AI practitioners, the principal implication is that the sG-MDP framework offers a structured method to decompose long-horizon reasoning tasks, creating denser reward signals and making complex problems more tractable for search algorithms without requiring pre-trained critic models. |
| Energy-Based Transformers are Scalable Learners and Thinkers (Read more on [arXiv](https://arxiv.org/abs/2507.02092) or [HuggingFace](https://huggingface.co/papers/2507.02092))| Peixuan Han, Md Mofijul Islam, Ganesh Nanduru, Alexi Gladstone, amanchadha | i) This paper introduces Energy-Based Transformers (EBTs), a new model paradigm that achieves superior scaling and generalization by framing prediction as an iterative, unsupervised energy minimization process analogous to System 2 thinking. ii) The primary research objective is to determine if it's possible to develop models that learn System 2 thinking capabilities, such as dynamic compute allocation and self-verification, entirely from unsupervised learning without modality- or problem-specific supervision. iii) The key methodology involves training Energy-Based Transformers (EBTs) to learn an energy function that evaluates the compatibility between an input and a candidate prediction, then generating predictions by iteratively minimizing this energy via gradient descent. iv) EBTs demonstrate superior scalability, achieving up to a 35% higher scaling rate than the Transformer++ approach during pretraining, and at inference, they can improve language modeling performance by 29% more than Transformer++ models by allocating additional computation. v) The principal implication for AI practitioners is that EBTs present a new, more data-efficient pretraining paradigm that generalizes better than standard Transformers, offering a promising approach for scaling future foundation models, especially as high-quality training data becomes a limiting factor. |
| Selecting and Merging: Towards Adaptable and Scalable Named Entity
  Recognition with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.22813) or [HuggingFace](https://huggingface.co/papers/2506.22813))| Wei Wei, Zhuojun Ding, Facico | The SaM framework improves Named Entity Recognition by dynamically selecting and merging pre-trained, domain-specific expert models at inference time for enhanced adaptability and scalability. The primary objective is to overcome the poor adaptability, scalability, and high cost associated with training unified, multi-domain Large Language Models for NER tasks. The proposed SaM framework first trains multiple expert models on distinct domains using parameter-efficient fine-tuning (LoRA). For a given target domain at inference, it selects relevant experts based on two criteria: domain similarity calculated via text embeddings and performance on a small, pseudo-labeled sample set. The LoRA parameters of these selected experts are then merged using Ties-Merging to create specialized models for final prediction. Experimental results on CrossNER and MIT benchmarks demonstrate that the SaM framework outperforms a unified model trained on all source data, achieving an average F1-score improvement of approximately 10%. The principal implication for AI practitioners is that they can develop more scalable and adaptable NER systems by maintaining a library of lightweight, domain-specific LoRA adapters and dynamically composing them at inference time, thus avoiding costly retraining or reliance on a single, sub-optimal monolithic model. |
| Self-Correction Bench: Revealing and Addressing the Self-Correction
  Blind Spot in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.02778) or [HuggingFace](https://huggingface.co/papers/2507.02778))| Ken Tsui | This research introduces Self-Correction Bench to demonstrate that LLMs exhibit a "Self-Correction Blind Spot," and shows that simple test-time interventions can activate this latent capability. The primary objective is to systematically quantify why LLMs fail to correct their own generated errors despite being able to correct identical errors presented externally. The methodology uses controlled error injection into either the model's own response or the user's prompt across three datasets (SCLI5, GSM8K-SC, PRM800K-SC) to compare internal versus external error correction. Results reveal an average 64.5% blind spot rate across 14 models, which is reduced by 89.3% simply by appending the token "Wait" to the model's output without any finetuning. The principal implication for AI practitioners is that an LLM's self-correction ability is often a problem of activation rather than knowledge, and it can be elicited at inference time using simple conditioning tokens to improve reliability. |
| ZeCO: Zero Communication Overhead Sequence Parallelism for Linear
  Attention (Read more on [arXiv](https://arxiv.org/abs/2507.01004) or [HuggingFace](https://huggingface.co/papers/2507.01004))| Tianjian Li, Xinyi Wan, Ruijie Zhu, Zehao Liu, Yuhong Chou | ZeCO is a sequence parallelism (SP) method for linear attention models that achieves near-linear scalability by eliminating communication bottlenecks. The paper's objective is to overcome the substantial communication overhead of existing SP methods, which impedes the training of models on ultra-long sequences. The key methodology is a novel collective communication primitive called "All-Scan," which uses a pipelined receive-scan-send pattern to transmit minimal state information between devices, enabling significant overlap between communication and computation. Empirically, ZeCO achieves a 60% throughput speedup over the state-of-the-art SP method on 256 GPUs with an 8M sequence length, and its All-Scan communication is 3.9x faster than the All-Gather used in prior work. For AI practitioners, this provides a direct path to efficiently pre-train linear attention models on previously intractable context lengths with near-ideal scaling, significantly accelerating the development of very-long-context LLMs. |
| AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM
  Post-Training (Read more on [arXiv](https://arxiv.org/abs/2507.01663) or [HuggingFace](https://huggingface.co/papers/2507.01663))| Guang Yang, Kui Luo, Haibo Wang, Ansheng You, Zhenyu Han | AsyncFlow is a task-separated, asynchronous streaming reinforcement learning framework designed to improve the efficiency and scalability of large language model post-training. The primary objective is to overcome the scalability bottlenecks, resource idling, and inflexible engine coupling of existing RL frameworks by developing a modular, high-throughput system for large-scale post-training. The key methodology combines a distributed data management module, TransferQueue, which provides centralized, fine-grained data scheduling to enable automated pipeline overlapping, with a producer-consumer asynchronous workflow that uses a delayed parameter update mechanism to minimize synchronization overhead between RL tasks. In experiments, AsyncFlow achieved an average throughput improvement of 1.59× over the state-of-the-art task-collocated baseline (verl); an ablation study on a 7B model using 512 NPUs showed that the TransferQueue module alone provided a 2.01x throughput gain, which increased to 2.74x with full asynchronous optimizations. The principal implication for AI practitioners is that implementing a task-separated architecture with asynchronous streaming dataflows and optimized, delayed parameter updates can substantially increase hardware utilization and training throughput, enabling more efficient and scalable RL-based fine-tuning of large models on industrial-scale clusters. |
