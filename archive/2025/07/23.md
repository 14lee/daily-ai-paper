

## Papers for 2025-07-23

| Title | Authors | Summary |
|-------|---------|---------|
| Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16784) or [HuggingFace](https://huggingface.co/papers/2507.16784))| Tina Li, Nathaniel Morgan, Hongyin Luo, thejackobrien, drkylj | This paper introduces TIM, a language model, and TIMRUN, an inference runtime, designed to enable long-horizon reasoning beyond LLM context limits by modeling tasks as recursive, prunable trees. The objective is to overcome context window, output token, and GPU memory constraints to support virtually unlimited working memory and multi-hop tool use within a single inference pass. The methodology involves training TIM to generate structured JSON representing a hierarchy of tasks and subtasks, which the TIMRUN runtime leverages to dynamically prune the KV cache of completed subtasks, thereby reusing memory and positional embeddings. Experiments show the system improves reasoning on certain tasks while significantly reducing memory load; on the AIME 2024 benchmark, accuracy increased from 40.0% to 46.7% while the system pruned 64.1% of the total KV cache. For AI practitioners, this co-designed model-runtime system provides a new architecture for building complex, memory-intensive agents that can handle long reasoning chains and tool use more efficiently than traditional multi-agent frameworks that rely on repetitive context prefilling. |
| Step-Audio 2 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.16632) or [HuggingFace](https://huggingface.co/papers/2507.16632))| Chao Yan, Boyong Wu, Insects, SmailAA, petronny | The paper presents Step-Audio 2, an end-to-end large audio language model that unifies audio understanding and generation by directly processing raw audio and outputting interleaved discrete text and audio tokens. The primary objective is to develop a model that overcomes the limitations of prior LALMs by comprehending paralinguistic cues, enabling genuine end-to-end speech conversation, and mitigating hallucinations through external tool integration. The methodology uses an architecture with a frozen audio encoder, an adapter, an LLM decoder, and an audio detokenizer, trained through a multi-stage process of pre-training, supervised fine-tuning, and reinforcement learning (PPO/GRPO), augmented with RAG and tool-calling. Evaluation results show state-of-the-art performance across multiple benchmarks, including a 3.11% average character error rate (CER) on general Chinese ASR test sets, surpassing other leading models. For AI practitioners, this work provides a robust architectural blueprint for building more natural and reliable spoken dialogue systems by generating interleaved audio-text tokens within a single model, bypassing traditional cascaded ASR-LLM-TTS pipelines. |
| MegaScience: Pushing the Frontiers of Post-Training Datasets for Science
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16812) or [HuggingFace](https://huggingface.co/papers/2507.16812))| Pengfei Liu, SinclairWang, Vfrz | This paper introduces MEGASCIENCE, a 1.25M-instance dataset for scientific reasoning, created by curating and combining textbook-derived data with other open-source sets to improve LLM performance on science tasks. The objective is to develop and release large-scale, high-quality, and verifiable open-source datasets to advance the scientific reasoning capabilities of LLMs, a domain the authors argue is neglected compared to math and coding. The methodology involves creating a base dataset, TEXTBOOKREASONING, by extracting 650k QA pairs from university textbooks using a pipeline with dual-standard extraction and LLM-based decontamination, then combining it with optimally selected subsets of public datasets to form MEGASCIENCE for supervised fine-tuning. Models fine-tuned on MEGASCIENCE consistently outperform official instruction-tuned versions; for instance, Qwen2.5-7B fine-tuned on MEGASCIENCE achieved a 61.01% average score across 14 benchmarks, surpassing the 58.80% of the official Qwen2.5-7B-Instruct model. For AI practitioners, this research demonstrates that targeted SFT on a high-quality, domain-specific dataset like MEGASCIENCE can yield superior scientific reasoning performance compared to relying on general-purpose instruction-tuned models, providing a direct path to create more capable "AI scientists". |
| Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2507.08422) or [HuggingFace](https://huggingface.co/papers/2507.08422))| Se Young Chun, Agorium, hirussell, ignow | The paper introduces Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates diffusion transformers by performing mixed-resolution sampling focused on spatially significant regions. The objective is to accelerate the inference of diffusion transformers along the spatial dimension, mitigating artifacts like aliasing and noise-timestep mismatches that arise from latent upsampling, without requiring model retraining. RALU employs a three-stage process: initial low-resolution denoising, selective early upsampling of artifact-prone edge regions identified via Canny edge detection, and final full-resolution refinement, using Noise-Timestep rescheduling with Distribution Matching (NT-DM) to stabilize generation across resolution changes. The method achieves up to a 7.0x speed-up on the FLUX.1-dev model with an FID score of 28.68, significantly outperforming the spatial baseline Bottleneck Sampling (FID 38.16) at a comparable acceleration level. For AI practitioners, RALU offers a practical, training-free method to significantly reduce the inference latency of large diffusion transformers, and its design allows it to be combined with temporal acceleration techniques for further performance gains. |
| Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16814) or [HuggingFace](https://huggingface.co/papers/2507.16814))| Songyang Gao, Harold-lkk, vanilla1116, haitengzhao, shenjunhao | This paper presents SOPHIA, a semi-off-policy reinforcement learning framework that enhances visual slow-thinking reasoning in large vision-language models (LVLMs). The objective is to overcome the constraints of on-policy RL and mitigate visual hallucination risks associated with pure off-policy RL. The key methodology involves a semi-off-policy behavior model that combines on-policy visual understanding from the trainable LVLM with off-policy reasoning from a separate language model, using propagated visual rewards to guide training. Extensive experiments show SOPHIA improves the InternVL3.0-38B model's average pass@1 accuracy by 8.50% and achieves 49.08% on the MathVision benchmark. For AI practitioners, SOPHIA provides a scalable, automated method to improve LVLM reasoning without relying on human or closed-source annotations, serving as a superior policy initialization for further on-policy training. |
| ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  Planning (Read more on [arXiv](https://arxiv.org/abs/2507.16815) or [HuggingFace](https://huggingface.co/papers/2507.16815))| Fu-En Yang, Yu-Chiang Frank Wang, Yueh-Hua Wu, cmhungsteve, jasper0314-huang | ThinkAct introduces a dual-system framework for vision-language-action (VLA) tasks that improves long-horizon planning and adaptability by separating high-level reasoning from low-level control. The primary objective is to enable an embodied agent to generate explicit reasoning plans guided by environmental feedback, rather than relying on end-to-end action prediction or supervised chain-of-thought data. The methodology involves a reasoning MLLM fine-tuned with reinforcement learning (specifically, Group Relative Policy Optimization) using a novel action-aligned reward signal derived from visual goal completion and trajectory consistency (measured via DTW), which generates a compact visual plan latent to condition a downstream Diffusion Policy action model. On the LIBERO manipulation benchmark, ThinkAct achieves an 84.4% overall success rate, outperforming previous state-of-the-art models and demonstrating effective long-horizon planning and self-correction capabilities. For AI practitioners, the key implication is that decoupling reasoning and action into two asynchronously operating modules—where a reasoning module is optimized via RL on task-grounded visual rewards to guide a separate policy—offers a scalable and robust approach to building agents that can handle complex, multi-step tasks in dynamic environments. |
| Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16746) or [HuggingFace](https://huggingface.co/papers/2507.16746))| Zikui Cai, Kaiyu Yue, deqing, charleslwang, leonli66 | ZEBRA-COT is a new, large-scale dataset of 182,384 samples with interleaved text-image reasoning traces designed to train vision-language models for Visual Chain of Thought (visual CoT). The primary objective is to address the lack of high-quality training data for visual CoT by creating a diverse dataset that enables VLMs to generate explicit, logically coherent visual aids as part of their reasoning process across scientific, 2D/3D, and strategic domains. The methodology involves curating the dataset by sourcing real-world and synthetic problems and then using VLMs (Gemini-2.5, GPT-4.1) to enhance raw data into structured, high-quality reasoning traces, which are then used to fine-tune existing VLM backbones like Anole-7B and Bagel-7B. Fine-tuning the Anole-7B model on ZEBRA-COT yielded up to a 13.3-point absolute performance gain on the VisuLogic benchmark, and a fine-tuned Bagel-7B model acquired the novel capability to inherently generate interleaved visual reasoning steps, which it previously could not. The principal implication for AI practitioners is that ZEBRA-COT provides a foundational dataset and open-source models for building and evaluating systems with innate visual reasoning, offering a strong initialization point for subsequent fine-tuning with reinforcement learning to improve logical consistency in visual thought processes. |
| HOComp: Interaction-Aware Human-Object Composition (Read more on [arXiv](https://arxiv.org/abs/2507.16813) or [HuggingFace](https://huggingface.co/papers/2507.16813))| Rynson W. H. Lau, Jinyuan Jia, Dong Liang, LeoLau | HOComp is a novel framework for human-object image composition that generates realistic interactions while preserving subject and object identity. The objective is to composite a foreground object onto a human-centric background image, ensuring the resulting human-object interaction is harmonious and plausible, while simultaneously maintaining the visual consistency of both the original person and the inserted object. The method employs a Diffusion Transformer (DiT) backbone guided by MLLMs-driven Region-based Pose Guidance (MRPG), which uses a multimodal large language model to define the interaction type and applies a localized pose loss, and Detail-Consistent Appearance Preservation (DCAP), which combines shape-aware attention modulation, a multi-view appearance loss for the object, and a background consistency loss to preserve identities. On the authors' proposed IHOC dataset, HOComp significantly outperforms nine state-of-the-art methods, achieving an HOI-Score of 87.39 compared to the next-best score of 75.22 from GPT-4o. For AI practitioners, this work provides a robust framework for controllable image synthesis in applications requiring nuanced human-object interaction like virtual try-on and advertising, demonstrating how combining MLLM-based semantic guidance with targeted, component-specific loss functions can create context-aware generative models that preserve fine-grained details. |
| Experience is the Best Teacher: Grounding VLMs for Robotics through
  Self-Generated Memory (Read more on [arXiv](https://arxiv.org/abs/2507.16713) or [HuggingFace](https://huggingface.co/papers/2507.16713))| Christopher E. Mower, Changan Chen, René Zurbrügg, Kaixian Qu, Guowei Lan | This paper introduces EXPTEACH, a framework that grounds Vision-Language Models (VLMs) to physical robots by enabling them to autonomously generate, store, and retrieve memories from real-world task experiences. The main objective is to overcome the challenge of grounding internet-trained VLMs to specific robotic embodiments by having the agent learn from its own successes and failures in a closed loop. The methodology combines a short-term memory (STM) for in-task reflection and a long-term memory (LTM) that stores summarized experiences, which are retrieved using Retrieval-Augmented Generation (RAG) to inform future planning. Across 12 real-world scenarios, grounding with LTM boosted single-trial success rates from 22% to 80%, demonstrating the framework's effectiveness and generalizability. The principal implication for AI practitioners is that implementing a self-generative memory system allows VLMs to adapt to specific hardware and environments, creating more robust and capable robotic agents by learning directly from embodied experience rather than relying solely on pre-trained knowledge. |
| RefCritic: Training Long Chain-of-Thought Critic Models with Refinement
  Feedback (Read more on [arXiv](https://arxiv.org/abs/2507.15024) or [HuggingFace](https://huggingface.co/papers/2507.15024))| Hongyu Lin, Bowen Yu, Le Yu, Hao Xiang, Qiaoyu Tang | RefCritic is a long chain-of-thought critic model trained with a dual-reward reinforcement learning framework to generate actionable feedback that improves policy model refinement. The primary objective is to develop a critic model that moves beyond superficial solution verification to produce in-depth, actionable critiques that demonstrably improve the performance of the policy model being critiqued. The methodology involves a two-stage process: first, supervised fine-tuning (SFT) on filtered data to create a cold-start critic, followed by reinforcement learning (RL) using a dual-reward system that jointly rewards the critic for the correctness of its judgment and for the performance improvement of a policy model after it refines its solution based on the critic's feedback. On the AIME25 benchmark, using feedback from RefCritic improved the base policy model's Pass@1 performance by 6.8% after a single round of refinement. Additionally, RefCritic achieves an average F1 score of 77.1 on ProcessBench for identifying error locations, outperforming methods that use explicit step-level supervision during training. The principal implication for AI practitioners is that training critic models should incorporate a direct feedback loop from the downstream task; explicitly rewarding critiques based on their ability to improve a policy model's performance is more effective for creating useful, actionable feedback than optimizing for critique accuracy alone. |
| SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced
  Academic Search (Read more on [arXiv](https://arxiv.org/abs/2507.15245) or [HuggingFace](https://huggingface.co/papers/2507.15245))| Jinxin Xie, Longbin Yu, Qian Kou, Yuduo Li, Xiaofeng Shi | This paper introduces SPAR, a modular, multi-agent framework designed to enhance academic paper retrieval through LLM-based agents. The primary research objective is to develop a flexible and effective search system that can handle complex, multi-intent queries by mimicking human research behaviors like following citation networks. Its key methodology is a multi-agent architecture comprising five specialized agents for query interpretation, multi-source retrieval with "RefChain" citation expansion, relevance judgment, iterative query evolution, and result reranking. SPAR demonstrated significant performance gains over strong baselines, achieving a +56% F1 score improvement on the AutoScholar benchmark and +23% on the newly introduced SPARBench. For AI practitioners, the principal implication is that decomposing complex retrieval tasks into specialized, coordinated agent functions that integrate symbolic planning (RefChain) with LLM-powered query evolution offers a more robust and effective approach than monolithic or single-agent systems. |
| Does More Inference-Time Compute Really Help Robustness? (Read more on [arXiv](https://arxiv.org/abs/2507.15974) or [HuggingFace](https://huggingface.co/papers/2507.15974))| Chawin Sitawarin, Weichen Yu, Jiachen T. Wang, Chong Xiang, Tong Wu | This paper demonstrates that while increasing inference-time compute can enhance LLM robustness when reasoning is hidden, it conversely reduces robustness when reasoning steps are exposed, revealing a critical security trade-off. The primary objective is to investigate how inference-time computation affects the robustness of open-source reasoning LLMs against adversarial attacks, critically examining the implicit assumption that intermediate reasoning steps are hidden. The study employs a "budget forcing" strategy to systematically control the length of reasoning chains (from 100 to 16,000 tokens) across 12 open-source models, evaluating robustness on benchmarks for prompt injection (SEP), prompt extraction (TENSORTRUST), and harmful requests (SORRY-BENCH). The primary results show that when reasoning is hidden, robustness improves with more computation (e.g., Qwen3-32B's prompt injection robustness increases from ~35% to ~75%); however, when reasoning is exposed, an *inverse scaling law* emerges where robustness consistently degrades (e.g., R1-QWEN-14B's prompt injection robustness drops from ~90% to below 20% as the budget increases). The principal implication for AI practitioners is that the benefits of inference-time scaling are context-dependent; increasing compute can introduce significant security vulnerabilities if intermediate reasoning steps are accessible, either directly, via tool-use APIs, or through extraction attacks. |
| Steering Out-of-Distribution Generalization with Concept Ablation
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2507.16795) or [HuggingFace](https://huggingface.co/papers/2507.16795))| Senthooran Rajamanoharan, Samuel Marks, Adam Karvonen, Caden Juang, Helena Casademunt | i) A 1-line summary The paper introduces Concept Ablation Fine-Tuning (CAFT), a method that uses interpretability tools to steer the out-of-distribution generalization of LLMs during fine-tuning without modifying the training data.  ii) Main research question or objective To develop a method for controlling how a large language model generalizes from a fine-tuning dataset to an out-of-distribution (OOD) one, particularly in a worst-case scenario where no OOD data is available to specify the intended generalization.  iii) Key methodology used CAFT identifies undesired concepts as linear directions in the model's latent space using either Principal Component Analysis (PCA) on activation differences or Sparse Autoencoders (SAEs). It then fine-tunes the model while continuously ablating (projecting away) these undesired directions from the model's activations during both the forward and backward passes.  iv) Primary results (include at least one specific quantitative finding) On an emergent misalignment task where fine-tuning on insecure code causes harmful general responses, CAFT with PCA reduced misaligned responses from 7.0% to 0.39% for the Qwen model—a reduction of over 10x—with only a minor degradation in performance on the original insecure code task. In multiple-choice tasks with spurious correlations, CAFT with SAEs often improved OOD accuracy from near 0% to over 50%, and in some cases near 100%.  v) Principal implication for AI practitioners CAFT provides a practical technique for AI engineers to mitigate unintended and potentially harmful behaviors that emerge during fine-tuning, especially when it is infeasible to curate specific training data to prevent such generalizations. It demonstrates a direct application of interpretability tools within the training process to improve model safety and control. |
| ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2507.15454) or [HuggingFace](https://huggingface.co/papers/2507.15454))| Yixuan Li, Lihan Jiang, Linning Xu, Mulin Yu, Ruijie Zhu | ObjectGS is a framework that unifies high-fidelity 3D scene reconstruction and semantic understanding by modeling individual objects with dedicated, ID-aware Gaussian primitives. The main objective is to overcome the lack of semantic understanding in standard 3D Gaussian Splatting by developing a method that jointly performs object-level reconstruction and segmentation. The key methodology involves initializing object-aware anchors from 2D segmented masks, assigning a fixed one-hot ID encoding to each generated Gaussian based on its object affiliation, and optimizing with a classification loss to enforce discrete semantic boundaries during rendering. On the 3DOVS open-vocabulary segmentation benchmark, ObjectGS achieved a mean IoU of 96.4%, outperforming prior methods like Gaussian Grouping (89.1%). The principal implication for AI practitioners is that using discrete one-hot ID encoding with a classification loss offers a robust and unambiguous method for embedding semantic data into Gaussian Splatting models, enabling cleaner object extraction and direct object-level manipulation for applications like scene editing and robotics. |
