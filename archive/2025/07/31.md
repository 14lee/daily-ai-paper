

## Papers for 2025-07-31

| Title | Authors | Summary |
|-------|---------|---------|
| ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents (Read more on [arXiv](https://arxiv.org/abs/2507.22827) or [HuggingFace](https://huggingface.co/papers/2507.22827))| Qunzhong Wang, Yuxuan Wan, Yaozhi Zheng, Yilei Jiang, csuhan | ScreenCoder introduces a modular multi-agent framework to convert UI design images into front-end code by breaking the process into grounding, planning, and generation. The research aims to overcome the limitations of end-to-end models by creating a robust, interpretable system that handles visual understanding, layout planning, and code synthesis as distinct sub-problems. The methodology employs a three-agent pipeline where a grounding agent detects UI components, a planning agent constructs a hierarchical layout tree, and a generation agent synthesizes HTML/CSS code; this system also functions as a data engine to fine-tune a VLM using supervised and reinforcement learning. The agentic ScreenCoder achieves state-of-the-art results, outperforming models like GPT-4o with a Block Match score of 0.755 versus 0.730. The principal implication for AI practitioners is the framework's dual function as a high-performance inference pipeline and a scalable data engine for generating synthetic image-code pairs, enabling the targeted improvement of VLMs for complex, domain-specific code generation tasks. |
| Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency
  and Performance (Read more on [arXiv](https://arxiv.org/abs/2507.22448) or [HuggingFace](https://huggingface.co/papers/2507.22448))| Maksim Velikanov, Iheb-Chaabane, ifarhat1993, ybelkada, JingweiZuo | The Falcon-H1 series introduces a new family of open-source, hybrid-head language models that combine parallel attention and Mamba-2 SSMs for superior performance and efficiency. The primary objective was to design and evaluate a novel hybrid architecture that achieves state-of-the-art performance while being significantly more parameter- and training-efficient than existing large language models. The models utilize a parallel architecture with independently tunable attention and Mamba-2 SSM heads, developed through extensive ablations on channel allocation, tokenizer design, and training dynamics, including a custom Maximal Update Parametrization (ÂµP) recipe. Falcon-H1 models demonstrate exceptional parameter efficiency, with the flagship 34B model rivaling 70B-scale models and showing significant efficiency gains in long-context tasks; specifically, Falcon-H1-34B achieves up to a 4x improvement in input throughput and an 8x speedup in output throughput over a comparable Transformer model at the longest tested sequence lengths. For AI practitioners, Falcon-H1 provides highly capable models for complex reasoning and long-context applications at a fraction of the size and computational cost of competitors, enabling deployment in resource-constrained environments without sacrificing performance. |
| BANG: Dividing 3D Assets via Generative Exploded Dynamics (Read more on [arXiv](https://arxiv.org/abs/2507.21493) or [HuggingFace](https://huggingface.co/papers/2507.21493))| Wei Yang, Yinuo Bai, Haoran Jiang, Qixuan Zhang, ZarkLngeW | BANG is a generative framework that decomposes 3D assets into constituent parts through a controllable, dynamic "exploding" process. The primary objective is to develop a generative model that can dynamically and controllably decompose a 3D object into its meaningful geometric parts, bridging the gap between 3D generation and structural understanding. The methodology involves fine-tuning a large-scale, pre-trained latent diffusion model on a curated dataset of 20k exploded 3D assets using a lightweight "Exploded View Adapter" and a temporal attention module to ensure smooth part transitions. The model produces high-quality part decompositions, with an ablation study demonstrating that its temporal attention module improves the weighted IoU metric for part trajectory tracking by 18.8% (from 0.6874 to 0.8163) compared to a baseline without it. For AI practitioners, this research shows that large, pre-trained generative models can be adapted for complex, dynamic 3D manipulation tasks like part decomposition using lightweight, task-specific modules, enabling part-level control in creative and engineering workflows. |
| VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced
  Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.22607) or [HuggingFace](https://huggingface.co/papers/2507.22607))| Sicong Leng, Chenghao Xiao, Ruifeng Yuan, 26hzhang, kenchan0226 | The paper introduces VL-COGITO, a multimodal reasoning model trained with a Progressive Curriculum Reinforcement Learning (PCuRL) framework to systematically improve performance on tasks of increasing complexity. The primary objective is to develop a training framework that addresses unstable performance in Multimodal Large Language Models (MLLMs) across diverse reasoning tasks by systematically guiding the model through a curriculum of gradually increasing difficulty. The work proposes the PCuRL framework, which integrates an Online Difficulty Soft Weighting mechanism to dynamically adjust training focus based on prompt difficulty and a Dynamic Length Reward mechanism to adaptively incentivize appropriate reasoning path lengths, all within a multi-stage training process based on Group Relative Policy Optimization (GRPO). VL-COGITO achieves state-of-the-art or highly competitive performance across multiple multimodal benchmarks, demonstrating absolute gains of 7.6% on Geometry@3K and 5.5% on MathVista over its backbone model. The principal implication for AI practitioners is that a multi-stage curriculum learning strategy that progresses from simple to complex tasks and dynamically rewards reasoning length can be applied directly to a base model via reinforcement learning to significantly enhance reasoning capabilities, bypassing a separate supervised fine-tuning phase. |
| Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with
  Weak Supervision (Read more on [arXiv](https://arxiv.org/abs/2507.20976) or [HuggingFace](https://huggingface.co/papers/2507.20976))| Celso de Melo, Stanislav Panev, Zheyang Qin, Min0326, xiaofanghf | A generative AI method that synthesizes labeled aerial imagery to adapt vehicle detectors to new geographic domains using weak supervision. The primary objective is to mitigate the performance degradation of vehicle detectors caused by domain shift when applied to new geographic regions, by generating high-quality, labeled synthetic data for the target domain using only weak (image-level) labels. The methodology involves a multi-stage framework that fine-tunes a latent diffusion model on both a fully-labeled source and a weakly-labeled target dataset, then leverages stacked cross-attention maps from object and learnable context tokens to automatically generate pseudo-bounding box labels for synthetic target-domain images, which are subsequently used to train a final detector. The proposed framework demonstrates significant performance gains, improving AP50 by 7-40% over unsupervised domain adaptation methods and 6-10% over weakly supervised methods; for instance, adapting from the DOTA to UGRC dataset, the method achieved a 75.7% AP50, surpassing the best-performing unsupervised method by 33.1 percentage points. The principal implication for AI practitioners is that fine-tuned generative models, particularly their internal cross-attention mechanisms, can be used to create an automated pipeline for generating domain-specific labeled data, substantially reducing the manual annotation cost required to adapt computer vision models to new deployment environments. |
| Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2507.22886) or [HuggingFace](https://huggingface.co/papers/2507.22886))| Yu-Gang Jiang, Guanquan Jie, Henghui Ding, Kaining Ying | This paper introduces OmniAVS, a new benchmark for omnimodal referring audio-visual segmentation, and OISA, a multimodal large language model-based method, to enhance reasoning and multimodal understanding in audiovisual scenes. The main objective is to extend the capabilities of referring audio-visual segmentation (RAVS) by enabling deeper understanding, complex reasoning, and multimodal integration across text, speech, sound, and image cues in expressions. The authors propose OmniAVS, a dataset with 8 types of multimodal referring expressions and detailed explanations, and OISA, a Multimodal Large Language Model (MLLM)-based segmentation assistant, which employs Audio-Visual Interleaving for temporal alignment and a query propagation mechanism for efficient segmentation. OISA-1B achieved a state-of-the-art average J&F score of 41.1% on the OmniAVS benchmark, surpassing the previous best method LISA-13B by 5.0%, and demonstrated superior reasoning capabilities with a METEOR score of 21.7% for explanation generation. OmniAVS and OISA collectively provide a practical framework and a challenging benchmark for developing omnimodal AI systems with fine-grained perception and reasoning capabilities, prompting the need for models capable of integrating and reasoning across diverse modalities for real-world applications. |
| Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2507.22565) or [HuggingFace](https://huggingface.co/papers/2507.22565))| Gilbert Fridgen, Ramin Bahmani, Igor Tchappi, Amir Sartipi, akhadangi | The paper introduces RLDP, a framework using reinforcement learning to dynamically manage clipping and noise for the differentially private (DP) fine-tuning of large language models. The primary objective is to improve the utility-privacy trade-off by reformulating the optimization of DP parameters as a closed-loop control problem instead of relying on static heuristics. The core methodology involves an online Soft Actor-Critic (SAC) hyper-policy that observes rich statistical summaries of the training dynamics and adjusts per-LoRA-adapter clip radii and noise levels to maximize a reward function that balances utility gains against privacy budget consumption. Across experiments on four LLMs, RLDP achieved an average 5.6% lower perplexity and reached the best baseline's final utility using, on average, 71% fewer training steps while upholding the same formal privacy guarantees. For AI practitioners, this enables the fine-tuning of LLMs on sensitive data with significantly higher model quality and drastically reduced computational cost, making privacy-preserving AI more practical and effective. |
| Repair-R1: Better Test Before Repair (Read more on [arXiv](https://arxiv.org/abs/2507.22853) or [HuggingFace](https://huggingface.co/papers/2507.22853))| Quanjun Zhang, Xiaochen Xie, Haichuan Hu | The paper introduces Repair-R1, a reinforcement learning framework that co-optimizes test case generation and code repair, improving automated program repair by first generating discriminative tests to understand bugs before fixing them. The main objective is to enhance Large Language Model (LLM)-based automated program repair (APR) by shifting from a "repair-then-validate" paradigm to a "test-before-repair" approach, explicitly training the model to first generate tests that expose a bug. The key methodology, Repair-R1, employs Group Relative Policy Optimization (GRPO) to jointly optimize test generation and patch generation, using rule-based rewards for output format, test quality, and repair correctness based on oracle test pass rates. Primary results show that Repair-R1 improves repair success rate by 2.68% to 48.29% and test generation success rate by 16.38% to 53.28% compared to vanilla models across four benchmarks. The principal implication for AI practitioners is that structuring generation tasks to include an explicit diagnostic step (like test case generation) before producing a final output (a code patch) can significantly improve model performance and reasoning, providing a more robust alternative to standard supervised fine-tuning, especially on imbalanced datasets. |
