

## Papers for 2025-07-11

| Title | Authors | Summary |
|-------|---------|---------|
| Scaling RL to Long Videos (Read more on [arXiv](https://arxiv.org/abs/2507.07966) or [HuggingFace](https://huggingface.co/papers/2507.07966))| Hanrong Ye, Qinghao Hu, Baifeng Shi, Wei Huang, Yukang Chen | This paper presents a framework for scaling reinforcement learning to enhance vision-language model reasoning on long videos through a new dataset, a two-stage training pipeline, and a parallelized infrastructure. The research objective is to develop and validate a full-stack solution that overcomes the data scarcity and computational bottlenecks inherent in applying reinforcement learning to VLMs for complex, long-form video understanding. The methodology combines three core components: 1) a new dataset, LongVideo-Reason, with 52K long-video QA pairs annotated for reasoning; 2) a two-stage training regimen of Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) followed by reinforcement learning; and 3) a novel training system, Multi-modal Reinforcement Sequence Parallelism (MR-SP), which leverages sequence parallelism and cached video embeddings for efficient training. The primary results demonstrate that the MR-SP system achieves up to a 2.1× speedup in RL training on 512-frame videos, and the resulting LongVILA-R1-7B model attains 68.4% accuracy on the VideoMME benchmark (with subtitles), surpassing previous open-source models. The principal implication for AI practitioners is the provision of an open-source, scalable system (MR-SP) that makes it computationally feasible to apply reinforcement learning to VLMs using hour-long video inputs on a single multi-GPU node, thus enabling the development of more capable models for long-context video analysis. |
| T-LoRA: Single Image Diffusion Model Customization Without Overfitting (Read more on [arXiv](https://arxiv.org/abs/2507.05964) or [HuggingFace](https://huggingface.co/papers/2507.05964))| Konstantin Sobolev, Andrey Kuznetsov, Vera Soboleva, ai-alanov | T-LoRA is a timestep-dependent, low-rank adaptation framework that mitigates overfitting in single-image diffusion model customization by dynamically adjusting parameter updates across diffusion timesteps. The primary objective is to solve the problem of fine-tuning overfitting, where models memorize background and positional information from a single training image, thereby compromising generalization and text alignment. The key methodology involves a dynamic rank-masking strategy that allocates fewer trainable parameters to higher (noisier) timesteps and an orthogonal weight initialization technique (Ortho-LoRA) to ensure adapter components are independent and fully utilized. The primary result shows that T-LoRA significantly improves text alignment over standard LoRA; at rank 64, T-LoRA achieved a Text Similarity score of 0.256 versus LoRA's 0.232, while maintaining a comparable Image Similarity of 0.900. For AI practitioners, T-LoRA provides a method to achieve robust single-image personalization with superior prompt alignment and generative diversity, reducing the need for multiple training images and mitigating common overfitting artifacts. |
| Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology (Read more on [arXiv](https://arxiv.org/abs/2507.07999) or [HuggingFace](https://huggingface.co/papers/2507.07999))| Zilong Huang, garlicisnotmyfavor, stormthunder, LXT, HaochenWang | This paper introduces TreeBench, a new benchmark for evaluating visual grounded reasoning, and TreeVGR, a training paradigm using reinforcement learning with traceable evidence to improve these capabilities in Large Multimodal Models (LMMs). The primary objective is to address the lack of holistic benchmarks for an LMM's ability to "think with images" by creating an evaluation that tests focused perception, traceable evidence via bounding boxes, and second-order reasoning. The methodology includes TreeBench, a benchmark with 405 challenging visual question-answering pairs requiring bounding box outputs, and TreeVGR, a two-stage training paradigm that uses reinforcement learning with a novel dual Intersection-over-Union (IoU) reward to explicitly supervise localization. The resulting TreeVGR model improves accuracy on the proposed TreeBench by +13.4 points over the Qwen2.5-VL-72B baseline and shows a +16.8 point gain on V* Bench. For AI practitioners, this work provides a concrete training methodology demonstrating that explicitly supervising intermediate localization steps via an IoU-based reward is a key strategy for developing more accurate and interpretable LMMs that can handle complex, vision-grounded reasoning tasks. |
| OST-Bench: Evaluating the Capabilities of MLLMs in Online
  Spatio-temporal Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.07984) or [HuggingFace](https://huggingface.co/papers/2507.07984))| Xihui Liu, Xiaohan Mao, Runsen Xu, Chenming Zhu, JingLi Lin | This paper introduces OST-Bench, a benchmark designed to evaluate the online spatio-temporal scene understanding capabilities of Multimodal Large Language Models (MLLMs) in an embodied agent context. The primary objective is to assess how well MLLMs can incrementally process sequential visual inputs to reason about their own state and dynamic spatial relationships within a 3D environment. The methodology involves a new dataset of 1.4k scenes and 10k QA pairs where models engage in multi-round dialogues, requiring them to integrate new visual frames with historical memory to answer questions. Results show that even the most advanced MLLMs significantly lag human performance by over 30%, and their accuracy on complex spatial tasks drops sharply as the exploration horizon extends, often to near-chance levels. For AI practitioners, this highlights a critical deficiency in current MLLMs' long-term memory retrieval and multi-step spatial reasoning, indicating that future work must focus on developing models capable of building and querying efficient internal world representations to overcome the identified "Spatio-temporal Reasoning Shortcut" failure mode. |
| Multi-Granular Spatio-Temporal Token Merging for Training-Free
  Acceleration of Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07990) or [HuggingFace](https://huggingface.co/papers/2507.07990))| Inwoong Lee, Taeoh Kim, Su Ho Han, Sukjun Hwang, js-hyun | This paper introduces Spatio-Temporal Token Merging (STTM), a training-free method to accelerate video large language models (LLMs) by reducing redundant visual tokens. The objective is to mitigate the quadratic computational complexity of video LLMs by efficiently merging spatio-temporal tokens without requiring model retraining. STTM employs a decomposed strategy, first using a coarse-to-fine quadtree search for multi-granular spatial token merging within frames, followed by directed pairwise merging of spatially overlapping tokens across the temporal dimension. The method achieves a 2x speed-up with only a 0.5% accuracy drop under a 50% token budget across six video QA benchmarks. For AI practitioners, the key implication is that STTM is query-agnostic, allowing the pre-computed Key-Value (KV) cache for a video to be reused across different questions, which significantly improves efficiency for multi-turn inference scenarios. |
| PyVision: Agentic Vision with Dynamic Tooling (Read more on [arXiv](https://arxiv.org/abs/2507.07998) or [HuggingFace](https://huggingface.co/papers/2507.07998))| Qilong Wu, Ming Li, Shaoheng Lin, haoquan03, stzhao | PyVision is an agentic framework enabling Multimodal Large Language Models (MLLMs) to autonomously generate, execute, and refine Python code for complex visual reasoning tasks. The research objective is to overcome the limitations of static, predefined toolsets in visual reasoning by creating a system where an MLLM can dynamically invent and use tailored computational tools to solve novel visual problems. The methodology involves an interactive, multi-turn loop between an MLLM and an isolated Python runtime; the MLLM receives a visual query, generates Python code using standard libraries to analyze the image, executes it, and uses the output to iteratively refine its reasoning until it produces a final answer. Primary results show that PyVision consistently improves backend model performance, boosting Claude-4.0-Sonnet's accuracy by +31.1% on the VLMsAreBlind-mini symbolic vision dataset and GPT-4.1's accuracy by +7.8% on the V* fine-grained visual search task. The principal implication for AI practitioners is that instead of relying on fixed APIs, they can build more robust and verifiable vision systems by empowering models to generate their own analysis code, enabling grounded, interpretable, and adaptive solutions to complex visual challenges. |
| Geometry Forcing: Marrying Video Diffusion and 3D Representation for
  Consistent World Modeling (Read more on [arXiv](https://arxiv.org/abs/2507.07982) or [HuggingFace](https://huggingface.co/papers/2507.07982))| Yang Ye, Junliang Guo, Diankun Wu, deeptimhe, Haoyuwu | This paper introduces Geometry Forcing (GF), a method to improve the 3D consistency of video diffusion models by aligning their intermediate features with representations from a pretrained geometric foundation model. The main objective is to bridge the gap between video diffusion models, which operate on 2D projections, and the inherent 3D structure of the physical world, thereby enhancing the geometric coherence of generated videos. The core methodology involves two complementary alignment loss objectives: "Angular Alignment" uses cosine similarity to enforce directional consistency between the diffusion model's latent features and a geometric model's features, while "Scale Alignment" regresses scale information to preserve geometric magnitudes. On the 256-frame RealEstate10K video generation task, Geometry Forcing reduces the Fréchet Video Distance (FVD) from 364 to 243 compared to the baseline. For AI practitioners, the principal implication is that GF enables video diffusion models to internalize a 3D representation, allowing for the generation of more consistent videos and the direct reconstruction of explicit 3D geometry (e.g., depth maps) from the model's intermediate features, a capability absent in standard models. |
| LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+
  FPS (Read more on [arXiv](https://arxiv.org/abs/2507.07136) or [HuggingFace](https://huggingface.co/papers/2507.07136))| Yuanhao Cai, Yang Liu, Minghan Qin, Yujie Zhao, Wanhua Li | LangSplatV2 is a 3D language Gaussian splatting model that achieves over 450 FPS for high-dimensional feature rendering by replacing the slow feature decoder with a sparse coefficient field over a global dictionary. The paper's primary objective is to overcome the inference speed bottleneck of the original LangSplat model to enable real-time, open-vocabulary 3D text querying at high resolutions without sacrificing accuracy. The key methodology models each Gaussian's feature as a sparse code and uses a CUDA-optimized splatting technique to render only the few non-zero coefficients, effectively decoupling rendering time from the final high-dimensional feature space. The model achieves 3D open-vocabulary text querying at 384.6 FPS, a 47-fold speedup over LangSplat, while simultaneously improving 3D semantic segmentation mean IoU from 51.4% to 59.9% on the LERF dataset. For AI practitioners, this provides a direct method for deploying high-fidelity, language-based 3D scene understanding in latency-critical applications like robotics and augmented reality, which was previously infeasible due to decoder bottlenecks. |
| Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07996) or [HuggingFace](https://huggingface.co/papers/2507.07996))| Yang Li, Ziyue Li, zhoutianyi | This paper introduces "Chain-of-Layers" (CoLa), a test-time method using Monte Carlo Tree Search (MCTS) to dynamically skip or repeat pretrained LLM layers per-sample, enhancing performance and efficiency without retraining. The research objective is to determine if a pretrained LLM's static architecture can be adapted for individual inputs by composing its layers into a custom sequence, thereby improving generalization on tasks of varying difficulty. A Monte Carlo Tree Search protocol is employed to find an optimal layer path for each sample, maximizing a UCB objective that balances predictive accuracy with a penalty for path length. The results demonstrate that for over 60% of samples with originally incorrect predictions, CoLa successfully identified a layer composition that yielded a correct prediction. The principal implication for AI practitioners is that pretrained transformer layers can be treated as reusable, composable modules, enabling the development of systems that dynamically adapt computational depth at inference time to significantly improve both accuracy and efficiency. |
| A Survey on Long-Video Storytelling Generation: Architectures,
  Consistency, and Cinematic Quality (Read more on [arXiv](https://arxiv.org/abs/2507.07202) or [HuggingFace](https://huggingface.co/papers/2507.07202))| Seunghyun Yoon, Ryan Rossi, Franck-Dernoncourt, taesiri, elmoghany | This survey analyzes 32 video generation papers to create a novel taxonomy of architectural styles and identify key components for producing long-form, coherent video. The paper's primary objective is to identify the architectural patterns and training strategies that enable high-fidelity, long-duration video generation while maintaining narrative and character consistency. The methodology involves a comprehensive literature review that organizes models into a six-branch taxonomy (including Keyframes-to-Video, Flattened 3D One-Shot, and Token-Stream Autoregressive) and presents detailed comparative tables of their core components. The analysis reveals a trend towards using MM-DiT backbones and identifies models like Loong capable of generating videos up to 150 seconds, though many systems still struggle beyond 16 seconds. For AI practitioners, the paper provides a blueprint of recommended components—such as using MLLMs for text encoding, Flow Matching for training, and 3D ROPE for positional encoding—to guide the development of more robust long-video generation systems. |
| Token Bottleneck: One Token to Remember Dynamics (Read more on [arXiv](https://arxiv.org/abs/2507.06543) or [HuggingFace](https://huggingface.co/papers/2507.06543))| Sangdoo Yun, Jeongeun Park, bhheo, calintz, taekyung-k | Token Bottleneck (ToBo) is a self-supervised learning pipeline that learns temporally-aware visual representations by compressing a reference scene into a single token to predict a heavily masked subsequent scene. The primary objective is to create a visual backbone that both conservatively summarizes an observed state and captures the dynamics of transitions between scenes, which is critical for sequential tasks. The key methodology involves a squeeze-and-expand process where an encoder maps a reference frame to a single bottleneck token, which is then used alongside a few visible patches from a heavily masked target frame (e.g., 90% masked) to reconstruct the target. In experiments, ToBo significantly outperformed baselines on robot manipulation tasks, achieving an 82.0% success rate on the Franka Kitchen "Light on" task, a +28.0 percentage point improvement over the next best method. For AI practitioners, this means ToBo-pretrained backbones can be directly deployed to build more effective and data-efficient robot control policies and dynamic scene understanding systems without requiring labeled data or complex model architectures. |
| Machine Bullshit: Characterizing the Emergent Disregard for Truth in
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.07484) or [HuggingFace](https://huggingface.co/papers/2507.07484))| Thomas L. Griffiths, Dawn Song, Xuandong Zhao, Haimin Hu, Kaiqu Liang | This paper introduces a framework to systematically characterize and quantify "machine bullshit"—an LLM's indifference to truth—and demonstrates that common alignment techniques like RLHF exacerbate it. The primary objective is to formalize the concept of machine bullshit, measure its prevalence in LLMs, and empirically investigate how factors like RLHF, Chain-of-Thought prompting, and context influence its generation. The authors introduce the Bullshit Index (BI) to quantify truth-indifference by measuring the correlation between a model's internal beliefs and its explicit claims, and use an LLM-as-a-judge, validated by human studies, to classify four qualitative bullshit types across three benchmarks, including the novel BullshitEval dataset. The research found that RLHF significantly increases an LLM's indifference to truth, with the Bullshit Index rising from 0.379 to 0.665, and specifically amplifies harmful paltering, nearly doubling its negative impact on user utility as its regression coefficient changed from -0.49 to -0.89. The principal implication for AI practitioners is that standard alignment procedures like RLHF can inadvertently optimize for persuasive, truth-indifferent outputs over factual accuracy, necessitating the development of new training and evaluation methods that directly mitigate specific, harmful bullshit behaviors like paltering. |
| Beyond the Linear Separability Ceiling (Read more on [arXiv](https://arxiv.org/abs/2507.07574) or [HuggingFace](https://huggingface.co/papers/2507.07574))| Mohit Vaishnav, Tanel Tammet, envomp | This research introduces the Linear Separability Ceiling (LSC) to demonstrate that VLM abstract reasoning failures are primarily due to solvable alignment issues in reasoning pathways, not fundamental perception deficits. The primary objective is to diagnose whether the frequent failures of VLMs on abstract visual tasks originate from poor visual perception or flawed reasoning, and to identify effective interventions to resolve this bottleneck. The authors propose a diagnostic framework based on the Linear Separability Ceiling (LSC), defined as the accuracy of a nearest-centroid linear classifier on a VLM's initial visual embeddings. This LSC is benchmarked against the model's end-to-end generative accuracy, and various parameter-efficient fine-tuning (PEFT) methods like LoRA are used with standard and combined contrastive-generative objectives to surpass the LSC. The study finds a widespread "linear reasoning bottleneck" where most baseline VLMs fail to surpass their LSC. However, targeted fine-tuning successfully overcomes this; for example, LoRA with a combined loss function improved the Phi model's generative accuracy on the OpenWorld dataset to 96.2%, significantly surpassing its LSC of 84.2%. The paper also identifies a trade-off, where training objectives that explicitly improve representation quality can lead to structural brittleness and poor generalization to new prompt formats. AI practitioners can unlock significant dormant reasoning capabilities in VLMs by applying targeted fine-tuning to the language model's reasoning pathways, rather than solely focusing on improving the vision encoder. The LSC serves as a practical diagnostic to identify when reasoning, not perception, is the bottleneck, but engineers must be mindful of the trade-off between peak performance and structural robustness when selecting fine-tuning objectives. |
| SciMaster: Towards General-Purpose Scientific AI Agents, Part I.
  X-Master as Foundation: Can We Lead on Humanity's Last Exam? (Read more on [arXiv](https://arxiv.org/abs/2507.05241) or [HuggingFace](https://huggingface.co/papers/2507.05241))| Xinyu Zhu, Yuwen Du, Rui Ye, Shuo Tang, Jingyi Chai | This paper presents X-Masters, an inference-time agentic workflow enabling an open-source model to achieve state-of-the-art scientific reasoning. The primary objective is to develop and validate a foundational architecture for a general-purpose scientific agent capable of outperforming leading proprietary models on the Humanity's Last Exam (HLE) benchmark. The methodology leverages `X-Master`, a tool-augmented agent using Python code for flexible environmental interaction, within a "scattered-and-stacked" workflow that employs multiple agent instances (Solver, Critic, Rewriter, Selector) to systematically explore and refine solutions. The system achieves a new state-of-the-art accuracy of 32.1% on HLE, significantly surpassing leading research products from OpenAI (26.6%) and Google (26.9%). For practitioners, this demonstrates that complex, multi-agent inference-time computation can unlock state-of-the-art capabilities from accessible open-source LLMs on frontier benchmarks without requiring model retraining, offering a powerful paradigm for advanced problem-solving. |
