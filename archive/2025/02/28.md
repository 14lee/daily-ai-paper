

## Papers for 2025-02-28

| Title | Authors | Summary |
|-------|---------|---------|
| Self-rewarding correction for mathematical reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.19613) or [HuggingFace](https://huggingface.co/papers/2502.19613))| Nan Jiang, Chenlu Ye, Hanning Zhang, Wei Xiong, Lichang-Chen | This paper introduces a self-rewarding reasoning framework for large language models (LLMs) that enables autonomous error detection and correction in mathematical reasoning without external feedback. The main research question is whether LLMs can simultaneously generate reasoning steps, evaluate their correctness, and revise their outputs during inference without external reward models. The key methodology involves a two-staged training approach using self-generated data: sequential rejection sampling to create training trajectories, followed by reinforcement learning with rule-based signals. Primary results show that on the MATH500 benchmark, the self-rewarding IFT + PPO model achieves a final accuracy of 80.2%, outperforming intrinsic self-correction and comparable to systems using external reward models. For AI practitioners, this framework offers a way to improve LLM reasoning accuracy and reduce computational overhead by integrating generation and evaluation within a single model, streamlining deployment for mathematical reasoning tasks.  |
| MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.19634) or [HuggingFace](https://huggingface.co/papers/2502.19634))| Jiayuan Zhu, Fenglin Liu, Jiazhen Pan, morson, che111 | MedVLM-R1 is a medical vision-language model that uses reinforcement learning to generate explicit reasoning alongside answers for radiology visual question answering. The main research objective is to develop a medical VLM that generates natural language reasoning to improve transparency and trustworthiness, without relying on supervised fine-tuning (SFT). The key methodology is a reinforcement learning framework, specifically Group Relative Policy Optimization (GRPO), that incentivizes the model to discover human-interpretable reasoning paths without using reasoning references. The model, trained on 600 visual question answering samples, boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models. For AI practitioners, this implies that training smaller, specialized models with reinforcement learning can achieve superior, robust, and transparent generalization in the medical domain relative to supervised fine-tuning approaches.  |
| R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts (Read more on [arXiv](https://arxiv.org/abs/2502.20395) or [HuggingFace](https://huggingface.co/papers/2502.20395))| Ziyue Li, zhoutianyi, Lzy01241010 | R2-T2 introduces a test-time re-routing method for multimodal Mixture-of-Experts (MoE) models that improves performance without retraining. The main research objective is to optimize the routing weights of a multimodal MoE model during inference to improve performance on challenging or out-of-distribution samples. The key methodology is "Re-Routing in Test-Time (R2-T2)," which locally optimizes routing weights by moving them toward those of correctly predicted neighbor samples, using strategies like Neighborhood Gradient Descent (NGD), kernel regression, and mode finding. Applying R2-T2 with NGD to MoAI-7B improved MMBench accuracy by 6.9%, TextVQA accuracy by 6.8%, and achieved a 66.1-point increase on MME-P. AI practitioners can use R2-T2 to enhance the performance and generalization of multimodal MoE models on diverse tasks in test-time, without costly retraining or modification of model parameters.  |
| LongRoPE2: Near-Lossless LLM Context Window Scaling (Read more on [arXiv](https://arxiv.org/abs/2502.20082) or [HuggingFace](https://huggingface.co/papers/2502.20082))| Gilsinia Lopez, Gaokai Zhang, Li Lyna Zhang, Ning Shang, OldKingMeister | LongRoPE2 extends LLMs' effective context window while preserving short-context performance through RoPE rescaling and mixed context window training. The main research objective is to address the out-of-distribution (OOD) issues in rotary positional embeddings (RoPE) and the performance degradation on short-context tasks when extending the context window of pre-trained large language models (LLMs). The key methodology involves an evolutionary search for optimal RoPE rescaling factors guided by "needle-driven" perplexity, combined with a mixed context window training approach that uses both original and rescaled RoPE. Primary results show that LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B training tokens. Principal implication is that AI practitioners can extend LLM context windows to 128K with near-lossless performance on both long and original context window, significantly reducing the data, and training costs compare to prior methods.  |
| FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving (Read more on [arXiv](https://arxiv.org/abs/2502.20238) or [HuggingFace](https://huggingface.co/papers/2502.20238))| Chaoqun Liu, Hou Pong Chan, Hao Zhang, Weiwen Xu, Guizhen Chen | FINEREASON introduces a logic-puzzle benchmark to evaluate and improve LLMs' deliberate reasoning through state checking and transition tasks. The main research objective is to assess and enhance LLMs' ability to reflect and rectify mistakes during multi-step reasoning processes, going beyond final-answer accuracy. The key methodology involves decomposing logic puzzles into atomic steps and evaluating models on two tasks: state checking (assessing if a state can lead to a solution) and state transition (determining the next valid move). Primary results show that models trained with state checking and transition data demonstrated gains in math reasoning by up to 5.1% on GSM8K, when starting from the DeepSeek-R1-Distill-Qwen-7B model, the accuracy increased from 82.3% to 87.4%. The principal implication for AI practitioners is that training LLMs with structured, puzzle-based data focusing on intermediate reasoning steps can significantly improve their performance on general mathematical reasoning tasks.  |
| CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale (Read more on [arXiv](https://arxiv.org/abs/2502.16645) or [HuggingFace](https://huggingface.co/papers/2502.16645))| Kaiyue Qiu, Zhaoyang Chu, Chenlong Wang, yxy0807, zx10086 | CODESYNC introduces a data engine and benchmark to assess large language models' (LLMs) ability to adapt to evolving Python library APIs. The main research question is: Can LLMs be effectively and efficiently updated to handle real-time API modifications? CODESYNC systematically identifies API updates, retrieves relevant code instances from GitHub, and uses an LLM to synthesize contrastive code for legacy/updated API versions, then builds a benchmark,CODESYNCBENCH. Evaluation of 14 LLMs shows they struggle with API updates even with knowledge updating methods, e.g. a maximum BLEU score of 31.59 on the code completion task across five models with SFT. The principal implication is that AI practitioners need to develop and employ techniques to improve LLMs' ability to synchronize with evolving code, as static pre-training datasets limit handling of real-time API updates.  |
| Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance (Read more on [arXiv](https://arxiv.org/abs/2502.16944) or [HuggingFace](https://huggingface.co/papers/2502.16944))| Zhixu Li, Pu Zhao, Lu Wang, Chenghua Huang, keanudicap | DVPO decouples value and policy optimization in RLHF to improve training efficiency and stability for large language models. The main research objective is to address the computational complexity and instability of traditional PPO-based RLHF caused by joint actor-critic training. The key methodology is Decoupled Value Policy Optimization (DVPO), which pre-trains a Global Value Model (GVM) on policy trajectories and uses it as a fixed guide for policy optimization via a standard RL objective. Primary results show that DVPO reduces GPU memory usage by 40% and training time by 35% compared to conventional RLHF, while achieving comparable performance to state-of-the-art PPO. The principal implication is that AI practitioners can achieve more efficient and stable RLHF training by decoupling value estimation from policy updates, simplifying the alignment of LLMs with human preferences.  |
| UniTok: A Unified Tokenizer for Visual Generation and Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.20321) or [HuggingFace](https://huggingface.co/papers/2502.20321))| Xin Yu, Jihan Yang, Junfeng Wu, Yi Jiang, Chuofan Ma | UniTok is a unified visual tokenizer designed for both visual generation and understanding tasks, bridging the representation gap between these two domains. The main research objective is to investigate whether reconstruction and contrastive losses truly conflict in unified tokenizer training, and to identify any underlying bottlenecks. The key methodology is multi-codebook quantization, which divides visual tokens into chunks and discretizes each with independent sub-codebooks, alongside attention factorization. UniTok achieves a remarkable rFID of 0.38 and a zero-shot accuracy of 78.6% on ImageNet. The principal implication for AI practitioners is that a unified visual tokenizer, enhanced with multi-codebook quantization, can match or surpass domain-specific tokenizers, enabling more efficient and integrated multimodal model development.  |
| FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute (Read more on [arXiv](https://arxiv.org/abs/2502.20126) or [HuggingFace](https://huggingface.co/papers/2502.20126))| Markos Georgopoulos, Jonas Kohler, Yeongmin Kim, Gregor Bachmann, Sotiris Anagnostidis | FlexiDiT enables Diffusion Transformers (DiTs) to generate high-quality images with reduced computational cost by dynamically adjusting the compute budget per denoising step. The main research objective is to overcome the fixed and large compute requirements of standard DiTs during inference by revisiting the static compute allocation paradigm. The key methodology is converting pre-trained DiT models into flexible ones (FlexiDiTs) that can process inputs at varying compute budgets by dynamically adjusting patch size during the denoising process, and using different LoRAs for each sequence. The primary result is that FlexiDiT models can reduce FLOPs by more than 40% compared to static counterparts for class-conditioned and text-conditioned image generation, without any drop in quality. AI practitioners can deploy more computationally efficient diffusion models by adopting FlexiDiT, enabling substantial savings in computational resources without compromising the quality of generated outputs, especially for high-resolution image and video generation.  |
| Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think (Read more on [arXiv](https://arxiv.org/abs/2502.20172) or [HuggingFace](https://huggingface.co/papers/2502.20172))| Haozhe Zhao, Weichu Xie, Wenhao Chai, Shuai Bai, Liang Chen | DREAM ENGINE enables arbitrary text-image interleaved control for image generation by aligning large multimodal models (LMMs) with diffusion models. The research objective is to develop a framework that can generate images based on complex instructions interweaving text and visual elements from multiple images. The key methodology involves replacing the text encoders of a diffusion model (SD3.5) with an LMM (QwenVL) and a two-stage training paradigm: joint text-image alignment and multimodal interleaved instruction tuning. The primary results show that DREAM ENGINE achieves a 0.69 overall score on the GenEval benchmark, matching state-of-the-art text-to-image models. For AI practitioners, the principal implication is that LMMs can be directly integrated into diffusion models to enable advanced text-image control, simplifying the creation of complex, multi-image-influenced generation systems.  |
| NeoBERT: A Next-Generation BERT (Read more on [arXiv](https://arxiv.org/abs/2502.19587) or [HuggingFace](https://huggingface.co/papers/2502.19587))| Sarath Chandar, Mariam El Mezouar, Quentin Fournier, Lola Le Breton | NeoBERT, a new BERT-like encoder model, integrates architectural, data, and pre-training advancements to improve bidirectional representation learning. The primary objective is to create a next-generation BERT model that outperforms existing encoders by leveraging modern advancements in language model design. The key methodology involves pre-training on the RefinedWeb dataset with modifications like RoPE, SwiGLU, RMSNorm, a 20% masking rate, and a two-stage sequence length increase (1,024 to 4,096 tokens). NeoBERT achieves an 89.0 average score on the GLUE benchmark and 51.3 on the MTEB benchmark after contrastive fine-tuning, outperforming all similarly-sized and even larger, models on MTEB. AI practitioners can adopt NeoBERT as a plug-and-play replacement for existing base encoders to obtain better performance in downstream NLP tasks that depend on their embeddins, notably for retrieval-augmented generation and toxicity classification, without needing architectural modifications.  |
| Mobius: Text to Seamless Looping Video Generation via Latent Shift (Read more on [arXiv](https://arxiv.org/abs/2502.20307) or [HuggingFace](https://huggingface.co/papers/2502.20307))| Xiaodong Cun, Yong Zhang, Bo Liu, Jianfei Yuan, Xiuli Bi | Mobius is a training-free method to generate seamless looping videos from text descriptions using pre-trained video diffusion models. The main research objective is to develop a method for generating seamless looping videos directly from text prompts, without requiring user annotations or additional training. The key methodology involves constructing a latent cycle and performing multi-frame latent denoising by iteratively shifting the first-frame latent towards the end in each step, while also using a frame-invariant latent decoding method. Primary results show that the proposed method achieves an MSE of 25.43 between the first and last frame, FVD of 40.78, a CLIP score of 32.24, and a Motion Smoothness score of 0.9850. For AI practitioners, this method provides a way to directly repurpose pre-trained text-to-video diffusion models for generating seamless looping videos, without the need for large scale training or annotated dataset.  |
| SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.20127) or [HuggingFace](https://huggingface.co/papers/2502.20127))| Yanzhen Zou, Xiangxin Meng, Pengfei Gao, Chao Peng, mizersy | SoRFT is a novel training approach that enhances large language models' (LLMs) issue-resolving capabilities through subtask decomposition and reinforced fine-tuning. The main research objective is to improve the performance and generalization of open-source LLMs on software issue resolution tasks, addressing limitations of existing methods. The key methodology involves decomposing issue resolving into subtasks (file/function/line localization, code edit generation) and using rejection-sampled supervised fine-tuning followed by rule-based proximal policy optimization (PPO) with ground-truth-based rewards. The primary result is that SoRFT-Qwen-7B achieves 21.4% resolution rate on SWE-Bench Verified, outperforming other open-source models of similar size. For AI practitioners, SoRFT offers a cost-effective way to leverage open-source development resources and substantially boost the performance of open-source LLMs in automated issue resolution.  |
| Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2502.19459) or [HuggingFace](https://huggingface.co/papers/2502.19459))| Song-Chun Zhu, Junfeng Ni, Ruijie Lu, Baoxiong Jia, Yu Liu | ArtGS introduces a method for reconstructing and modeling complex articulated objects using 3D Gaussian Splatting. The main research objective is to effectively integrate information across different object states to improve part-mesh reconstruction and articulation parameter estimation, especially for multi-part articulated objects. The key methodology involves using canonical Gaussians with coarse-to-fine initialization and updates, alongside a skinning-inspired part dynamics modeling module. Primary results show that on the PARIS dataset, ArtGS achieves a mean angular error (Axis Ang.) of 0.01 degrees and a mean Chamfer Distance for movable parts (CD-m) of 0.03, outperforming existing methods. For AI practitioners, this implies a more efficient and accurate approach to creating digital twins of articulated objects, facilitating applications in robotics and virtual environments.  |
| R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning (Read more on [arXiv](https://arxiv.org/abs/2502.19735) or [HuggingFace](https://huggingface.co/papers/2502.19735))| Hongyong Zeng, Yuanchang Luo, Shimin Tao, Yilun Liu, boommmmm | R1-T1 is a novel framework that enhances machine translation (MT) in large language models (LLMs) through reinforcement learning (RL) with human-aligned chain-of-thoughts (CoTs). The main research objective is to improve the adaptability of LLMs to diverse translation scenarios by incorporating inference-time reasoning into general MT, going beyond specific sub-tasks. The key methodology involves formalizing six expert-curated CoT templates, reflecting human translation strategies, and using RL with KL-constrained rewards for self-evolving CoT discovery and anti-forgetting adaptation. Primary results demonstrate steady translation performance improvement across 21 languages and 80 translation directions on the Flores-101 test set, with a COMETScore of 0.626 on trained languages using RL, surpassing supervised fine-tuning (SFT) and other baselines. Principal implication for AI practioners: It provides a method for using RL to adapt LLMs to new machine translation tasks without relying on the SFT data and avoiding the Catastrophic Forgetting issue.  |
