

## Papers for 2025-02-12

| Title | Authors | Summary |
|-------|---------|---------|
| Competitive Programming with Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2502.06807) or [HuggingFace](https://huggingface.co/papers/2502.06807))| Borys Minaev, Andre Saraiva, Alexander Wei, Ahmed El-Kishky, OpenAI | Reinforcement learning significantly improves large language models' performance on complex coding and reasoning tasks. The main research question is how domain-specific, hand-engineered inference strategies compare to learned approaches in competitive programming. The key methodology involved fine-tuning large language models with reinforcement learning and comparing performance with and without hand-crafted test-time strategies. The primary result was that OpenAI's o3 model achieved a Codeforces rating of 2724 (99.8th percentile) and an IOI 2024 score of 395.64, surpassing a gold medal threshold without hand-engineered strategies. Scaling general-purpose reinforcement learning presents a robust method toward state-of-the-art AI in reasoning tasks like competitive programming.  |
| CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction (Read more on [arXiv](https://arxiv.org/abs/2502.07316) or [HuggingFace](https://huggingface.co/papers/2502.07316))| Yu Wu, Runxin Xu, Dejian Yang, Daya Guo, Junlong Li | CODEI/O systematically condenses diverse reasoning patterns in code for improved performance on reasoning tasks. The main research objective is to improve the performance of Large Language Models (LLMs) on a broad range of reasoning tasks by leveraging code-based training data. The key methodology involves transforming raw code files into an input-output prediction format and training LLMs to predict either the output given code and input, or feasible input given code and output, entirely in natural language as Chain-of-Thought rationales. Primary results demonstrate consistent improvements across 14 benchmarks spanning symbolic, scientific, logic, math & numerical, and commonsense reasoning, with CODEI/O++ achieving an average score improvement of 2.9 points, compared to single stage training on Qwen 2.5 Coder 7B. For AI practitioners, this implies that training on code input-output prediction tasks can enhance LLMs' general reasoning capabilities beyond code-specific applications.  |
| Magic 1-For-1: Generating One Minute Video Clips within One Minute (Read more on [arXiv](https://arxiv.org/abs/2502.07701) or [HuggingFace](https://huggingface.co/papers/2502.07701))| Qingyu Yin, Jiantong Zhao, Shitong Shao, Hongwei Yi, Owen777 | Magic 1-For-1 is an efficient video generation model that optimizes memory consumption and inference latency. The main objective is to reduce the computational cost and time required for text-to-video generation while maintaining high video quality. The key methodology involves factorizing the text-to-video task into text-to-image and image-to-video subtasks, alongside model convergence speedup, adversarial step distillation, and parameter sparsification. The primary results show the model can generate 5-second video clips within 3 seconds, and achieves an average score of 0.8134 on a customized VBench, outperforming other models. The principal implication for AI practitioners is that it offers an approach for generating minute-long videos within one minute, optimizing the tradeoff between computational cost and video quality for diffusion-based video generation.  |
| Teaching Language Models to Critique via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.03492) or [HuggingFace](https://huggingface.co/papers/2502.03492))| Jingjing Xu, Weichao Mao, Liyu Chen, Jie chen, Zhihui | CTRL trains large language models (LLMs) to provide effective feedback on code, improving iterative code generation. The main research objective is to develop a framework, CTRL, that trains a critic model to generate feedback that maximizes correction performance for a fixed generator model, without human supervision. The methodology uses a two-stage approach: supervised finetuning using execution feedback to synthesize critiques, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to optimize the critic. The results demonstrate that critics trained with CTRL significantly enhance pass rates, achieving up to 106.1% relative improvement on the CodeContests benchmark when using the same base model for generation and critique, and 23.5% improvement when paired with a better generator. For AI practitioners, CTRL provides a method to create specialized critics that can substantially improve code generation performance through effective, targeted feedback, enabling more autonomous AI systems.  |
| Expect the Unexpected: FailSafe Long Context QA for Finance (Read more on [arXiv](https://arxiv.org/abs/2502.06329) or [HuggingFace](https://huggingface.co/papers/2502.06329))| Mateusz Russak, Dmytro Mozolevskyi, Melisa Russak, muayad, kiranr | FailSafeQA, a new long-context financial benchmark, evaluates LLM robustness and context-awareness against variations in human-interface interactions.  i) This paper introduces FailSafeQA, a new benchmark for evaluating the robustness of Large Language Models (LLMs) in financial question-answering systems, particularly when dealing with long contexts and imperfect user inputs.  ii) The main research objective is to assess the resilience of LLMs against six variations in human-input interactions, such as query failure (misspelled, incomplete and out-of-domain) and context failure (degraded, irrelevant, and missing).  iii) The key methodology uses the LLM-as-a-Judge approach with Qwen2.5-72B-Instruct and defines fine-grained rating criteria to calculate Robustness, Context Grounding, and Compliance scores for 24 LLMs. The input consists of truncated 10k filings.  iv) The most robust model, OpenAI 03-mini, fabricated information in 41% of tested cases, while Palmyra-Fin-128k-Instruct, the most compliant model, failed robust predictions in 17% of test cases.  v) AI practitioners should be aware that high-performing LLMs still have significant room for improvement in terms of balancing robustness and context grounding. Practitioners must carefully assess the trade-off between a model's ability to handle imperfect inputs and its tendency to hallucinate.  |
| LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! (Read more on [arXiv](https://arxiv.org/abs/2502.07374) or [HuggingFace](https://huggingface.co/papers/2502.07374))| Xiangxi Mo, Shu Liu, Tyler Griggs, Shiyi Cao, Dacheng Li | Large language models (LLMs) can be efficiently fine-tuned to perform complex reasoning by learning the structural patterns of long chain-of-thought (CoT) demonstrations. The main research question is how to effectively elicit Long CoT reasoning capabilities in LLMs and what aspects of training data are most important. The key methodology involved supervised fine-tuning and low-rank adaptation (LoRA) on LLMs, with controlled experiments perturbing either the content or structure of Long CoT training samples. A primary result was that a Qwen2.5-32B-Instruct model achieved 56.7% accuracy on AIME 2024 after fine-tuning with only 17k Long CoT samples. AI practitioners can elicit strong reasoning performance in LLMs with relatively small, structurally sound datasets, without needing perfect accuracy in the content of individual reasoning steps.  |
| Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents (Read more on [arXiv](https://arxiv.org/abs/2502.04223) or [HuggingFace](https://huggingface.co/papers/2502.04223))| Lukas Voegtle, Ilia Karmanov, jseppanen, katerynaCh, amalad | ÉCLAIR, a multi-modal large language model (MLLM), extracts structured text, bounding boxes, and semantic classes from documents in integrated reading order. The main research objective is to develop a general-purpose text-extraction tool capable of processing diverse document types and extracting formatted text, spatial information, and semantic class labels simultaneously. The key methodology involves a transformer encoder-decoder architecture with a ViT-like encoder and an autoregressive decoder, pre-trained on a newly generated arXiv-5M dataset and fine-tuned on diverse public datasets. The primary results include achieving state-of-the-art accuracy on the new DROBS benchmark with a 0.937 Counting F1 score and outperforming other methods on established benchmarks. The principal implication for AI practitioners is that ÉCLAIR provides a new model for document OCR, enabling the extraction of more structured data from documents.  |
| CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing (Read more on [arXiv](https://arxiv.org/abs/2502.03997) or [HuggingFace](https://huggingface.co/papers/2502.03997))| Jiang Bian, Qi Liu, Yu Yuan, ShizhaoSun | CAD-Editor is a framework for automatically modifying CAD models based on textual instructions, using an automated data synthesis pipeline and a locate-then-infill approach. The main research objective is to develop a system for text-based editing of CAD models, addressing the lack of support for text-based control in existing design variation methods and the absence of consideration for existing CAD models as constraints. The methodology involves generating synthetic training data using design variation models and LVLMs and decomposing the task into locating regions for modification and infilling those regions with LLMs. Primary results show that CAD-Editor achieves a 95.6% Valid Ratio and a 0.27 Directional CLIP Score, outperforming baseline methods in generation validity, text-CAD alignment, and overall quality. AI practitioners can leverage the proposed framework and data synthesis pipeline to enable more intuitive and efficient CAD model editing through natural language instructions, accelerating the design workflow.  |
| Enhance-A-Video: Better Generated Video for Free (Read more on [arXiv](https://arxiv.org/abs/2502.07508) or [HuggingFace](https://huggingface.co/papers/2502.07508))| Wenqi Shao, Kaipeng Zhang, Mengzhao Chen, Xuanlei Zhao, Yang Luo | Enhance-A-Video is a training-free method to improve the temporal consistency and visual quality of diffusion transformer (DiT)-based video generation. The main research objective is to develop a method to enhance the coherence and quality of DiT-based generated videos without retraining or fine-tuning. The key methodology involves introducing a "Enhance Block" that calculates a Cross-Frame Intensity (CFI) from temporal attention maps and uses an "enhance temperature" parameter to scale and integrate this CFI, thereby strengthening cross-frame correlations. User studies demonstrated that models incorporating Enhance-A-Video were preferred across metrics including temporal consistency, prompt-video consistency, and overall visual quality, and VBench scores consistently improved across all tested models. AI practitioners can integrate this plug-and-play method into existing DiT-based video generation frameworks to improve video quality at minimal computational cost, without any retraining or fine tuning of models.  |
| NatureLM: Deciphering the Language of Nature for Scientific Discovery (Read more on [arXiv](https://arxiv.org/abs/2502.07527) or [HuggingFace](https://huggingface.co/papers/2502.07527))| Chuan Cao, Liang He, Shufang Xie, Peiran Jin, Yingce Xia | NatureLM is a sequence-based science foundation model designed for scientific discovery across multiple domains.  Main research question or objective: To develop a unified, versatile model capable of handling various scientific applications, including generation and optimization, across multiple scientific domains using a sequence-based approach.  Key methodology used: A Transformer decoder architecture pre-trained on 143 billion tokens from multiple scientific domains (small molecules, proteins, DNA, RNA, materials, and text), followed by post-training with instruction-response pairs.  Primary results: NatureLM (8x7B) achieved state-of-the-art performance in retrosynthesis (71.9% top-1 accuracy on USPTO-50K) and SMILES-to-IUPAC translation (0.607 top-5 accuracy), significantly outperforming general-purpose foundation models.  Principal implication for AI practitioners: Practitioners can utilize NatureLM as a foundation model for diverse scientific tasks, particularly where cross-domain interactions and sequence-based representations are crucial, potentially accelerating scientific discovery through a generalist model approach.  |
| Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2502.06589) or [HuggingFace](https://huggingface.co/papers/2502.06589))| Kewei Cheng, Xin Liu, Haoming Jiang, Jingfeng Yang, yczhuang | Hephaestus introduces a continual pre-training method to enhance the fundamental capabilities of LLM-based agents.  Main research question or objective: How can continual pre-training on a large-scale, agent-oriented corpus improve the API function calling, intrinsic reasoning, and environmental feedback adaptation capabilities of large language models?  Key methodology used: A two-stage continual pre-training framework on the Hephaestus-Forge corpus (103B tokens, 76,537 APIs), leveraging scaling law experiments to optimize data mixing ratios, followed by instruction fine-tuning.  Primary results: Hephaestus-8B outperforms LLAMA-3-8B by 9.6% and rivals commercial LLMs on three agent benchmarks, achieves comparable performance with GPT-3.5-turbo, excelling particularly in complex multi-turn tasks (BFCL-v3).  Principal implication for AI practitioners: Continual pre-training with a well-curated, agent-specific corpus like Hephaestus-Forge can significantly enhance fundamental agent capabilities of open-source LLMs, bridging the performance gap with commercial models and providing a more robust and generalizable foundation for LLM-based agent development.  |
| Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon (Read more on [arXiv](https://arxiv.org/abs/2502.07445) or [HuggingFace](https://huggingface.co/papers/2502.07445))| Seffi Cohen, Lior Rokach, Bracha Shapira, Yehonatan Elisha, Nurit Cohen-Inger | This paper introduces a meta-evaluation framework, Chameleon Benchmark Overfit Detector (C-BOD), to detect overfitting in Large Language Models (LLMs) on benchmark datasets. The central research question is whether LLMs over-rely on benchmark-specific cues, exhibiting surface-level performance rather than true language understanding. The methodology involves systematically perturbing benchmark prompts using a parametric transformation (controlled by parameter µ) and assessing performance changes with statistical significance tests (McNemar's test). A primary result is that 20 out of 26 tested LLMs showed statistically significant performance degradation on the MMLU benchmark under modest perturbations, with an average accuracy drop of 2.15%. AI practitioners should integrate C-BOD's perturbation methods into evaluation pipelines to ensure robust generalization and mitigate superficial memorization in LLMs, prioritizing model resilience over high scores on fixed benchmarks.  |
| VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.07531) or [HuggingFace](https://huggingface.co/papers/2502.07531))| Hang Xu, Yi Zhu, Yanpeng Zhou, Zimian Peng, Sixiao Zheng | VidCRAFT3 is a novel image-to-video generation framework enabling precise control over camera motion, object motion, and lighting direction. The main research objective is to develop a model that can simultaneously control multiple visual elements (camera motion, object motion, and lighting) in image-to-video generation, overcoming the limitations of existing methods. The key methodology involves a Spatial Triple-Attention Transformer integrating lighting, text, and image features, along with 3D point cloud rendering and trajectory-based motion encoding, and using a three-stage training process. Primary results show the model achieves a CamMC score of 4.07 on the RealEstate10K dataset, outperforming existing methods like CameraCtrl, CamI2V and MotionCtrl. The principal implication is that AI practitioners can use VidCRAFT3 to create high-quality videos with fine-grained and disentangled control over multiple aspects.  |
| Retrieval-augmented Large Language Models for Financial Time Series Forecasting (Read more on [arXiv](https://arxiv.org/abs/2502.05878) or [HuggingFace](https://huggingface.co/papers/2502.05878))| Yueru He, Zhengyu Chen, Lingfei Qian, Zihao Jiang, Mengxi Xiao | This paper introduces a retrieval-augmented generation (RAG) framework, FinSeer, for financial time-series forecasting, specifically stock movement prediction. The main research objective is to develop a RAG framework that effectively integrates financial time-series data with large language models (LLMs) to improve stock movement prediction accuracy. The key methodology involves a fine-tuned 1B parameter LLM (StockLLM), a novel candidate selection method using LLM feedback, and a training objective maximizing similarity between queries and historically significant sequences. The RAG framework with FinSeer achieved an 8% higher accuracy on the BIGDATA22 benchmark compared to a general-purpose LLM-feedback-based retriever. For AI practitioners, this framework demonstrates the importance of using dedicated retrieval models designed to process and filter financial time-series data, to improve the performance of the LLMs in financial forecasting tasks.  |
| Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More (Read more on [arXiv](https://arxiv.org/abs/2502.07490) or [HuggingFace](https://huggingface.co/papers/2502.07490))| Li Shen, Zhenyu Zhang, Jianjin Li, Zhikai Jia, Xialie Zhuang | Mask-Enhanced Autoregressive Prediction (MEAP) integrates masked language modeling into next-token prediction to improve large language models' in-context retrieval capabilities without extra computational cost. The main research objective is to enhance LLMs' ability to retrieve key information and perform long-context reasoning without compromising their fundamental language modeling capabilities. MEAP randomly masks a fraction of input tokens and then performs standard next-token prediction using a decoder-only Transformer. In pre-training, MEAP outperformed NTP on the Needle in a Haystack evaluation by 11% on average using 140B less training token. This demonstrates MEAP's superior performance in key information retrieval tasks, and thus provides AI practitioners with a more data- and compute-efficient training paradigm for large language models.  |
| FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks (Read more on [arXiv](https://arxiv.org/abs/2502.04465) or [HuggingFace](https://huggingface.co/papers/2502.04465))| Mirco Ravanelli, Cem Subakan, Francesco Paissan, lucadellalib | FocalCodec is a low-bitrate speech codec based on focal modulation that uses a single binary codebook for compression. The research objective is to develop a speech codec that achieves high compression rates while preserving both semantic and acoustic information for downstream tasks. The key methodology involves a compressor-quantizer-decompressor architecture utilizing focal modulation, binary spherical quantization (BSQ), and a pretrained self-supervised encoder (WavLM). Primary results show that FocalCodec@50 achieves a dWER of 2.18 on the LibriSpeech test-clean set, outperforming several baselines at comparable bitrates. AI practitioners can use FocalCodec as an efficient and low-bitrate option that can be deployed to preserve sufficient semantic and acoustic information for downstream tasks, such as speech resynthesis, voice conversion, or speech enhancement model development.  |
| Auditing Prompt Caching in Language Model APIs (Read more on [arXiv](https://arxiv.org/abs/2502.07776) or [HuggingFace](https://huggingface.co/papers/2502.07776))| Percy Liang, Rohith Kuditipudi, Xiang Lisa Li, Chenchen Gu, thashim | Prompt caching in large language model APIs can leak private and proprietary information through timing differences, which can be detected by auditing. The main research objective was to develop and conduct statistical audits to detect prompt caching and determine the level of cache sharing (per-user, per-organization, or global) in real-world LLM API providers. The key methodology was using statistical hypothesis testing on response times from two procedures: one to generate cache hits, and one to generate cache misses, analyzing differences using the two-sample Kolmogorov-Smirnov test. The primary results revealed that prompt caching was detected in 8 out of 17 API providers, with 7 exhibiting global cache sharing across users, where it was detected with an average precision of around 0.8. AI practitioners should be aware of prompt caching implementation details and cache-sharing levels in LLM APIs to mitigate potential privacy leakage, since the caching can be identified from timing data.  |
| Gemstones: A Model Suite for Multi-Faceted Scaling Laws (Read more on [arXiv](https://arxiv.org/abs/2502.06857) or [HuggingFace](https://huggingface.co/papers/2502.06857))| Abhinav Bhatele, Siddharth Singh, David Yu Miller, John Kirchenbauer, smcleish | Gemstones provides a dataset of over 4000 transformer checkpoints to study scaling laws across various architectural and training hyperparameters. The main research question is how model design (width, depth) and model selection impact scaling law parameters and interpretations. The key methodology involves training transformers, up to 2 billion parameters, with diverse widths, depths, learning rates, and cooldown schedules, then fitting and analyzing scaling laws on this data. The primary results show scaling law prescriptions are highly sensitive to model selection and fitting procedures; for example, the optimal tokens-per-parameter ratio is slightly higher than that proposed in previous works. The principal implication for AI practitioners is that scaling laws should be approached with awareness for fragility, with a recommendation to err on wider and, surprisingly, over-trained models, especially when considering time optimality.  |
| Skill Expansion and Composition in Parameter Space (Read more on [arXiv](https://arxiv.org/abs/2502.05932) or [HuggingFace](https://huggingface.co/papers/2502.05932))| Yixing Lan, Haoyi Niu, Yinan Zheng, Jianxiong Li, LTL07 | i) The paper introduces Parametric Skill Expansion and Composition (PSEC), a framework for iteratively expanding agent capabilities. ii) The research aims to develop an autonomous agent that can efficiently acquire new skills by leveraging prior knowledge and dynamically composing existing skills. iii) PSEC employs parameter-efficient finetuning using Low-Rank Adaptation (LoRA) modules for skill expansion and a context-aware module for skill composition in parameter space. iv) Experiments on D4RL show PSEC demonstrates the superior capacity to efficiently tackle new challenges. v) PSEC provides AI practitioners with a method for continual learning and efficient skill transfer in reinforcement learning agents, mitigating catastrophic forgetting through parameter isolation.  |
