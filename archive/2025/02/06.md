

## Papers for 2025-02-06

| Title | Authors | Summary |
|-------|---------|---------|
| SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.02737) or [HuggingFace](https://huggingface.co/papers/2502.02737))| Gabriel Martín Blázquez, Elie Bakouch, Anton Lozhkov, Loubna Ben Allal, lvwerra | SmolLM2 is a 1.7 billion parameter language model trained on 11 trillion tokens to achieve state-of-the-art performance among small language models. The main research objective was to develop a performant small language model (SmolLM2) through a data-centric approach, optimizing for resource-constrained settings. The key methodology involved multi-stage training with a curated dataset mixing web text, code, math data, and instruction-following data, including newly created datasets (FineMath, Stack-Edu, SmolTalk) and manual refinement of mixing rates. A primary result is that SmolLM2 outperforms other small LMs like Qwen2.5-1.5B and Llama3.2-1B on several benchmarks; for instance achieving a score of 68.7 on HellaSwag compared to 66.4 by Qwen. AI practitioners can leverage the released SmolLM2 model and associated datasets to deploy or further research efficient, high-performing small LMs, particularly beneficial in settings with limited computational resources.  |
| TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets (Read more on [arXiv](https://arxiv.org/abs/2502.01506) or [HuggingFace](https://huggingface.co/papers/2502.01506))| Yunmiao Zhang, Kaidi Zhang, Minghao Wu, Yifei Zhang, Yuzhe Yang | TwinMarket, a multi-agent framework leveraging large language models (LLMs), simulates investor behavior and socio-economic dynamics in a stock market environment. The main research objective is to examine how individual behaviors, through interactions and feedback mechanisms in a simulated stock market, give rise to collective dynamics and emergent phenomena such as financial bubbles. The key methodology involves using LLMs within a Belief-Desire-Intention (BDI) framework to structure agent cognitive processes, coupled with a simulated social network for information exchange and social influence. Primary results show that in a 100-agent simulation, the model replicates stylized facts of financial markets, and rumor-exposed markets experienced a 2.02x increase in Sell/Buy ratio compared to the baseline, indicating amplified panic-driven selling behavior. Principal implication for AI practitioners: simulating human financial behavior by leveraging BDI framework to structure the cognitive process of agents can better predict market behavior under stress.  |
| Demystifying Long Chain-of-Thought Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2502.03373) or [HuggingFace](https://huggingface.co/papers/2502.03373))| Xiang Yue, Graham Neubig, Morry Niu, Yuxuan Tong, Edward Yeo | This paper investigates the mechanics of long chain-of-thought (CoT) reasoning in large language models (LLMs) and identifies key factors influencing its generation and stability. The main research question is what factors enable LLMs to generate long CoT trajectories and how can their emergence be stabilized? The key methodology involves extensive supervised fine-tuning (SFT) and reinforcement learning (RL) experiments, including ablations on reward design and data composition. A primary result is that RL can improve long CoT SFT models by over 3% absolute accuracy on the MATH-500 benchmark, whereas short CoT SFT models showed minimal improvement. The principle implication for AI practitioners is that reward shaping, particularly using a cosine length-scaling reward with a repetition penalty, and scaling verifiable reward signals using a mix of gold and silver supervision data, are crucial for stabilizing long CoT growth and enhancing performance.  |
| LIMO: Less is More for Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.03387) or [HuggingFace](https://huggingface.co/papers/2502.03387))| Shijie Xia, Ethan Chern, Yang Xiao, Zhen Huang, Yixin Ye | LIMO demonstrates that large language models can achieve strong mathematical reasoning with surprisingly few, high-quality training examples. The main research question is whether minimal but precisely orchestrated demonstrations of cognitive processes can elicit sophisticated reasoning in foundation models with comprehensive domain knowledge. The key methodology involves curating a small, high-quality dataset (817 samples) of mathematical problems and solutions, and fine-tuning a pre-trained Qwen2.5-32B-Instruct model. The primary result is that LIMO achieves 57.1% accuracy on the AIME benchmark and 94.8% on MATH, significantly outperforming models trained on much larger datasets. The principal implication for AI practitioners is that focusing on the quality of reasoning demonstrations, rather than sheer data volume, is a more effective approach for developing robust reasoning capabilities in LLMs.  |
| Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking (Read more on [arXiv](https://arxiv.org/abs/2502.02339) or [HuggingFace](https://huggingface.co/papers/2502.02339))| Feihu Che, Ruihan Jin, Shuai Zhang, Mingkuan Feng, Jinyang Wu | AStar, an automated structured thinking paradigm, enhances multimodal reasoning in large language models via Monte Carlo Tree Search (MCTS). The main research objective is to address the limitations of existing multimodal large language models (MLLMs) in complex visual reasoning, balancing performance and efficiency. The key methodology involves automatically deriving high-level cognitive reasoning patterns using MCTS-powered hierarchical structures, then integrating these patterns into a unified reasoning framework. The primary result is that AStar achieves a 54.0% accuracy on the MathVerse benchmark with a 7B backbone, surpassing GPT-4O (50.2%). For AI practitioners, AStar provides an effective way to boost MLLMs reasoning performance by leveraging structured patterns derived through the use of MCTS, which in turn, enhance the capability in solving complex problems that require structured thinking.  |
| A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods (Read more on [arXiv](https://arxiv.org/abs/2502.01618) or [HuggingFace](https://huggingface.co/papers/2502.01618))| Akash Srivastava, Kai Xu, Guangxuan Xu, Shivchander Sudalairaj, ishapuri-mit | This paper introduces a probabilistic inference framework for scaling large language models (LLMs) at inference time using particle-based Monte Carlo methods. The main research objective is to develop a more robust inference-time scaling approach that is less susceptible to reward hacking compared to existing search-based methods. The key methodology is casting inference-time scaling as probabilistic inference over a state-space model and applying particle filtering to estimate the latent states, leveraging a language model and a process reward model. The primary result is that the proposed method achieves a 4-16x faster scaling rate than deterministic search counterparts on mathematical reasoning tasks, enabling Qwen2.5-Math-1.5B-Instruct to surpass GPT-40 accuracy with only 4 rollouts. The principal implication for AI practioners is that they can leverage this probabilistic inference approach for more efficient and robust inference-time scaling of LLMs, particularly in domains with imperfect reward models, achieving better performance with smaller models and limited compute budgets.  |
| Jailbreaking with Universal Multi-Prompts (Read more on [arXiv](https://arxiv.org/abs/2502.01154) or [HuggingFace](https://huggingface.co/papers/2502.01154))| Shang-Tse Chen, Hsuan Su, Yu-Ling Hsu | JUMP, a prompt-based method, jailbreaks Large Language Models (LLMs) using optimized universal multi-prompts and can also be adapted for defense. The main research objective is to optimize a universal attacker to achieve the best attack results on a set of malicious instructions, outperforming existing techniques. The methodology involves a prompt-based framework named JUMP, decomposing the training pipeline into Selector, Mutator, Constraints, and Evaluator stages, using an additional model as an attacker to generate adversarial suffixes through beam search. Primary results include JUMP++ achieving an Attack Success Rate (ASR@10) of 64.4% on Llama2-7b, significantly outperforming several baselines including AdvPrompter in the universal attack setting. Principal implication is to guide practitioners to use JUMP for a more efficient, high-performing method for jailbreaking and defending LLMs by optimizing universal multi-prompts, reducing computational costs when dealing with massive data.  |
| LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2502.01105) or [HuggingFace](https://huggingface.co/papers/2502.01105))| Danze Chen, Yiren Song, mikeshou | LayerTracer is a diffusion transformer-based framework for generating layered Scalable Vector Graphics (SVGs) from text or images, mimicking professional design processes. The main research objective is to generate cognitive-aligned, editable layered SVGs that meet professional design standards, overcoming limitations of existing methods. The key methodology involves a dual-phase approach: first, a text-conditioned DiT generates multi-phase rasterized blueprints; second, layer-wise vectorization with path deduplication creates editable SVGs. In the SVG generation task, LayerTracer achieves the highest CLIP-Score of 33.76 with the lowest average number of paths (35.39) and shortest time cost (27s) relative to baselines such as VectorFusion and SVGDreamer. For AI practitioners, LayerTracer provides a novel approach and dataset for generating high-quality, editable layered SVGs, directly aligning AI-generated vectors with professional design cognition.  |
| Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.03275) or [HuggingFace](https://huggingface.co/papers/2502.03275))| Yuandong Tian, Jiantao Jiao, Yingchen Xu, Hanlin Zhu, DiJia Su | This paper proposes a method to improve language model reasoning by mixing latent and text tokens in the reasoning trace. The main research question is whether representing initial reasoning steps with discrete latent tokens, while retaining later steps as text, can improve reasoning performance and efficiency in Large Language Models (LLMs). The key methodology involves training a VQ-VAE to convert text tokens into latent codes, then fine-tuning LLMs on reasoning traces where initial text tokens are replaced by these codes, using a randomized replacement strategy. The primary result is that the proposed approach outperforms baseline methods on various benchmarks, such as GSM8K (+4.1% accuracy with Llama-3.2-3B) and an average reduction of 17% of reasoning trace length. The principal implication for AI practioners is that using a mixed representation of latent and text tokens during reasoning trace training can lead to improved accuracy and efficiency compared to using text-only reasoning traces.  |
| On Teacher Hacking in Language Model Distillation (Read more on [arXiv](https://arxiv.org/abs/2502.02671) or [HuggingFace](https://huggingface.co/papers/2502.02671))| Nino Vieillard, Sarah Perrin, Johan Ferret, Daniele Calandriello, Daniil Tiapkin | Language model distillation can exhibit "teacher hacking," where a student model exploits imperfections in the teacher instead of approximating the true data distribution. The main research question is whether teacher hacking occurs during knowledge distillation in language models, and if so, when and how it can be mitigated. A controlled experimental setup is used, involving an oracle (ground-truth) language model, a teacher model distilled from the oracle, and a student model distilled from the teacher. Results show that teacher hacking occurs when using a fixed offline dataset for distillation, observable when optimization deviates from polynomial convergence laws; for example KL divergence between student and teacher decreases, but divergence from Oracle increases. The implication for AI practioners is to utilize online data generation, prioritize prompt diversity, or increase generation budget to mitigate teacher hacking during language model distillation.  |
