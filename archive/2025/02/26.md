

## Papers for 2025-02-26

| Title | Authors | Summary |
|-------|---------|---------|
| OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference (Read more on [arXiv](https://arxiv.org/abs/2502.18411) or [HuggingFace](https://huggingface.co/papers/2502.18411))| Jiaqiwang, Weiyun1025, UniverseCA, ChrisDing1105, PhoenixZ | OmniAlign-V introduces a new dataset and benchmark to improve the alignment of multi-modal large language models (MLLMs) with human preferences. The main research objective is to address the gap in human preference alignment observed in existing open-source MLLMs, despite their strong performance on foundational capability benchmarks. The key methodology involves constructing OmniAlign-V, a dataset of ~200K high-quality training samples with diverse images and complex question-answer pairs, and MM-AlignBench, a human-annotated benchmark for evaluating MLLM alignment. Finetuning MLLMs with OmniAlign-V via Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) improved the win rate against Qwen2VL-72B on MM-AlignBench, achieving a 72.6 win rate. The principal implication is that AI practitioners should utilize curated, human-aligned multi-modal datasets like OmniAlign-V during SFT and DPO to significantly enhance the human preference alignment of MLLMs while maintaining or enhancing fundamental capabilities.  |
| SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference (Read more on [arXiv](https://arxiv.org/abs/2502.18137) or [HuggingFace](https://huggingface.co/papers/2502.18137))| Haofeng Huang, surfingtomchen, hxi0408, Xiang-cd, jt-zhang | SpargeAttn is a universal sparse and quantized attention mechanism designed to accelerate inference in various AI models. The paper's main objective is to design a training-free sparse attention operator that accelerates all models without metric loss. The key methodology involves a two-stage online filter that predicts sparse blocks in the attention map using selective token compression and a sparse warp online softmax, integrated with 8-bit quantization. SpargeAttn achieved a 1.83x speedup on Mochi on an L40 GPU without loss of video quality and is 2.5x to 5x faster than existing dense/sparse attention models. AI practitioners can use SpargeAttn to significantly accelerate the inference of diverse models, including language, image, and video generation, without sacrificing end-to-end performance metrics.  |
| KV-Edit: Training-Free Image Editing for Precise Background Preservation (Read more on [arXiv](https://arxiv.org/abs/2502.17363) or [HuggingFace](https://huggingface.co/papers/2502.17363))| Yansong Tang, jewelshaw, shiyi0408, xilluill | KV-Edit is a training-free image editing method that achieves precise background preservation by utilizing KV cache in diffusion models. The main research objective is to address the challenge of maintaining background consistency during image editing tasks while generating content aligned with modified text prompts. The key methodology involves caching and reusing key-value pairs of background tokens in Diffusion Transformers (DiTs) during the inversion and denoising processes, and optional mask-guided inversion and reinitialization strategies. Primary results show that KV-Edit achieves a PSNR of 35.87 in masked region preservation, outperforming existing methods. For AI practitioners, this method provides a way to perform image editing with perfect background preservation, without additional training or complex mechanisms, thereby facilitating more practical AI image editing applications.  |
| ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation (Read more on [arXiv](https://arxiv.org/abs/2502.18364) or [HuggingFace](https://huggingface.co/papers/2502.18364))| JianminBao, DongChen06, 131131yhx, 2JZ, yifanpu001 | This paper introduces the Anonymous Region Transformer (ART) for generating variable multi-layer transparent images from a global text prompt and an anonymous region layout. The main research objective is to develop a method for generating high-quality, multi-layer transparent images that overcomes the limitations of existing methods requiring detailed semantic layouts. The key methodology involves using an anonymous region layout, a layer-wise region crop mechanism, and a multi-layer transparent image autoencoder. The method achieves a speed improvement of over 12 times compared to the full attention approach, and user studies show it outperforms existing methods (LayerDiffuse and COLE) in multiple aspects. The principal implication is that AI practitioners can generate multi-layer images more efficiently and with greater scalability, allowing for more precise control in interactive content creation and editing of individual elements within generative models.  |
| SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution (Read more on [arXiv](https://arxiv.org/abs/2502.18449) or [HuggingFace](https://huggingface.co/papers/2502.18449))| RishabhSingh021, gsynnaeve, lingming, JadeCopet, yuxiang630 | SWE-RL is a reinforcement learning approach that enhances LLM reasoning for software engineering tasks using open-source software evolution data. The main research objective is to improve LLMs' performance on real-world software engineering tasks, specifically issue resolution, using reinforcement learning. The key methodology is training LLMs on GitHub pull request data with a rule-based reward function based on the similarity between predicted and oracle code patches, optimized via Group Relative Policy Optimization (GRPO). The primary result is that Llama3-SWE-RL-70B achieves a 41.0% solve rate on the SWE-bench Verified dataset. The principal implication for AI practitioners is that reinforcement learning on software evolution data can significantly enhance LLM reasoning capabilities for software engineering and also improve performance on out-of-domain tasks.  |
| Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective (Read more on [arXiv](https://arxiv.org/abs/2502.17262) or [HuggingFace](https://huggingface.co/papers/2502.17262))| Chenggang Li, Xiao Li, shenke18, Lucky2022, JerryXu98 | The paper introduces a Clustering-On-Difficulty (COD) framework to predict downstream task performance of Large Language Models (LLMs). The main research objective is to accurately predict LLM performance on downstream tasks prior to extensive model training, addressing the challenges of emergent abilities and uneven task difficulty distributions. The key methodology involves clustering tasks based on difficulty features, fitting performance-compute curves on predictable clusters, and mapping these predictions to the full evaluation set. The primary result is that COD achieves a mean absolute prediction error of 1.36% across eight LLM evaluation benchmarks on a 70B-parameter model. The principal implication is that AI practitioners can use COD for efficient resource allocation and monitoring during LLM training, by reliably predicting downstream task performance using smaller models.  |
| Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.15499) or [HuggingFace](https://huggingface.co/papers/2502.15499))| Ya Wang, LLIXQ, xunzhou, Taoer, BryceZhuo | Scale-Distribution Decoupling (SDD) is a novel approach that stabilizes and improves the training of large language models by separating the scale and distribution of weight matrices. The main research objective is to address training instability issues, such as gradient explosion and vanishing gradients, in large language models (LLMs), particularly in Post-Norm Transformer architectures. SDD uses a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients in fully-connected layers. SDD-1B achieves a training loss of 2.65, outperforming OLMo2-1B (2.70), PostNorm-1B (2.69), and DeepNorm-1B (2.72), also achieving the highest average accuracy of 54.04% across multiple downstream tasks. For AI practitioners, SDD provides a lightweight and compatible solution for stabilizing LLM training, improving convergence, and enabling more efficient large-scale pre-training.  |
| K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs (Read more on [arXiv](https://arxiv.org/abs/2502.18461) or [HuggingFace](https://huggingface.co/papers/2502.18461))| Qibin Hou, Zhen Li, oyzh2005 | K-LoRA is a training-free method for merging subject and style LoRAs to generate images that preserve both characteristics. The paper's objective is to develop a method for effectively combining content and style LoRAs without requiring additional training or manual parameter tuning. The key methodology is a Top-K selection process within attention layers that identifies and selects the most representative features from each LoRA for fusion, combined with a scaling factor that prioritizes content or style at different diffusion timesteps. The method achieved a CLIP score of 69.4% and a DINO score of 46.9% for subject similarity, outperforming existing methods. AI practitioners can use K-LoRA to effectively fuse separately trained subject and style LoRAs, enabling efficient customized image generation without retraining, simplifying the process of generating images with specific content and styles.  |
| WebGames: Challenging General-Purpose Web-Browsing AI Agents (Read more on [arXiv](https://arxiv.org/abs/2502.18356) or [HuggingFace](https://huggingface.co/papers/2502.18356))| Fraser, semitable, BiggieW, XanderJC, georgethomas | WebGames introduces a benchmark suite for evaluating general-purpose web-browsing AI agents. The primary objective is to assess AI limitations in web interactions using 50+ interactive challenges designed to be human-intuitive yet AI-challenging. The methodology involves evaluating vision-language models like GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL in a hermetic, client-side environment, measuring their success against human baselines. The best AI system achieved a 41.2% success rate compared to 95.7% human performance, revealing a substantial capability gap. This highlights the need for improvements in AI's ability to handle common web interaction patterns, thereby directing future development efforts for web-browsing agents by AI practitioners.  |
| Introducing Visual Perception Token into Multimodal Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.17425) or [HuggingFace](https://huggingface.co/papers/2502.17425))| wxcTest, horseee, rp-yu | This paper introduces Visual Perception Tokens to enhance Multimodal Large Language Models' (MLLMs) control over visual perception processes. The main research objective is to enable MLLMs to autonomously control their visual perception, such as selecting specific image regions or refining features. The key methodology involves designing two types of Visual Perception Tokens (Region Selection and Vision Re-Encoding) that MLLMs generate and use to trigger additional visual processing steps. Results show that adding Visual Perception Tokens to a 2B parameter model improves its average performance across various VQA tasks by 30.9%, achieving a score of 0.749 compared to 0.572 without the tokens. AI practitioners can utilize these tokens to improve MLLMs' performance in tasks requiring fine-grained visual understanding and spatial reasoning, by giving models a mechanism to actively control their visual input.  |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? (Read more on [arXiv](https://arxiv.org/abs/2502.17535) or [HuggingFace](https://huggingface.co/papers/2502.17535))| Peijie Dong, Qian Wang, Xiang Liu, wenxinsiju, coolzhtang | This paper proposes a "lottery LLM hypothesis" suggesting that smaller, compressed large language models (LLMs) can achieve comparable performance to original LLMs using external tools and reasoning. The main research objective is to identify the essential capabilities that compressed LLMs and key-value (KV) cache compression methods should preserve to maintain performance. The methodology involves a review of recent LLM advancements (retrieval-augmented generation, external tools, multi-step reasoning, computational expressivity) and proposes a recursive multi-step reasoning algorithm (Algorithm 1) for the "lottery LLM". Primary results include showing that retrieval augmented generation can provide a compressed model equivalent performance. For instance Table 2 shows that Llama-3-Ins8B with RAG achieves a 59.8 accuracy score in the PopQA. The principal implication for AI practitioners is to focus on preserving specific abilities, like retrieval from prompts and long-context reasoning when developing LLM compression techniques, rather than solely focusing on perplexity or basic task accuracy.  |
| AAD-LLM: Neural Attention-Driven Auditory Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.16794) or [HuggingFace](https://huggingface.co/papers/2502.16794))| Ashesh Mehta, Stephan Bickel, vchoudhari, susameddin, xi-j | i) AAD-LLM is a brain-computer interface that integrates neural signals with an auditory large language model to improve auditory scene understanding aligned with listener attention.  ii) The main research objective is to develop a system that can process and respond to auditory scenes based on a listener's attentional focus, rather than treating all sound inputs equally.  iii) The key methodology involves decoding a listener's attended speaker from intracranial electroencephalography (iEEG) recordings and integrating this information into an auditory LLM to generate responses aligned with the listener's perception.  iv) AAD-LLM achieved a word error rate (WER) of 10.6% on transcribing the attended speech in a two-speaker scenario with background noise, significantly outperforming baseline models.  v) AI practitioners can leverage this work to develop more human-centered auditory AI systems that prioritize listener intent, enhancing applications such as assistive hearing devices and human-computer interaction.  |
| Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI (Read more on [arXiv](https://arxiv.org/abs/2502.17092) or [HuggingFace](https://huggingface.co/papers/2502.17092))| KartikAngadi, kruthika, SyedAbdul | Shakti-VLM, a family of 1B and 4B parameter vision-language models, achieves competitive multimodal performance with enhanced data efficiency through architectural innovations and a three-stage training strategy. The primary objective was to develop efficient vision-language models (VLMs) that achieve strong performance with reduced training data requirements. The methodology includes QK-Normalization, hybrid normalization, enhanced positional encoding, and a three-stage training process (text-only pretraining, vision-language alignment, and full model fine-tuning). Shakti-VLM-4B achieved 59.78% on the MMMU validation set, surpassing comparable models like Qwen2VL-7B and MiniCPM-V-2.6-8B. AI practitioners can leverage Shakti-VLM's design and training strategies to build high-performing multimodal models with significantly less computational resources and training data, especially in enterprise-scale deployments.  |
