

## Papers for 2025-02-03

| Title | Authors | Summary |
|-------|---------|---------|
| s1: Simple test-time scaling (Read more on [arXiv](https://arxiv.org/abs/2501.19393) or [HuggingFace](https://huggingface.co/papers/2501.19393))| Xiang Lisa Li, percyliang, swj0419, zitongyang, Muennighoff | i) The paper introduces "s1", a straightforward method for enhancing language model reasoning and achieving test-time scaling by using a small, carefully curated dataset and a novel budget-forcing technique. ii) Main research question or objective: What is the simplest approach to achieve both test-time scaling and strong reasoning performance in language models? iii) Key methodology used: The authors curated a 1,000-sample dataset (s1K) based on difficulty, diversity, and quality, and developed a test-time budget forcing technique to control model thinking time. iv) Primary results: The s1-32B model, finetuned on s1K and equipped with budget forcing, outperformed the o1-preview model on competition math questions by up to 27% on MATH and AIME24 benchmarks and demonstrated test-time scaling, improving from 50% to 57% on AIME24 with increased thinking time. v) Principal implication for AI practitioners: AI practitioners can leverage the s1K dataset and budget forcing technique to significantly improve the reasoning capabilities and test-time performance of language models with minimal training data and a simple test-time intervention.  |
| Reward-Guided Speculative Decoding for Efficient LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2501.19324) or [HuggingFace](https://huggingface.co/papers/2501.19324))| doyensahoo, JunnanLi, hendrydong, yuhuixu, baohao | Reward-Guided Speculative Decoding (RSD) is introduced to improve the efficiency of large language model (LLM) inference, particularly for multi-step reasoning tasks. The main research question is how to balance efficiency and accuracy in LLM inference by integrating lightweight "draft" evaluations with reward-driven refinements from a more capable "target" model. The key methodology involves using a process reward model to evaluate intermediate decoding steps from a draft model and dynamically deciding whether to accept them or invoke the target model for correction based on reward thresholds. Primary results show that RSD achieves up to 4.4× fewer FLOPs compared to using the target model alone, while achieving up to 3.5 higher accuracy than standard speculative decoding on reasoning benchmarks. For AI practitioners, RSD provides a robust framework to deploy LLMs more efficiently in resource-intensive scenarios by optimizing the trade-off between computational cost and output quality.  |
| Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2501.18119) or [HuggingFace](https://huggingface.co/papers/2501.18119))| Fangzhi Xu, Zhen Peng, Kai He, Tianzhe Zhao, Qika | The paper introduces a method for integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) using quantized representations. The main research question is how to effectively bridge the gap between KG structures and the natural language format of LLMs to achieve seamless integration. The key methodology involves a self-supervised quantized representation (SSQR) method that compresses KG structural and semantic knowledge into discrete codes, followed by constructing KG instruction-following data to fine-tune LLMs. Primary results show that SSQR outperforms existing unsupervised quantized methods, achieving a 9.28% improvement in Mean Reciprocal Rank (MRR) compared to the previous best performance on the WN18RR dataset. The principal implication for AI practitioners is that they can leverage the SSQR method to seamlessly integrate KGs with LLMs by using the learned quantized codes as input features, enhancing model performance on KG-related tasks such as link prediction and triple classification without requiring significant architectural modifications.  |
| Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming (Read more on [arXiv](https://arxiv.org/abs/2501.18837) or [HuggingFace](https://huggingface.co/papers/2501.18837))| Primusa, euanong, sgoodfriend, jayelm, meg-tong | Constitutional Classifiers are synthetic safeguards that defend large language models (LLMs) against universal jailbreaks by using a constitution of natural language rules. The main research question is whether Constitutional Classifiers can effectively defend LLMs against universal jailbreak strategies that systematically bypass model safeguards and extract harmful information. The key methodology involves training classifiers on synthetic data generated by prompting LLMs with a constitution that specifies permitted and restricted content, followed by extensive red teaming to test robustness. The primary results show that in over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information at a similar level of detail to an unguarded model across most target queries, and enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks, with an absolute 0.38% increase in production-traffic refusals. The principal implication for AI practitioners is that Constitutional Classifiers offer a viable defense against universal jailbreaks while maintaining practical deployment feasibility, and thus can play a crucial role in safely deploying capable AI systems.  |
| Trading Inference-Time Compute for Adversarial Robustness (Read more on [arXiv](https://arxiv.org/abs/2501.18841) or [HuggingFace](https://huggingface.co/papers/2501.18841))| Sam Toyer, Stephanie Lin, Boaz Barak, Evgenia Nitishinskaya, Wojciech Zaremba | Here is a concise summary of the research paper:  This paper investigates the impact of increased inference-time computation on the adversarial robustness of reasoning models. The main research question is whether increasing inference-time compute can improve the robustness of large language models (LLMs) against adversarial attacks without adversarial training. The key methodology involves testing various adversarial attacks on OpenAI's reasoning models (01-preview and 01-mini) and measuring attack success rates as a function of inference-time compute. The primary results show that increased inference-time compute generally improves robustness across a range of attacks, with the attack success rate often decreasing to zero as test-time compute grows; for example, in a many-shot attack on a math task, increasing inference-time compute reduced the success rate of an adversary aiming to output the correct answer multiplied by 7 to near zero. The principal implication for AI practitioners is that scaling inference-time compute can be a viable strategy for enhancing the adversarial robustness of LLMs, offering a complementary approach to traditional adversarial training.  |
| INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation (Read more on [arXiv](https://arxiv.org/abs/2501.18753) or [HuggingFace](https://huggingface.co/papers/2501.18753))| Shaogang Gong, Zixu Cheng, Jian Hu | Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) is introduced to improve segmentation accuracy using a single task-generic prompt. The main research question is how to generate accurate instance-specific prompts for image segmentation from a single task-generic prompt without per-instance supervision. The key methodology involves instance-specific prompt generation using negative mining on Vision-Language Model (VLM) outputs and semantic mask generation using GroundingDINO and SAM, refined iteratively. The primary results show that INT achieves a mean Intersection over Union (mIoU) of 0.808 on the CHAMELEON dataset for camouflaged object detection, outperforming existing methods. The principal implication for AI practitioners is that INT provides a method to enhance the accuracy of promptable segmentation models by effectively leveraging a single task-generic prompt across diverse images without requiring instance-specific annotations, thereby simplifying the segmentation process and potentially broadening its application in scenarios with limited labeled data.  |
| Unraveling the Capabilities of Language Models in News Summarization (Read more on [arXiv](https://arxiv.org/abs/2501.18128) or [HuggingFace](https://huggingface.co/papers/2501.18128))| Göksel Biricik, odabashi | This research paper benchmarks 20 language models for news summarization across three datasets using zero-shot and few-shot learning. The main research question is how effectively smaller-scale language models handle news summarization compared to larger models, balancing efficiency and performance. The key methodology involves a multifaceted evaluation approach including automatic metrics (ROUGE, METEOR, BERTScore), human evaluation, and AI-based evaluation using GPT-3.5-Turbo and GPT-4 as a judge. Primary results indicate that GPT-3.5-Turbo achieved the highest scores in automated metrics on the CNN/DM dataset in the zero-shot setting, with a ROUGE-L score of 0.2077, but including demonstration examples in the few-shot setting did not enhance the performance of the models, and in some cases, led to worse quality of the generated summaries. The principal implication for AI practitioners is that while large models like GPT-3.5-Turbo and GPT-4 dominate in news summarization tasks, smaller models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta show promising results, offering competitive alternatives.  |
| Fast Encoder-Based 3D from Casual Videos via Point Track Processing (Read more on [arXiv](https://arxiv.org/abs/2404.07097) or [HuggingFace](https://huggingface.co/papers/2404.07097))| Haggai Maron, Wuyue Lu, Yoni Kasten | Here is a concise summary of the research paper "Fast Encoder-Based 3D from Casual Videos via Point Track Processing":  TRACKSTO4D, a learning-based approach, reconstructs 3D structures and camera positions from 2D point tracks extracted from casual videos in a single feed-forward pass. The main research question is how to efficiently infer 3D structure and camera positions from dynamic content in casual videos without relying on lengthy optimization processes. The key methodology involves a novel encoder architecture that processes 2D point track tensors as input, incorporating symmetry-aware attention mechanisms and a low-rank assumption for movement patterns to predict 3D point clouds and camera poses. The primary results show that TRACKSTO4D achieves comparable accuracy to state-of-the-art methods while reducing runtime by up to 95%, with a specific finding that it reduces inference time by 95% compared to the baseline. The principal implication for AI practitioners is that they can leverage TRACKSTO4D for significantly faster 3D reconstruction from casual videos, enabling more efficient development of applications in areas like robot navigation and autonomous driving without sacrificing accuracy.  |
| DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning (Read more on [arXiv](https://arxiv.org/abs/2411.04983) or [HuggingFace](https://huggingface.co/papers/2411.04983))| Lerrel Pinto, Yann LeCun, Hengkai Pan, Gaoyue Zhou | DINO-WM is a method for training visual world models using pretrained DINOv2 embeddings for task-agnostic behavior planning. The main research question is whether a world model can be trained offline on pre-collected trajectories to support test-time behavior optimization and task-agnostic reasoning using only passive data. The key methodology involves using DINOv2 patch features to model visual dynamics without reconstructing the visual world, predicting future patch features from offline behavioral trajectories. The primary result is that DINO-WM achieves a 90% success rate on the Push-T task, compared to 4% for DreamerV3. For AI practitioners, DINO-WM demonstrates that pretrained visual features can be leveraged to create world models capable of zero-shot planning across diverse tasks without task-specific data, enabling more generalizable and efficient robot learning.  |
