

## Papers for 2025-02-27

| Title | Authors | Summary |
|-------|---------|---------|
| Kanana: Compute-efficient Bilingual Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.18934) or [HuggingFace](https://huggingface.co/papers/2502.18934))| seopbo, Doohae, daniel-rl2, jiyeonham, bzantium | Kanana is a series of bilingual language models demonstrating strong performance in Korean and competitive performance in English at a significantly lower computational cost than comparable state-of-the-art models. The main research objective was to develop compute-efficient bilingual language models that maintain strong performance in both Korean and English. The key methodologies employed include high-quality data filtering, staged pre-training, depth up-scaling, pruning, and distillation, combined with supervised fine-tuning and preference optimization for instruction tuning. Primary results show that the Kanana Flag 32.5B model outperforms Llama 3.1 70B on MMLU and KMMLU, while using substantially fewer computational resources, costing similiar to Gemma 2 9B. AI practitioners can leverage Kanana's training techniques such as staged pre-training and depth-up scaling to build high-performing, resource-efficient language models, especially for languages with limited data availability.  |
| GHOST 2.0: generative high-fidelity one shot transfer of heads (Read more on [arXiv](https://arxiv.org/abs/2502.18417) or [HuggingFace](https://huggingface.co/papers/2502.18417))| Andrey Kuznetsov, Denis Dimitrov, Pavel Paramonov, Alexander Groshev, nastasia-y | GHOST 2.0 is a two-module framework for high-fidelity one-shot head swapping, addressing limitations in existing face-swapping and head-reenactment methods. The main research objective is to develop a system that can realistically swap entire heads between source and target images, preserving identity, pose, and expression while seamlessly blending the result. The key methodology involves an "Aligner" module for head reenactment and a "Blender" module for integrating the reenacted head into the target background, using StyleGAN-based architecture and correlation learning. Primary results show that at 512x512 resolution in cross-reenactment, GHOST 2.0 achieves a CSIM score of 0.628 and a FID score of 29.57, outperforming one of the baselines (StyleHEAT) and indicating better performace than another baseline (HeSer) at identity preservation. AI practitioners can use GHOST 2.0 to improve the realism and robustness of head-swapping applications, particularly in scenarios with significant variations in head pose, hairstyle, and background.  |
| TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.19400) or [HuggingFace](https://huggingface.co/papers/2502.19400))| Jonathan Leung, AlvinYuVotee, KrishKrosh, chongcht, vinesmsuic | TheoremExplainAgent, a novel agentic system, generates multimodal theorem explanation videos, and a new benchmark, TheoremExplainBench, evaluates them. The main research objective is to assess if AI systems can effectively generate multimodal theorem explanations. The key methodology involves a two-agent pipeline (planner and coding agent) using Manim to create videos, and a benchmark of 240 theorems across STEM, evaluated across five dimensions. The o3-mini agent achieved a 93.8% success rate and an overall score of 0.77, but visual element layout exhibited minor issues. AI practitioners can leverage this agentic approach for enhanced theorem understanding, though refinement is needed in visual structuring and consistency of generated video outputs.  |
| Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2502.19361) or [HuggingFace](https://huggingface.co/papers/2502.19361))| Weixun Wang, Jiaheng Liu, Shilong Li, Yancheng He, zhangysk | DeltaBench, a new benchmark, evaluates large language models' (LLMs) ability to detect errors in long chain-of-thought (CoT) reasoning. The main research objective is to assess the quality of long CoTs generated by o1-like models and to measure the critique abilities of existing LLMs, process reward models (PRMs) and critic models on these CoTs. The key methodology involves creating DeltaBench, a dataset of long CoTs with fine-grained error annotations, and evaluating various LLMs, including PRMs and critic models, on their ability to identify these errors. Primary results show that even the top-performing model (GPT-4-turbo-128k) achieved a low F1-score of only 40.8% in error detection, and that o1-like models do not show any advantage over non-o1-like models on critique abilities. Principal implication for AI practitioners is that current LLMs, including PRMs, have limited ability to identify errors in long CoT reasoning, highlighting a need for significant improvements in critique capabilities for robust AI system development.  |
| Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems (Read more on [arXiv](https://arxiv.org/abs/2502.19328) or [HuggingFace](https://huggingface.co/papers/2502.19328))| Bin Xu, Zijun Yao, Xiaozhi Wang, Yunjia Qi, Hao Peng | This paper proposes a new reward modeling approach, "agentic reward modeling," that combines human preferences with verifiable correctness signals for more reliable reward systems in large language models (LLMs).  The main research objective is to develop a reward system that mitigates the limitations of existing reward models, which primarily focus on subjective human preferences and often neglect verifiable correctness.  The key methodology involves implementing a reward agent, REWARDAGENT, that integrates human preference rewards with two verifiable signals: factuality (assessed via pairwise comparison and evidence verification) and instruction-following (verified through constraint parsing and Python code execution).  The primary results show that REWARDAGENT significantly outperforms existing reward models on benchmarks like RM-Bench, JudgeBench, and a newly constructed IFBench, achieving an overall score of 72.5% in one configuration.  The principal implication for AI practitioners is that integrating verifiable correctness signals with human preference feedback can lead to more reliable and robust reward models, improving LLM performance in downstream tasks and alignment with intended behavior, particularly during the inference and training phases.  |
| Language Models' Factuality Depends on the Language of Inquiry (Read more on [arXiv](https://arxiv.org/abs/2502.17955) or [HuggingFace](https://huggingface.co/papers/2502.17955))| Hamid Palangi, Kumar Ayush, Kumar Tanmay, ayush1801, AggarwalTushar | Language models (LMs) exhibit inconsistent factual recall across different languages, failing to transfer knowledge even when possessing it in one language. The main research question is whether multilingual LMs truly internalize and transfer factual knowledge across languages or encode isolated linguistic silos. The key methodology involves creating a benchmark of 10,000 country-related facts across 13 languages and proposing metrics (Factual Recall Score, Knowledge Transferability Score, Cross-Lingual Factual Knowledge Transferability Score) to quantify factual recall and knowledge transferability. A primary result is that Llama-3-70B achieved the highest X-FaKT score of 0.848, demonstrating superior balanced performance in both factual recall and knowledge transfer. The principal implication is that AI practitioners must recognize language-specific factual reliability in multilingual LMs and leverage the most trustworthy information across languages, moving beyond the assumption of consistent cross-lingual knowledge access.  |
| Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation (Read more on [arXiv](https://arxiv.org/abs/2502.19414) or [HuggingFace](https://huggingface.co/papers/2502.19414))| Matthias Bethge, Jonas Geiping, Ponnurangam Kumaraguru, Shashwat Goel, Shiven Sinha | Language models (LMs) are evaluated on their ability to generate counterexamples that falsify incorrect algorithmic solutions, introducing a new benchmark called REFUTE. The main research question is: Can LMs create counterexamples for incorrect solutions to algorithmic problems? The key methodology involves sourcing incorrect submissions from programming competitions, filtering them for non-trivial errors, and prompting LMs to generate inputs that cause these solutions to fail, validated through code execution. The primary result is that the best reasoning agents, including OpenAI 03-mini (high), can only create counterexamples for less than 9% of incorrect solutions in REFUTE, despite having a much higher success rate at solving those same problems. The principal implication for AI practitioners is that verification, including falsification of subtly incorrect solutions, is significantly harder for current LMs than generating correct solutions, highlighting a limitation in capabilities relevant for self-improvement and reliable reasoning.  |
| Towards an AI co-scientist (Read more on [arXiv](https://arxiv.org/abs/2502.18864) or [HuggingFace](https://huggingface.co/papers/2502.18864))| Anil Palepu, Tao Tu, Alexander Daryin, Wei-Hung Weng, Juraj Gottweis | Here's a summary of the paper, strictly adhering to your guidelines:  The paper introduces an AI co-scientist, a multi-agent system built on Gemini 2.0, designed to assist in scientific discovery by generating and evaluating novel research hypotheses. The main research objective is to develop an AI system capable of formulating demonstrably novel research hypotheses and proposals, building upon existing evidence and aligned with scientist-provided goals. The key methodology involves a multi-agent architecture with an asynchronous task execution framework, utilizing a generate, debate, and evolve approach with specialized agents for hypothesis generation, refinement, and ranking via simulated scientific debates and tournaments. The system demonstrates, across 203 diverse research goals, improved hypothesis quality (measured by an internal Elo rating system) as a function of increased test-time compute, and hypotheses for acute myeloid leukemia were validated to show tumor inhibition *in vitro* at clinically applicable concentrations. AI practitioners can leverage the multi-agent architecture and test-time compute scaling paradigm presented to build systems capable of complex reasoning and iterative improvement, although specific external validation metrics remain limited within the paper.  |
| VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model (Read more on [arXiv](https://arxiv.org/abs/2502.18906) or [HuggingFace](https://huggingface.co/papers/2502.18906))| Lingrui Mei, Lu Wang, Jiani Zheng, vyokky, keanudicap | VEM decouples value estimation from policy optimization for training GUI agents, enabling environment-free reinforcement learning. The main research objective is to develop an environment-free RL framework that can effectively train GUI agents without costly real-world interactions. The key methodology involves pretraining a Value Environment Model (VEM) to predict state-action values from offline data and then using this frozen VEM to guide policy exploration. The method achieves 28.0% offline task success rate on the General domain of the Android-in-the-Wild benchmark, surpassing environment-free baselines by 12-28%. AI practitioners can leverage this approach to train GUI agents with greater sample efficiency and stability, bypassing the need for direct environment interactions.  |
| Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance (Read more on [arXiv](https://arxiv.org/abs/2502.18772) or [HuggingFace](https://huggingface.co/papers/2502.18772))| Polydoros Giannouris, Efstathia Soufleri, Triantafillos Papadopoulos, Xueqing Peng, jiminHuang | The paper introduces Plutus-ben, a Greek financial benchmark, and Plutus-8B, a Greek financial LLM, to address the lack of resources for Greek financial NLP.  The main research question is: How do current language models perform on core Greek financial tasks, and how can fine-tuning on Greek financial data enhance performance?  Key methodology involved creating Plutus-ben, comprising five financial NLP tasks (numeric and textual NER, QA, abstractive summarization, topic classification), and fine-tuning Llama-Krikri-8B with Greek domain-specific data to create Plutus-8B, evaluating 22 LLMs.  The primary result is that Plutus-8B achieved the best performance on Plutus-ben, surpassing GPT-4 by 15.38% and outperforming all baseline models in the evaluation.  Principal implication for AI practitioners is that fine-tuning on language-specific and domain-specific data is crucial for LLM performance in low-resource languages like Greek, significantly improving performance in tasks like financial numeric reasoning.  |
| Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator (Read more on [arXiv](https://arxiv.org/abs/2502.19204) or [HuggingFace](https://huggingface.co/papers/2502.19204))| Ying Cui, Ruibo Li, Hongji Li, Dongyan Guo, Xiankang He | This paper introduces a new distillation framework for improving monocular depth estimation (MDE) using unlabeled data. The main research objective is to enhance zero-shot MDE by addressing the limitations of existing depth normalization strategies in pseudo-label distillation. The key methodology involves Cross-Context Distillation, integrating global and local depth cues, and a multi-teacher distillation framework using diverse depth estimation models. The primary result shows that the proposed method outperforms state-of-the-art methods on benchmark datasets; for instance, on the DIODE dataset, the AbsRel improves by 14.1% using the Local-Global and Shared-Context Distillation strategies. For AI practitioners, this method provides an effective way to train more robust and accurate MDE models by leveraging unlabeled data and combining the strengths of multiple teacher models, especially improving generalization in varied scenarios.  |
| Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs (Read more on [arXiv](https://arxiv.org/abs/2502.19413) or [HuggingFace](https://huggingface.co/papers/2502.19413))| Andreas Hochlehnert, Tawsif Ahmed, Ameya Prabhu, Gollam Rabby, Christoph Schuhmann | This paper proposes converting copyrighted scientific texts into structured "Knowledge Units" using LLMs to make factual information freely accessible while respecting copyright. The main research question is whether converting scientific texts into Knowledge Units preserves factual information and adheres to copyright laws. The key methodology involves using LLMs to extract entities, attributes, and relationships from paragraphs of scientific papers into structured data, and evaluating the legal defensibility and information retention via question-answering experiments. Primary results show that language models answering multiple-choice questions using Knowledge Units achieved nearly the same accuracy (within 3-5% variance) as when using original texts across several scientific domains. AI practitioners can utilize this framework to build and use datasets containing facts from copyrighted scientific text, potentially democratizing access to scholarly knowledge without infringing on the original expression.  |
| AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement (Read more on [arXiv](https://arxiv.org/abs/2502.16776) or [HuggingFace](https://huggingface.co/papers/2502.16776))| Xijie Huang, Junxiao Yang, Leqi Lei, Zhexin Zhang, LLLeo612 | AISafetyLab is a unified framework and toolkit for AI safety that integrates attack, defense, and evaluation methodologies. The main objective is to provide a standardized platform to evaluate and improve AI safety by addressing the lack of comprehensive tools and inconsistent experimental setups. The methodology involves implementing 13 attack methods (including black-box, gray-box, and white-box), 16 defense mechanisms (both inference-time and training-time), and 7 evaluation scorers, alongside auxiliary modules for model interaction, data management, utilities, and logging. In evaluations using Vicuna-7B-v1.5, AutoDAN achieved an average attack success rate of 56.4% across various defenses, while some other methods had varying performance depending on the defense used. For AI practitioners, AISafetyLab provides a flexible, extensible platform with comprehensive method coverage for systematically assessing and enhancing the robustness of AI models against adversarial attacks.  |
| BIG-Bench Extra Hard (Read more on [arXiv](https://arxiv.org/abs/2502.19187) or [HuggingFace](https://huggingface.co/papers/2502.19187))| Chrysovalantis Anastasiou, John Palowitch, Hritik Bansal, Mehran Kazemi, baharefatemi | BIG-Bench Extra Hard (BBEH) is a new benchmark to evaluate the general reasoning capabilities of large language models (LLMs). The main research objective is to address the saturation of existing LLM reasoning benchmarks, particularly BIG-Bench Hard (BBH), by creating a more challenging and diverse set of tasks. The methodology involves replacing each of the 23 tasks in BBH with a novel, more difficult task that probes similar reasoning capabilities, using a semi-adversarial approach with two reference models to ensure sufficient difficulty. The primary result is that the best general-purpose model achieved a harmonic mean accuracy of 9.8% on BBEH, while the best reasoning-specialized model achieved 44.8%, indicating significant room for improvement. AI practitioners should use BBEH to evaluate LLMs for robust general reasoning, revealing current limitations and driving improvements instead of using other benchmarks where LLMs have reached ceiling performance.  |
| CritiQ: Mining Data Quality Criteria from Human Preferences (Read more on [arXiv](https://arxiv.org/abs/2502.19279) or [HuggingFace](https://huggingface.co/papers/2502.19279))| Zhiheng Xi, Tianyi Liang, Qipeng Guo, Kai Lv, KYLN24 | CritiQ is a novel data selection method that automatically mines data quality criteria from human preferences and performs efficient data selection. The main research objective is to develop a method for automatically extracting data quality criteria from human preferences with minimal human annotation effort. The key methodology, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments based on a knowledge base and a reflection process. Accuracies on human-annotated test sets reach 89.33% for code, 84.57% for math, and 88.06% for logic, outperforming baselines such as TextGrad and single-criterion methods. AI practitioners can use CritiQ to automatically derive data quality criteria and select high-quality subsets, improving model performance on downstream tasks with reduced reliance on manually designed heuristics or extensive human annotation.  |
| MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra (Read more on [arXiv](https://arxiv.org/abs/2502.16284) or [HuggingFace](https://huggingface.co/papers/2502.16284))| Qiang Liu, Deli Zhao, Yu Rong, Shaozhen Liu, AzureLeon1 | MolSpectra enhances pre-training of 3D molecular representations by incorporating multi-modal energy spectra. The main research objective is to establish the relationship between 3D molecular structures and energy states using spectral data to improve molecular representation learning. The key methodology involves a multi-spectrum encoder, SpecFormer, trained with masked patch reconstruction, and a contrastive objective aligning 3D and spectral representations. Pre-training with MolSpectra achieved state-of-the-art performance on the QM9 dataset, achieving a mean absolute error (MAE) of 0.011 D on the dipole moment (μ) prediction, outperforming the baseline Coord method in 10 out of 12 properties. For AI practitioners, MolSpectra provides a pre-training framework that leverages molecular spectra to learn more informative 3D molecular representations, enhancing performance on downstream tasks like property prediction.  |
| PosterSum: A Multimodal Benchmark for Scientific Poster Summarization (Read more on [arXiv](https://arxiv.org/abs/2502.17540) or [HuggingFace](https://huggingface.co/papers/2502.17540))| Frank Keller, Pasquale Minervini, rohitsaxena | POSTERSUM, a new benchmark, evaluates multimodal models on summarizing scientific posters into research paper abstracts, revealing limitations in current models and introducing a hierarchical approach for improvement.  **Main research question or objective:** How effectively can Multimodal Large Language Models (MLLMs) understand and summarize the complex, visually-rich content of scientific posters into concise textual abstracts, and can a hierarchical approach improve this performance?  **Key methodology used:** The authors created a new dataset, POSTERSUM, consisting of 16,305 scientific posters paired with their corresponding abstracts. They benchmarked state-of-the-art MLLMs (including GPT-4o, Claude-3.5 Sonnet, Gemini 2.0, and various open-source models) on this dataset using metrics like ROUGE, SacreBLEU, METEOR, and BERTScore. They then proposed "SEGMENT & SUMMARIZE," a hierarchical approach involving segmentation of the poster into coherent regions, localized summarization of each region, and global summarization to combine the localized summaries.  **Primary results:** State-of-the-art MLLMs struggle to accurately summarize scientific posters. The best-performing closed-source model, GPT-4o, achieved a ROUGE-L score of only 22.30. The proposed SEGMENT & SUMMARIZE method significantly outperformed all other models, including closed-source MLLMs, achieving a ROUGE-L score of 24.18.  **Principal implication for AI practitioners:** Current MLLMs, while strong on various tasks, have significant limitations when handling the complex multimodal information presented in scientific posters. The POSTERSUM dataset provides a valuable benchmark for advancing multimodal understanding, and the "SEGMENT & SUMMARIZE" approach demonstrates a promising direction for improving performance by incorporating a divide-and-conquer strategy, handling the complexity inherent in poster summarization. AI/ML/Software Engineers and Data Scientist working with scientific documents should prioritize models and architectures that are capable of understanding a variety of modalities and their combinations.  |
