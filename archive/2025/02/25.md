

## Papers for 2025-02-25

| Title | Authors | Summary |
|-------|---------|---------|
| DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks (Read more on [arXiv](https://arxiv.org/abs/2502.17157) or [HuggingFace](https://huggingface.co/papers/2502.17157))| Zhiyue Zhao, Mingyu Liu, Z-MU-Z, zhyya, Canyu | DICEPTION is a generalist diffusion model for various visual perception tasks like segmentation, depth, and normal estimation. The primary objective is to create a single diffusion-based model capable of performing multiple visual perception tasks efficiently, leveraging pre-trained text-to-image models. The methodology involves unifying various perception tasks as conditional image generation in RGB space, using point prompts, task prompts, and a DiT architecture. Results demonstrate performance on par with state-of-the-art models, achieving comparable results to SAM-vit-h using only 0.06% of its training data (600K vs. 1B pixel-level annotated images). AI practitioners can leverage the priors of pre-trained diffusion models to create efficient and effective multi-task visual generalist models, significantly reducing the data and computational requirements compared to conventional training from scratch.  |
| Thus Spake Long-Context Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.17129) or [HuggingFace](https://huggingface.co/papers/2502.17129))| Yuerong Song, Zhigeng Liu, Mianqiu Huang, Ruixiao Li, LiuXR | i) This survey paper presents a comprehensive overview of the long-context large language model (LLM) lifecycle. ii) The paper aims to provide a global picture of long-context LLMs, covering architectures, infrastructure, training, and evaluation technologies. iii) The methodology involves analyzing existing literature and categorizing long-context LLM technologies into architecture, infrastructure, training, and evaluation perspectives. iv) The survey showcases a spectrum of long-context technologies and identifies 10 unanswered questions currently faced by long-context LLMs; the context length of open-source LLMs has grown from 2k to 2M tokens between April 2023 and February 2024. v) The principal implication is to offer AI researchers and practitioners a systematic introduction to the research landscape of long-context LLMs, highlighting key challenges and future research directions.  |
| Slamming: Training a Speech Language Model on One GPU in a Day (Read more on [arXiv](https://arxiv.org/abs/2502.15814) or [HuggingFace](https://huggingface.co/papers/2502.15814))| Yossi Adi, avishai-elmakies, gallilmaimon | The paper introduces *Slam*, a recipe for training speech language models (SLMs) on a single GPU within 24 hours. The main research objective is to determine if high-quality SLMs can be trained using a single GPU within 24 hours. The methodology involves empirical analysis of model initialization, architecture, synthetic training data, and preference optimization, systematically ablating each training pipeline component. A key result is that the *Slam* recipe, utilizing a Qwen2.5-0.5B model and synthetic data, achieves a Topic-StoryCloze score of 82.04 on a single A5000 GPU. The principal implication is that AI practitioners can train high-quality SLMs with significantly reduced computational resources, improving accessibility of SLM research and development.  |
| Audio-FLAN: A Preliminary Release (Read more on [arXiv](https://arxiv.org/abs/2502.16584) or [HuggingFace](https://huggingface.co/papers/2502.16584))| Shuai Fan, Zixuan Li, Jiahao Pan, Ziya Zhou, Liumeng Xue | Audio-FLAN is a large-scale instruction-tuning dataset for unified audio-language models covering 80 diverse tasks across speech, music, and sound domains. The main research objective is to create a comprehensive dataset to enable unified audio-language models to perform both understanding and generation tasks in a zero-shot manner. The key methodology involves collecting and standardizing nearly all publicly available academic audio datasets into a common instruction-based format, normalizing the heterogeneous datasets and varying instructions using LLaMA and GPT. The primary result is a dataset with approximately 80 tasks, and over 100 million instances, significantly surpassing prior efforts in both quantity and diversity. AI practitioners can use Audio-FLAN to train and evaluate unified audio-language models capable of performing a wide range of understanding and generation tasks, potentially leading to models with zero-shot generalization abilities across speech, music and other audios.  |
| GCC: Generative Color Constancy via Diffusing a Color Checker (Read more on [arXiv](https://arxiv.org/abs/2502.17435) or [HuggingFace](https://huggingface.co/papers/2502.17435))| Yu-Chee Tseng, Yi-Chen Lo, Chia-Che Chang, Cheng-De Fan, Chen-Wei Chang | GCC is a method for estimating scene illumination in images by inpainting a color checker using diffusion models. The main research objective is to develop a color constancy method that generalizes well across different camera sensors without requiring sensor-specific training. The key methodology involves fine-tuning a diffusion-based inpainting model to insert a color checker into an image, then using Laplacian decomposition to maintain checker structure and extract illumination color from the inpainted checker's achromatic squares. In cross-dataset evaluations, GCC achieved a worst-25% error rate of 5.15° and 4.32° in bi-directional evaluations. AI practitioners can leverage this method to estimate the illumination with good accuracy, across a wide range of sensors without specific sensor training data.  |
| CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.16614) or [HuggingFace](https://huggingface.co/papers/2502.16614))| Yejie Wang, Wei Zhang, Jiaheng Liu, Marcus Dong, Alexander Zhang | CodeCriticBench is a benchmark for evaluating large language models' (LLMs) ability to critique code, assessing both code generation and code question-answering tasks. The main research objective is to establish a comprehensive framework for evaluating LLMs' code critique capabilities across different dimensions and difficulty levels. The methodology involves collecting code tasks from various sources, constructing basic and advanced critique evaluation protocols, and designing fine-grained evaluation checklists. Primary results show that, on advanced evaluations, DeepSeek-R1 achieves an MSE of 3.92 on code generation, while Claude3.5-Sonnet leads in code QA with an MSE of 1.02; generally models increased in Accuracy (ACC) as parameters increased. The principal implication is that AI practitioners can use CodeCriticBench to systematically assess and compare the code critique performance of different LLMs, driving improvements in coding assistance tools and automated code review systems.  |
| Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.17407) or [HuggingFace](https://huggingface.co/papers/2502.17407))| James Thorne, Jiwoo Hong, Guijin Son, Cartinoe5930 | The paper introduces MCLM, a multilingual math benchmark, and evaluates the linguistic generalizability of test-time scaling methods in mathematical reasoning. The main research question is whether test-time scaling confers cross-lingual benefits in mathematical reasoning similar to those observed with pre-training scaling. The authors test three test-time scaling methods (Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing) on multilingual LLMs using a new benchmark, MCLM, featuring competition-level problems in 55 languages. A primary result is that using Qwen2.5-1.5B Math with Outcome Reward Modeling achieves a score of 35.8 on MCLM, while Budget Forcing on MR1-1.5B attains 35.2, showing that gains from test-time scaling do not consistently extend to multiple languages. The principal implication is that AI practitioners should be aware that test-time scaling methods may not generalize effectively to multilingual tasks, and improving multilingual robustness requires methods beyond simply increasing inference-time compute.  |
| Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.16894) or [HuggingFace](https://huggingface.co/papers/2502.16894))| Wei Wei, Xiaoye Qu, Sichen Liu, Zhenyi Lu, Facico | GOAT enhances LoRA fine-tuning for large language models by using adaptive singular value decomposition and Mixture-of-Experts optimization alignment. The primary research question is how to mitigate the performance gap between LoRA and full fine-tuning, particularly in Mixture-of-Experts (MoE) architectures. The key methodology involves initializing LoRA MoE experts with distinct SVD segments of pre-trained weights and aligning optimization with a theoretical scaling factor derived from full fine-tuning. Primary results show that GOAT achieves 99.07% of full fine-tuning performance on image classification and outperforms all LoRA variants. The principal implication for AI practitioners is that GOAT offers a more efficient and effective fine-tuning approach, closing the performance gap with full fine-tuning while maintaining scalability.  |
| Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2502.16033) or [HuggingFace](https://huggingface.co/papers/2502.16033))| Yang Zhao, Shan Jiang, Hongquan Li, Yue Fan, Qianqi Yan | The paper introduces MMIR, a new benchmark for evaluating multimodal reasoning models' ability to detect semantic inconsistencies in layout-rich visual-textual content. The main research objective is to assess how well Multimodal Large Language Models (MLLMs) can identify and reason about semantic mismatches in artifacts like webpages and slides. The key methodology involves creating 534 samples with synthetically injected errors across five reasoning-heavy categories and evaluating six state-of-the-art MLLMs. The primary result is that the proprietary model, o1, achieved the best performance with over 50% accuracy in detecting inconsistencies, significantly outperforming open-source models which scored below 25%. The paper's principle implication, therefore, is that there is a crucial need for development in advancing multimodal reasoning in current MLLMs, particularly for handling inconsistencies, to make the models more reliable.  |
| Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration (Read more on [arXiv](https://arxiv.org/abs/2502.17110) or [HuggingFace](https://huggingface.co/papers/2502.17110))| Ji Zhang, Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy | Mobile-Agent-V is a framework that leverages video guidance to enhance mobile device automation through multi-agent collaboration. The main research objective is to address the limitations of existing mobile automation frameworks by providing rich and cost-effective operational knowledge. The key methodology involves a sliding window video input mechanism, a video agent for adaptive frame selection, and a deep-reflection agent for refining decision outputs. Primary results show that Mobile-Agent-V achieves a 30% performance improvement over existing frameworks in tasks requiring operational knowledge. The principal implication for AI practitioners is that they can use video demonstrations to effectively inject operational knowledge into mobile agents, enabling more efficient and scalable automation.  |
| RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2502.15894) or [HuggingFace](https://huggingface.co/papers/2502.15894))| Chongxuan Li, Yixiao Chen, Guande He, Min Zhao, zhuhz22 | RIFLEX improves length extrapolation in video diffusion transformers by reducing a key intrinsic frequency in positional embeddings. The main research objective is to understand and mitigate the failure modes (temporal repetition and slow motion) of existing length extrapolation methods in video diffusion transformers. The key methodology is analyzing the role of frequency components in Rotational Position Embedding (RoPE) and reducing the "intrinsic frequency" component that governs repetition patterns. Primary results show that RIFLEX achieves 2x extrapolation on CogVideoX-5B in a training-free manner, with a NoRepeat Score of 54.2 and Dynamic Degree of 59.4. The principal implication is that AI practitioners can achieve high-quality length extrapolation in video generation without additional training or significant modifications to existing models by simply adjusting the intrinsic frequency in the positional encoding.  |
| Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties (Read more on [arXiv](https://arxiv.org/abs/2502.16922) or [HuggingFace](https://huggingface.co/papers/2502.16922))| Deyu Zhou, Yong Jiang, Pengfei LI, Jialong Wu, wzl0228 | The paper introduces CTM, a new benchmark for evaluating temporal reasoning in large language models (LLMs) within the context of Chinese dynastic chronology. The main objective is to assess LLMs' ability to understand and align temporal relationships across various Chinese historical entities and events. The methodology involves constructing a dataset of 8,750 question-answer pairs and 60 Timeline Ito Game instances, focusing on contextualization, cross-entity relationships, and pairwise temporal alignment. Evaluation of various LLMs revealed that the Time Interval Calculation (TIC) task was the most challenging, and the best performing model (Deepseek-R1) achieved an accuracy of 64.02% on question answering,. This suggests that CTM can provide a culturally rich resource for enhancing temporal reasoning capabilities and structured knowledge integration in large language models.  |
| Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2502.16707) or [HuggingFace](https://huggingface.co/papers/2502.16707))| Sergey Levine, Xiangyu Yue, Zhuoran Yang, csuhan, yunhaif | This paper introduces Reflective Planning, a framework that enhances vision-language models (VLMs) for multi-stage, long-horizon robotic manipulation tasks by incorporating a reflection mechanism. The main research question is how to improve VLMs' physical reasoning and long-horizon planning capabilities for complex robotic manipulation. The key methodology involves using a diffusion-based dynamics model for visual look-ahead and an iterative reflection process, enabling the VLM to critique and refine its actions based on imagined future states. The proposed method, ReflectVLM, achieved an 85.4% success rate on a challenging set of manipulation tasks, significantly outperforming state-of-the-art commercial VLMs and Monte Carlo Tree Search. AI practitioners can leverage this framework to develop more robust and efficient robotic planning systems that require visual understanding and long-horizon reasoning, without extensive task-specific training.  |
| Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam (Read more on [arXiv](https://arxiv.org/abs/2502.17055) or [HuggingFace](https://huggingface.co/papers/2502.17055))| Xiang Li, Gaojie Jin, Zhenyu Zhang, Haotian Hu, Tianjin Huang | Stable-SPAM, a new optimizer, enhances stability in 4-bit large language model (LLM) training. The main research objective is to evaluate and improve the stability of 4-bit LLM training using recently proposed optimizers. The key methodology involves introducing Stable-SPAM, which incorporates adaptive gradient normalization (AdaGN), adaptive spike-aware clipping (AdaClip), and inherits momentum reset from SPAM. Primary results show that a 4-bit LLaMA-1B model trained with Stable-SPAM outperforms a BF16 LLaMA-1B trained with Adam by up to 2 perplexity points. The principal implication is that AI practitioners can use Stable-SPAM to achieve more stable and efficient training of LLMs with 4-bit quantization, matching or exceeding 16-bit Adam performance with significantly reduced memory and computational costs.  |
| Can Community Notes Replace Professional Fact-Checkers? (Read more on [arXiv](https://arxiv.org/abs/2502.14132) or [HuggingFace](https://huggingface.co/papers/2502.14132))| Isabelle Augenstein, Desmond Elliott, gretawarren, Nadav | This research investigates the reliance of Twitter/X's Community Notes on professional fact-checking for combating misinformation. The main research questions are to what extent community notes rely on the work of professional fact-checkers and what are the traits of posts and notes that reference fact-checking sources. The researchers annotated a corpus of Twitter/X community notes using language models and performed manual annotations, classifying cited sources and identifying attributes like topic and refutation strategies. A primary result is that at least 5% of all English community notes contain an external link to professional fact-checkers, rising to 7% for notes rated as 'helpful'. This suggests that, to improve community-based moderation quality, AI practitioners could consider integrating and/or prioritize content from verified professional fact-checking organizations within community moderation systems.  |
| Forecasting Open-Weight AI Model Growth on Hugging Face (Read more on [arXiv](https://arxiv.org/abs/2502.15987) or [HuggingFace](https://huggingface.co/papers/2502.15987))| Jianxi Gao, Pin-Yu Chen, KBhandari11 | The paper adapts a scientific citation model to predict the adoption dynamics of open-weight AI models on Hugging Face. The main research question is, "Can we predict the trajectory of influence an open-weight model will have on the AI community?". The key methodology adapts Wang et al.'s citation model, using immediacy, longevity, and relative fitness parameters to track the cumulative number of fine-tuned models. The results show that most models cluster around narrow bands of parameters but models like `openai/whisper-large-v3` demonstrate a high relative fitness (λi) of 528070.6635. AI practitioners can use this framework to anticipate model prominence and understand the long-term impact of open-weight models, guiding strategic decisions and governance.  |
| TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.15425) or [HuggingFace](https://huggingface.co/papers/2502.15425))| Balázs Kégl, Albert Thomas, Hamza Cherkaoui, Abdelhakim Benechehab, Giuseppe Paolo | TAG is a decentralized framework for constructing multi-agent hierarchical reinforcement learning systems of arbitrary depth. The main research objective is to develop a framework enabling scalable and adaptable multi-agent systems through hierarchical organization and decentralized control. The key methodology is the LevelEnv abstraction, which presents each hierarchy level as an environment to the agents above it, standardizing information flow and enabling bidirectional communication. The experiments on MPE-Spread and VMAS Balance environments show that depth-three agents (3PPO and 2MAPPO-PPO) match a hand-designed heuristic performance with 95% confidence interval. AI practitioners can use TAG to build scalable multi-agent systems that decompose complex tasks across multiple hierarchical levels, improving learning efficiency and coordination without centralized control.  |
| VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing (Read more on [arXiv](https://arxiv.org/abs/2502.17258) or [HuggingFace](https://huggingface.co/papers/2502.17258))| Yi Yang, Hehe Fan, Linchao Zhu, Xiangpeng Yang | VideoGrain introduces a zero-shot approach for multi-grained video editing by modulating space-time attention mechanisms in diffusion models. The main research question is: Can attention be modulated to ensure accurate distribution of each local edit's attention weights in the intended regions for multi-grained video editing? The key methodology is Spatial-Temporal Layout-Guided Attention (ST-Layout Attn), which modulates both cross-attention (for text-to-region control) and self-attention (for feature separation) within a diffusion model. The method achieves an Edit-Accuracy of 88.4, a Temporal-Consistency of 85.0 and an Overall score of 83.0 on a dataset of 76 video-text pairs. AI practitioners can leverage this method to perform precise, multi-grained video editing (class-level, instance-level, and part-level) without requiring parameter tuning or additional training data.  |
| Beyond Release: Access Considerations for Generative AI Systems (Read more on [arXiv](https://arxiv.org/abs/2502.16701) or [HuggingFace](https://huggingface.co/papers/2502.16701))| Yacine Jernite, Ariel Herbert-Voss, Dan Hendrycks, Rishi Bommasani, irenesolaiman | Generative AI system access, beyond component release, determines stakeholder engagement and risk-benefit tradeoffs through resourcing, technical usability, and utility. The main research question is how accessibility of generative AI system components, beyond their mere availability, influences their use, potential risks, and benefits. The key methodology involves deconstructing access along three axes (resourcing, technical usability, and utility) and analyzing access variables for four high-performance language models (Llama 3.1 405B Instruct, DeepSeek v3, GPT-4, Claude 3.5 Sonnet). A primary result is that Llama 3.1 405B Instruct requires at least 8 NVIDIA H100 GPUs and 405 GB VRAM to run locally in 8-bit precision. Principal implication is that, for AI practitioners, release decisions must consider access variables for effective risk assessment and deployment.  |
| X-Dancer: Expressive Music to Human Dance Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.17414) or [HuggingFace](https://huggingface.co/papers/2502.17414))| Chenxu Zhang, You Xie, Guoxian Song, Hongyi Xu, Zeyuan Chen | X-Dancer is a transformer-diffusion framework for generating music-driven human dance videos from a single image. The main research objective is to create diverse, long-range, and lifelike human dance videos synchronized with music, starting from a single static image. The key methodology involves a transformer that generates 2D pose sequences, and a diffusion model that translates these poses into video frames. The X-Dancer achieves a FVD score of 507.06 and FID-VID of 61.94 on their in-house dataset, surpassing all baselines in visual synthesis quality, which is a direct result of the method. AI practitioners can leverage this framework as a scalable solution for high-quality and expressive human image animation, with direct application in video content creation and customizable choreography.  |
| MONSTER: Monash Scalable Time Series Evaluation Repository (Read more on [arXiv](https://arxiv.org/abs/2502.15122) or [HuggingFace](https://huggingface.co/papers/2502.15122))| Amish Mishra, Lynn Miller, Chang Wei Tan, Navid Mohammadi Foumani, angus924 | MONSTER introduces a new benchmark for time series classification using larger datasets to address limitations of current benchmarks.  The main research objective is to create and evaluate a collection of large-scale time series datasets to improve benchmarking in time series classification.  Key methodologies include compiling 29 univariate and multivariate datasets, processing them into a common format, and evaluating baseline methods (ConvTran, FCN, HInceptionTime, TempCNN, HYDRA, QUANT, and ET) using 5-fold cross-validation.  Primary results show that QUANT achieved the lowest overall mean 0-1 loss (0.1880) across all datasets, closely followed by ConvTran (0.1954), although performance varied significantly across different data categories.  Principal implication for AI practioners is that that the field has artificially disadvanted low-bias methods and MONSTER can improve development and application in time series classification by training models on larger datasets.  |
| The snake in the Brownian sphere (Read more on [arXiv](https://arxiv.org/abs/2502.13074) or [HuggingFace](https://huggingface.co/papers/2502.13074))| Grégory Miermont, Brett Kolesnik, Emmanuel Jacob, Omer Angel | The paper describes the inverse of the continuous Cori-Vauquelin-Schaeffer (CVS) bijection, mapping the Brownian sphere to the Brownian snake. The main research objective is to construct the Brownian snake as a measurable function of the Brownian sphere, thereby inverting the continuous CVS bijection. The key methodology involves using the geometric notion of a cut locus on the Brownian sphere, defining a metric on the closure of the cut locus, and leveraging the induced orientation to define a planar order. The primary result is that, given a Brownian sphere (X,d,µ) and two independent points drawn from µ, there exists a measurable function outputting an R-tree T and label function Z such that T has the law of the Continuum Random Tree (CRT), and applying the continuum CVS mapping to (T, Z) recovers (X, d, μ). The paper proves that the orientation of the Brownian Sphere has a Rademacher distribution (equal to ±1 with equal probability), independently of the random variables ψ(h). AI/ML/Software Engineers/Data Scientist, can measurably recover the Brownian Snake and its associated tree structure from a given a Brownian Sphere, which provides new mathematical tooling and foundational understanding for models related to random planar maps.  |
| M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment (Read more on [arXiv](https://arxiv.org/abs/2502.15167) or [HuggingFace](https://huggingface.co/papers/2502.15167))| Weiming Zhang, Wen Shen, Zhihua Wei, Kejiang Chen, Chuan Cui | M3-AGIQA is a framework for assessing AI-generated image quality using multimodal inputs, multi-round interactions, and considering multiple quality aspects. The main research objective is to develop a comprehensive method for evaluating AI-generated images (AGIs) that aligns with human perceptual judgments across quality, correspondence, and authenticity. The key methodology involves distilling multi-aspect image captioning capabilities from online Multimodal Large Language Models (MLLMs) into a local MLLM via LoRA fine-tuning, and employing an xLSTM feature extractor with a regression head to predict Mean Opinion Scores (MOSs). The method achieved a Spearman's Rank-Order Correlation Coefficient (SRCC) of 0.9045 and a Pearson Linear Correlation Coefficient (PLCC) of 0.9317 on the quality aspect of the AGIQA-3k dataset. AI practitioners can utilize this framework to more accurately and comprehensively evaluate the quality of generated images, considering multiple factors that go beyond simple perceptual quality.  |
