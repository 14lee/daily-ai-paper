

## Papers for 2025-02-04

| Title | Authors | Summary |
|-------|---------|---------|
| The Differences Between Direct Alignment Algorithms are a Blur (Read more on [arXiv](https://arxiv.org/abs/2502.01237) or [HuggingFace](https://huggingface.co/papers/2502.01237))| Boris Shaposhnikov, kefirski, ZeL1k7, ummagumm-a, Myashka | The paper investigates Direct Alignment Algorithms (DAAs) for aligning language models with human preferences, focusing on their performance and key distinctions. The main research objective is to clarify the relationships and comparative advantages among various DAAs, particularly regarding the impact of an explicit Supervised Fine-Tuning (SFT) phase and a scaling parameter, β. The methodology involves incorporating an SFT phase and the β parameter into single-stage DAAs (ORPO and ASFT) and empirically evaluating their performance on benchmarks like Alpaca Eval 2 using Llama 3.1 8B and Llama 3.2 3B models. A primary result is that these modifications improved ORPO's performance on Alpaca Eval 2 by +3.46 and ASFT's by +8.27. The principal implication for AI practitioners is that incorporating an explicit SFT phase and tuning the β parameter can significantly enhance the alignment quality of single-stage DAAs, making them competitive with two-stage methods like DPO, and that pairwise methods often outperform pointwise objectives.  |
| Process Reinforcement through Implicit Rewards (Read more on [arXiv](https://arxiv.org/abs/2502.01456) or [HuggingFace](https://huggingface.co/papers/2502.01456))| Wendi Li, Zefan Wang, Lifan Yuan, hanbin, ganqu | The paper introduces PRIME, a scalable reinforcement learning framework for enhancing reasoning in large language models using dense token-level rewards. The main research question is how to acquire and utilize high-quality dense rewards at scale for efficient online process reward model (PRM) updates in reinforcement learning of large language models (LLMs). The key methodology is the use of implicit process rewards derived from an Implicit PRM, which is trained with outcome labels only and allows online updates using policy rollouts and outcome labels. The primary result is that Eurus-2-7B-PRIME, trained using PRIME, achieves a 15.1% average improvement across several reasoning benchmarks over the SFT model. The principal implication for AI practitioners is that PRIME offers an efficient way to incorporate dense rewards into reinforcement learning for LLMs, improving sample efficiency and performance without the need for dedicated reward model training or step-level annotations.  |
| OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models (Read more on [arXiv](https://arxiv.org/abs/2502.01061) or [HuggingFace](https://huggingface.co/papers/2502.01061))| Chao Liang, Zerong Zheng, Jiaqi Yang, Jianwen Jiang, Gaojie Lin | OmniHuman-1 is a diffusion-based model for generating human animation videos conditioned on multiple modalities, including text, audio, and pose. The main research objective is to address the challenge of scaling up training data for end-to-end human animation models. The key methodology is a mixed-condition training strategy using a Diffusion Transformer model that integrates text, audio, and pose as conditions, along with an "omni-conditions" approach to leverage data across different conditioning strengths. The primary results show that OmniHuman outperforms existing methods on portrait and body animation tasks, achieving a FID score of 16.970 on the RAVDESS dataset for portrait animation. The principal implication for AI practitioners is that the proposed omni-conditions training strategy effectively scales up human animation models by leveraging mixed-condition data, enabling the development of more versatile and realistic human video generation systems.  |
| Preference Leakage: A Contamination Problem in LLM-as-a-judge (Read more on [arXiv](https://arxiv.org/abs/2502.01534) or [HuggingFace](https://huggingface.co/papers/2502.01534))| Bohan Jiang, Ming Zhong, Yue Huang, Dawei Li, RLSNLP | This paper investigates preference leakage, a contamination issue in LLM-as-a-judge systems where evaluator LLMs exhibit biases towards related data generator LLMs. The main research question is whether preference leakage introduces systematic biases in LLM-based evaluations and, if so, to what extent. The key methodology involves training student models on synthetic data generated by different LLMs and then evaluating them using related and unrelated LLM judges, quantifying the bias through a "preference leakage score". A primary result is that the average preference leakage score for the Mistral-GPT-40 vs Mistral-Gemini-1.5 model pair on AlpacaEval 2.0 was 18.4%, indicating significant bias. The principal implication for AI practitioners is that using closely related LLMs for data generation and evaluation can lead to significant biases, artificially inflating performance metrics and compromising the reliability of assessments.  |
| SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2501.18636) or [HuggingFace](https://huggingface.co/papers/2501.18636))| Sensen Zhang, Zhiyu Li, Simin Niu, Xun Liang, UglyToilet | SafeRAG is a new benchmark to evaluate the security of retrieval-augmented generation (RAG) systems against data injection attacks. The main research question is: How vulnerable are RAG systems to attacks that manipulate external knowledge sources? The key methodology involves constructing a dataset, SafeRAG, with four attack types (silver noise, inter-context conflict, soft ad, and white Denial-of-Service) and evaluating 14 RAG components across different stages (indexing, retrieval, generation). A primary result is that the Baichuan 13B model achieved an attack failure rate (AFR) of 1.00 under the Denial-of-Service task, indicating complete resistance. The principal implication for AI practitioners is that current RAG systems, even advanced ones, are vulnerable to sophisticated data injection attacks, highlighting the need to develop more robust retrievers, filters, and generators when building RAG applications.  |
| FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation (Read more on [arXiv](https://arxiv.org/abs/2502.01068) or [HuggingFace](https://huggingface.co/papers/2502.01068))| Jae-Joon Kim, Yulhwa Kim, jiwonsong, dongwonjo | FastKV introduces a novel KV cache compression method for large language models (LLMs) to improve efficiency in long-context processing. The main research question is how to enhance the latency and throughput of LLMs handling long-context sequences while maintaining accuracy. The key methodology is Token-Selective Propagation (TSP), which retains full context in initial layers and selectively propagates crucial tokens in deeper layers, alongside grouped-query attention (GQA)-aware KV cache compression. The primary results show that FastKV achieves 2.00x improvement in time-to-first-token (TTFT) and 1.40x improvement in throughput compared to HeadKV. The principal implication for AI practitioners is that FastKV can be used as a drop-in replacement in existing LLMs to significantly reduce latency and increase throughput in long-context processing without sacrificing accuracy.  |
| Almost Surely Safe Alignment of Large Language Models at Inference-Time (Read more on [arXiv](https://arxiv.org/abs/2502.01208) or [HuggingFace](https://huggingface.co/papers/2502.01208))| Jun Wang, Ilija Bogunovic, Matthieu Zimmer, Shyam Sundhar Ramesh, Xiaotong Ji | This paper introduces InferenceGuard, a novel inference-time alignment method that ensures large language models (LLMs) generate safe responses with a probability approaching one. The main research question is how to guarantee safe outputs from LLMs during inference without modifying model weights. The key methodology involves framing safe inference-time alignment as a constrained Markov decision process (cMDP), augmenting the state space with a safety constraint tracker, and training a critic in the latent space to guide a lookahead search algorithm. The primary results show that InferenceGuard achieved safety rates of 98.02% on Alpaca-7B and 100% on Beaver-7B-v3 while maintaining strong task performance. The principal implication for AI practitioners is that InferenceGuard offers a practical and theoretically sound approach for safely aligning LLMs during inference, enhancing their usability in real-world applications without the need for retraining.  |
| DeepRAG: Thinking to Retrieval Step by Step for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.01142) or [HuggingFace](https://huggingface.co/papers/2502.01142))| Yaojie Lu, Chunlei Xin, Fandong Meng, Jiali Zeng, xinyan233333 | DeepRAG is a retrieval-augmented generation framework that models retrieval-augmented reasoning as a Markov Decision Process for improved efficiency and accuracy. The main research question is how to optimize retrieval-augmented reasoning in large language models by dynamically determining when to retrieve external knowledge versus relying on parametric reasoning. The key methodology is a Markov Decision Process framework called DeepRAG, which uses binary tree search, imitation learning, and chain of calibration to enable strategic and adaptive retrieval. Primary results show that DeepRAG improves answer accuracy by 21.99% while also enhancing retrieval efficiency. The principal implication for AI practitioners is that DeepRAG provides a more effective framework for retrieval-augmented reasoning compared to existing methods, and it achieves superior performance by using dynamic cognitive decision-making.  |
| ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.01100) or [HuggingFace](https://huggingface.co/papers/2502.01100))| Radha Poovendran, Ashish Sabharwal, Kyle Richardson, ronanlb, yuchenlin | ZebraLogic is a framework for evaluating the logical reasoning abilities of large language models (LLMs) using logic grid puzzles. The main research question is how LLM performance on logical reasoning tasks scales with problem complexity. The key methodology involves generating logic grid puzzles with controllable complexity using constraint satisfaction problems and evaluating various LLMs' performance. Primary results show a significant decline in accuracy as problem complexity increases, with most models struggling when the puzzle's search space exceeds 10^7 possibilities (e.g., gpt-40-mini achieves only 20.1% overall accuracy). The principal implication for AI practitioners is that scaling model size or training data alone is insufficient for solving complex logical reasoning tasks, and increasing test-time compute via more reasoning steps can improve performance.  |
| The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles (Read more on [arXiv](https://arxiv.org/abs/2502.01081) or [HuggingFace](https://huggingface.co/papers/2502.01081))| Soujanya Poria, Deepanway Ghosal, Yew Ken Chia, Vernon Y. H. Toh | The paper tracks the evolution of multimodal reasoning in GPT-[n] and o-[n] models using visual puzzles. The main research question is how the reasoning performance of these models evolves over time on multimodal puzzles. The key methodology involves evaluating the models on PUZZLEVQA and ALGOPUZZLEVQA datasets using multiple-choice and open-ended questions, with a two-stage prompting strategy for answer extraction. Primary results show that the o1 model achieved 79.2% accuracy on PUZZLEVQA in the multiple-choice setting, but all models performed significantly worse in open-ended settings. The principal implication for AI practitioners is that despite improvements, current models still have limitations in visual perception and abstract reasoning, suggesting a need for further development in these areas.  |
| Improving Transformer World Models for Data-Efficient RL (Read more on [arXiv](https://arxiv.org/abs/2502.01591) or [HuggingFace](https://huggingface.co/papers/2502.01591))| Wolfgang Lehrach, Carter Wendelken, Xinghua Lou, Joseph Ortiz, Antoine Dedieu | This paper introduces a model-based reinforcement learning (MBRL) agent that achieves state-of-the-art performance on the Craftax-classic benchmark. The main research question is how to improve the sample efficiency of MBRL agents in complex, open-world environments like Craftax-classic. The key methodology involves combining a novel policy architecture (CNNs and RNNs) with three main improvements to transformer world models (TWMs): "Dyna with warmup", "nearest neighbor tokenizer" on image patches, and "block teacher forcing". The primary result is that the proposed MBRL agent achieves a reward of 67.42% after only 1 million environment steps, significantly outperforming DreamerV3, which achieves 53.2%. The principal implication for AI practitioners is that the combination of these techniques provides a more sample-efficient approach to training reinforcement learning agents in environments requiring strong generalization, deep exploration, and long-term reasoning.  |
| Improved Training Technique for Latent Consistency Models (Read more on [arXiv](https://arxiv.org/abs/2502.01441) or [HuggingFace](https://huggingface.co/papers/2502.01441))| Dimitris Metaxas, Di Liu, Khanh Doan, trungleuc, quandao10 | This paper introduces an improved training technique for latent consistency models (CMs) to address their suboptimal performance in the latent space compared to pixel space. The main research question is: How can the performance of consistency models in latent space be improved? The key methodology involves replacing Pseudo-Huber loss with Cauchy loss to mitigate the impact of impulsive outliers in latent data, introducing a diffusion loss at early timesteps, employing optimal transport (OT) coupling, using an adaptive scaling-c scheduler, and adopting Non-scaling LayerNorm. The primary result is that the proposed method achieves a FID score of 7.27 for 1-NFE sampling on the CelebA-HQ dataset, a significant improvement over the baseline iLCT model's FID of 37.15. For AI practitioners, this improved training technique enables the development of more effective latent consistency models capable of generating high-quality samples with one or two steps.  |
| Scaling Embedding Layers in Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.01637) or [HuggingFace](https://huggingface.co/papers/2502.01637))| Pritish Kamath, Yangsibo Huang, Badih Ghazi, Edith Cohen, Da Yu | The paper introduces SCONE, a method for scaling input embedding layers in language models without increasing inference-time cost. The main research question is how to enhance language model performance by extending input embedding layers while retaining the original vocabulary and avoiding increased decoding costs. The key methodology involves introducing embeddings for frequent n-grams (f-grams) that are learned with a separate model during training and precomputed/stored off-accelerator for inference. A primary result is that a 1B parameter model using SCONE with 1B f-grams outperformed a 1.9B parameter baseline on the OLMo evaluation mixture, achieving a perplexity of 14.581 compared to 14.598 for the baseline. The principal implication for AI practitioners is that SCONE enables more efficient scaling of language models by leveraging larger embedding layers without impacting inference-time FLOPS, allowing for improved performance within a fixed computational budget.  |
| PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.01584) or [HuggingFace](https://huggingface.co/papers/2502.01584))| Molly Q Feldman, Federico Cassano, Aleksander Boruch-Gruszecki, Joydeep Biswas, Carolyn Jane Anderson | This paper introduces a benchmark based on the NPR Sunday Puzzle Challenge to evaluate reasoning in large language models using general knowledge questions. The main research objective is to develop a benchmark that tests reasoning capabilities of large language models on problems that are challenging yet require only general knowledge, unlike existing benchmarks that rely on specialized, "PhD-level" knowledge. The key methodology involves curating a dataset of nearly 600 problems from the NPR Sunday Puzzle, prompting models to answer these problems zero-shot, and evaluating their accuracy. The primary results show that OpenAI's o1 model achieves 59% accuracy, significantly outperforming other models, including DeepSeek R1, which achieved 35% accuracy. The principal implication for AI practitioners is that this benchmark reveals capability gaps in reasoning models that are not evident in benchmarks requiring specialized knowledge, and it highlights specific failure modes like models "giving up" or getting stuck in reasoning.  |
| Lifelong Sequential Knowledge Editing without Model Degradation (Read more on [arXiv](https://arxiv.org/abs/2502.01636) or [HuggingFace](https://huggingface.co/papers/2502.01636))| Thomas Hartvigsen, Ahmed Alaa, Maochuan Lu, Phudish Prateepamornkul, akshat57 | This paper introduces a method for lifelong sequential knowledge editing in large language models without significant model degradation. The main research question is how to perform sequential knowledge edits on large language models without causing catastrophic forgetting or loss of downstream performance. The key methodology used is a novel approach called ENCORE, which combines Most-Probable Early Stopping (MPES) during gradient descent with a Frobenius-norm constraint on the weight updates during the least-squares optimization step. The primary results show that ENCORE can perform 10,000 sequential edits without loss of downstream performance and is 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B. The principal implication for AI practitioners is that ENCORE enables more efficient and robust sequential knowledge editing, allowing for continual updating of models without significant degradation in performance on downstream tasks.  |
| Current Pathology Foundation Models are unrobust to Medical Center Differences (Read more on [arXiv](https://arxiv.org/abs/2501.18055) or [HuggingFace](https://huggingface.co/papers/2501.18055))| Jonas Teuwen, Eric Marcus, EdwinDdeJong | Here is a concise summary of the research paper:  i) This paper evaluates the robustness of current pathology foundation models (FMs) to medical center differences, finding significant sensitivity to this confounding factor. ii) The main research objective is to measure whether pathology FMs focus on biological features like tissue and cancer type, or on confounding medical center signatures. iii) The key methodology used is the introduction of a "Robustness Index" to quantify the degree to which biological features dominate confounding features in the FM embedding space, along with an analysis of the impact of unrobustness on downstream model performance. iv) The primary results show that all evaluated pathology FMs represent the medical center to a strong degree, with the Virchow2 model achieving the highest Robustness Index of 1.20, indicating that it is the only model where biological information dominated the medical center information for the first 50 neighbors. v) The principal implication for AI practitioners is that current pathology FMs are highly sensitive to medical center variations, and this sensitivity affects downstream tasks such as cancer type classification, highlighting the need for models that are more robust to such confounding factors for reliable clinical applications.  |
| A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation (Read more on [arXiv](https://arxiv.org/abs/2502.00314) or [HuggingFace](https://huggingface.co/papers/2502.00314))| Rebecca Scalabrino, Daniel Hsu, Alexander Manzella, Ehsan Khodapanah Aghdam, Moein Heidari | Here is a summary of the research paper based on the provided guidelines:  This study evaluates U-Net variants for segmenting retroperitoneal tumors in CT images, introducing a novel architecture called ViLU-Net. The main research question is how the performance of U-Net-based models incorporating convolutional neural networks (CNNs), Vision Transformers (ViTs), Mamba, and xLSTM components compares in segmenting retroperitoneal tumors. The key methodology involves implementing and training various U-Net modifications, including the proposed ViLU-Net which integrates Vision x-LSTM (ViL) blocks within a U-shaped encoder-decoder framework, on a new dataset of 82 retroperitoneal tumor CT cases and the public FLARE 2022 dataset. The primary results show that ViLU-Net achieved the highest average Dice Similarity Coefficient (DSC) of 0.8594 on the abdomen CT dataset among the tested models. The principal implication for AI practitioners is that xLSTM-based architectures like ViLU-Net offer a promising approach for medical image segmentation, demonstrating superior performance with reduced complexity compared to existing models.  |
