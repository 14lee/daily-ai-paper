

## Papers for 2025-02-21

| Title | Authors | Summary |
|-------|---------|---------|
| SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines (Read more on [arXiv](https://arxiv.org/abs/2502.14739) or [HuggingFace](https://huggingface.co/papers/2502.14739))| Liam-Liu, kangz, aaabiao, BingliW, mkj69 | SuperGPQA is a new benchmark for evaluating LLMs across 285 graduate-level disciplines, utilizing a human-LLM collaborative filtering mechanism.  i) SuperGPQA is a new challenging benchmark for evaluating large language model knowledge and reasoning at the graduate level.  ii) Main research question/objective: To assess the capabilities of LLMs across a wide range of specialized, graduate-level academic disciplines, exceeding the scope of existing benchmarks.  iii) Key methodology: A human-LLM collaborative filtering system was employed, involving crowd-sourcing annotators, experts, and SOTA LLMs with iterative refinement of questions based on LLM responses and expert feedback, followed by a 3-stage quality inspection process.  iv) Primary results: The reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA, demonstrating significant room for improvement for current LLMs.  v) Principal implication for AI practitioners: The benchmark reveals a substantial gap between current LLM capabilities and graduate-level human expertise, highlighting the need for developing models with enhanced reasoning and specialized domain knowledge to advance research towards Artificial General Intelligence.  |
| MLGym: A New Framework and Benchmark for Advancing AI Research Agents (Read more on [arXiv](https://arxiv.org/abs/2502.14499) or [HuggingFace](https://huggingface.co/papers/2502.14499))| Nikolay Bashlykov, Nicholas Roberts, Lovish Madaan, rraileanu, dnathani | MLGYM is a new Gym environment and benchmark, MLGYM-Bench, for evaluating and developing LLM agents on 13 diverse, open-ended AI research tasks. The main research objective is to create a standardized framework for evaluating LLM agents on their ability to perform realistic AI research tasks, enabling research on reinforcement learning algorithms. The key methodology is a Gym environment that integrates diverse AI research tasks, allowing agents to interact with a shell environment using tools, with performance evaluated via task-specific scripts. A primary result is that OpenAI's O1-preview model achieved the highest Best Submission AUP@4 score of 1.176 across all tasks, followed by Gemini-1.5-Pro at 1.125. AI practitioners can utilize MLGYM to develop and assess AI research agents, driving progress in automating complex machine-learning research workflows, and apply different training algorithms for AI agents such as reinforcement learning.  |
| SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features (Read more on [arXiv](https://arxiv.org/abs/2502.14786) or [HuggingFace](https://huggingface.co/papers/2502.14786))| Xiao Wang, talfanevans, ibomohsin, AlexeyG, mitsch | SigLIP 2, a family of multilingual vision-language encoders, improves upon SigLIP with enhanced semantic understanding, localization, and dense features. The main research objective is to develop vision-language encoders that outperform existing models, including SigLIP, across various tasks while supporting multiple languages. The key methodology involves combining the original SigLIP training recipe with decoder-based pretraining, self-distillation, masked prediction, and online data curation, applied in a staged training approach. Primary results show that SigLIP 2 outperforms SigLIP and other open-weight baselines on ImageNet zero-shot classification; for example a SigLIP 2 B/16 model achieves 79.1% accuracy compared to SigLIP's 76.7% at 256x256 resolution. AI practitioners can leverage SigLIP 2's improved encoders for enhanced performance in vision-language tasks, particularly benefiting from multilingual capabilities, strong dense features, and backward compatibility with SigLIP.  |
| S*: Test Time Scaling for Code Generation (Read more on [arXiv](https://arxiv.org/abs/2502.14382) or [HuggingFace](https://huggingface.co/papers/2502.14382))| Shangyin Tan, Xiuyu Li, Chengkun Cao, Dacheng Li, eva98 | S* is a hybrid test-time scaling framework that improves code generation by combining parallel and sequential scaling with adaptive input synthesis for selection. The main research objective is to improve the coverage and selection accuracy of generated code by extending existing test-time scaling paradigms. The key methodology involves augmenting parallel sampling with sequential scaling via iterative debugging, and introducing a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison of candidate solutions, grounded in execution results. Results show that S* consistently improves performance across 12 Large Language Models, with DeepSeek-R1-Distill-Qwen-32B achieving 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. The principal implication for AI practitioners is that combining parallel and sequential scaling with execution-grounded adaptive input synthesis during test-time significantly improves code generation performance, enabling smaller or instruction-based models to surpass larger or reasoning models.  |
| How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM? (Read more on [arXiv](https://arxiv.org/abs/2502.14502) or [HuggingFace](https://huggingface.co/papers/2502.14502))| Vasily Konovalov, Daniil Moskovskiy, Maria Marina, msalnikov, memyprokotow | This paper investigates how much new factual knowledge can be incorporated into a Large Language Model (LLM) using Low-Rank Adaptation (LoRA) without compromising pre-existing knowledge. The main research objective is to determine the extent to which new facts can be integrated into an LLM via a LoRA adapter while preserving general capabilities. The key methodology involves fine-tuning a Llama-3.1-8B-Instruct model using LoRA with varying amounts of new knowledge (DBpedia triples) and evaluating performance on external benchmarks (MMLU, TruthfulQA) and internal metrics (knowledge shifts). A primary result is that a model trained on 500 unknown facts, achieved 100% reliability on test, while models trained with additional highly-known data could see minimized negative shifts; Accuracy of models trained on MMLU with added 10 HighlyKnown or paraphrased sample show a significant drop in accuracy. The principal implication for AI practitioners is that while LoRA is effective for incorporating new knowledge, there is a trade-off between new knowledge integration, reduced truthfulness and general reasoning capabilities, requiring careful consideration of training data composition.  |
| Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information (Read more on [arXiv](https://arxiv.org/abs/2502.14258) or [HuggingFace](https://huggingface.co/papers/2502.14258))| Jaewoo Kang, Minbyul Jeong, Jungwoo Park, Chanwoong Yoon, Yein Park | Language models possess specialized attention heads, termed "Temporal Heads," that are primarily responsible for processing time-specific factual knowledge. The research objective is to identify and analyze the mechanisms within large language models (LLMs) that handle temporally-changing facts. The methodology utilizes Circuit Analysis, specifically Temporal Knowledge Circuits and attention head ablation, to isolate and evaluate the contribution of specific attention heads. Ablating identified Temporal Heads reduced the model's temporal knowledge accuracy in Llama2 by 3-9%, while its performance on time-invariant tasks remains unchanged. AI practitioners can leverage identified Temporal Heads to edit or control temporal aspects of LLM outputs, minimizing retraining.  |
| LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.14834) or [HuggingFace](https://huggingface.co/papers/2502.14834))| Jifan Yu, Yushi Bai, Daniel Zhang-Li, Yucheng Wang, Shangqing Tu | LongWriter-V enhances vision-language models (VLMs) for generating ultra-long, high-fidelity text from visual inputs.  The main research objective is to address the limitation of existing VLMs in generating coherent outputs beyond 1,000 words, despite their ability to process long visual and textual contexts.  Key methodology involved creating a new dataset, LongWriter-V-22k, with 22,158 examples of multi-image inputs and long text outputs (up to 10,000 words), and proposing IterDPO, a modified direct preference optimization method for long text.  Primary results show that the 7B parameter model trained with LongWriter-V-22k and IterDPO outperformed larger proprietary models like GPT-4o on the MMLongBench-Write benchmark, achieving an overall score of 84.6, including component scores of 86.2 (length) and 82.9 (quality).  Principal implication for AI practitioners is that using specialized datasets with long-output examples and iterative preference optimization can significantly improve the long-text generation capabilities of VLMs, enabling more effective real-world applications requiring detailed visual descriptions or reports.  |
| Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.14768) or [HuggingFace](https://huggingface.co/papers/2502.14768))| Yuqian Hong, Haoming Luo, Qingnan Ren, Zitian Gao, Tian Xie | Logic-RL explores rule-based reinforcement learning (RL) to enhance reasoning in large language models (LLMs) using synthetic logic puzzles. The main research objective is to investigate if rule-based RL can improve LLM reasoning abilities and generalization to unseen tasks. The key methodology involves training a 7B parameter LLM with a modified REINFORCE++ algorithm, using a system prompt, a stringent format reward, and procedurally generated Knights and Knaves logic puzzles. The primary result is that after training on 5,000 logic problems, the model improved by 125% on the AIME math benchmark and 38% on the AMC, demonstrating cross-domain generalization. For AI practitioners, this demonstrates that RL, even with limited synthetic data, can significantly enhance an LLM's abstract reasoning and generalization capabilities, offering a potentially more effective approach than supervised fine-tuning for specialized reasoning tasks.  |
| PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC (Read more on [arXiv](https://arxiv.org/abs/2502.14282) or [HuggingFace](https://huggingface.co/papers/2502.14282))| Junyang Wang, Yuyang Wanyan, Haiyang Xu, Xi Zhang, Haowei Liu | PC-Agent is a hierarchical multi-agent framework designed to automate complex tasks on PCs by improving perception and decision-making.  The main research objective is to develop a system that can handle complex user instructions and interdependent sub-tasks in PC environments, overcoming limitations of existing methods in perception and workflow management.  The key methodology is a hierarchical multi-agent collaboration architecture that decomposes decision-making into Instruction-Subtask-Action levels, with specialized agents (Manager, Progress, Decision, Reflection) and an Active Perception Module (APM).  The primary result is that PC-Agent achieved a 56.0% task success rate on the PC-Eval benchmark, a 32% absolute improvement over previous state-of-the-art methods.  Principal implication for AI practitioners is that the proposed framework significantly enhances the capability of agents to automate real-world, complex tasks on PCs.  |
| S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.12853) or [HuggingFace](https://huggingface.co/papers/2502.12853))| Jiaqi Chen, Xingyan Liu, Cheng Liu, Peisong Wang, Ruotian Ma | S$^2$R is a framework that enhances Large Language Model (LLM) reasoning by teaching models to self-verify and self-correct during inference via reinforcement learning. The main research objective is to develop an efficient framework that improves LLM reasoning abilities, particularly in mathematical problem-solving, without requiring large-scale data or extensive training. The key methodology involves initializing LLMs with self-verification and self-correction behaviors through supervised fine-tuning, then strengthening these skills using outcome-level and process-level reinforcement learning. Results demonstrate that a Qwen2.5-math-7B model, trained with only 3.1k initialization samples, achieved an accuracy improvement from 51.0% to 81.6% on the MATH500 test set. For AI practitioners, this implies that implementing self-verification and self-correction via reinforcement learning offers a resource-efficient approach to substantially improve the mathematical reasoning capabilities of LLMs, potentially using process-level RL for weaker base models and outcome-level RL for stronger ones.  |
| Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning (Read more on [arXiv](https://arxiv.org/abs/2502.14372) or [HuggingFace](https://huggingface.co/papers/2502.14372))| Zi-Wen Liu, basil2115 | This paper introduces a reinforcement learning (RL) based method for discovering highly efficient low-weight quantum error-correcting (QEC) codes. The main research objective is to develop a method that optimizes the weight of measurements in stabilizer codes while preserving code distance, targeting practically relevant parameter regimes. The key methodology is a Proximal Policy Optimization (PPO) RL algorithm with action masking, operating on Tanner graphs of stabilizer codes, guided by a reward function that balances node degree reduction and code distance preservation. A primary result is that the RL-based method achieves up to a 73x reduction in physical qubit overhead compared to previous weight reduction methods like Sabo et al. (for a [1109,9,14](8,13) code). AI practitioners can adapt this RL framework to design low-weight QEC codes with constraints tailored to specific quantum computing architectures, potentially accelerating the implementation of fault-tolerant quantum technologies.  |
| Dynamic Concepts Personalization from Single Videos (Read more on [arXiv](https://arxiv.org/abs/2502.14844) or [HuggingFace](https://huggingface.co/papers/2502.14844))| Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Or Patashnik, Rameen Abdal | The paper introduces "Set-and-Sequence," a framework for personalizing text-to-video models with dynamic concepts from single videos, enabling high-fidelity generation, editing, and composition. The main objective is to personalize diffusion transformer-based generative video models to capture dynamic concepts, defined by both appearance and motion, from single video examples. The key methodology is a two-stage LoRA training process: (i) "Identity Basis" learning using an unordered set of frames to capture appearance, and (ii) "Motion Residual" encoding using the full video sequence to capture motion dynamics, implemented within a shared spatio-temporal weight space. In editing tasks, the proposed method achieved a mean squared error (MSE) of 0.0221, an identity preservation (ID) score of 0.680, a clip text similarity (C-T) score of 0.239 and a temporal coherency (TC) score of 0.9972. AI practitioners can leverage this framework to embed personalized dynamic concepts into video generation models, improving control over both appearance and motion for enhanced editing and composition capabilities.  |
| Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation (Read more on [arXiv](https://arxiv.org/abs/2502.14846) or [HuggingFace](https://huggingface.co/papers/2502.14846))| Luca Weihs, Tanmay Gupta, Matt Deitke, Ajay Patel, Yue Yang | The paper introduces CoSyn, a framework for generating synthetic text-rich multimodal data to improve vision-language model (VLM) performance.  Main research question or objective: Can leveraging the coding capabilities of text-only large language models (LLMs) automatically generate synthetic text-rich multimodal data to address the limited availability of such data for training VLMs?  Key methodology used: The CoSyn framework prompts LLMs to generate code (e.g., Python, HTML, LaTeX) that renders synthetic images, and uses this code as a textual representation to create instruction-tuning data.  Primary results: Models trained on CoSyn synthetic data achieved state-of-the-art performance among competitive open-source models on seven text-rich image benchmarks, and models trained on synthetic data boosted average accuracy by 3.6%.  Principal implication for AI practitioners: AI practitioners can use the CoSyn framework to generate targeted synthetic text-rich data efficiently, improving VLM performance in specific domains and mitigating the limitations of scarce real-world data.  |
| AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO (Read more on [arXiv](https://arxiv.org/abs/2502.14669) or [HuggingFace](https://huggingface.co/papers/2502.14669))| Dinh Bach Vu, Alan Dao | AlphaMaze trains large language models (LLMs) on tokenized maze representations to improve spatial reasoning for navigation. The research investigates how to equip standard LLMs with visual reasoning abilities for maze navigation using a two-stage training framework. The methodology combines Supervised Fine-Tuning (SFT) on tokenized maze data and Group Relative Policy Optimization (GRPO) with a custom reward function. Results show the SFT-trained model achieved 86% accuracy on a maze navigation benchmark, which increased to 93% after GRPO fine-tuning. AI practitioners can leverage this two-stage training approach (SFT and GRPO) with tokenized visual representations to enhance LLMs' spatial reasoning capabilities in tasks requiring sequential decision-making.  |
| How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild (Read more on [arXiv](https://arxiv.org/abs/2502.12769) or [HuggingFace](https://huggingface.co/papers/2502.12769))| Goran Glavaš, Anne Lauscher, saadob12 | This paper investigates the extent of hallucination in large language models (LLMs) across 30 languages in open-domain, knowledge-intensive question answering.  The main research question is: How frequently do LLMs hallucinate across different languages and model sizes in a "real-world" question-answering setting, and how does this relate to language resource availability?  Key methodology: The researchers trained a multilingual hallucination detection model using machine-translated English data and created a multilingual evaluation dataset (MFAVA) with LLM-generated and human-annotated examples. They then estimated hallucination rates for six open-source LLM families across 30 languages using a novel protocol based on the detection model's performance.  Primary results: Smaller LLMs and those supporting more languages exhibited significantly higher hallucination rates. The average hallucination rate across languages varied from 7% to 12%. However, there was no correlation between language-normalized hallucination rates and digital language representation.  Principal implication for AI practitioners: AI practitioners should be aware that smaller LLM model sizes and models designed for broad multilingual support may be more prone to generating non-factual or unfaithful content in question-answering tasks, necessitating careful model selection and potentially requiring additional mitigation strategies.  |
| Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework (Read more on [arXiv](https://arxiv.org/abs/2502.13759) or [HuggingFace](https://huggingface.co/papers/2502.13759))| Zeyu Zhang, Jonathan Tonglet, Yuan Huang, Jingpu Yang, Ziruibest | This paper introduces a new geolocation framework, including a large-scale dataset, a novel reasoning method, and an evaluation metric, to address challenges in image geolocation. The main research objective is to improve the accuracy and interpretability of image geolocation using real human gameplay data and a human-like reasoning approach. The key methodology involves collecting data from a geolocation game platform (GeoComp dataset), proposing a multi-step reasoning framework (Geographical Chain-of-Thought, GeoCoT), and developing an evaluation metric (GeoEval). The primary results show that GeoCoT improves geolocation accuracy by up to 25% compared to existing methods, achieving a city-level accuracy of 0.118. AI practitioners can leverage the GeoComp dataset and GeoCoT framework to develop and evaluate more robust and interpretable geolocation models, particularly for applications requiring fine-grained localization and human-like reasoning.  |
| RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2502.14377) or [HuggingFace](https://huggingface.co/papers/2502.14377))| Zhanjie Zhang, Jiasong Feng, Ao Ma, Jing Wang, Ke Cao | RelaCtrl is a framework for efficient controllable generation in Diffusion Transformers, optimizing the integration of control signals. The main objective is to address the high parameter and computational overhead of existing controlled diffusion transformer methods, and their inefficient resource allocation. The key methodology involves evaluating layer relevance to control information using a "ControlNet Relevance Score," tailoring control layer positioning/capacity, and replacing self-attention/FFN with a Two-Dimensional Shuffle Mixer (TDSM). The approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-δ, as per quantitative experimental results. For AI practitioners, RelaCtrl offers a method for significantly improving the efficiency of controlled image and video generation using Diffusion Transformers, reducing resource demands without compromising output quality.  |
| LLM-based User Profile Management for Recommender System (Read more on [arXiv](https://arxiv.org/abs/2502.14541) or [HuggingFace](https://huggingface.co/papers/2502.14541))| Hwanjun Song, Breadbang | PURE is an LLM-based recommendation framework that constructs and maintains evolving user profiles for zero-shot recommendation. The main research objective is to develop a system that can effectively leverage user-generated textual data, beyond purchase history, to improve recommendation accuracy in a continuously evolving setting. The key methodology is PURE, composed of a Review Extractor (extracting preferences from reviews), a Profile Updater (refining user profiles), and a Recommender (generating recommendations using updated profiles). Experimental results on Amazon datasets show that PURE (ICL) achieves an N@10 score of 35.60 on Games and 32.03 on Movies, outperforming baselines that only use purchase history or naively combine reviews. For AI practitioners, PURE demonstrates the concrete value of incorporating long-term review data and user preference through structured profiles.  |
| Unstructured Evidence Attribution for Long Context Query Focused Summarization (Read more on [arXiv](https://arxiv.org/abs/2502.14409) or [HuggingFace](https://huggingface.co/papers/2502.14409))| David Jurgens, Isabelle Augenstein, Lu Wang, Zain Muhammad Mujahid, dwright37 | Here's a 4-5 sentence summary of the provided AI research paper, adhering to your guidelines:  1.  **1-Line Summary:** This paper introduces the task of long-context, query-focused summarization with unstructured evidence citation, and proposes a synthetic dataset (SUnsET) to improve models' ability to extract and cite relevant evidence spans.  2.  **Main Research Question/Objective:** The primary objective is to investigate how well LLMs can generate query-focused summaries from long contexts while citing unstructured evidence, and how to mitigate positional biases (like "lost-in-the-middle") affecting evidence selection.  3.  **Key Methodology:** The authors create SUnsET, a synthetic dataset generated via a novel domain-agnostic pipeline, and use it to fine-tune LLMs with LoRA adapters. They evaluate on four datasets of varying document types/lengths, using position-aware and position-agnostic training.  4.  **Primary Results:** Fine-tuning on SUnsET significantly improves evidence extraction and citation accuracy across multiple LLMs and datasets. A key quantitative finding is citation rates increase dramatically: (6.8× for Mixtral 8x7B with position-aware training). Training also improves summary quality, though shuffling document sections during training can mitigate positional biases.  5.  **Principal Implication for AI Practitioners:** AI practitioners can use the SUnsET dataset and fine-tuning approach to adapt LLMs for improved unstructured evidence citation in long-context summarization, leading to more transparent and reliable summaries, but must be aware that current methods are prone to errors.  |
