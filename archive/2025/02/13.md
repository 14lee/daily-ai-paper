

## Papers for 2025-02-13

| Title | Authors | Summary |
|-------|---------|---------|
| TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation (Read more on [arXiv](https://arxiv.org/abs/2502.07870) or [HuggingFace](https://huggingface.co/papers/2502.07870))| Zhuobai Dong, Weiming Han, Jiawei Zhang, Dongxing Mao, Alex Jinpeng Wang | TextAtlas5M is a large-scale dataset designed for generating images with dense, complex, and long-form text. The main research objective is to address the limitations of existing datasets, which often focus on shorter and simpler text, thereby hindering the development of models capable of generating images with comprehensive textual content. The key methodology involves curating 5 million long-text generated and collected images across diverse data types, including synthetic and real-world images, and creating a human-improved test set (TextAtlasEval) of 3,000 samples across 3 data domains. Primary results include the finding that evaluations demonstrate even advanced proprietary models (e.g., GPT4o with DallE-3) are significantly challenged by TextAtlasEval benchmarks, while showing an even large gap in their open-source counterparts. This dataset and benchmarks provide AI practitioners with a valuable resource for training and evaluating text-conditioned image generation models, specifically focusing on dense and long-form text rendering, thus, advancing the capacity to control visual outputs.  |
| Light-A-Video: Training-free Video Relighting via Progressive Light Fusion (Read more on [arXiv](https://arxiv.org/abs/2502.08590) or [HuggingFace](https://huggingface.co/papers/2502.08590))| Pan Zhang, Pengyang Ling, Jiazi Bu, Yujie Zhou, yuhangzang | Light-A-Video is a training-free approach for temporally smooth video relighting that leverages image relighting and video diffusion models. The main research objective is to achieve temporally consistent video relighting without requiring training or optimization, addressing the limitations of existing methods. The key methodology involves a Consistent Light Attention (CLA) module for stable light source generation and a Progressive Light Fusion (PLF) strategy to blend relighted appearances, incorporating motion priors from a video diffusion model. Primary results show that Light-A-Video achieves a FID score of 29.63 while maintaining a temporal consistency CLIP score of 0.9655, superior to baseline methods that apply image relighting frame-by-frame. For AI practitioners, Light-A-Video provides a training-free pipeline for high-quality video relighting, directly applicable with existing image relighting and video diffusion models, enabling zero-shot illumination control of video sequences.  |
| BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.07346) or [HuggingFace](https://huggingface.co/papers/2502.07346))| Lei Li, Conghui He, Hanxu Hu, Wenhao Zhu, ggdcr | BenchMAX is a multi-way multilingual evaluation benchmark for assessing advanced capabilities of large language models (LLMs) across 17 languages. The main research objective is to create a benchmark that fairly compares LLM capabilities like instruction following, reasoning, and code generation across diverse languages and script systems. The methodology involves machine-translating English tasks into 16 other languages, followed by independent annotation by three native speakers for each sample and task, and final version selection using a strong LLM. A key finding is that DeepSeek-V3 671B model achieved 84.2% on Math and 47.4 on Science reasoning tasks, respectively. For AI practitioners, BenchMAX provides a platform to evaluate LLM performance across languages to improve their multilingual capabilities.  |
| CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.08639) or [HuggingFace](https://huggingface.co/papers/2502.08639))| Huchuan Lu, Xu Jia, Xiaoyu Shi, Yawen Luo, Qinghe Wang | CineMaster is a novel framework for 3D-aware and controllable text-to-video generation, enabling cinematic video creation with precise object placement and camera control. The main research objective is to provide users with 3D-aware and intuitive control over text-to-video generation, similar to the control wielded by film directors. The proposed two-stage framework first allows users to construct 3D scenes and camera movements via an interactive workflow, then uses the generated depth maps, camera trajectories, and object labels to guide a text-to-video diffusion model. CineMaster achieves a mean Intersection over Union (mIoU) of 0.551 and a trajectory deviation (Traj-D) of 66.29, outperforming existing methods in object-box alignment. For AI practitioners, this framework provides a new paradigm for controllable video generation, using a 3D-native approach to enable precise manipulation of scene elements and camera movement directly from textual input and 3D scene descriptions.  |
| WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2502.08047) or [HuggingFace](https://huggingface.co/papers/2502.08047))| Mike Zheng Shou, Difei Gao, Henry Hengyuan Zhao | WorldGUI introduces a new benchmark and framework, GUI-Thinker, for dynamic testing of desktop GUI automation agents. The main research objective is to evaluate and improve GUI agents' ability to handle diverse initial states and dynamic environments in real-world computer interactions. The methodology involves creating a benchmark (WorldGUI) with 315 tasks across 10 applications, each with varied starting states, and proposing a critical-thinking-based framework (GUI-Thinker) with five core components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. Experimental results demonstrate that GUI-Thinker significantly outperforms existing agents, with the Claude-3.5 based GUI-thinker achieving a 32.4% overall success rate, and GPT-40 based agent achieving 36.2%, exceeding a baseline by 14.9%. For AI practitioners, WorldGUI provides a robust benchmark to test and enhance agent adaptability in varied, dynamic states.  |
| LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid (Read more on [arXiv](https://arxiv.org/abs/2502.07563) or [HuggingFace](https://huggingface.co/papers/2502.07563))| Yu Cheng, Xiaoye Qu, Yiran Zhong, landisen, weigao266 | LASP-2 improves sequence parallelism for linear attention in transformers by optimizing communication and computation. The main research objective is to enhance the efficiency of sequence parallelism (SP) when training linear attention transformer models with very long input sequences. The key methodology is LASP-2, which reorganizes the communication-computation workflow to require only one AllGather collective communication on intermediate memory states independent of sequence length, and extends this to hybrid models (LASP-2H). Primary results show that LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention on a Linear-Llama3 model with a 2048K sequence length across 64 GPUs. For AI practitioners, LASP-2 provides a more efficient way to train linear attention-based and hybrid transformer models on long sequences, reducing training time and resource consumption.  |
| TransMLA: Multi-head Latent Attention Is All You Need (Read more on [arXiv](https://arxiv.org/abs/2502.07864) or [HuggingFace](https://huggingface.co/papers/2502.07864))| Muhan Zhang, Zengwei Yao, fxmeng | TransMLA converts GQA-based language models to MLA-based models, improving expressiveness without increasing KV cache size. The main research objective is to demonstrate that Multi-head Latent Attention (MLA) offers greater expressive power than Group Query Attention (GQA) for the same key-value (KV) cache overhead. The key methodology involves transforming pre-trained GQA models (e.g., LLaMA, Qwen) into equivalent MLA models via low-rank matrix factorization, followed by fine-tuning. Primary results show that the transformed TransMLA model outperformed the original Qwen2.5-7B GQA model on the GSM8K benchmark (87% vs 81%). The main implication is that the TransMLA transformation provides AI practitioners using open-source, GQA-based LLMs with a low cost method to shift to more effective MLA architecture without changes in KV cache size, enhancing performance.  |
| Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance (Read more on [arXiv](https://arxiv.org/abs/2502.08127) or [HuggingFace](https://huggingface.co/papers/2502.08127))| Yan Wang, Weipeng Zhou, Lingfei Qian, QianqianXie1994, jiminHuang | The paper evaluates the performance of reasoning-enhanced and general large language models (LLMs) on financial tasks and introduces a new financial reasoning-enhanced model. The main research question is how transferable general-domain reasoning enhancements in LLMs are to the financial domain, and what impact they have across different financial tasks. The methodology involves a comprehensive evaluation of 16 LLMs on three financial datasets (FinQA, DocMath-Simplong, XBRL-Math) encompassing numerical reasoning, tabular interpretation, and financial terminology, followed by developing a model called Fino1. A primary result is that Finol-8B achieved an average score of 61.03 across all datasets, outperforming Llama3.1-8B-Instruct by 10.91 points, with an XBRL-Math score reaching 82.22. The key implication for AI practitioners is that domain-specific fine-tuning with curated financial data, even on a small scale, can significantly improve LLM performance on financial reasoning tasks, surpassing general reasoning enhancements.  |
| Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.06533) or [HuggingFace](https://huggingface.co/papers/2502.06533))| lecraquito, Nbeau, supertardigrade | This paper investigates how varying pre-training levels affect language model exploration in reinforcement learning (RL) fine-tuning, and proposes a modified KL penalty to improve exploration. The main research question is how pre-training data distribution impacts exploration efficiency during RL fine-tuning of language models on tasks requiring out-of-distribution generalization. The key methodology involves pre-training a small language model on an arithmetic addition task with varying digit lengths, then fine-tuning it with RL and a modified KL penalty that prioritizes exploration on "critical tokens". Primary results show the model with the prioritized KL penalty achieved higher accuracy; for example the accuracy during testing with N=7 was higher when the KL penalty took into account the confidence of the old policy. The principal implication for AI practitioners is that adjusting the KL penalty based on pre-trained model certainty on specific tokens can enhance the efficiency of RL fine-tuning, particularly for tasks requiring generalization beyond the pre-training distribution.  |
| Distillation Scaling Laws (Read more on [arXiv](https://arxiv.org/abs/2502.08606) or [HuggingFace](https://huggingface.co/papers/2502.08606))| Etai Littwin, Jason Ramapuram, Floris Weers, Amitis Shidani, Dan Busbridge | This paper provides a distillation scaling law that estimates distilled model performance based on compute budget and student/teacher allocation. The main research objective is to determine optimal distillation recipes and understand how to allocate compute resources between teacher and student models to maximize student performance. The key methodology involves a large-scale, controlled study of distillation with students and teachers ranging from 143M to 12.6B parameters, trained on up to 512B tokens, fitting a distillation scaling law to predict student cross-entropy. The primary result is that distillation outperforms supervised pretraining only when the total compute is below a student-size-dependent threshold and a teacher already exists or has uses beyond a single distillation, and student cross-entropy follows a broken power law. The principal implication for AI practitioners is that distillation is beneficial for resource-constrained scenarios or when leveraging existing teachers, guiding optimal model and data scaling during distillation pretraining.  |
| SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation (Read more on [arXiv](https://arxiv.org/abs/2502.08168) or [HuggingFace](https://huggingface.co/papers/2502.08168))| HaiPeng Wang, Peidong Wang, Sihao Dong, Xiayang Xiao, JimmyMa99 | SARChat-Bench-2M is a new benchmark for evaluating vision-language models (VLMs) on synthetic aperture radar (SAR) image interpretation tasks. The main research objective is to develop a large-scale multimodal dialogue dataset and benchmark for evaluating VLMs' capabilities in SAR image understanding. The key methodology involves constructing a dataset (SARChat-2M) of 2 million SAR image-text pairs and defining six core tasks (classification, description, counting, localization, recognition, and referring) with specific evaluation metrics. Primary results show that the mPLUG-Owl3-7B model achieved the best performance among tested VLMs, with single-target and multi-target cross-modal identification accuracy rates reaching 99.27% and 99.51%, respectively. The principal implication is that AI practitioners can use SARChat-2M and SARChat-Bench to train, evaluate, and advance VLMs for SAR-specific applications, addressing the existing gap in large-scale, high-quality aligned SAR image-text datasets.  |
| LLM Pretraining with Continuous Concepts (Read more on [arXiv](https://arxiv.org/abs/2502.08524) or [HuggingFace](https://huggingface.co/papers/2502.08524))| Andrew Cohen, Jane Yu, Jack Lanchantin, Jihoon Tack, xlxxl | LLM Pretraining with Continuous Concepts introduces a novel pretraining framework, CoCoMix, that combines discrete next-token prediction with continuous concept learning to enhance language models. The main research objective is to investigate whether augmenting the next token prediction objective with explicit concept modeling in a latent space can improve language model pretraining. The key methodology involves extracting concepts from a pretrained sparse autoencoder, predicting these concepts, and mixing them into the model's hidden state by interleaving them with token hidden representations. The primary results show that CoCoMix achieves comparable performance to standard next-token prediction with 21.5% fewer training tokens on a 1.38B parameter model. For AI practitioners, CoCoMix offers a more sample-efficient pretraining approach, enhances model interpretability and steerability by allowing direct inspection and modification of the predicted concept, and improves performance in weak-to-strong supervision scenarios.  |
| Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance (Read more on [arXiv](https://arxiv.org/abs/2502.06145) or [HuggingFace](https://huggingface.co/papers/2502.06145))| Dechao Meng, Xin Gao, Zhen Shen, Guangyuan Wang, Hookszdp | Animate Anyone 2 introduces a diffusion-based framework for character image animation that incorporates environmental context to achieve realistic character-environment interactions. The main research objective is to animate characters with environment affordance, ensuring consistent and interactive relationships between the character and its surroundings. The key methodology involves extracting both motion signals and environmental representations from a source video, using a shape-agnostic mask strategy, an object guider with spatial blending for object interactions, and depth-wise pose modulation. Primary results include a superior SSIM score of 0.812 and FVD of 144.65 on the TikTok benchmark, outperforming existing methods in quantitative evaluations. For AI practitioners, this framework offers a robust method to generate high-fidelity character animations that seamlessly integrate with their environments, useful for applications in filmmaking and advertising.  |
| NoLiMa: Long-Context Evaluation Beyond Literal Matching (Read more on [arXiv](https://arxiv.org/abs/2502.05167) or [HuggingFace](https://huggingface.co/papers/2502.05167))| Ryan A. Rossi, Trung Bui, Hanieh Deilamsalehy, Franck-Dernoncourt, amodaresi | NOLIMA, a new benchmark, evaluates large language models' (LLMs) long-context understanding by minimizing literal keyword overlap between questions and answers, emphasizing associative reasoning.  Main research question/objective: To assess how well LLMs perform long-context reasoning when they cannot rely on simple literal matches between the question and the context, unlike typical Needle-In-A-Haystack (NIAH) tests.  Key methodology: The authors created the NOLIMA benchmark, extending NIAH, where questions and corresponding "needles" (answers) have minimal lexical overlap, requiring models to infer latent associations to locate the needle within a long "haystack" (irrelevant text). They tested 12 LLMs, including GPT-40, and conducted analyses with variations of reasoning complexity, context length, needle placement, and with the presence/absence of literal matching.  Primary results: Model performance degraded significantly with increasing context length; at 32K tokens, 10 of the 12 models dropped below 50% of their short-length baseline scores. GPT-4o's performance decreased from 99.3% baseline to 69.7% at 32K. The presence of literal matches drastically simplified the task, and distractors with literal matches drastically impaired the task.  Principal implication for AI practitioners: Current LLMs, even those claiming to support very long contexts, struggle with long-context associative reasoning tasks that lack surface-level (literal) cues, indicating a critical limitation that practitioners should consider when deploying these models in long-context applications.  |
| Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing (Read more on [arXiv](https://arxiv.org/abs/2502.04411) or [HuggingFace](https://huggingface.co/papers/2502.04411))| Peijie Dong, Xinglin Pan, Zhenheng Tang, Kunfeng Lai, Dominic789654 | Mediator is a framework for merging multiple fine-tuned large language models (LLMs) efficiently by adaptively averaging layers with minimal parameter conflicts and routing layers with significant conflicts. The main research objective is to develop a method for merging LLMs that minimizes parameter conflicts and system costs while preserving performance across diverse tasks. The key methodology involves quantifying layer-wise parameter conflicts, adaptively averaging layers with low conflict and routing layers with high conflict, employing sparse expert decomposition, and using uncertainty-based routing for out-of-distribution samples. Primary results show that Mediator achieves significant performance improvements over existing methods; e.g. on LLaMA-3.2-8B, it achieved 71.80% average on multiple tasks. The principal implication is that AI practitioners can merge fine-tuned LLMs more efficiently to improve the performance and adaptability while reducing the storage and computational costs compared to maintaining separate models.  |
| Next Block Prediction: Video Generation via Semi-Autoregressive Modeling (Read more on [arXiv](https://arxiv.org/abs/2502.07737) or [HuggingFace](https://huggingface.co/papers/2502.07737))| Furu Wei, Xu Sun, Shuming Ma, Shuhuai Ren | The paper proposes a semi-autoregressive framework called Next-Block Prediction (NBP) for video generation that improves upon traditional next-token prediction. The main research objective is to develop a video generation framework that improves spatial dependency modeling and inference efficiency compared to autoregressive next-token prediction models. The key methodology shifts the generation unit from individual tokens to blocks (e.g., rows or frames), using bidirectional attention within each block and predicting multiple tokens in parallel. The NBP model achieved FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4, with an 11x inference speedup. For AI practitioners, this framework provides a more efficient and scalable solution for video generation, maintaining or improving quality while accelerating inference through parallelization.  |
| DPO-Shift: Shifting the Distribution of Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2502.07599) or [HuggingFace](https://huggingface.co/papers/2502.07599))| Xiao Li, Lei Zhao, Qianen Zhang, Feng Jiang, Xiliang Yang | DPO-Shift controllably shifts the distribution of chosen probabilities in Direct Preference Optimization (DPO) to mitigate likelihood displacement. The main research objective is to address the likelihood displacement issue in DPO, where probabilities of chosen responses decrease during training. The key methodology is introducing a parameter function, f(x), added to the rejected reward in the Bradley-Terry model, called DPO-Shift. Experimentally, DPO-Shift with f(x)=0.95 achieved a reward accuracy of 0.743 on the UltraFeedback test set, comparable to DPO's 0.739, while demonstrably increasing chosen response probability. For AI practioners, DPO-Shift offers a simple, theoretically grounded solution to improve alignment with human preferences by mitigating the likelihood displacement of standard DPO, enabling a trade-off between chosen probability and reward margin.  |
| LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention (Read more on [arXiv](https://arxiv.org/abs/2502.08213) or [HuggingFace](https://huggingface.co/papers/2502.08213))| kkolomeitsev | The paper introduces LLM Modules, an architecture for transferring knowledge from a large, frozen language model to a smaller, trainable one using Enhanced Cross-Attention. The main objective is to develop a method that enables smaller models to achieve performance comparable to larger models by leveraging the knowledge of pre-trained large language models (LLMs) without full fine-tuning. The key methodology involves using a frozen Qwen2-1.5B model as a "knowledge source" and a GPT-Neo-125M model as a "generation module," connected by Enhanced Cross-Attention layers that include linear projections, an adapter block, and a gating mechanism. Training on the Bespoke-Stratos-17k dataset for 15 epochs reduced training loss from 13.8 to 2.3 in the first epoch and to 1.1 in subsequent ones. For AI practitioners, the principal implication is that this modular approach can significantly reduce computational costs associated with training large language models while still achieving substantial performance improvements on specific tasks.  |
| MetaSC: Test-Time Safety Specification Optimization for Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.07985) or [HuggingFace](https://huggingface.co/papers/2502.07985))| vicgalle | MetaSC is a framework that optimizes language model safety reasoning at inference time by dynamically updating safety prompts. The research objective is to improve language model safety performance without modifying model weights. The key methodology is a "meta-critique" mechanism that iteratively updates safety prompts (specifications) to adaptively drive the critique and revision process of a self-critique loop. Primary results show that MetaSC significantly improves safety scores compared to fixed system prompts and static self-critique defenses, achieving a safety score of 1.00 on the jailbreak defense task using the Hermes-3-Llama-3.1-405B model. For AI practitioners, MetaSC offers a way to enhance model safety dynamically at inference time, without retraining or fine-tuning.  |
