

## Papers for 2025-02-10

| Title | Authors | Summary |
|-------|---------|---------|
| VideoRoPE: What Makes for Good Video Rotary Position Embedding? (Read more on [arXiv](https://arxiv.org/abs/2502.05173) or [HuggingFace](https://huggingface.co/papers/2502.05173))| Pan Zhang, Xiaoyi Dong, Xilin Wei, yuhangzang, LiuXR | VideoRoPE introduces a novel rotary position embedding method for video data that outperforms existing methods by preserving spatio-temporal relationships. The main research objective is to identify and address the limitations of existing Rotary Position Embedding (RoPE) methods when applied to video data with complex spatio-temporal structures. The key methodology involves analyzing four essential characteristics (2D/3D structure, frequency allocation, spatial symmetry, temporal index scaling) for effective RoPE adaptation to video and proposing VideoRoPE, which features a 3D structure, low-frequency temporal allocation, diagonal layout, and adjustable temporal spacing. Primary results show that VideoRoPE outperforms previous RoPE variants on various benchmarks, achieving a 12.44% performance improvement over M-ROPE on the Video Retrieval task in both V-NIAH and V-NIAH-D settings. The principal implication for AI practitioners is that VideoRoPE provides a more robust and effective positional encoding scheme for video-based models, enhancing performance in tasks such as video retrieval, understanding, and hallucination reduction.  |
| Fast Video Generation with Sliding Tile Attention (Read more on [arXiv](https://arxiv.org/abs/2502.04507) or [HuggingFace](https://huggingface.co/papers/2502.04507))| Ion Stoica, Hangliang Ding, Runlong Su, Peiyuan Zhang, BrianChen1129 | Sliding Tile Attention (STA) accelerates video diffusion models by efficiently computing attention within local spatiotemporal windows. The paper introduces STA to address the high computational cost of 3D full attention in video diffusion transformers (DiTs). STA operates tile-by-tile, utilizing a hardware-aware sliding window design and kernel-level optimizations. STA reduces end-to-end latency of a video DiT (HunyuanVideo) from 945s to 685s without quality degradation, and to 268s with finetuning (0.09% drop on VBench). AI practitioners can deploy STA to significantly reduce inference time for video generation DiTs while maintaining output quality, or trade minimal quality loss for substantial speed gains.  |
| AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting (Read more on [arXiv](https://arxiv.org/abs/2502.05176) or [HuggingFace](https://huggingface.co/papers/2502.05176))| Jie-Ying Lee, Ying-Huan Chen, Yang-Jung Chen, Chung-Ho Wu, cmhungsteve | AuraFusion360 is a reference-based method for 360° unbounded scene inpainting that removes objects and fills holes in 3D scenes represented by Gaussian Splatting. The main research objective is to achieve high-quality object removal and hole filling in 360° unbounded scenes, maintaining view consistency and geometric accuracy. The methodology introduces depth-aware unseen mask generation, Adaptive Guided Depth Diffusion for initial point placement, and SDEdit-based detail enhancement for multi-view coherence. The method achieves an average PSNR of 17.661 and LPIPS of 0.388 on the 360-USID dataset, outperforming existing methods. AI practitioners can use this method and the provided 360-USID dataset for improved 3D scene inpainting, particularly in applications requiring consistent and accurate object removal in 360° environments.  |
| Goku: Flow Based Video Generative Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2502.04896) or [HuggingFace](https://huggingface.co/papers/2502.04896))| Fengda Zhu, Yida Zhang, Yuqi Zhang, Chongjian Ge, ShoufaChen | Goku is a family of rectified flow Transformer models for joint image-and-video generation that achieves industry-leading performance. The main research objective is to develop a state-of-the-art joint image-and-video generation model with industry-leading performance using rectified flow Transformers. The key methodology involves a data curation pipeline, a 3D joint image-video variational autoencoder (VAE), a Transformer architecture with full attention, rectified flow formulation, and infrastructure optimization for large-scale training. Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. This work demonstrates a pathway toward industry-grade performance in visual generation, enabling practitioners to build more efficient and high-performing generative models using Rectified Flows.  |
| QuEST: Stable Training of LLMs with 1-Bit Weights and Activations (Read more on [arXiv](https://arxiv.org/abs/2502.05003) or [HuggingFace](https://huggingface.co/papers/2502.05003))| Jiale Chen, d-alistarh, mnikdan97, soroushtabesh, BlackSamorez | QuEST introduces a quantization-aware training method for large language models (LLMs) enabling stable training with extremely low-precision weights and activations. The main research objective is to determine the Pareto-optimal frontier for training LLMs with low-bitwidth weights and activations, minimizing representation size while maintaining accuracy. The key methodology, QuEST, combines Hadamard normalization and MSE-optimal fitting for quantization, with a "trust" gradient estimator minimizing the difference between quantized and full-precision gradients. Primary results show stable training of Llama-family models down to 1-bit weights and activations, with 4-bit QuEST models achieving superior accuracy compared to BF16 models almost 4x larger in size. The principal implication for AI practitioners is that QuEST enables training and deploying accurate LLMs at significantly reduced precision and model size, potentially leading to more efficient inference.  |
| Agency Is Frame-Dependent (Read more on [arXiv](https://arxiv.org/abs/2502.04403) or [HuggingFace](https://huggingface.co/papers/2502.04403))| Shi Dong, Will Dabney, Michael Bowling, André Barreto, David Abel | i) The paper argues that agency, a system's capacity to steer outcomes toward a goal, is fundamentally frame-dependent. ii) The main objective is to demonstrate that the attribution of agency to a system is relative to the choice of a reference frame. iii) The methodology involves a philosophical argument, illustrating that the essential properties of agency (individuality, source of action, normativity, adaptivity) are frame-dependent. iv) The paper does not present specific quantitative findings. v) Any basic science of agency requires frame-dependence, impacting how AI practitioners should approach reinforcement learning.  |
| FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.05179) or [HuggingFace](https://huggingface.co/papers/2502.05179))| Peize Sun, Chongjian Ge, Wenbo Li, Shilong Zhang, ShoufaChen | FlashVideo introduces a two-stage framework for efficient high-resolution text-to-video generation. The research aims to decouple prompt fidelity and visual quality optimization in video generation. It utilizes a two-stage DiT architecture with a large model for low-resolution generation followed by flow matching with a smaller model for high-resolution detail enhancement. FlashVideo achieves a top-tier performance on VBench-Long (82.99 score) with significantly reduced function evaluation time (102.3s for 1080p video generation). The two-stage design allows AI practitioners to preview initial output before committing to full-resolution generation, reducing computational costs and wait times.  |
| Linear Correlation in LM's Compositional Generalization and Hallucination (Read more on [arXiv](https://arxiv.org/abs/2502.04520) or [HuggingFace](https://huggingface.co/papers/2502.04520))| Chengyu Dong, Shibo Hao, Chenyang An, Letian Peng, shangjingbo | i) This paper unveils linear correlations in language models (LMs) during knowledge composition. ii) The research investigates the extent to which linear transformations can approximate the relationships between the output logits of related next token prediction (NTP) tasks. iii) The methodology involves fitting a linear transformation between logits of source and target knowledge prompts using a subset of data, then evaluating the transformation on the remaining data using Pearson correlation. iv) Results indicate that the fitted linear transformation is resilient to fine-tuning, with successful generalization for simultaneous knowledge updates requiring high correlation intensity and transformation precision; in City-Country relationships, 42% of cities learn the top-1 weight with their influenced countries. v) The implication for AI practitioners is the understanding that compositional generalization in LMs relies on linear correlations between vocabulary representations, which can be leveraged for knowledge composition tasks but also may lead to hallucinations when misaligned.  |
| Generating Symbolic World Models via Test-time Scaling of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.04728) or [HuggingFace](https://huggingface.co/papers/2502.04728))| Fuxiang Frank Xia, Tim Z. Xiao, Yuhuan Yuan, Zhouliang Yu, zhangysk | i) This paper introduces a test-time scaling approach for generating Planning Domain Definition Language (PDDL) domains using Large Language Models (LLMs). ii) The main objective is to enhance PDDL reasoning in LLMs for generating high-quality PDDL domains without additional training data. iii) The methodology employs a Best-of-N sampling approach followed by iterative refinement using Instance Verbalized Machine Learning (iVML). iv) The method achieves an 85.2% success rate on the NL2Domain task and 71.4% on Prob2Domain with Qwen2.5-Coder-7B, exceeding ol-mini's performance. v) AI practitioners can leverage this approach to generate symbolic world models for robust planning, particularly in complex domains where existing LLM-based planners struggle.  |
| On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices (Read more on [arXiv](https://arxiv.org/abs/2502.04363) or [HuggingFace](https://huggingface.co/papers/2502.04363))| Yeojin Lee, Jungmin Cheon, Isu Jeong, Kyuhwan Lee, Bosung Kim | On-device Sora is a framework for diffusion-based text-to-video generation that operates efficiently on smartphone-grade devices. The main research objective is to enable efficient and high-quality text-to-video generation on resource-constrained mobile devices, addressing limitations of current diffusion-based video generation models. Key methodologies include Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to minimize token-processing computation, and Concurrent Inference with Dynamic Loading (CI-DL) for efficient model inference. Results demonstrate that On-device Sora generates videos on an iPhone 15 Pro with quality comparable to Open-Sora running on NVIDIA A6000 GPUs, achieving up to 1.94x speedup with LPL. AI practitioners can leverage On-device Sora's techniques to deploy and accelerate diffusion-based video generation models on mobile and embedded devices, expanding accessibility and enabling on-device applications.  |
| CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference (Read more on [arXiv](https://arxiv.org/abs/2502.04416) or [HuggingFace](https://huggingface.co/papers/2502.04416))| Wulong Liu, Xianzhi Yu, Hui-Ling Zhen, Lancheng Zou, Eleven-P | CMoE is a framework that efficiently creates sparse Mixture-of-Experts models from dense large language models (LLMs) for improved inference efficiency. The main objective is to transform dense LLMs into sparse MoE architectures without extensive retraining. The methodology involves grouping feed-forward network (FFN) neurons into shared and routed experts based on activation rates, constructing a training-free routing mechanism using representative neurons, and optional lightweight adaptation. Results show that, with a 25% activation ratio, CMoE achieved 76.59% of the dense model's accuracy on some downstream benchmarks with lightweight fine-tuning on 2,048 samples. For AI practitioners, CMoE offers a method to deploy LLMs more efficiently in resource-constrained environments by significantly reducing computational overhead while maintaining performance.  |
| Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.04404) or [HuggingFace](https://huggingface.co/papers/2502.04404))| Jie-Jing Shao, Ding-Chu Zhang, Wen-Da Wei, Xuan-Yi Zhu, yangxw | This paper introduces Self-Backtracking, a technique that enables language models to autonomously backtrack during reasoning. The main research objective is to address the limitations of current slow-thinking mechanisms in large language models, specifically inefficient overthinking and over-reliance on auxiliary reward models. The key methodology involves training the model to recognize suboptimal reasoning paths and backtrack to earlier states, using a specialized dataset format and a modified loss function during training, and an inference algorithm combining expansion, backtracking, and selection steps during inference. The primary result shows that Self-Backtracking improves reasoning accuracy on the Countdown task by over 40% compared to optimal-path supervised fine-tuning, using the Llama3.2-1B model. The principal implication for AI practitioners is that integrating self-backtracking into language models can significantly enhance reasoning capabilities and efficiency, and reduce the need for external reward models.  |
| Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More (Read more on [arXiv](https://arxiv.org/abs/2502.03738) or [HuggingFace](https://huggingface.co/papers/2502.03738))| Yuyin Zhou, Wei Shao, Guoyizhe Wei, Yaodong Yu, Feng Wang | This paper investigates the impact of patchification, an image tokenization method, on the performance of vision models. The main research objective is to examine the information loss caused by the patchification-based compressive encoding paradigm in vision models and how it affects visual understanding. The key methodology involves extensive scaling experiments by varying patch sizes in ViT and Mamba-based architectures across different vision tasks and input scales. The primary result is that model performance consistently improves as patch size decreases, achieving a test accuracy of 84.6% on ImageNet-1k with a base-sized model using a 1x1 patch size (50,176 tokens). The principal implication is that AI practitioners should consider reducing or eliminating spatial compression in vision encoders to improve model accuracy, as computational resources allow.  |
| QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2502.05178) or [HuggingFace](https://huggingface.co/papers/2502.05178))| Yuke Zhu, Linxi Fan, Scott Reed, Fuzhao Xue, zhaoyue-zephyrus | QLIP is a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. The main research objective is to develop a visual tokenizer that excels at both capturing image semantics and reconstructing high-quality visuals for multimodal language modeling. The key methodology involves training a Binary Spherical Quantization (BSQ)-based autoencoder with a contrastive objective for text-image alignment, using a two-stage training process to balance reconstruction and alignment. A primary result is that QLIP-B achieves a zero-shot classification accuracy of 74.3% on ImageNet, while achieving a reconstruction FID of 3.21, comparable to state-of-the-art methods. AI practitioners can use QLIP as a drop-in replacement for visual encoders in existing models like LLaVA or image tokenizers in models like LlamaGen, achieving improved or comparable performance in multimodal understanding and generation tasks.  |
| ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.04689) or [HuggingFace](https://huggingface.co/papers/2502.04689))| Giuseppe Carenini, yuweiyin | ARR is a zero-shot prompting method that improves question-answering (QA) performance of Large Language Models (LLMs) by explicitly guiding them through analyzing, retrieving, and reasoning steps. The main research objective is to evaluate the effectiveness of the ARR prompting method compared to baseline and Chain-of-Thought (CoT) prompting in multiple-choice QA tasks. The key methodology involves comparing the accuracy of LLMs using different trigger sentences representing ARR, baseline (no specific trigger), and zero-shot CoT prompting across ten multiple-choice QA datasets. Primary results show that ARR achieves an average accuracy of 69.58% across all datasets, outperforming the baseline (65.48%) and CoT (68.14%) when using the LLaMA3-8B-Chat model. AI practitioners can leverage the ARR prompting strategy to enhance LLM performance in QA tasks without needing model fine-tuning or few-shot examples, leading to better results in various applications, including information retrieval and decision support.  |
