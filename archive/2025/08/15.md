

## Papers for 2025-08-15

| Title | Authors | Summary |
|-------|---------|---------|
| We-Math 2.0: A Versatile MathBook System for Incentivizing Visual
  Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10433) or [HuggingFace](https://huggingface.co/papers/2508.10433))| Xiaowan Wang, Yanzi Wang, Peiqing Yang, Qiuna Tan, Runqi Qiao | WE-MATH 2.0 introduces a unified system featuring a structured knowledge base, model-centric datasets, and a reinforcement learning paradigm to enhance the visual mathematical reasoning capabilities of Multimodal Large Language Models (MLLMs). The research aims to address deficiencies in existing approaches, specifically the lack of comprehensive knowledge-driven design, model-centric data difficulty modeling, and an emphasis on reasoning generalization. The core methodology integrates a five-level "MathBook Knowledge System" with 491 knowledge points, two new datasets (MathBook-Standard and MathBook-Pro) built with a three-dimensional difficulty space, and a two-stage "MathBook-RL" framework that uses cold-start fine-tuning followed by progressive alignment reinforcement learning. Experimental results show the resulting model, MathBook-7B, achieves an average score of 48.7 across four benchmarks, representing over a 5% improvement compared to its backbone model using only 9.8K training samples. The principal implication for AI practitioners is that a knowledge-driven, curriculum-based approach to data construction and RL training can yield significant improvements in complex reasoning and generalization, suggesting that high-quality, structured data is more efficient than massive, less-structured datasets for these tasks. |
| NextStep-1: Toward Autoregressive Image Generation with Continuous
  Tokens at Scale (Read more on [arXiv](https://arxiv.org/abs/2508.10711) or [HuggingFace](https://huggingface.co/papers/2508.10711))| Quan Sun, Jingwei Wu, Guopeng Li, Chunrui Han, NextStep Team | NextStep-1 is a 14B autoregressive model that advances text-to-image generation by directly predicting continuous visual tokens with a lightweight flow matching head, aiming to bypass the quantization loss of discrete methods. Its objective is to unify text and continuous image generation under a single next-token prediction paradigm, leveraging a causal transformer to process the mixed-modality sequence. The key methodology involves using a standard language model head for discrete text tokens and a patch-wise flow matching head to autoregressively denoise a conditioned noise sample into the next continuous image token. NextStep-1 achieves state-of-the-art performance for an autoregressive model, scoring 85.28 on the DPG-Bench benchmark for compositional fidelity, competitive with leading diffusion models. The principal implication for AI practitioners is that the transformer backbone performs the core generative modeling while the flow matching head is a surprisingly size-insensitive sampler, suggesting that architectural focus should be on the transformer and a robust, regularized tokenizer to ensure generation stability, rather than on a complex final prediction head. |
| ToonComposer: Streamlining Cartoon Production with Generative
  Post-Keyframing (Read more on [arXiv](https://arxiv.org/abs/2508.10881) or [HuggingFace](https://huggingface.co/papers/2508.10881))| Xiaoyu Li, Yaowei Li, Zhaoyang Zhang, Guangzhi Wang, Lingen Li | ToonComposer is a generative model that unifies cartoon inbetweening and colorization into a single "post-keyframing" stage using a Diffusion Transformer (DiT) architecture controlled by sparse inputs. The objective is to streamline cartoon production by replacing separate, error-prone AI stages with a unified process that requires minimal manual input, such as a single colored frame and a few keyframe sketches. The model's key methodology involves a sparse sketch injection mechanism for precise temporal control and a novel Spatial Low-Rank Adapter (SLRA) to adapt the video foundation model's spatial characteristics to the cartoon domain while preserving its temporal priors. Primary results demonstrate superior performance, with ToonComposer achieving a DISTS score of 0.1785 on a synthetic benchmark (versus >0.37 for prior methods) and winning 70.99% of user preferences for aesthetic quality. The principal implication for AI practitioners is the SLRA technique, which provides a framework for domain-adapting large DiT video models by modifying only spatial behavior, a pattern applicable to other specialized video generation tasks. |
| UI-Venus Technical Report: Building High-performance UI Agents with RFT (Read more on [arXiv](https://arxiv.org/abs/2508.10833) or [HuggingFace](https://huggingface.co/papers/2508.10833))| Shuheng Shen, Xingran Zhou, Zhenyu Xu, Zhengwen Zeng, Zhangxuan Gu | UI-Venus is a state-of-the-art, screenshot-based UI agent developed using Reinforcement Finetune (RFT) that excels at UI grounding and complex navigation tasks. The primary objective is to build a high-performance UI agent by leveraging RFT with carefully designed reward functions, rigorous data cleaning, and a novel framework for improving multi-step planning. The agent is trained using Group Relative Policy Optimization (GRPO) and a "Self-Evolving Trajectory History Alignment & Sparse Action Enhancement" framework, which refines historical reasoning traces and upsamples rare but critical actions between training epochs. The UI-Venus-72B model achieves SOTA performance, including a 65.9% success rate on the AndroidWorld online navigation benchmark and 95.3% accuracy on the ScreenSpot-V2 grounding benchmark. For AI practitioners, this work demonstrates that RFT, combined with self-evolving trajectory refinement and targeted data strategies, is a more effective paradigm than supervised fine-tuning for creating robust UI agents capable of complex, long-horizon tasks. |
| PRELUDE: A Benchmark Designed to Require Global Comprehension and
  Reasoning over Long Contexts (Read more on [arXiv](https://arxiv.org/abs/2508.09848) or [HuggingFace](https://huggingface.co/papers/2508.09848))| Rui Lu, Tong Li, Chulun Zhou, Tsz Ting Chung, Mo Yu | This paper introduces PRELUDE, a novel benchmark that evaluates long-context reasoning in LLMs by requiring them to determine if a hypothetical character prequel is consistent with a book's canonical narrative. The main objective is to create a task that demands global comprehension and deep reasoning, mitigating shortcuts like memorization and shallow information retrieval found in existing benchmarks. The methodology involves generating hypothetical prequels for characters from 13 novels and having human experts annotate their consistency, resulting in a dataset where 88% of instances require non-local evidence. The primary result shows that even state-of-the-art systems lag human performance by over 15% in F1-score, and a human study reveals models have a reasoning accuracy gap of over 30% compared to humans, often reaching correct conclusions via flawed logic. The principal implication for AI practitioners is that current LLMs and techniques like RAG are fundamentally limited in performing deep, holistic reasoning over long contexts, indicating that simply scaling context windows or improving retrieval is insufficient for genuine comprehension. |
| STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.10893) or [HuggingFace](https://huggingface.co/papers/2508.10893))| Honghua Chen, Shangchen Zhou, Fangzhou Hong, Yihang Luo, Yushi Lan | STREAM3R introduces a causal, decoder-only Transformer framework for scalable, sequential 3D reconstruction from streaming images. The main objective is to overcome the scalability and computational limitations of prior methods by developing an efficient framework for online, incremental 3D reconstruction. The key methodology reformulates pointmap prediction as a sequential regression task, where a causal Transformer processes each incoming frame while efficiently reusing cached features from past frames as context. Experiments show the proposed method outperforms prior work; for instance, on the 7-Scenes 3D reconstruction benchmark, STREAM3Rβ achieves a mean accuracy of 0.122, surpassing the 0.126 of the RNN-based CUT3R, while also being faster. The principal implication for AI practitioners is that LLM-style decoder-only architectures, along with their optimized training and inference infrastructure like KVCache, can be effectively adapted to create scalable and real-time systems for 3D visual understanding in streaming environments. |
| Pass@k Training for Adaptively Balancing Exploration and Exploitation of
  Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2508.10751) or [HuggingFace](https://huggingface.co/papers/2508.10751))| Qinghao Ye, Yue Ling, Youbin Wu, Xiaobo Qin, Zhipeng Chen | This paper presents Pass@k Training, a method that uses the Pass@k metric as a reward signal in Reinforcement Learning with Verifiable Rewards (RLVR) to improve the reasoning capabilities of large language models by enhancing exploration. The primary objective is to overcome the tendency of standard Pass@1-based RLVR to converge to local optima by encouraging the model to explore a more diverse solution space. The key methodology involves replacing the Pass@1 reward with a Pass@k reward and developing an efficient implementation through an analytical derivation of the advantage function, which avoids the computational overhead of sampling. The primary result is that sequential training with Pass@k and then Pass@1 significantly improves final task performance; for instance, a Qwen2.5-7B model achieved a 30.8 Pass@1 score on the Enigmata benchmark, surpassing a powerful Claude-3.7-Sonnet model that scored 22.7. For AI practitioners, the principal implication is that employing Pass@k Training as an intermediate step before standard Pass@1 fine-tuning can unlock higher performance ceilings in reasoning models by first broadening their exploratory capabilities to escape local optima, a strategy made practical by the paper's efficient analytical formulation. |
| HumanSense: From Multimodal Perception to Empathetic Context-Aware
  Responses through Reasoning MLLMs (Read more on [arXiv](https://arxiv.org/abs/2508.10576) or [HuggingFace](https://huggingface.co/papers/2508.10576))| Yi Yuan, Tianqi Li, Yabing Wang, Ruobing Zheng, Zheng Qin | This paper introduces HumanSense, a benchmark to evaluate MLLMs on human-centered perception and interaction, and proposes a reinforcement learning approach to enhance these capabilities. The main objective is to systematically assess and improve an MLLM's ability to process multimodal context and generate empathetic, context-aware responses. The methodology consists of the 15-task HumanSense benchmark for evaluation and a multi-stage, modality-progressive reinforcement learning strategy to build a reasoning-focused omni-model. Results show that the best-performing MLLM (57.8% average accuracy) significantly trails the human baseline (87.5%), with omni-modal models outperforming visual-only models on complex interaction tasks. The principal implication for AI practitioners is that progress in human-centered AI requires omni-modal architectures, as the key performance bottleneck is not low-level perception but high-level, long-context reasoning. |
| A Survey on Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.10875) or [HuggingFace](https://huggingface.co/papers/2508.10875))| Zhiqiang Shen, Bowei Guo, Mingda Chen, Tianyi Li | This paper presents a comprehensive survey of Diffusion Language Models (DLMs), systematically categorizing their architectures, training methodologies, inference optimizations, and applications as a parallel-generation alternative to autoregressive models. The primary objective is to provide a holistic overview and structured taxonomy of the DLM landscape, analyzing its core principles, state-of-the-art techniques, and fundamental challenges. The authors conduct an extensive literature review, organizing findings into a detailed taxonomy that classifies DLMs by paradigm (e.g., continuous vs. discrete space), training strategies, and inference techniques. The survey finds that modern discrete DLMs achieve performance comparable to similarly-sized autoregressive models, particularly on reasoning benchmarks, while inference optimizations like confidence-aware parallel decoding can yield up to a 27.6x speed-up. The principal implication for AI practitioners is that DLMs are a viable alternative for high-throughput inference applications, but require managing the critical trade-off between parallelism and generation quality, known as the "Parallel Decoding Curse," within a less mature infrastructure ecosystem. |
| From Black Box to Transparency: Enhancing Automated Interpreting
  Assessment with Explainable AI in College Classrooms (Read more on [arXiv](https://arxiv.org/abs/2508.10860) or [HuggingFace](https://huggingface.co/papers/2508.10860))| Ziyin Zhang, Zhaokun Jiang | This research develops an explainable AI framework for the automated assessment of interpreting quality by integrating feature engineering, Variational Autoencoder (VAE)-based data augmentation, and SHAP analysis. The main objective is to create a transparent and robust model that accurately predicts multi-dimensional interpreting quality (fidelity, fluency, language use) while overcoming the challenges of data scarcity and imbalance inherent in educational datasets. The methodology involves extracting linguistic features from a novel English-Chinese interpreting dataset, augmenting it from 117 to 500 samples using a VAE, and training machine learning regressors whose predictions are explained using SHAP. The primary results demonstrate that VAE-based augmentation significantly improves model performance, with the Random Forest model's Spearman correlation for fidelity prediction increasing from 0.51 to 0.68, and SHAP analysis identifying neural-based metrics like BLEURT and pause-related features as the strongest predictors for fidelity and fluency respectively. The principal implication for AI practitioners is the validation of a combined VAE and SHAP workflow to build robust, interpretable models on small, specialized NLP datasets, providing a practical template for developing trustworthy AI systems in domains where data is limited and model transparency is critical for user adoption. |
| Processing and acquisition traces in visual encoders: What does CLIP
  know about your camera? (Read more on [arXiv](https://arxiv.org/abs/2508.10637) or [HuggingFace](https://huggingface.co/papers/2508.10637))| Giorgos Tolias, Yuta Nakashima, Giorgos Kordopatis-Zilos, Vladan Stojnić, Ryan Ramos | This paper demonstrates that foundational visual encoders, particularly Contrastive Vision-Language (CVL) models, systematically encode subtle image processing and camera acquisition metadata, which can interfere with or even dominate semantic understanding. The primary objective is to investigate whether these imperceptible "metadata labels" are embedded in image representations and how they impact the performance of downstream semantic tasks. The methodology involves training linear classifiers on the frozen embeddings of 47 different visual encoders to predict metadata attributes (e.g., JPEG quality, camera model) and evaluating semantic task performance under controlled conditions where metadata and semantic labels are deliberately correlated or anti-correlated. The study finds that metadata is strongly encoded, with some CVL models predicting camera type (smartphone vs. non-smartphone) with 70.1% accuracy from embeddings of images that have been 90% masked to remove semantic content. The principal implication for AI practitioners is that models can exhibit strong, hidden biases toward non-semantic, acquisition-related artifacts, leading to unreliable performance when deployed on data from diverse sources and potentially causing failures when the correlation between metadata and semantics shifts. |
| When Explainability Meets Privacy: An Investigation at the Intersection
  of Post-hoc Explainability and Differential Privacy in the Context of Natural
  Language Processing (Read more on [arXiv](https://arxiv.org/abs/2508.10482) or [HuggingFace](https://huggingface.co/papers/2508.10482))| Gjergji Kasneci, Florian Matthes, Ege Erdogan, Stephen Meisenbacher, Mahdi Dhaini | This research empirically investigates the trade-off between post-hoc explainability and differentially private (DP) text rewriting in NLP by evaluating how three DP methods affect the faithfulness of four explanation techniques across three datasets. The primary research question is to quantify the impact of DP text rewriting on the post-hoc explainability of fine-tuned language models and to characterize the privacy-explainability trade-off. The methodology involves fine-tuning five encoder-only models (e.g., BERT, RoBERTa) on DP-rewritten text and measuring performance using a composite score that combines model F1-score with explanation faithfulness metrics (Comprehensiveness and Sufficiency) for Gradient, IG, LIME, and SHAP explainers. The primary result shows that smaller "base" models consistently outperform larger models on privatized data; for the SST2 dataset, the composite score for large models dropped by an average of -0.286 compared to base models when utility was prioritized (α=0.75), demonstrating that increased model size is detrimental. The principal implication for AI practitioners is that when building privacy-preserving systems that require explainability, smaller models should be selected as they offer a more stable and effective balance of utility and explanation faithfulness under DP constraints. |
