

## Papers for 2025-08-19

| Title | Authors | Summary |
|-------|---------|---------|
| Ovis2.5 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.11737) or [HuggingFace](https://huggingface.co/papers/2508.11737))| Yang Li, cqgwin, Suikong, xxyyy123, runninglsy | Ovis2.5 is a new multimodal large language model (MLLM) developed for native-resolution visual perception and advanced multimodal reasoning. The primary objective was to overcome limitations of prior MLLMs in handling visually dense content and deep reasoning by addressing rigid vision front-ends and linear chain-of-thought training. This was achieved by integrating a Native-Resolution ViT (NaViT), augmenting training with reflective deep-reasoning data for an optional "thinking mode," and implementing a five-phase curriculum with multimodal data packing and hybrid parallelism for 3-4x end-to-end speedup. Ovis2.5-9B achieved an average of 78.3 on the OpenCompass multimodal leaderboard, establishing state-of-the-art performance among open-source MLLMs under 40B parameters, with Ovis2.5-2B scoring 73.9. These advancements enable more accurate analysis of complex visuals like charts and diagrams and robust problem-solving with a controllable trade-off between latency and accuracy, making it suitable for high-performance and resource-constrained AI applications. |
| ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long
  Narrative Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10419) or [HuggingFace](https://huggingface.co/papers/2508.10419))| Yufeng Wang, Wei Wei, Rongchen Zhao, Juyuan Wang, lxucs | ComoRAG is a cognitive-inspired RAG framework designed for stateful long narrative reasoning. It addresses the limitation of traditional stateless RAG methods in comprehending intricate plotlines and evolving relations within long narratives, especially for LLMs with limited context windows. ComoRAG mimics human Prefrontal Cortex functions, employing a dynamic memory workspace, hierarchical knowledge source (Veridical, Semantic, Episodic layers), and a metacognitive control loop to conduct iterative reasoning cycles for evidence acquisition and knowledge consolidation. The framework consistently outperforms strong RAG baselines on long-context narrative benchmarks, achieving up to an 11% relative gain over the strongest baseline and a 19% relative F1 improvement on challenging narrative query types. This offers a principled, robust, and generalizable paradigm for AI practitioners to enhance retrieval-based long context comprehension, particularly for applications requiring stateful reasoning over complex narratives. |
| 4DNeX: Feed-Forward 4D Generative Modeling Made Easy (Read more on [arXiv](https://arxiv.org/abs/2508.13154) or [HuggingFace](https://huggingface.co/papers/2508.13154))| Zeng Tao, Jiawei Ren, Long Zhuo, Tianqi Liu, Zhaoxi Chen | 4DNeX introduces the first feed-forward framework for efficient, end-to-end generation of dynamic 3D scene representations from a single image. The method fine-tunes a pretrained video diffusion model using a novel 4DNeX-10M dataset, which includes a unified 6D video representation jointly modeling RGB and XYZ sequences with width-wise fusion and modality-aware token encoding. Quantitative results demonstrate 4DNeX's superior efficiency, generating dynamic 4D scenes in 15 minutes compared to 60-90 minutes for existing approaches like Free4D and 4Real. User studies further validate its effectiveness, showing a 56% user preference for 4DNeX's consistency over Free4D and 93% for aesthetics over 4Real. This work provides a scalable solution for image-to-4D modeling, facilitating the development of generative 4D world models for applications in AR/VR and digital content creation. |
| Next Visual Granularity Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12811) or [HuggingFace](https://huggingface.co/papers/2508.12811))| Kang Liao, Qingyi Tao, Zhonghua Wu, Zhouxia Wang, yikaiwang | This paper introduces Next Visual Granularity (NVG), a novel image generation framework that represents images as structured sequences of varying granularity, enabling coarse-to-fine generation with explicit structure control. The primary objective is to overcome limitations of existing generative models in handling complex spatial structures by explicitly modeling hierarchical visual structure for more controllable and interpretable image generation. NVG employs a multi-granularity quantized autoencoder to decompose images into content and structure pairs across multiple stages, iteratively generating structure maps (using a lightweight rectified flow model) and content tokens (via progressive canvas refinement) from an empty image to refine details. Quantitative comparisons show NVG consistently outperforms VAR models in FID scores, achieving 2.06 for NVG-d24 compared to VAR-d24's 2.09, and demonstrating comparable or superior performance to other state-of-the-art methods with fewer parameters and training steps. This framework provides AI practitioners with a method for fine-grained, iterative control over image generation, offering practical advantages in domains requiring structured and hierarchical generation processes, such as design or scientific visualization, without relying on post-hoc modules. |
| Speed Always Wins: A Survey on Efficient Architectures for Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.09834) or [HuggingFace](https://huggingface.co/papers/2508.09834))| Jusen Du, Yucheng Zhou, Jiaxi Hu, Weigao Sun, landisen | This survey systematically examines innovative and efficient architectures for Large Language Models (LLMs). It addresses the computational and memory bottlenecks of traditional Transformer LLMs for large-scale training and deployment, particularly in long-context, multimodal, and reasoning scenarios. The paper categorizes and reviews recent advancements including linear and sparse sequence modeling, efficient full attention, sparse Mixture-of-Experts, hybrid architectures, and diffusion LLMs. Key results demonstrate significant efficiency gains, such as FlashAttention-2 achieving 2-4x speedups over dense FlashAttention for sequences up to 64K, and MoBA providing 16x speedup for 10M-token sequences at 95% sparsity. This provides AI practitioners with a comprehensive blueprint for developing more efficient and scalable LLMs by leveraging diverse architectural strategies and optimization techniques. |
| Has GPT-5 Achieved Spatial Intelligence? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.13142) or [HuggingFace](https://huggingface.co/papers/2508.13142))| Ruisi Wang, Qingping Sun, Yubo Wang, yl-1993, caizhongang | This empirical study evaluates the spatial intelligence of state-of-the-art multi-modal large language models (MLLMs), including GPT-5, to determine their current capabilities in this critical dimension of artificial general intelligence. The research proposed a taxonomy of six fundamental spatial capabilities and evaluated models on eight benchmarks using Chance-Adjusted Accuracy (CAA) and standardized zero-shot Chain-of-Thought (CoT) prompting, consuming over one billion tokens. Empirical findings reveal GPT-5 establishes a new state-of-the-art in spatial intelligence, for example, achieving 64.18 CAA on the SITE benchmark. However, GPT-5 still significantly lags human performance across a broad spectrum of spatial tasks, particularly in Mental Reconstruction, Perspective-taking, Deformation & Assembly, and Comprehensive Reasoning. The study concludes that spatial intelligence remains an underexplored frontier, as proprietary models do not exhibit a decisive advantage over open-source models on the most challenging spatial problems, indicating a need for continued research and development in this domain. |
| HeroBench: A Benchmark for Long-Horizon Planning and Structured
  Reasoning in Virtual Worlds (Read more on [arXiv](https://arxiv.org/abs/2508.12782) or [HuggingFace](https://huggingface.co/papers/2508.12782))| Artyom Sorokin, Viktor Volkov, Stefan Rebrikov, Petr Anokhin, roxal | This paper introduces HeroBench, a novel benchmark designed to evaluate long-horizon planning and structured reasoning capabilities of LLMs in a complex, RPG-inspired virtual environment. The research objective is to assess how effectively LLMs and agentic systems can formulate and execute extended, interdependent action sequences, a capability underexplored by existing benchmarks. The methodology involves evaluating 25 state-of-the-art LLMs on a rigorously constructed dataset of tasks by prompting them to generate executable Python code, which is then validated in a simulated environment using success and progress metrics. Results reveal significant performance disparities, with Grok-4 achieving the highest success rate of 91.7% on base tasks, substantially outperforming other models, including GPT-5, particularly as task difficulty increased. The principal implication for AI practitioners is that current models, even top-tier reasoning-enhanced ones, exhibit critical weaknesses in robust high-level planning and reliable execution for complex tasks, suggesting that performance on standard benchmarks is not a reliable indicator for autonomous planning in intricate real-world scenarios. |
| When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness
  Methods for LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.11383) or [HuggingFace](https://huggingface.co/papers/2508.11383))| Elena Tutubalina, Gleb Ershov, Mikhail Chaichuk, apanc, myyycroft | This paper presents the first systematic comparison of five prompt robustness methods for Large Language Models (LLMs) to address their sensitivity to subtle formatting variations. The study evaluates Few-shot, Batch Calibration (BC), Template Ensembles (TE), Sensitivity-Aware Decoding (SAD), and LoRA with format augmentations across 8 Llama, Qwen, and Gemma models (1.5B-9B parameters) as well as GPT-4.1 and DeepSeek V3, on 52 Natural Instructions tasks under various distribution shifts. Key findings include Batch Calibration significantly reducing prompt spread for 6/8 open-source models while improving accuracy, though its effectiveness degrades under covariate (class imbalance) shifts, exemplified by a 50.2% Matthews correlation coefficient drop for Llama 3.1 8B. Additionally, greedy decoding consistently increased format sensitivity compared to probability ranking across models. For practitioners, Batch Calibration is recommended for its robustness and low overhead in balanced settings, probability ranking should be preferred over greedy decoding, and for black-box frontier models, Template Ensembles with majority voting prove effective in reducing spread. |
| Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive
  World Model (Read more on [arXiv](https://arxiv.org/abs/2508.13009) or [HuggingFace](https://huggingface.co/papers/2508.13009))| Yifan Zhang, Boyang Wang, Zexiang Liu, Chunli Peng, Xianglong He | Matrix-Game 2.0 is an open-source, real-time, and streaming interactive world model for long video generation. The research addresses challenges in interactive video generation, including latency, error accumulation, and the lack of large-scale interactive video datasets. Key methodologies involve a scalable data production pipeline for Unreal Engine and GTA5, an action injection module for mouse and keyboard inputs, and few-step distillation based on a causal architecture with Self-Forcing. Matrix-Game 2.0 achieves an ultra-fast generation speed of 25 FPS on a single H100 GPU for minute-level videos. This work represents a significant advancement for AI practitioners in developing efficient frameworks for real-time world simulation and interactive applications. |
| Lumen: Consistent Video Relighting and Harmonious Background Replacement
  with Video Generative Models (Read more on [arXiv](https://arxiv.org/abs/2508.12945) or [HuggingFace](https://huggingface.co/papers/2508.12945))| Zixiang Gao, Chenxuan Miao, Yutong Feng, Yuxuan Liu, Jianshu Zeng | Lumen is an end-to-end video relighting framework that also handles harmonious background replacement using video generative models. The primary objective is to adjust foreground lighting while preserving intrinsic attributes and replace backgrounds harmoniously, maintaining temporal consistency under flexible textual control. Lumen leverages a large-scale video generative model (Wan2.1 DiT-based) and a unique multi-domain joint training curriculum on a mixed synthetic and realistic video dataset, incorporating a style adapter to decouple domain distributions. Quantitative evaluation shows Lumen's superior performance; for instance, on realistic paired videos, it achieves a PSNR of 23.06 and a Subject Consistency of 0.9808, outperforming baselines. This framework offers AI practitioners a robust solution for cinematic video editing, e-commerce content generation, and other applications requiring high-fidelity video synthesis with consistent relighting and background modifications. |
| G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior
  Integration (Read more on [arXiv](https://arxiv.org/abs/2508.11379) or [HuggingFace](https://huggingface.co/papers/2508.11379))| Evgeny Burnaev, Peter Wonka, Artem Komarichev, rusrakhimov, smileyenot983 | G-CUT3R introduces a novel feed-forward method for guided 3D scene reconstruction by integrating camera and depth prior information into the CUT3R framework. The objective is to enhance 3D scene reconstruction accuracy and consistency by effectively incorporating diverse prior information, such as camera intrinsics, poses, and depth maps, into feed-forward models like CUT3R. G-CUT3R extends CUT3R by encoding prior information (K, P, D) into ray images or composite representations, processing them with modality-specific convolutional layers and ViT encoders, and fusing the extracted features with RGB image features via zero-initialized convolutional layers within the decoder stage. Experiments demonstrate significant performance improvements across multiple benchmarks; for instance, incorporating pose guidance reduced Absolute Translation Error (ATE) by 61% on Sintel (from 0.077 to 0.030) compared to the no-guidance variant. G-CUT3R provides AI practitioners a versatile and robust solution for 3D vision tasks by enabling more reliable 3D reconstructions for complex real-time applications that can leverage available multi-modal data. |
| S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.12880) or [HuggingFace](https://huggingface.co/papers/2508.12880))| Meiqi Wu, Nisha Huang, Xiaokun Feng, Jiashu Zhu, Chubin Chen | S^2-Guidance introduces a training-free stochastic self-guidance method for enhancing diffusion model performance. This method addresses the suboptimal results of Classifier-free Guidance (CFG), which often leads to semantic incoherence and low-quality outputs in text-to-image and text-to-video generation. S^2-Guidance leverages stochastic block-dropping during the forward process to construct sub-networks, effectively guiding the model away from potential low-quality predictions. Quantitative experiments demonstrate that S^2-Guidance consistently surpasses CFG, achieving an average HPSv2.1 score of 31.09% on SD3 models compared to CFG's 30.48%, and a Total Score of 80.93 on VBench for Wan1.3B models versus CFG's 80.29. For AI practitioners, S^2-Guidance offers a robust, training-free approach to significantly improve generative model quality and prompt adherence without requiring auxiliary training or specialized architectural modifications. |
| Representing Speech Through Autoregressive Prediction of Cochlear Tokens (Read more on [arXiv](https://arxiv.org/abs/2508.11598) or [HuggingFace](https://huggingface.co/papers/2508.11598))| Daniel L. K. Yamins, Evelina Fedorenko, Greta Tuckute, klemenk | AuriStream is a biologically-inspired, two-stage model for speech representation learning via autoregressive prediction of cochlear tokens. The main objective is to learn versatile speech representations using a simple, scalable autoregressive prediction objective on a time-frequency representation inspired by the human cochlea. The methodology involves WavCoch tokenizing raw audio into discrete cochlear tokens, which then serve as input for an autoregressive GPT-style Transformer that predicts upcoming tokens. AuriStream-1B achieved a Spearman correlation of 10.64 on the ZeroSpeech 2021 Lexical Semantic Benchmark's synthetic audio subset, outperforming all other comparison models. This demonstrates that AI practitioners can leverage a simple autoregressive prediction objective on biologically-inspired discrete tokens to achieve state-of-the-art lexical semantic representations and versatile performance for various speech tasks, with the added benefit of interpretable audio generation. |
| Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision
  Mapping (Read more on [arXiv](https://arxiv.org/abs/2508.12466) or [HuggingFace](https://huggingface.co/papers/2508.12466))| Tyler Derr, xuhuizhan5 | Inverse-LLaVA proposes a novel multimodal learning approach that eliminates alignment pre-training by mapping text embeddings into continuous visual representation space instead of projecting visual features to text tokens. The main objective is to challenge the necessity of expensive alignment pre-training and demonstrate if inverting the modality mapping direction can achieve competitive multimodal understanding. The methodology involves using learnable projections to map text embeddings to the visual dimensionality, performing fusion within transformer intermediate layers via selective additive attention components, and training in a single stage with only instruction tuning data. Inverse-LLaVA eliminated alignment pre-training, reducing computational requirements by 45%, and achieved notable improvements on reasoning tasks like MM-VET (+0.2%) and cognitive reasoning on MME (+27.2%), while showing trade-offs in perception tasks (e.g., celebrity recognition -49.5%). This implies that AI practitioners can develop efficient multimodal architectures that preserve modality-specific characteristics and significantly reduce computational overhead by eliminating the data-intensive alignment pre-training stage for tasks that benefit from continuous visual representations. |
| Precise Action-to-Video Generation Through Visual Action Prompts (Read more on [arXiv](https://arxiv.org/abs/2508.13104) or [HuggingFace](https://huggingface.co/papers/2508.13104))| Minghan Qin, Sida Peng, Haoyu Guo, walsvid, angshineee | This paper proposes visual action prompts, specifically 2D skeletons, as a unified and precise representation for action-to-video generation involving complex, high-degree-of-freedom interactions. The main objective is to enable precise and generalizable action-driven video generation across diverse domains, addressing the precision-generality tradeoff of existing action representations. The methodology involves robust pipelines to extract 2D skeletons from human-object interaction videos and render robotic gripper skeletons from state logs, then integrating these visual prompts into pretrained video generation models via lightweight fine-tuning using ControlNet. Experiments demonstrate that skeleton-based control significantly improves dynamic correctness; for instance, on the DROID dataset, their unified skeleton model achieved an ST-IoU of 0.478, outperforming text-based control (0.239). This approach allows AI practitioners to train unified action-driven generative models across heterogeneous datasets, facilitating cross-domain knowledge transfer and precise control for complex interaction scenarios. |
| Reinforcement Learning with Rubric Anchors (Read more on [arXiv](https://arxiv.org/abs/2508.12790) or [HuggingFace](https://huggingface.co/papers/2508.12790))| Haokai Xu, Zeyu Qin, Guoshan Lu, Zenan Huang, utdawn | Rubicon introduces a novel rubric-based Reinforcement Learning framework to enable Large Language Models (LLMs) to effectively learn from non-verifiable rewards in open-ended tasks. The core objective is to extend the Reinforcement Learning from Verifiable Rewards (RLVR) paradigm beyond strictly verifiable domains by integrating subjective, open-ended tasks using rubric-based feedback. The methodology involves constructing the largest rubric reward system to date (over 10,000 rubrics), designing advanced reward aggregation strategies, and employing a multi-stage RL training protocol with adaptive defense against reward hacking. Quantitatively, the Rubicon-preview model achieved a +5.2% absolute improvement on open-ended, humanities-centric benchmarks, outperforming a 671B DeepSeek-V3 model by +2.4% points with only 5K training samples. This approach provides AI practitioners with fine-grained stylistic control over LLM outputs, enabling more human-like and expressive responses for applications in subjective domains where traditional verifiable rewards are unfeasible. |
| Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning
  Models to Ask for Information (Read more on [arXiv](https://arxiv.org/abs/2508.11252) or [HuggingFace](https://huggingface.co/papers/2508.11252))| Xi Yang, Duanyu Feng, Chen Huang, Bowen Qin, YouchengHuang | This paper introduces CRITIC-math to systematically evaluate Large Reasoning Models' (LRMs) ability to proactively ask for information on incomplete mathematical problems. The research investigates LRMs' capacity to identify incomplete problems and raise clarification questions, and the effectiveness of supervised fine-tuning (SFT) in teaching this skill. Using CRITIC-math, a new benchmark dataset of 1.3K test and 5.3K training data with two types of incomplete problems (missing goal/premises), the study found that LRMs achieved low clarification ratios (around 25%) and accuracies (around 40%) with implicit prompts. Overthinking and hallucination were identified as primary failure modes, accounting for 80% and 95% of failures respectively, yet SFT significantly improved this ability, with CRITIC-Qwen achieving 87.86% accuracy. This work highlights that current LRM development is biased towards solving well-defined problems and emphasizes the need for AI practitioners to shift focus towards developing genuinely intelligent agents capable of proactive information-seeking for real-world scenarios. |
