

## Papers for 2025-08-19

| Title | Authors | Summary |
|-------|---------|---------|
| Ovis2.5 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.11737) or [HuggingFace](https://huggingface.co/papers/2508.11737))| Yang Li, cqgwin, Suikong, xxyyy123, runninglsy | Ovis2.5 is a new multimodal large language model (MLLM) designed for native-resolution visual perception and enhanced reasoning. The paper addresses shortcomings in previous MLLMs, specifically rigid vision front-ends hindering analysis of dense content (e.g., charts) and linear chain-of-thought training lacking self-correction for deeper reasoning. Ovis2.5 integrates a native-resolution Vision Transformer (NaViT) for variable image resolutions and employs a five-phase training curriculum including DPO and GRPO, incorporating "thinking-style" data for reflection. Comprehensive evaluations show Ovis2.5-9B achieved an average OpenCompass score of 78.3, establishing state-of-the-art performance among open-source MLLMs in the sub-40B parameter range. AI practitioners can leverage Ovis2.5 for improved performance in visually dense and complex reasoning tasks, including STEM and chart analysis, and utilize its resource-efficient training infrastructure for faster model development and deployment in constrained environments. |
| ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long
  Narrative Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10419) or [HuggingFace](https://huggingface.co/papers/2508.10419))| Yufeng Wang, Wei Wei, Rongchen Zhao, Juyuan Wang, lxucs | ComoRAG introduces a cognitive-inspired Retrieval-Augmented Generation framework for stateful long narrative reasoning. The main objective is to address the challenge of global context comprehension in long narratives, which traditional stateless RAG methods fail to capture. ComoRAG employs a Metacognitive Regulation process inspired by the human Prefrontal Cortex, featuring a Hierarchical Knowledge Source, Dynamic Memory Workspace, and a Metacognitive Control Loop with iterative operations like Self-Probe and Mem-Fuse. The framework achieves significant performance gains, for instance, increasing accuracy on the EN.MC benchmark from a static-retrieval baseline of 64.6% to 72.9%. This demonstrates that ComoRAG offers a principled, robust, and flexible plug-and-play solution for AI practitioners to enhance complex query resolution in long-context narrative comprehension. |
| 4DNeX: Feed-Forward 4D Generative Modeling Made Easy (Read more on [arXiv](https://arxiv.org/abs/2508.13154) or [HuggingFace](https://huggingface.co/papers/2508.13154))| Zeng Tao, Jiawei Ren, Long Zhuo, Tianqi Liu, Zhaoxi Chen | 4DNeX is a novel feed-forward framework for generating dynamic 3D scene representations (4D) from a single image. The primary objective is to enable efficient, end-to-end image-to-4D generation, addressing limitations of existing computationally intensive or multi-frame input methods. This is achieved by fine-tuning a pretrained video diffusion model, utilizing a newly constructed 4DNeX-10M dataset, employing a unified 6D video representation (RGB+XYZ sequences), and applying simple adaptation strategies like width-wise fusion and XYZ normalization. Extensive experiments demonstrate 4DNeX's superior efficiency and generalizability; for example, it generates 4D scenes in 15 minutes, significantly faster than optimization-based methods like Free4D (60 minutes), while achieving competitive metrics such as 97.2% consistency and 58.3% dynamic degree in image-to-4D tasks. 4DNeX provides a scalable and accessible solution for image-to-4D modeling, laying the foundation for efficient generative 4D world models that simulate dynamic scene evolution. |
| Next Visual Granularity Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12811) or [HuggingFace](https://huggingface.co/papers/2508.12811))| Kang Liao, Qingyi Tao, Zhonghua Wu, Zhouxia Wang, yikaiwang | This paper introduces Next Visual Granularity (NVG), a novel image generation framework representing images as structured sequences of varying granularity levels. The primary objective is to advance image generation by explicitly modeling hierarchical visual structure, addressing the limitation of existing methods treating images as unstructured data, and enabling fine-grained control. NVG decomposes images into content and structure pairs across multiple stages using a multi-granularity quantized autoencoder and a residual, pyramid-like token construction. It iteratively refines the image by generating structure maps with a lightweight rectified flow model and content with a transformer, incorporating Structure-Aware RoPE. Compared to VAR models, NVG consistently outperforms them in FID scores, with NVG-d24 achieving an FID of 2.06 versus VAR-d24's 2.09, and its tokenizer demonstrates superior reconstruction quality (rFID 0.74 for NVG vs 1.06 for VAR). For AI practitioners, NVG offers a scalable, more controllable generative system that supports explicit structure control directly during generation without requiring additional post-hoc modules, proving beneficial for applications where structural and hierarchical control is essential. |
| Speed Always Wins: A Survey on Efficient Architectures for Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.09834) or [HuggingFace](https://huggingface.co/papers/2508.09834))| Jusen Du, Yucheng Zhou, Jiaxi Hu, Weigao Sun, landisen | "Speed Always Wins" surveys innovative architectures optimizing Large Language Models (LLMs) for efficiency. The paper systematically examines how to overcome the Transformer's quadratic complexity and high resource demands to achieve more efficient and scalable LLMs. It categorizes and reviews recent advancements into seven areas, including linear/sparse sequence modeling, efficient full attention, sparse Mixture-of-Experts, hybrid architectures, and Diffusion LLMs. For example, hybrid models like Jamba [185] achieve 3x higher throughput than Mixtral while supporting 256K context with only 4GB KV cache. This survey serves as a blueprint for AI practitioners to develop scalable, resource-aware, and versatile LLM systems by integrating these architectural principles. |
| Has GPT-5 Achieved Spatial Intelligence? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.13142) or [HuggingFace](https://huggingface.co/papers/2508.13142))| Ruisi Wang, Qingping Sun, Yubo Wang, yl-1993, caizhongang | This empirical study assesses GPT-5's spatial intelligence across eight recent benchmarks, revealing significant advancements but also persistent limitations compared to human performance. The paper aims to examine the extent to which GPT-5 and other state-of-the-art multi-modal large language models (MLLMs) have achieved spatial intelligence. It proposes a comprehensive taxonomy of spatial tasks, evaluating models on eight key benchmarks (e.g., VSI-Bench, SITE, MindCube) using standardized prompts and Chance-Adjusted Accuracy (CAA), consuming over one billion tokens. Findings indicate GPT-5 sets a new state-of-the-art in spatial intelligence, achieving a Chance-Adjusted Accuracy (CAA) of 21.67 on MindCube, yet still falls significantly short of human performance (Human MindCube CAA: 91.94). For AI practitioners, this research clarifies fundamental spatial task categories and identifies the remaining unique challenges for MLLMs, emphasizing the need for continued development to bridge the human-model gap in complex spatial reasoning. |
| HeroBench: A Benchmark for Long-Horizon Planning and Structured
  Reasoning in Virtual Worlds (Read more on [arXiv](https://arxiv.org/abs/2508.12782) or [HuggingFace](https://huggingface.co/papers/2508.12782))| Artyom Sorokin, Viktor Volkov, Stefan Rebrikov, Petr Anokhin, roxal | HeroBench is a novel benchmark for evaluating large language models' (LLMs) long-horizon planning and structured reasoning in complex virtual worlds. Its main objective is to assess LLMs' ability to generate and execute extended, interdependent action sequences, addressing the limitations of simpler algorithmic benchmarks. The benchmark utilizes a grid-based, RPG-style simulated environment, presenting JSON-serialized tasks that require LLMs to generate Python code for actions like resource gathering, crafting, and combat, with performance evaluated by Success and Progress scores. Evaluations of 25 state-of-the-art LLMs revealed substantial performance disparities, with Grok-4 achieving the highest success rate of 91.7% on base tasks, demonstrating superior robustness across difficulty levels. This work highlights persistent challenges in robust long-horizon autonomous planning for LLMs, underscoring the need for continued research into planning architectures and the careful design of multi-agent systems for complex sequential tasks. |
| When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness
  Methods for LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.11383) or [HuggingFace](https://huggingface.co/papers/2508.11383))| Elena Tutubalina, Gleb Ershov, Mikhail Chaichuk, apanc, myyycroft | This paper presents a large-scale comparative evaluation of five prompt robustness methods for Large Language Models. The core objective was to systematically compare the effectiveness of existing prompt robustness methods across diverse LLM families and sizes. The study benchmarked five in-context learning and supervised fine-tuning techniques on 8 open-source and 2 frontier LLMs across 52 Natural Instructions tasks, evaluating their performance against diverse prompt formats and under various distribution shifts. Key findings indicate that Batch Calibration significantly reduced prompt sensitivity (spread) for 6/8 open-source models while improving accuracy, and a majority voting-based Template Ensembles method reduced spread for frontier models by at least 44% in 9 of 20 cases. AI practitioners should consider calibration for open-source LLMs in balanced classification, prefer probability ranking over greedy decoding, and apply majority voting-based Template Ensembles for black-box frontier models to mitigate prompt sensitivity. |
| Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive
  World Model (Read more on [arXiv](https://arxiv.org/abs/2508.13009) or [HuggingFace](https://huggingface.co/papers/2508.13009))| Yifan Zhang, Boyang Wang, Zexiang Liu, Chunli Peng, Xianglong He | Matrix-Game 2.0 is an open-source, auto-regressive diffusion world model that generates interactive, long-form video streams in real-time from visual and user-action inputs. The main objective is to develop a framework for real-time, streaming interactive video generation that overcomes the latency of bidirectional models and the error accumulation of traditional auto-regressive approaches. The methodology involves training a bidirectional vision-only diffusion transformer on a large-scale (~1200 hours) dataset from game environments, then distilling it into a causal, few-step auto-regressive model using a Self-Forcing technique and KV-caching. The primary result is the model's ability to generate minute-level, 352×640 resolution video at 25.15 FPS on a single H100 GPU, demonstrating high-fidelity, real-time user interaction. The principal implication for AI practitioners is that it provides an open-source model and a data production pipeline for building real-time interactive simulations, offering a viable foundation for applications requiring on-the-fly, generative virtual environments without reliance on traditional rendering engines. |
| Lumen: Consistent Video Relighting and Harmonious Background Replacement
  with Video Generative Models (Read more on [arXiv](https://arxiv.org/abs/2508.12945) or [HuggingFace](https://huggingface.co/papers/2508.12945))| Zixiang Gao, Chenxuan Miao, Yutong Feng, Yuxuan Liu, Jianshu Zeng | Lumen is an end-to-end video relighting framework that also performs harmonious background replacement using large-scale video generative models. The objective is to relight video foregrounds with harmonious blending while preserving intrinsic attributes and replacing backgrounds based on textual descriptions, overcoming data scarcity and ensuring temporal consistency. Lumen leverages a multi-domain dataset of 3D-rendered and HDR-simulated realistic paired videos and employs a DiT-based generative model with a domain-aware style adapter trained via a two-stage curriculum. Quantitative evaluations demonstrate Lumen's superior performance, achieving a PSNR of 23.06 on realistic paired videos, indicating enhanced foreground preservation and lighting harmonization compared to existing methods. This framework provides AI practitioners with a robust, text-guided solution for high-quality video relighting and background replacement, applicable across diverse real-world scenarios due to its generalization capabilities. |
| S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.12880) or [HuggingFace](https://huggingface.co/papers/2508.12880))| Meiqi Wu, Nisha Huang, Xiaokun Feng, Jiashu Zhu, Chubin Chen | i) S²-Guidance is a training-free method that enhances diffusion model outputs by using stochastically generated sub-networks for self-correction during inference. ii) The primary objective is to mitigate the semantic incoherence and low-quality artifacts produced by Classifier-Free Guidance (CFG) in diffusion models without requiring additional training or hand-crafted architectural modifications. iii) The key methodology, S²-Guidance, modifies the standard CFG update step by subtracting a corrective term. This term is the output of a temporary sub-network created on-the-fly at each denoising step via stochastic block-dropping, which guides the model away from suboptimal predictions. iv) The method demonstrates superior performance across multiple benchmarks; for instance, on the T2I-CompBench benchmark with the SD3 model, S²-Guidance achieved a color composition score of 59.63%, significantly outperforming the baseline CFG score of 53.61%. v) For AI practitioners, S²-Guidance can be implemented as a drop-in, training-free replacement for standard CFG in existing generation pipelines to improve output quality and prompt adherence. The most impactful finding is that a single stochastic sub-network sample per timestep is sufficient for effective guidance, making the enhancement computationally efficient and practical for deployment. |
| Representing Speech Through Autoregressive Prediction of Cochlear Tokens (Read more on [arXiv](https://arxiv.org/abs/2508.11598) or [HuggingFace](https://huggingface.co/papers/2508.11598))| Daniel L. K. Yamins, Evelina Fedorenko, Greta Tuckute, klemenk | AuriStream is a two-stage, biologically-inspired model learning speech representations through autoregressive prediction of discrete cochlear tokens. The model's objective is to learn versatile speech representations using a simple and scalable autoregressive prediction objective on a human cochlea-inspired time-frequency representation. Its methodology involves WavCoch, which transforms raw audio into discrete cochlear tokens via a 13-bit LFQ bottleneck, followed by AuriStream, a GPT-style Transformer autoregressively predicting upcoming cochlear tokens. AuriStream-1B achieved state-of-the-art lexical semantics with an sSIMI score of 12.52 on the LibriSpeech Audio subset and competitive performance across diverse SUPERB speech tasks, including 4.20 ASR and 98.01 IC. This framework demonstrates that an autoregressive objective on biologically-inspired inputs can yield versatile representations, offering an interpretable alternative to current speech AI models through its ability to generate audio from predictions. |
| Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision
  Mapping (Read more on [arXiv](https://arxiv.org/abs/2508.12466) or [HuggingFace](https://huggingface.co/papers/2508.12466))| Tyler Derr, xuhuizhan5 | Inverse-LLaVA eliminates expensive alignment pre-training in large vision-language models by inverting the conventional modality mapping direction. The primary objective is to investigate if mapping text embeddings into continuous visual space, instead of projecting visual features to text, can maintain or improve performance while removing the alignment stage. This is achieved by projecting text embeddings into continuous visual representation space and performing fusion within transformer intermediate layers via selective additive attention components during single-stage instruction tuning. Empirically, Inverse-LLaVA demonstrates +27.2% improvement on cognitive reasoning tasks (MME benchmark) and reduces training computational requirements by 45% by eliminating alignment pre-training. This suggests that architectural innovation can effectively substitute for data-intensive alignment procedures, allowing AI practitioners to develop powerful multimodal models with significantly lower computational and data demands, particularly for complex reasoning tasks. |
| Precise Action-to-Video Generation Through Visual Action Prompts (Read more on [arXiv](https://arxiv.org/abs/2508.13104) or [HuggingFace](https://huggingface.co/papers/2508.13104))| Minghan Qin, Sida Peng, Haoyu Guo, walsvid, angshineee | This paper introduces visual action prompts for precise action-to-video generation in complex, high-degree-of-freedom interaction scenarios. The primary objective is to develop a generalizable action-to-video model that accurately depicts interaction outcomes while balancing action precision and dynamic transferability across domains, addressing the lack of a unified precise action representation. The methodology involves "rendering" actions into domain-agnostic visual prompts, specifically 2D skeletons, which are robustly recovered from human-object interaction and robotic manipulation datasets via specialized pipelines, then integrated into a pretrained CogVideoX model using ControlNet. Quantitative experiments on EgoVid, RT-1, and DROID datasets show that visual action prompts outperform alternative control signals; for instance, on RT-1, the unified skeleton approach achieved a Spatio-temporal IoU of 0.576, exceeding text (0.267) and raw state (0.507) controls. This approach enables training unified action-driven generative models across heterogeneous datasets, facilitating crucial cross-domain knowledge transfer for AI practitioners. |
| G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior
  Integration (Read more on [arXiv](https://arxiv.org/abs/2508.11379) or [HuggingFace](https://huggingface.co/papers/2508.11379))| Evgeny Burnaev, Peter Wonka, Artem Komarichev, rusrakhimov, smileyenot983 | G-CUT3R is a novel feed-forward method that enhances the CUT3R framework for 3D scene reconstruction by integrating auxiliary prior information like camera parameters and depth maps. The main objective is to improve the accuracy of feed-forward reconstruction by leveraging commonly available geometric data that existing models typically ignore. The methodology modifies the CUT3R decoder by introducing dedicated encoders for each prior modality and fusing their features with RGB image tokens via zero-initialized convolutional layers, enabling stable and flexible integration of any combination of priors. The proposed method demonstrates significant performance improvements, achieving a 61% reduction in Absolute Translation Error (from 0.077 to 0.030) on the Sintel dataset when incorporating pose guidance. For AI practitioners, this provides a lightweight and versatile solution to boost 3D reconstruction quality in real-world applications by utilizing available sensor data (e.g., from LiDAR or IMU) without fundamentally altering existing feed-forward architectures. |
| Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning
  Models to Ask for Information (Read more on [arXiv](https://arxiv.org/abs/2508.11252) or [HuggingFace](https://huggingface.co/papers/2508.11252))| Xi Yang, Duanyu Feng, Chen Huang, Bowen Qin, YouchengHuang | This paper introduces the `CRITIC-math` benchmark to evaluate the inability of Large Reasoning Models (LRMs) to ask for information when faced with incomplete mathematical problems. The research aims to systematically assess to what extent LRMs can identify problem incompleteness and proactively ask for clarification, and whether this skill can be improved through supervised fine-tuning. A new dataset, `CRITIC-math`, was constructed by rewriting well-defined math problems into two types of incomplete problems ("missing goal" and "missing premises"), which was then used to evaluate several state-of-the-art LRMs. The primary result is that LRMs perform poorly, achieving Clarification Ratios of only around 25% with implicit prompts, and when failing to ask, they exhibit overthinking, hallucination of missing information, and "thoughts-to-answer unfaithfulness." The principal implication for AI practitioners is that the current LRM development paradigm, which focuses exclusively on solving well-defined problems, is insufficient and should be augmented with methodologies that train models to identify and query for missing information to build more robust and genuinely intelligent systems. |
