

## Papers for 2025-08-01

| Title | Authors | Summary |
|-------|---------|---------|
| Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving (Read more on [arXiv](https://arxiv.org/abs/2507.23726) or [HuggingFace](https://huggingface.co/papers/2507.23726))| Zhicheng Jiang, Wenhao Huang, Liankai Huang, Jinming Gu, Luoxin Chen | The paper introduces Seed-Prover and Seed-Geometry, two systems that integrate large language models with the Lean formal proof assistant to advance automated theorem proving. The objective is to solve highly complex mathematical problems by developing a system capable of both broad, exploratory conjecture generation and deep, iterative proof refinement. The methodology combines a "lemma-style" whole-proof generation model with a three-tiered inference strategy (light, medium, heavy) that leverages iterative refinement based on Lean compiler feedback, proved lemmas, and self-summarization. The system achieved state-of-the-art results, including proving 78.1% of 155 formalized past IMO problems and, post-competition, solving 5 out of 6 problems at the IMO 2025. The principal implication for AI practitioners is that integrating LLMs with formal verification environments and employing multi-stage, iterative refinement strategies enables the solution of complex, structured reasoning tasks with verifiable correctness, surpassing the capabilities of single-pass or natural language-based approaches. |
| Phi-Ground Tech Report: Advancing Perception in GUI Grounding (Read more on [arXiv](https://arxiv.org/abs/2507.23779) or [HuggingFace](https://huggingface.co/papers/2507.23779))| Kai Qiu, Qi Dai, Jialiang Zhu, Ziqiang Xu, Miaosen Zhang | This research details the Phi-Ground model family, which advances GUI grounding by systematically optimizing data processing, training strategies, and model architecture to achieve state-of-the-art performance. The objective is to improve the perception capabilities of GUI grounding models for Computer Use Agents (CUAs) by investigating factors from data collection to training protocols, addressing the low accuracy of existing methods. The methodology involves fine-tuning MLLMs on a 40M+ sample dataset, emphasizing text-first modality input, random resize data augmentation, and uniform spatial data distribution, and leveraging a two-stage approach where a planner model generates detailed instructions for the specialized grounding model. In an agent setting, the Phi-Ground-7B-16C-DPO model achieves 55.0% click accuracy on the challenging ScreenSpot-pro benchmark, and post-training with Direct Preference Optimization (DPO) further enhances performance. For AI practitioners, the key implication is that for multimodal perception tasks, performance depends critically on data strategy (distribution and augmentation) and computational budget (balancing model size and image tokens), and that decoupling high-level reasoning (planning) from low-level perception (grounding) is a highly effective design pattern. |
| C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring
  Challenges in Complex Conversations (Read more on [arXiv](https://arxiv.org/abs/2507.22968) or [HuggingFace](https://huggingface.co/papers/2507.22968))| Yiwen Guo, Wei Tao, Chengqian Ma | This paper introduces CÂ³, a bilingual benchmark for evaluating Spoken Dialogue Models (SDMs) on complex conversational challenges. The primary objective is to assess the capabilities of current SDMs in handling five key phenomena: phonological ambiguity, semantic ambiguity, omission, coreference, and multi-turn interaction in both English and Chinese. The methodology involves a new dataset of 1,079 instances designed to test these phenomena, evaluated using an LLM-based method (GPT-4o and DeepSeek-R1 as judges) that shows high correlation (Pearson > 0.87) with human judgments. The study's primary result is that SDMs struggle significantly with ambiguity, achieving an overall accuracy of just 3.97% on semantic ambiguity tasks in Chinese, and generally perform better in English (overall accuracy 35.15%) than in Chinese (23.33%). The principal implication for AI practitioners is that the selection of an SDM must be carefully tailored to the specific language and conversational complexity of the application, as model performance varies drastically; for example, GPT-4o-Audio-Preview excels in English (55.68% accuracy) while Qwen2.5-Omni is superior for Chinese (40.08% accuracy). |
| RecGPT Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.22879) or [HuggingFace](https://huggingface.co/papers/2507.22879))| Jian Wu, Jiakai Tang, Gaoyang Guo, Dian Chen, Chao Yi | This paper presents RecGPT, a large language model-based framework that redesigns the recommender system pipeline to be intent-centric, moving beyond traditional log-fitting. The primary objective is to overcome the limitations of log-fitting approaches, such as filter bubbles and the Matthew effect, by explicitly modeling user intent through LLMs for interest mining, item retrieval, and explanation generation. The key methodology involves a multi-stage workflow using three specialized LLMs for user interest mining, item tag prediction, and explanation generation, integrated into a tag-aware tri-tower (User, Item, Tag) retrieval architecture and trained via a progressive paradigm guided by a Human-LLM cooperative judge system. Online A/B experiments on the Taobao App demonstrated that RecGPT achieved a +6.33% increase in Click-Through Rate (CTR), a +9.47% increase in Item Page Views (IPV), and a +6.96% increase in Clicked Item Category Diversity (CICD) over the baseline. The principal implication for AI practitioners is that shifting from purely log-fitting models to an explicit, LLM-driven, intent-centric paradigm can create a more sustainable recommendation ecosystem, simultaneously boosting user engagement, commercial metrics, and content diversity for long-tail merchants. |
| villa-X: Enhancing Latent Action Modeling in Vision-Language-Action
  Models (Read more on [arXiv](https://arxiv.org/abs/2507.23682) or [HuggingFace](https://huggingface.co/papers/2507.23682))| Kaixin Wang, Chuheng Zhang, Pushi Zhang, Hangxing Wei, Xiaoyu Chen | The paper introduces villa-X, a Visual-Language-Latent-Action (ViLLA) framework that improves latent action learning and its integration into VLA models by jointly modeling latent and robot actions. The primary objective is to improve how latent actions are learned from visual data and how they are incorporated into Vision-Language-Action (VLA) pre-training to create more generalizable robot manipulation policies. The methodology features a Latent Action Model (LAM) augmented with a proprioceptive Forward Dynamics Model (proprio FDM) to ground latent actions in robot dynamics, and an Actor (ACT) module that jointly models latent and robot action sequences using a joint diffusion process, with robot action generation explicitly conditioned on the latent action plan. The framework achieves superior performance across simulated and real-world tasks, notably attaining a 90.1% average success rate on the four LIBERO benchmark suites, outperforming prior methods like OpenVLA (76.5%). For practitioners, this work demonstrates that explicitly grounding latent actions in robot proprioceptive data and using a structured, hierarchical diffusion model provides a more effective method for leveraging large-scale, action-free video data to pre-train robust and generalizable robot policies. |
| Scalable Multi-Task Reinforcement Learning for Generalizable Spatial
  Intelligence in Visuomotor Agents (Read more on [arXiv](https://arxiv.org/abs/2507.23698) or [HuggingFace](https://huggingface.co/papers/2507.23698))| Anji Liu, Bowei Zhang, Haiwen Xia, Zhancun Mu, Shaofei Cai | This paper presents a scalable framework for post-training visuomotor agents with multi-task reinforcement learning, significantly enhancing their generalizable spatial reasoning and zero-shot transfer capabilities. The primary objective is to determine if RL post-training can enable a pre-trained visuomotor policy to generalize spatial intelligence to novel tasks and unseen 3D environments, overcoming typical overfitting issues. The methodology uses cross-view goal specification as a unified task representation, automatically synthesizes over 100,000 tasks in Minecraft, and fine-tunes an imitation-learned policy with a distributed Proximal Policy Optimization (PPO) algorithm constrained by KL-divergence. The primary results show that RL post-training boosts average interaction success rates by 4x (from 7% to 28%) and enables the agent to achieve a 48% success rate on challenging invisible-target tasks where other baselines failed. For AI practitioners, this work provides a paradigm of imitation learning pre-training followed by large-scale RL fine-tuning, demonstrating that complex spatial skills learned in a single customizable simulator can generalize effectively to different virtual and real-world environments without requiring domain-specific adaptation. |
| Persona Vectors: Monitoring and Controlling Character Traits in Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2507.21509) or [HuggingFace](https://huggingface.co/papers/2507.21509))| Jack Lindsey, Owain Evans, Henry Sleight, Andy Arditi, Runjin Chen | This research introduces "persona vectors," linear directions in a large language model's activation space that correspond to specific character traits, and demonstrates their use for monitoring and controlling model personality. The primary objective is to develop an automated method to identify these vectors and use them to predict, monitor, and mitigate undesirable persona shifts induced by prompting or finetuning. The methodology involves automatically generating contrastive prompt pairs from a natural language trait description, then computing the persona vector as the difference-in-means of the resulting response activations. The results show that finetuning-induced shifts along these vectors strongly predict behavioral changes (e.g., Pearson's r up to 0.97 between activation shift and trait expression) and that preventative steering during training can mitigate these shifts while preserving capabilities. For AI practitioners, this provides a scalable method for pre-finetuning data screening; by calculating the "projection difference" on training samples, developers can identify and filter data likely to cause unwanted emergent behaviors like sycophancy or maliciousness. |
| On the Expressiveness of Softmax Attention: A Recurrent Neural Network
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2507.23632) or [HuggingFace](https://huggingface.co/papers/2507.23632))| Eric C. Larson, Gabriel Mongaras | This paper derives a recurrent neural network (RNN) formulation for softmax attention, clarifying its expressiveness compared to linear attention. The main objective is to understand why softmax attention is more expressive than linear attention, which typically lags in downstream accuracy despite being derived from softmax. The authors achieve this by deriving the recurrent form of softmax attention via its Taylor series expansion, analyzing the numerator, and reinterpreting the denominator as a gate or norm, alongside conducting ablation studies. Key findings show that linear attention is a first-order approximation of softmax attention, and adding higher-order Taylor series terms up to n=10 can make the recurrent approximation mirror softmax with negligible differences, while a simple vector norm for the denominator can suffice. This work provides a theoretical basis for understanding the performance bounds of softmax attention and suggests avenues for developing more performant or efficient attention mechanisms by leveraging higher-order interactions or alternative normalization schemes. |
| TARS: MinMax Token-Adaptive Preference Strategy for Hallucination
  Reduction in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2507.21584) or [HuggingFace](https://huggingface.co/papers/2507.21584))| Jiasheng Tang, Chang Liu, Zhiming Luo, Keda Tao, Kejia Zhang | TARS is a min-max token-adaptive preference strategy that reformulates Direct Preference Optimization (DPO) to reduce hallucinations in Multimodal Large Language Models (MLLMs). The primary objective is to address DPO's overfitting to superficial linguistic cues, which leads to distributional rigidity and ungrounded outputs in MLLMs. TARS reformulates DPO as a min-max optimization problem, maximizing adaptability via controlled perturbations of visual-agnostic tokens while minimizing preference loss, and incorporating spectral preference alignment for semantic consistency. Using 4.8k preference samples, TARS reduces hallucination rates on the AMBER benchmark from 26.4% to 13.2% for LLaVA-v1.5-7B models, outperforming DPO baselines and matching GPT-4o. TARS offers a data-efficient method for MLLM developers to enhance factual grounding and trustworthiness by mitigating hallucinations without extensive datasets or expert feedback. |
| Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for
  Culturally Diverse Art Style Classification (Read more on [arXiv](https://arxiv.org/abs/2507.23436) or [HuggingFace](https://huggingface.co/papers/2507.23436))| Abdelmalik Taleb-Ahmed, Cosimo Distante, Salah Eddine Bekhouche, Abdellah Zakaria Sellam | This paper proposes enhancing a dual-teacher knowledge distillation framework for art style classification by replacing linear MLP projection heads with spline-based Kolmogorov-Arnold Networks (KANs). The objective is to improve self-supervised art style classification by better modeling the complex, nonlinear interactions of stylistic features that linear projections fail to capture. The key methodology involves integrating KANs into all three network branches (student, momentum teacher, style teacher) and training the student network using a composite loss function that includes relation alignment, Gram matrix-based style alignment, and KAN-specific regularization. On the Pandora18k dataset, using a ConvNeXt-Base backbone, the KAN-based approach achieved a 66.26% Top-1 accuracy, representing a 1.03% improvement over the identical architecture using standard MLP heads. For AI practitioners, this research demonstrates that substituting MLP heads with KANs in self-supervised contrastive learning frameworks can yield superior feature representations for tasks involving complex data manifolds, such as art style, without altering the core training paradigm. |
| Enhanced Arabic Text Retrieval with Attentive Relevance Scoring (Read more on [arXiv](https://arxiv.org/abs/2507.23404) or [HuggingFace](https://huggingface.co/papers/2507.23404))| Abdenour Hadid, Fadi Dornaika, Yazid Bounab, Azeddine Benlamoudi, Salah Eddine Bekhouche | This paper presents Adaptive Passage Retrieval (APR), an enhanced dense retrieval framework for Arabic that integrates a lightweight transformer with a novel Attentive Relevance Scoring (ARS) module to improve ranking accuracy. The primary objective is to develop a dense retrieval model specifically for the Arabic language that surpasses standard systems by using a more sophisticated, learned relevance function instead of simple vector similarity to better handle Arabic's linguistic complexities. The system employs a dual-encoder architecture with a lightweight Arabic-specific transformer (MiniBERT) and introduces an Attentive Relevance Scoring (ARS) module that computes a relevance score via a learned, non-linear interaction, trained with a composite loss function. The paper does not provide an ablation study to isolate the performance gains from the ARS module versus the MiniBERT encoder. On the ArabicaQA test set, the APR model achieved a Top-10 retrieval accuracy of 63.17%, an absolute improvement of +4.77% over the state-of-the-art AraDPR baseline. For AI practitioners, this work demonstrates that augmenting a standard dual-encoder architecture with a lightweight, trainable relevance scoring module can yield significant performance gains over relying solely on dot-product similarity, providing a more robust method for semantic matching in morphologically complex languages. |
| NeRF Is a Valuable Assistant for 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2507.23374) or [HuggingFace](https://huggingface.co/papers/2507.23374))| ZeSheng Wang, Yufeng Wang, Takeo Igarashi, I-Chao Shen, Shuangkang Fang | The paper introduces NeRF-GS, a framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) to enhance 3D scene representation performance. The objective is to develop a hybrid framework that systematically integrates a full NeRF pipeline into the training of a 3DGS model to mitigate the inherent limitations of 3DGS, such as initialization sensitivity and weak inter-Gaussian correlations. The NeRF-GS methodology involves three core components: a shared hash-based feature encoding network for both NeRF and GS branches, the optimization of residual vectors for features and positions to model discrepancies between the two representations, and a joint optimization process using "GS-Rays" and shared attribute losses to align the branches and enable NeRF-assisted adaptive Gaussian growth. The proposed NeRF-GS framework demonstrates state-of-the-art performance, surpassing existing methods on benchmark datasets; for instance, on one scene, NeRF-GS achieves a PSNR of 30.5, outperforming the vanilla 3DGS result of 28.7 by 1.8dB. The primary implication for AI practitioners is that NeRF can be used as an effective auxiliary component during the training phase to significantly enhance the rendering quality and robustness of 3D Gaussian Splatting models, particularly for sparse-view scenes, while the final optimized GS branch can be deployed independently to retain its real-time rendering performance. |
| AgroBench: Vision-Language Model Benchmark in Agriculture (Read more on [arXiv](https://arxiv.org/abs/2507.20519) or [HuggingFace](https://huggingface.co/papers/2507.20519))| Yoshitaka Ushiku, Masaki Onishi, Hirokatsu Kataoka, Nakamasa Inoue, Risa Shinoda | This paper introduces AgroBench, a comprehensive, expert-annotated vision-language benchmark for evaluating VLM capabilities in the agricultural domain. The primary objective is to develop a robust benchmark to assess the practical knowledge and applicability of Vision-Language Models (VLMs) across diverse, real-world agricultural scenarios, overcoming the limitations of existing synthetically-generated datasets. The methodology involved creating a question-answering dataset of 4,342 QA pairs manually annotated by agronomist experts, covering seven tasks including 682 disease and 108 weed categories. The primary results show that VLMs struggle with fine-grained identification; specifically, in weed identification, most open-source models performed near random chance, and the highest accuracy achieved by Gemini 1.5-Pro was 55.17%, with error analysis showing 51.92% of failures are due to a "Lack of Knowledge". For AI practitioners, this implies that deploying VLMs in agriculture requires intensive domain-specific fine-tuning with expert-verified data to address the significant knowledge gaps of current models. |
| Flow Equivariant Recurrent Neural Networks (Read more on [arXiv](https://arxiv.org/abs/2507.14793) or [HuggingFace](https://huggingface.co/papers/2507.14793))| T. Anderson Keller | This paper introduces Flow Equivariant Recurrent Neural Networks (FERNNs), a novel architecture that enforces equivariance to continuous, time-parameterized transformations (flows) to improve sequence model generalization. The research objective is to formalize 'flow equivariance' and develop an RNN architecture that is provably equivariant to these dynamic transformations, a property standard group-equivariant RNNs (G-RNNs) lack. The key methodology achieves this by lifting the RNN's hidden state to a product space of flow generators and group elements, then applying a flow-specific transformation at each recurrent step to perform computation in the signal's moving reference frame. FERNNs are shown to significantly outperform G-RNNs, reducing Mean Squared Error by an order of magnitude (from 8.1e-3 to 1.5e-4) on a Translating MNIST prediction task and exhibiting zero-shot generalization to unseen flow velocities. The principal implication for AI practitioners is a parameter-efficient framework to build models for dynamic data (e.g., video, robotics) with superior generalization to new motions and longer sequences. |
| Efficient Machine Unlearning via Influence Approximation (Read more on [arXiv](https://arxiv.org/abs/2507.23257) or [HuggingFace](https://huggingface.co/papers/2507.23257))| Enhong Chen, Defu Lian, Chenwang Wu, Jiawei Liu | This paper presents Influence Approximation Unlearning (IAU), an efficient algorithm for machine unlearning that reframes data removal as an incremental learning task. The research objective is to develop a computationally efficient unlearning method that avoids the prohibitive costs of full model retraining or the Hessian matrix calculations required by traditional influence-based approaches. The key methodology establishes a theoretical link between unlearning and incremental learning, enabling the approximation of a data point's removal by applying a corrective gradient update derived from both the forgotten and remaining data, further enhanced by a novel gradient restriction loss during the initial model training. The primary results show that on a ResNet18 model with the CIFAR10 dataset, IAU achieves the best overall performance with an average rank of 0.3 across utility, time, and efficacy metrics, significantly outperforming the next best baseline (USGD with a rank of 1.7). The principal implication for AI practitioners is a scalable and practical method for executing data deletion requests, making privacy-compliant machine learning more feasible for large models and high-frequency unlearning scenarios without the significant overhead of retraining or Hessian inversion. |
