

## Papers for 2025-08-07

| Title | Authors | Summary |
|-------|---------|---------|
| VeriGUI: Verifiable Long-Chain GUI Dataset (Read more on [arXiv](https://arxiv.org/abs/2508.04026) or [HuggingFace](https://huggingface.co/papers/2508.04026))| Zhenyu Cui, Huichi Zhou, Shunyu Liu, weihao1115, Liam-Liu | The paper introduces VeriGUI, a new human-annotated dataset for benchmarking autonomous agents on long-chain, verifiable Graphical User Interface (GUI) tasks. The primary objective is to evaluate and foster the development of generalist GUI agents on complex, realistic computer tasks that require long-horizon planning, addressing the limitations of existing datasets which focus on short-term interactions and outcome-only verification. The methodology involves constructing a dataset of web and desktop tasks (averaging 214.4 steps) that are decomposed into a sequence of interdependent subtasks, each with an explicitly defined and verifiable goal. These tasks were generated using a combination of LLM-based instruction creation and expert human demonstration to collect detailed trajectories. Experimental results show that current state-of-the-art agents struggle significantly, with no agent configuration achieving an average task success rate (SR) above 10%; the highest average SR across all tasks was 8.5%, achieved by deep research agents, indicating a substantial performance gap in handling long-horizon tasks. The principal implication for AI practitioners is that current agent architectures and foundation models lack the robust planning and decision-making capabilities required for complex, multi-step GUI workflows, and VeriGUI provides a challenging benchmark with granular, subtask-level feedback to diagnose failures and guide the development of more capable systems. |
| Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens (Read more on [arXiv](https://arxiv.org/abs/2508.01191) or [HuggingFace](https://huggingface.co/papers/2508.01191))| Zhen Tan, Bohan, wjldw, ympc08, chengshuaizhao | Chain-of-Thought (CoT) reasoning in LLMs is primarily a brittle pattern-matching phenomenon, not genuine logical inference, fundamentally bounded by training data distribution. This research questions the nature of CoT reasoning, hypothesizing it reflects structured inductive biases learned from in-distribution data. The study uses DATAALCHEMY, a controlled environment for training LLMs from scratch, to systematically probe CoT across task, length, and format generalization. Findings reveal significant performance degradation under distribution shifts; for example, the exact match for transformation generalization dropped from 100% (in-distribution) to 0% (partial/out-of-distribution). Consequently, AI practitioners should be wary of over-reliance on CoT for robust reasoning, especially in critical applications, and prioritize rigorous out-of-distribution testing. |
| Efficient Agents: Building Effective Agents While Reducing Cost (Read more on [arXiv](https://arxiv.org/abs/2508.02694) or [HuggingFace](https://huggingface.co/papers/2508.02694))| Yue Hou, He Zhu, Pai Liu, Xavier Hu, Ningning Wang | This research systematically analyzes the efficiency-effectiveness trade-off in LLM-driven agents and proposes EFFICIENT AGENTS, a framework that achieves near state-of-the-art performance with significantly reduced operational cost. The main objective is to quantify the impact of different architectural components (LLM backbone, planning, tools, memory, test-time scaling) on agent performance and cost, and to identify an optimal configuration for cost-effective agent design on complex tasks. The study employs an empirical analysis on the GAIA benchmark, systematically varying individual agent components while using the cost-of-pass metric—the ratio of inference cost to success rate—to evaluate the efficiency-performance trade-off of each design choice. The primary result is the EFFICIENT AGENTS framework, which retains 96.7% of the performance of the OWL framework while achieving a 28.4% improvement in the cost-of-pass metric. The analysis reveals that simpler designs, such as a memory module that only retains historical observations and actions, can outperform more complex architectures in both effectiveness and efficiency. The principal implication for AI practitioners is that agent systems can be made more economically viable by avoiding over-engineering; specifically, by choosing a moderately complex planning horizon (e.g., a maximum of 8 steps), using simple memory configurations, and simplifying tool operations, as adding complexity often yields diminishing returns at a high computational cost. |
| SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from
  Experience (Read more on [arXiv](https://arxiv.org/abs/2508.04700) or [HuggingFace](https://huggingface.co/papers/2508.04700))| Xiaoyi Dong, Yuhang Cao, Ziyu Liu, yuhangzang, Zery | SEAgent is a self-evolving framework that enables Computer Use Agents (CUAs) to autonomously learn to operate unfamiliar software through experiential learning, curriculum generation, and a specialist-to-generalist training strategy. The primary objective is to develop a system that allows CUAs to master novel software environments by learning directly from interaction experience, thus eliminating the need for human-labeled data. The methodology combines a `World State Model` for step-wise trajectory evaluation, a `Curriculum Generator` for producing progressively difficult tasks, and a reinforcement learning policy updated via Group Relative Policy Optimization (GRPO) for successful actions and adversarial imitation for failures. On the OS-World benchmark, the specialist-to-generalist SEAgent achieved a 34.5% overall success rate, representing a 23.2% absolute improvement over the 11.3% success rate of the baseline UI-TARS agent. For AI practitioners, this work provides a blueprint for creating agents that can adapt to new software tools on-the-fly, reducing dependency on static, human-curated datasets and enabling more versatile, continuously evolving autonomous systems through self-generated experience. |
| Agent Lightning: Train ANY AI Agents with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.03680) or [HuggingFace](https://huggingface.co/papers/2508.03680))| Zilong Wang, Xufang Luo, SiyunZhao, hzy46, ultmaster | The paper presents Agent Lightning, a framework that enables reinforcement learning-based training for any AI agent by completely decoupling agent execution from the training process. The primary objective is to create a universal method for optimizing complex, multi-turn agents by formulating their execution as a Markov Decision Process (MDP) and defining a unified data interface for collecting transitions. The key methodology is a hierarchical algorithm, LightningRL, which decomposes multi-step agent trajectories into individual transitions, and a "Training-Agent Disaggregation" architecture that separates the RL training server from the agent runtime. Experiments show stable performance gains, with a text-to-SQL agent on the Spider dataset improving its test reward score from approximately 0.1 to over 0.55. The principal implication for AI practitioners is the ability to apply RL fine-tuning to existing agents developed with diverse frameworks (e.g., LangChain, AutoGen) with almost zero code modification, dramatically lowering the barrier to optimizing deployed agentic systems. |
| CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2508.03159) or [HuggingFace](https://huggingface.co/papers/2508.03159))| Donghyeon Lee, Soyon Park, Minju Song, Jueon Park, P-YI | This paper presents CoTox, a Chain-of-Thought-based framework for interpretable molecular toxicity prediction using Large Language Models (LLMs). Its objective is to improve prediction accuracy and explainability over existing methods by integrating chemical structures with biological context. The methodology uses a structured prompt containing a compound's IUPAC name, biological pathways, and Gene Ontology (GO) terms to guide an LLM through step-by-step reasoning for six organ toxicity types. CoTox, using GPT-4o, achieved a mean F1-score of 0.663, outperforming a Chemprop deep learning baseline (0.619), with Gemini-2.5-Pro obtaining the highest score of 0.700. For AI practitioners, the key implication is that for complex scientific domains, LLM performance is significantly enhanced by using Chain-of-Thought prompts with human-readable, multi-modal domain data (IUPAC names, pathways) over raw symbolic representations (SMILES). |
| Training Long-Context, Multi-Turn Software Engineering Agents with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.03501) or [HuggingFace](https://huggingface.co/papers/2508.03501))| Maksim Nekrashevich, Ibragim Badertdinov, Sergei Polezhaev, Maria Trofimova, Alexander Golubev | This paper details the application of reinforcement learning to train a long-context, multi-turn software engineering agent, improving its performance on real-world coding tasks. The research objective was to demonstrate that RL can effectively train LLMs in stateful, interactive environments, moving beyond simpler single-turn problems. The methodology involved a two-phase process on a Qwen2.5-72B-Instruct model: initial Rejection Fine-Tuning (RFT) followed by multi-turn RL training using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm with sparse terminal rewards and context windows up to 131k tokens. This approach increased the agent's Pass@1 success rate on the SWE-bench Verified benchmark from a 20% RFT baseline to 39.0%. For AI practitioners, this work presents a viable, teacher-free method to significantly enhance open-weight model capabilities for complex, interactive tasks like software engineering by optimizing directly on environmental feedback, offering an alternative to supervised fine-tuning on demonstration data. |
| Sotopia-RL: Reward Design for Social Intelligence (Read more on [arXiv](https://arxiv.org/abs/2508.03905) or [HuggingFace](https://huggingface.co/papers/2508.03905))| Keyang Xuan, Kolby Nottingham, Yining Zhao, Zhengyang Qi, Haofei Yu | This paper introduces SOTOPIA-RL, a reinforcement learning framework that trains socially intelligent agents by refining coarse, episode-level feedback into utterance-level, multi-dimensional rewards. The research objective is to develop an effective RL training methodology for social agents that overcomes the challenges of partial observability (delayed effects of utterances) and multi-dimensionality (indirect contributions to goals) inherent in social interactions. The key methodology involves an offline phase where an LLM attributes episode-level outcomes across multiple dimensions (goal completion, relationship, knowledge) to individual utterances, and an online RL phase where a reward model is trained on these attributed rewards to guide policy optimization using Group Relative Policy Optimization (GRPO). Experiments demonstrate that SOTOPIA-RL achieves a state-of-the-art social goal completion score of 7.17 on the SOTOPIA-hard benchmark, significantly outperforming baselines. For AI practitioners, the principal implication is that designing fine-grained, multi-dimensional reward signals, generated offline by a capable LLM, is a critical strategy for stabilizing RL training and improving agent performance in complex, interactive tasks with sparse rewards. |
| LaTCoder: Converting Webpage Design to Code with Layout-as-Thought (Read more on [arXiv](https://arxiv.org/abs/2508.03560) or [HuggingFace](https://huggingface.co/papers/2508.03560))| Tianpeng Lv, Guohao Wang, Zhongyi Zhang, Zhen Li, starmage520 | LaTCoder proposes a Layout-as-Thought (LAT) approach to convert webpage designs to code, significantly improving layout preservation using Multimodal Large Language Models (MLLMs). The research aims to overcome MLLM limitations in accurately preserving webpage layout during design-to-code generation, specifically minimizing the visual discrepancy between generated and original designs. LaTCoder utilizes a three-component methodology: layout-aware division of designs into image blocks, block-wise code synthesis via CoT-based prompting of MLLMs (e.g., DeepSeek-VL2, Gemini, GPT-4o), and layout-preserved assembly using absolute positioning or MLLM-based strategies with dynamic selection. On the CC-HARD dataset, LaTCoder with GPT-4o improved TreeBLEU by 60% and reduced Mean Absolute Error (MAE) by 43.23% compared to direct prompting, while human evaluators preferred LaTCoder-generated webpages in over 60% of cases. This approach provides AI/ML/Software Engineers with a robust strategy for UI automation, demonstrating that decomposing design-to-code tasks into layout-aware blocks significantly enhances MLLM performance and accuracy in complex webpage generation. |
| Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web
  Agents (Read more on [arXiv](https://arxiv.org/abs/2508.01858) or [HuggingFace](https://huggingface.co/papers/2508.01858))| Xinyu Yang, Hongliang He, Aiwen Sun, Cong Guo, Gnonymous | This paper introduces Web-CogReasoner, a web agent trained on a structured, multi-layered knowledge framework to improve cognitive reasoning for web navigation tasks. The research objective is to enhance agent performance by systematically building its capabilities through distinct stages of Factual, Conceptual, and Procedural knowledge acquisition, inspired by Bloom's Taxonomy. The methodology involves constructing the Web-CogDataset for training, the Web-CogBench for evaluation, and developing a knowledge-driven Chain-of-Thought (CoT) reasoning process to guide the agent. The Web-CogReasoner achieves a state-of-the-art success rate of 30.2% for open-source agents on the WebVoyager benchmark and an overall score of 84.4% on the authors' Web-CogBench, outperforming baselines like Gemini 2.5 Pro (80.2%). The principal implication for AI practitioners is that a curriculum-based training approach, which explicitly teaches an agent different cognitive layers of knowledge (perception, comprehension, and planning), is a highly effective strategy for building more robust and generalizable web agents compared to monolithic fine-tuning. |
| HPSv3: Towards Wide-Spectrum Human Preference Score (Read more on [arXiv](https://arxiv.org/abs/2508.03789) or [HuggingFace](https://huggingface.co/papers/2508.03789))| Hongsheng Li, Keqiang Sun, Xiaoshi Wu, Yuhang Ma | This research introduces HPSv3, a human preference score, and HPDv3, a wide-spectrum dataset, for evaluating and improving text-to-image generation models. The primary objective is to develop a more robust, human-aligned evaluation metric that addresses the narrow data coverage and suboptimal design of existing preference models. The methodology involves creating the HPDv3 dataset, which contains 1.08M text-image pairs including high-quality real photos, and training the HPSv3 model on it using a Vision-Language Model (VLM) backbone and an uncertainty-aware ranking loss. HPSv3 achieves state-of-the-art alignment with human judgments, attaining a Spearman correlation of 0.94 with human rankings and a 76.9% preference prediction accuracy on the HPDv3 test set. AI practitioners can leverage the HPSv3 model as a superior automated metric for text-to-image evaluation and use the proposed Chain-of-Human-Preference (CoHP) method as a training-free technique to iteratively refine image generation quality. |
| Gaussian Variation Field Diffusion for High-fidelity Video-to-4D
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2507.23785) or [HuggingFace](https://huggingface.co/papers/2507.23785))| Feng Zhao, Jiaolong Yang, Chuxin Wang, Sicheng Xu, BwZhang | This paper introduces a novel framework, GVFDiffusion, for generating high-fidelity, temporally coherent 4D objects from single video inputs by modeling temporal variations in a compact latent space. The main objective is to overcome the challenges of expensive data acquisition and high-dimensional modeling in 4D synthesis. The methodology involves a Direct 4DMesh-to-GS Variation Field VAE to efficiently encode motion into a latent space, and a temporal-aware Diffusion Transformer to generate these latent variations conditioned on video and a canonical 3D Gaussian Splatting (GS) representation. The model achieves state-of-the-art performance, demonstrating a Fréchet Video Distance (FVD) of 476.83, which is a significant improvement over the next-best prior method (529.10). For AI practitioners, this work provides an efficient pipeline that decomposes 4D generation into static geometry and dynamic variation, enabling the creation of high-quality animated 3D assets from video with substantially lower computational cost than alternative methods. |
| LeanK: Learnable K Cache Channel Pruning for Efficient Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.02215) or [HuggingFace](https://huggingface.co/papers/2508.02215))| Yuqing Yang, Chengruidong Zhang, Huiqiang Jiang, hzy46, zhangyik21 | i) LeanK is a learning-based method that prunes unimportant channels in the Key (K) cache of Large Language Models (LLMs) to accelerate long-context decoding by leveraging static channel sparsity.  ii) The objective is to develop a method to prune the K cache channel dimension to reduce GPU memory usage and improve decoding speed for long-context LLM inference without significant performance degradation.  iii) The methodology is a two-stage training process: first, a continuous scaling factor representing global channel importance is learned using L2 distillation loss and L1 regularization; second, this factor is converted into a static, hardware-aligned binary mask for efficient deployment.  iv) Experiments on models like Llama-3.1-8B show LeanK achieves up to 70% K cache and 16%-18% V cache memory reduction, with a custom kernel enabling a 1.3x speedup in attention computation while maintaining near-lossless model accuracy on benchmarks like RULER.  v) The principal implication for AI practitioners is that they can use LeanK's pre-trained static mask to significantly reduce the memory footprint and latency of long-context inference, enabling the deployment of large models on more resource-constrained hardware or allowing for larger batch sizes without runtime overhead for calculating sparsity. |
| Sculptor: Empowering LLMs with Cognitive Agency via Active Context
  Management (Read more on [arXiv](https://arxiv.org/abs/2508.04664) or [HuggingFace](https://huggingface.co/papers/2508.04664))| Yunxin Liu, Ting Cao, Qitai Tan, L. H. Xu, Mor-Li | The paper introduces Sculptor, a framework of cognitive tools that enables LLMs to actively manage their internal context, thereby mitigating proactive interference and improving performance on long-context reasoning tasks.  The primary objective is to investigate if empowering LLMs with tools for Active Context Management (ACM)—such as fragmenting, hiding, and searching the input context—can overcome performance degradation caused by proactive interference in long-sequence tasks.  The methodology introduces the Sculptor tool suite, featuring functions like `fragment_context`, `fold_fragment`, and `search_context`. Its effectiveness is evaluated by leveraging the zero-shot tool-calling capabilities of LLMs like Claude-4-Sonnet and GPT-4.1 on the PI-LLM and NeedleBench benchmarks. The paper explicitly states that results from a proposed Reinforcement Learning training approach are not yet available.  Primary results show that Sculptor-augmented models achieved significant gains on multi-needle reasoning tasks; on the NeedleBench benchmark, Claude-4-Sonnet’s accuracy increased from 67.0% to 94.0%. However, on the PI-LLM benchmark, results were mixed, with GPT-4.1 improving by 5.54 points while DeepSeek-V3's performance decreased by 5.93 points, indicating that zero-shot tool-use generalization varies across models.  The principal implication for AI practitioners is that implementing explicit context management mechanisms is a viable strategy for improving LLM robustness in long-context scenarios. The most impactful finding is that equipping models with tools to actively curate their "working memory" can dramatically improve performance on tasks requiring the integration of sparse information, suggesting that engineering active attentional control is as critical as expanding raw context capacity. |
| Position: The Current AI Conference Model is Unsustainable! Diagnosing
  the Crisis of Centralized AI Conference (Read more on [arXiv](https://arxiv.org/abs/2508.04586) or [HuggingFace](https://huggingface.co/papers/2508.04586))| Jiaying Wu, Qian Wang, Andre Huikai Lin, Moming Duan, nuojohnchen | i) This paper provides a data-driven diagnosis of the unsustainability of the current centralized AI conference model and proposes a decentralized, community-federated alternative. ii) The objective is to quantify the structural crisis in AI conferences across scientific, environmental, psychological, and logistical dimensions, and to propose a new, more sustainable model. iii) The study employs a multi-pronged methodology, including quantitative analysis of publication trends from CSRankings.org, carbon footprint modeling based on author affiliations, computational sentiment analysis of 405 Reddit threads using VADER, and systemic strain analysis using official conference statistics. iv) The paper identifies a crisis characterized by unsustainable growth, with key findings including a doubling of per-author publication rates to over 4.5 papers annually in the last decade and a significant psychological toll, where over 71% of analyzed online community discourse about conferences reflects negative sentiment and 35% of those negative threads reference mental health concerns. v) The principal implication for AI practitioners is that the current hyper-competitive publication environment incentivizes incremental "SOTA-hacking" over deep, innovative research, impacting project selection and career progression for engineers and scientists while contributing to widespread burnout. The proposed Community-Federated Conference (CFC) model suggests a fundamental shift in how research is reviewed and disseminated, which would alter the mechanisms for collaboration and knowledge exchange practitioners rely on. |
| Enhancing Vision-Language Model Training with Reinforcement Learning in
  Synthetic Worlds for Real-World Success (Read more on [arXiv](https://arxiv.org/abs/2508.04280) or [HuggingFace](https://huggingface.co/papers/2508.04280))| Ruslan Rakhimov, Viacheslav Sinii, Stanislav Dereka, kefirski, GeorgeBredis | This paper introduces Vision-Language Decoupled Actor-Critic (VL-DAC), a reinforcement learning algorithm designed to enhance Vision-Language Model (VLM) training in synthetic environments for improved real-world performance. The primary objective is to develop a robust, hyperparameter-free RL algorithm capable of training VLMs for multi-turn interactive tasks, overcoming limitations in long-horizon reasoning and credit assignment, and ensuring generalization beyond training simulators. VL-DAC employs a decoupled training approach, applying Proximal Policy Optimization (PPO) updates token-wise for actions while learning value only at the environment-step level, with gradients stopped at the VLM backbone, and incorporates stabilization techniques including KL regularization and value warm-up. VL-DAC training in inexpensive simulators yields policies with wide generalization, including a +50% relative gain on BALROG for agentic control, +5% relative on VSI-Bench for spatial planning, and +2% on VisualWebBench for web navigation, without degrading image understanding accuracy. This work demonstrates that VLMs can acquire transferable real-world competence by training entirely in cost-effective synthetic worlds using a straightforward RL algorithm, providing a practical and scalable path for developing interactive multimodal agents. |
| EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.04295) or [HuggingFace](https://huggingface.co/papers/2508.04295))| Dong Chen, Jie Wang, Tingrui Yu, Chaofan Wang, YerbaPage | EVOC2RUST is a hybrid framework using Large Language Models (LLMs) and static analysis for automated, project-level C-to-Rust code translation. The objective is to create an automated system that translates entire C projects into semantically equivalent and memory-safe Rust code, addressing challenges of linguistic differences and cross-module dependencies. The methodology involves a three-stage pipeline: 1) It constructs a compilable Rust "skeleton" by decomposing the C project and translating definitions and function signatures using a feature-mapping-enhanced LLM. 2) It incrementally translates function bodies to populate the skeleton. 3) It uses a cascading, compilation-driven repair process integrating LLMs and rule-based static analysis to fix errors. On the industrial C2R-Bench dataset, EVOC2RUST achieved a 93.84% incremental compilation pass rate and a 97.41% code safety rate, significantly outperforming purely LLM-based and rule-based baselines. For AI practitioners, this research provides a blueprint for applying LLMs to large-scale, safety-critical code migration tasks by demonstrating that a structured, hybrid approach—combining skeleton-guided generation, LLMs augmented with expert-defined transformation rules, and iterative repair—is substantially more effective than unconstrained, end-to-end LLM generation. |
| DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a
  Stage-Wise Diffusion Transformer Framework (Read more on [arXiv](https://arxiv.org/abs/2508.02807) or [HuggingFace](https://huggingface.co/papers/2508.02807))| Chao Liang, Ente Lin, Shuliang Ning, Zaiyu Huang, Tongchun Zuo | DreamVVT is a two-stage Diffusion Transformer framework that generates realistic and temporally coherent video virtual try-on for in-the-wild scenarios. The research objective is to preserve fine-grained garment details and maintain temporal consistency in unconstrained videos, addressing the failures of existing end-to-end methods. The key methodology involves first synthesizing high-fidelity try-on images for select keyframes, and second, using a LoRA-adapted pretrained video generation model conditioned on these keyframes, pose data, and VLM-generated text to produce the final video. Quantitatively, the framework achieves a state-of-the-art VFID(I3D) score of 11.0180 on the ViViD dataset, outperforming prior works. The principal implication for AI practitioners is that a modular, two-stage approach using parameter-efficient LoRA fine-tuning on pretrained backbones can achieve superior generalization and fidelity for specialized video synthesis tasks, reducing reliance on large, paired training datasets. |
| A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.01197) or [HuggingFace](https://huggingface.co/papers/2508.01197))| Jianke Zhu, Junbo Chen, Zhan Shi, songw-zju | This paper introduces GroundingOcc, a coarse-to-fine multi-modal model, and the Talk2Occ benchmark for the novel task of 3D occupancy grounding from natural language. The primary objective is to move beyond bounding-box-based visual grounding by developing a method to predict fine-grained, voxel-level 3D occupancy for objects described in language, enabling more precise spatial perception. The proposed GroundingOcc model fuses features from images, LiDAR, and text, employing auxiliary tasks like 2D grounding and a depth predictor supervised by occupancy-rendered depth maps to enhance geometric understanding. On the new Talk2Occ benchmark, the refined model (GroundingOcc-Refine) achieves 32.68% accuracy at an IoU of 0.25, significantly outperforming the strongest baseline's 21.10%. For AI practitioners, this work provides a public benchmark and a validated approach for developing systems that require detailed, non-axis-aligned understanding of object shapes, which is critical for advanced robotic interaction and motion planning. |
| RL-PLUS: Countering Capability Boundary Collapse of LLMs in
  Reinforcement Learning with Hybrid-policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.00222) or [HuggingFace](https://huggingface.co/papers/2508.00222))| Kechi Zhang, Huanyu Liu, Yongding Tao, Xue Jiang, Yihong Dong | RL-PLUS is a novel hybrid-policy optimization approach that synergizes internal exploitation with external data to counter the capability boundary collapse in LLMs trained with Reinforcement Learning with Verifiable Reward (RLVR). The primary objective is to address how on-policy RLVR methods often narrow an LLM's problem-solving scope, preventing it from acquiring new reasoning abilities that surpass the base model's inherent boundaries. The key methodology involves two components: Multiple Importance Sampling (MIS) to resolve distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model toward novel, high-value reasoning paths. The paper reports that on six math reasoning benchmarks, RL-PLUS achieves state-of-the-art performance, outperforming a strong SFT+GRPO baseline by an average of 5.2 points. The principal implication for AI practitioners is that standard on-policy RL can be insufficient for expanding a model's core capabilities; the RL-PLUS framework offers a more effective method to integrate external knowledge, break through performance ceilings, and resolve the capability boundary collapse problem. |
| Reasoning Language Models for Root Cause Analysis in 5G Wireless
  Networks (Read more on [arXiv](https://arxiv.org/abs/2507.21974) or [HuggingFace](https://huggingface.co/papers/2507.21974))| Haozhe Zhang, Yibin Kang, Antonio De Domenico, Mohamed Sana, nicopi | This paper proposes a framework to fine-tune Large Language Models for Root Cause Analysis (RCA) in 5G networks, supported by a new synthetic dataset called TeleLogs. The objective is to improve the accuracy and reasoning quality of LLMs for network troubleshooting by integrating domain knowledge and generating structured, multi-step diagnostic explanations. The key methodology is a two-stage training process that combines Supervised Fine-Tuning (SFT) on high-quality data generated by a multi-agent pipeline, followed by Reinforcement Learning (RL) using Group Relative Policy Optimization (GRPO). The primary result shows that a fine-tuned Qwen2.5-32B model achieves 95.86% pass@1 accuracy, significantly outperforming state-of-the-art reasoning models like DeepSeek-R1 (29.42%) and its own base model (18.85%). The principal implication for AI practitioners is that a targeted, two-stage fine-tuning approach can enable LLMs to perform highly specialized, complex reasoning tasks with high accuracy and explainability, making them viable for practical deployment in critical domains like network operations. |
| IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with
  Verifiable Rewards (Read more on [arXiv](https://arxiv.org/abs/2508.04632) or [HuggingFace](https://huggingface.co/papers/2508.04632))| Ling-I Wu, Xiaogui Yang, Tong Jian, Tianyi Liang, Xu Guo | The paper introduces IFDecorator, a framework that wraps Reinforcement Learning with Verifiable Rewards (RLVR) to improve instruction following by automatically calibrating data difficulty and using dedicated modules to enforce intent and detect reward hacking.  The primary objective is to mitigate over-optimization (reward hacking) and improve training efficiency in RLVR for instruction following (RLVR4IF), where models exploit verification shortcuts instead of adhering to the user's actual intent.  The methodology combines three components: a cooperative-adversarial data flywheel that generates progressively challenging instruction-verification pairs for curriculum learning; an "IntentCheck" module that directly assesses intent alignment to provide a more robust reward; and "trip wires," diagnostic trap instructions used to measure reward hacking behaviors without influencing the training signal.  The framework significantly improves instruction following capabilities; the Qwen2.5-32B-Instruct-IFDecorator model achieves 87.43% accuracy on the IFEval benchmark, a +7.95 percentage point improvement over its baseline, while preserving the model's general capabilities.  For AI practitioners, IFDecorator provides a robust method to fine-tune LLMs for more reliable instruction adherence, directly counteracting the common failure mode of reward hacking in RL-based alignment and resulting in models that better fulfill user intent. |
| OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers
  for Biomedical NER Across 12 Public Datasets (Read more on [arXiv](https://arxiv.org/abs/2508.01630) or [HuggingFace](https://huggingface.co/papers/2508.01630))| MaziyarPanahi | This paper presents OpenMed NER, an open-source framework for biomedical named-entity recognition (NER) that achieves state-of-the-art performance with high computational efficiency. The primary objective is to create a suite of accessible, high-performing models that can surpass closed-source systems on a wide array of biomedical tasks. The methodology combines lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA) on strong transformer backbones like DeBERTa-v3, adapting them to a 350k-passage biomedical corpus before task-specific fine-tuning. The models establish new state-of-the-art micro-F1 scores on 10 of 12 public datasets, including a +9.72 percentage point improvement on the challenging CLL corpus. The principal implication for AI practitioners is that strategic, parameter-efficient adaptation of existing open-source models can yield superior performance to resource-intensive, from-scratch training, enabling the development of SOTA specialized models in under 12 hours on a single GPU. |
| SonicMaster: Towards Controllable All-in-One Music Restoration and
  Mastering (Read more on [arXiv](https://arxiv.org/abs/2508.03448) or [HuggingFace](https://huggingface.co/papers/2508.03448))| Ambuj Mehrish, Jan Melechovsky, dorienh | The paper introduces SonicMaster, a unified, text-controllable, flow-matching generative model for simultaneous music restoration and mastering. The research objective is to develop a single framework that corrects a broad spectrum of audio degradations—including equalization, dynamics, reverb, and clipping—guided by natural language prompts, replacing traditional multi-tool workflows. The methodology involves training a Multimodal Diffusion Transformer (MM-DiT) using a rectified flow paradigm on the novel *SonicMaster dataset*, which contains 175,000 pairs of programmatically degraded audio and corresponding text instructions. Results show the model significantly improves audio quality, reducing Kullback-Leibler (KL) divergence from 5.131 (degraded input) to 0.888 and increasing the Production Quality (PQ) score from 7.026 to 7.705 on a comprehensive test set. For AI practitioners, this work validates using a single, text-conditioned generative model to consolidate complex, multi-stage processing pipelines, providing a paradigm for creating unified solutions to multifaceted restoration tasks. |
| IAUNet: Instance-Aware U-Net (Read more on [arXiv](https://arxiv.org/abs/2508.01928) or [HuggingFace](https://huggingface.co/papers/2508.01928))| Dmytro Fishman, Ali Zeynalli, Illia Tsiporenko, YaroslavPrytula | This paper introduces IAUNet, a query-based U-Net architecture featuring a lightweight convolutional Pixel decoder and a multi-scale Transformer decoder for biomedical instance segmentation. The primary objective is to enhance the standard U-Net for complex instance segmentation tasks, such as identifying overlapping cells, by integrating instance-aware query refinement across multiple feature scales. The methodology combines a U-Net backbone with a custom Pixel decoder and a Transformer decoder that iteratively updates learnable object queries using multi-scale mask features, with deep supervision applied at each decoder stage. On the newly introduced Revvity-25 dataset, IAUNet with a ResNet-50 backbone achieves an Average Precision (AP) of 49.7, outperforming models like Mask2Former (46.4 AP) while using fewer parameters (39M vs. 44M). For AI practitioners, this work provides a blueprint for creating efficient and high-performing instance segmentation models by hybridizing the well-established U-Net convolutional framework with modern, lightweight query-based mechanisms, offering a resource-efficient alternative to larger, purely Transformer-based architectures. |
| Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.00428) or [HuggingFace](https://huggingface.co/papers/2508.00428))| Hao Huang, Shiqi Jiang, Haiwen Huang, Nan Xiang, tianyilt | Sel3DCraft is an interactive visual prompt engineering system that transforms unstructured text-to-3D (T23D) generation into a guided, user-friendly process. The research objective is to develop a visual approach that replaces the costly trial-and-error prompting common in T23D tools with structured exploration and iterative refinement. Its methodology features a dual-branch architecture for candidate synthesis (combining retrieval and generation), a multi-view hybrid scoring function leveraging Multi-modal Large Language Models (MLLMs) to assess eight semantic dimensions of 3D models, and a visual analytics suite with a treemap wordle for prompt recommendation. A user study demonstrated that Sel3DCraft reduces model creation time by 70.5% (118.83s vs 402.17s) and prompt iterations by 66.2% compared to baseline systems, while significantly improving output quality ratings. The principal implication for AI practitioners is the provision of a framework that integrates MLLMs as automated, multi-dimensional evaluators within a human-in-the-loop system to enhance the controllability and efficiency of complex generative models. |
| The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in
  Text-to-Image Models (Read more on [arXiv](https://arxiv.org/abs/2507.23313) or [HuggingFace](https://huggingface.co/papers/2507.23313))| Elisabetta Rocchetti, Alfio Ferrara, sergiopicascia | This research quantitatively analyzes how transformer-based text-to-image diffusion models internally represent and disentangle "content" and "style" concepts from artistic prompts. The main objective is to investigate how models interpret stylistic instructions and spatially separate the representation of *what* is depicted from *how* it is depicted without explicit supervision. The key methodology uses the Diffusion Attentive Attribution Maps (DAAM) technique to extract cross-attention heatmaps for content and style tokens, then computes the Intersection over Union (IoU) between their corresponding image regions to measure conceptual overlap. The primary result is that models demonstrate an emergent, but highly variable, content-style separation; the IoU for content-style token pairs was, on average, 0.64 standard deviations lower than a baseline IoU, but certain styles like 'Rembrandt' showed negative separation (Δ = -0.07), indicating entanglement. The principal implication for AI practitioners is that a model's capacity for content-style disentanglement is inconsistent and heavily influenced by training data biases, where frequent co-occurrences between subjects and artists can lead to conceptual blending and unpredictable generative behavior. |
| MiDashengLM: Efficient Audio Understanding with General Audio Captions (Read more on [arXiv](https://arxiv.org/abs/2508.03983) or [HuggingFace](https://huggingface.co/papers/2508.03983))| Yadong Niu, Jian Luan, Jizhong Liu, Gang Li, Heinrich Dinkel | MiDashengLM is an open-source large audio-language model that uses a novel "general audio captioning" approach for efficient and comprehensive audio understanding, outperforming baselines in speed and many non-ASR tasks. The paper's primary objective is to develop an efficient and transparent audio-language model that overcomes the limitations of ASR-centric pretraining by creating a holistic textual representation ("general captions") that fuses speech, sound, and music information from audio. The key methodology involves aligning a Dasheng audio encoder with a Qwen2.5-Omni language model using a newly created dataset, ACAVCaps, which contains "general audio captions" generated by a multi-expert annotation pipeline. The model architecture is optimized for efficiency with variable-length inputs and a low 5 Hz audio feature framerate. The primary result is a significant improvement in efficiency, with MiDashengLM achieving up to 20.2x higher inference throughput and 4x faster time-to-first-token than the Qwen2.5-Omni-7B baseline. The model's Dasheng encoder also outperforms the Whisper-Large v3 encoder on 18 out of 22 diverse audio tasks. The principal implication for AI practitioners is that MiDashengLM provides an open, highly efficient foundation model for applications needing broad audio understanding, demonstrating that pretraining on rich "general captions" is a powerful alternative to ASR-based alignment for developing versatile and fast audio-language systems. |
| Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and
  Self-Checking for Complex Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2508.03178) or [HuggingFace](https://huggingface.co/papers/2508.03178))| Liang Xu, Xiangzheng Zhang, Shousheng Jia, Liang Wen, Chenyang Wang | i) 1-line summary: The Light-IF framework improves LLM complex instruction-following by inducing a generalizable "preview and self-checking" reasoning pattern through a multi-stage training process. ii) Main research question or objective: The paper's primary objective is to mitigate the "lazy reasoning" pattern observed in LLMs when faced with complex instructions, aiming to instill a more rigorous and generalizable reasoning process that ensures strict constraint adherence. iii) Key methodology used: The framework uses a multi-stage pipeline: it first synthesizes hardness-aware prompts, then applies Zero-RL to a base model to elicit detailed reasoning, extracts high-quality responses for a cold-start dataset, and finally trains the model using Entropy-Preserving SFT (Entropy-SFT) and Token-wise Entropy-Adaptive RL (TEA-RL) with dense rewards. iv) Primary results (include at least one specific quantitative finding): The resulting Light-IF-32B model substantially outperforms other models on instruction-following benchmarks, achieving a score of 0.575 on SuperClue, which is 13.9 points higher than the next-best open-source model evaluated. v) Principal implication for AI practitioners: The key implication is that practitioners can instill complex, generalizable reasoning behaviors in LLMs using targeted RL and novel entropy control techniques (Entropy-SFT, TEA-RL), providing a practical and data-efficient method to enhance reliability for constraint-heavy tasks without relying on massive supervised datasets. |
