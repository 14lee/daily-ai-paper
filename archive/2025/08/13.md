

## Papers for 2025-08-13

| Title | Authors | Summary |
|-------|---------|---------|
| WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent (Read more on [arXiv](https://arxiv.org/abs/2508.05748) or [HuggingFace](https://huggingface.co/papers/2508.05748))| zhaoyd, callanwu, zhzhen23, richardxp888, Ornamentt | This paper introduces WebWatcher, a multimodal agent designed for deep research tasks that require integrated vision-language reasoning and multi-tool interaction. The research objective is to develop an agent that overcomes the text-centric limitations of prior work by effectively reasoning over and synthesizing information from both visual and textual sources using external tools. The key methodology involves a three-stage training process: first, automatically generating tool-use trajectories with GPT-4o; second, using these for a supervised fine-tuning (SFT) cold start; and third, refining the agent's policy via Group-Relative Policy Optimization (GRPO) reinforcement learning. The primary result shows that WebWatcher-32B achieves state-of-the-art performance, scoring 27.0% on the new challenging BrowseComp-VL benchmark, significantly outperforming proprietary RAG workflows (13.4%) and other open-source agents. For AI practitioners, the principal implication is that combining SFT on synthetic trajectories with subsequent RL refinement provides an effective framework for building agents that can execute complex, multi-step reasoning with tools, a necessary step for tackling real-world multimodal problems beyond simple retrieval-augmented generation. |
| Matrix-3D: Omnidirectional Explorable 3D World Generation (Read more on [arXiv](https://arxiv.org/abs/2508.08086) or [HuggingFace](https://huggingface.co/papers/2508.08086))| Yuqi Li, Wenhang Ge, Zhongqi Yang, kangfei, dearamy | Matrix-3D is a framework for generating omnidirectional, explorable 3D worlds from a single image or text prompt by leveraging a trajectory-guided panoramic video diffusion model and subsequent 3D reconstruction. The main objective is to overcome the limited field-of-view in existing 3D world generation methods by creating wide-coverage, geometrically consistent, and fully explorable scenes from minimal user input. The methodology involves a trajectory-guided panoramic video diffusion model, conditioned on scene mesh renders, to generate geometrically consistent videos; these are then lifted to a 3D world using either a rapid feed-forward reconstruction model or a high-fidelity optimization-based pipeline, all trained on the newly introduced Matrix-Pano dataset of 116K annotated panoramic videos. The framework achieves state-of-the-art results, with the optimization-based 3D reconstruction pipeline attaining a PSNR of 27.62, significantly outperforming the prior ODGS baseline's 22.04 PSNR, while the feed-forward variant reduces reconstruction time to just 10 seconds. For AI practitioners, Matrix-3D provides a validated framework and a large-scale annotated dataset to generate high-quality 3D virtual worlds for applications in embodied AI, simulation, and digital content creation, significantly lowering the barrier to producing explorable environments from simple prompts. |
| Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale
  Asynchronous RL (Read more on [arXiv](https://arxiv.org/abs/2508.07976) or [HuggingFace](https://huggingface.co/papers/2508.07976))| Chuyi He, Shusheng Xu, Minyang Xie, Wei Fu, Jiaxuan Gao | This paper introduces ASearcher, an open-source project enabling large-scale asynchronous RL training for long-horizon agentic search. It addresses key challenges in online RL for search agents: limited search turns and insufficient high-quality QA pairs. The methodology involves a fully asynchronous RL training system, which decouples trajectory execution from model updates to support extended turn limits (e.g., up to 128 turns/trajectory), and an LLM-based agent for autonomous generation of challenging QA datasets. Primary results demonstrate that ASearcher-Web-QwQ achieves substantial improvements, including 46.7% and 20.8% Avg@4 gains on xBench and GAIA respectively, and attains Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. This work provides a scalable training pipeline for AI practitioners to develop more capable LLM-based search agents that can perform complex, long-horizon tasks and handle real-world uncertainties. |
| CharacterShot: Controllable and Consistent 4D Character Animation (Read more on [arXiv](https://arxiv.org/abs/2508.07409) or [HuggingFace](https://huggingface.co/papers/2508.07409))| Fei Shen, Yanhong Zeng, Wenran Liu, LiJiaxing, Gaojunyao | CharacterShot is a novel framework for controllable and consistent 4D character animation from a single reference image and a 2D pose sequence. The main objective is to democratize 4D character animation, enabling individual designers to create dynamic 3D characters with precise motion control without specialized hardware. CharacterShot's methodology involves enhancing a DiT-based image-to-video model (CogVideoX) with pose conditions, extending it to multi-view generation via a dual-attention module and camera prior, and optimizing 4D representations using a novel neighbor-constrained 4D Gaussian Splatting (4DGS), supported by a new large-scale Character4D dataset. Extensive experiments on CharacterBench demonstrate CharacterShot's SOTA performance; for instance, it achieved an LPIPS of 0.025 for 4D generation, significantly outperforming STAG4D (0.082). This framework offers AI practitioners a robust and efficient solution for generating high-quality, spatio-temporally and spatio-view consistent 4D character animations, thereby lowering the barrier for 3D content creation in various applications. |
| Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.09138) or [HuggingFace](https://huggingface.co/papers/2508.09138))| Chenchen Jing, Bozhen Fang, Wen Wang, qiuyuu, tricktreat | This paper exploits temporal dynamics in diffusion Large Language Models (dLLMs) to address temporal oscillation, where correct intermediate predictions are overwritten. The primary objective is to overcome this phenomenon in dLLMs, where correct answers often appear during intermediate denoising steps but are subsequently discarded for later, incorrect iterations. The paper proposes two complementary methods: Temporal Self-Consistency Voting, a training-free test-time strategy aggregating predictions via weighted voting, and Temporal Consistency Reinforcement, a post-training method using negative Temporal Semantic Entropy (TSE) as a self-supervised reward signal within a reinforcement learning framework. Temporal Self-Consistency Voting achieved an average improvement of 1.5% over the LLaDA-8B-Instruct baseline, and Temporal Consistency Reinforcement yielded absolute gains of 2.0% on GSM8K and 25.3% on Countdown when combined with accuracy reward. AI practitioners developing or deploying dLLMs can significantly improve model accuracy and stability by incorporating intermediate predictions, either through the proposed training-free voting strategy or by fine-tuning models with a temporal consistency-based reward signal. |
| HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating
  Local and Web Searches (Read more on [arXiv](https://arxiv.org/abs/2508.08088) or [HuggingFace](https://huggingface.co/papers/2508.08088))| Qiang Ju, Jiehan Cheng, Yan Yu, Zhicheng Dou, zstanjj | HierSearch is a hierarchical agentic deep search framework integrating local and Web knowledge sources using hierarchical reinforcement learning and a knowledge refiner. The work addresses challenges in enterprise deep search systems that require selective use and cross-supplementation of knowledge from both local private corpuses and the public Web. It employs a hierarchical agentic architecture with low-level local and Web deep search agents, coordinated by a high-level planner agent. The framework is trained using hierarchical reinforcement learning (HRL) with Group Relative Policy Optimization (GRPO) and rule-based rewards, augmented by a reasoning-aware knowledge refiner to filter irrelevant or hallucinated evidence. Experiments across six benchmarks (general, finance, medical) demonstrate that HierSearch consistently outperforms flat reinforcement learning solutions and various baselines; for instance, achieving an F1-score of 62.83 on MuSiQue, significantly higher than the R1-Searcher parallel search baseline's 57.19. This approach offers a more data-efficient and stable method for developing robust deep search systems that can effectively integrate and reason over heterogeneous, noisy knowledge sources, crucial for real-world enterprise applications. |
| VertexRegen: Mesh Generation with Continuous Level of Detail (Read more on [arXiv](https://arxiv.org/abs/2508.09062) or [HuggingFace](https://huggingface.co/papers/2508.09062))| Jakob Engel, Chris Xie, Armen Avetisyan, Yawar Siddiqui, zx1239856 | VertexRegen introduces a novel generative framework for producing 3D triangle meshes with a continuous, controllable level of detail. The paper's objective is to enable "anytime" mesh generation, where the process can be halted at any step to yield a valid, complete mesh, unlike standard partial-to-complete methods. The key methodology reframes generation as the learned reversal of the edge collapse operation from progressive meshes, using a Transformer to autoregressively predict a sequence of vertex split operations that refine a coarse base mesh. Results demonstrate comparable quality to state-of-the-art models, achieving a superior Jensen-Shannon Divergence (JSD) of 2.89 in unconditional generation tasks. The principal implication for AI practitioners is the ability to dynamically control mesh complexity and generation time by simply stopping the generation process, which is highly valuable for real-time graphics, interactive content creation, and resource-constrained environments. |
| Test-Time Reinforcement Learning for GUI Grounding via Region
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2508.05615) or [HuggingFace](https://huggingface.co/papers/2508.05615))| Zhengxi Lu, Fei Tang, tricktreat, yanyc, DIONG1024 | This paper introduces GUI-RC, a test-time scaling method, and GUI-RCPO, a test-time reinforcement learning approach, to enhance GUI grounding accuracy without requiring additional labeled data. The main objective is to leverage test-time computation to improve GUI grounding performance, addressing the limitations of existing train-time optimization methods that rely heavily on extensive labeled data. The core methodology for GUI-RC involves constructing spatial voting grids from multiple sampled predictions to identify consensus regions, while GUI-RCPO transforms these consistency patterns into self-supervised reward signals for test-time policy optimization using Group Relative Policy Optimization (GRPO). GUI-RC consistently improves grounding accuracy by 2-3% on average, boosting Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, and GUI-RCPO further improves it to 85.14% on the same benchmark through self-supervised optimization. This approach reveals the untapped potential of test-time scaling and reinforcement learning for GUI grounding, offering a promising direction for AI practitioners to develop more robust and data-efficient autonomous GUI agents. |
| UNCAGE: Contrastive Attention Guidance for Masked Generative
  Transformers in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.05399) or [HuggingFace](https://huggingface.co/papers/2508.05399))| Kevin Galim, Minjae Lee, Byeongkeun Ahn, Wonjun Kang, JakeOh | UNCAGE introduces a training-free method to enhance compositional text-to-image generation in Masked Generative Transformers (MGTs). The primary objective is to address inaccurate attribute binding and improve text-image alignment in compositional T2I generation using MGTs. UNCAGE leverages attention maps to compute contrastive attention scores, which then guide the token unmasking order by prioritizing tokens that clearly represent individual objects. Quantitatively, UNCAGE (ours) achieved an average CLIP text-image similarity of 33.03, outperforming the Meissonic baseline (32.72), with a negligible inference overhead of 0.13% of total runtime. For AI practitioners, UNCAGE offers an efficient, training-free solution to improve compositional fidelity in existing MGT-based T2I systems without substantial computational cost or model retraining. |
| Aryabhata: An exam-focused language model for JEE Math (Read more on [arXiv](https://arxiv.org/abs/2508.08665) or [HuggingFace](https://huggingface.co/papers/2508.08665))| Sandeep Varma, Sachin Dharashivkar, RitvikPW | The paper presents Aryabhata 1.0, a compact 7B parameter open-source model specialized for mathematical reasoning on India's Joint Entrance Examination (JEE). The primary objective is to develop a model that achieves high accuracy and pedagogical value on domain-specific math problems while remaining computationally efficient. The methodology involves linearly merging three Qwen-based models, followed by supervised fine-tuning using curriculum learning on verified chain-of-thought traces, and reinforcement learning with verifiable rewards (RLVR) employing an A2C objective with adaptive exploration strategies. Aryabhata 1.0 achieves 90.2% accuracy on the JEE April session benchmark and 83.6% on the MATH 500 out-of-distribution benchmark, outperforming its base models. For AI practitioners, this research provides a blueprint for creating highly specialized, efficient, open-source models for niche domains by combining model merging with advanced fine-tuning and RL techniques, demonstrating a viable alternative to larger, general-purpose models for high-stakes applications. |
| Train Long, Think Short: Curriculum Learning for Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.08940) or [HuggingFace](https://huggingface.co/papers/2508.08940))| Marzyeh Ghassemi, Elie Bou-Zeid, Abed Hammoud, Kumail Alhamoud, Hasan Abed Al Kader Hammoud | This paper introduces a curriculum learning framework using Group Relative Policy Optimization (GRPO) to train large language models for efficient, length-controlled reasoning. The main objective is to determine if a curriculum learning strategy, where token budgets gradually tighten during training, can enhance LLM reasoning capabilities and efficiency compared to fixed-budget approaches. The methodology involves fine-tuning QWEN-2.5-7B with GRPO, incorporating a reward function balancing task correctness, length efficiency (via a triangular reward and an exponentially decaying token budget), and formatting adherence through structural tags. Experiments show that curriculum learning consistently outperforms fixed-budget training; for instance, on GSM8K, accuracy improved from 82.71% to 86.20% with nearly identical token usage. AI practitioners can leverage curriculum-driven compression as a powerful inductive bias to train efficient reasoning models, enabling significant computational cost savings without runtime user hints or prompt overhead. |
| Towards Affordance-Aware Robotic Dexterous Grasping with Human-like
  Priors (Read more on [arXiv](https://arxiv.org/abs/2508.08896) or [HuggingFace](https://huggingface.co/papers/2508.08896))| Haoran Xu, Cheng Zeng, Xingyue Zhao, Linghao Zhuang, Haoyu Zhao | This paper introduces AffordDex, a two-stage framework for learning a universal dexterous grasping policy that is both human-like and functionally aware of object affordances. The objective is to develop a grasping policy that moves beyond simple stability metrics to incorporate human-like kinematics and an understanding of functionally inappropriate contact regions (negative affordances). The methodology first pre-trains a base policy on a human motion dataset, then fine-tunes it with a residual module using reinforcement learning, guided by a Negative Affordance-aware Segmentation (NAA) module that identifies unsafe regions. On the UniDexGrasp dataset (state-based, seen objects), AffordDex achieved an 89.2% success rate and an Affordance Score of 4, outperforming the UniDexGrasp++ baseline's 87.9% success rate and Affordance Score of 28. For AI practitioners, the principal implication is that explicitly modeling and penalizing negative affordances, in tandem with learned human motion priors, is a powerful technique for developing robotic manipulation policies that are not only successful but also functionally correct and safe. |
| DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech
  Recognition (Read more on [arXiv](https://arxiv.org/abs/2508.08938) or [HuggingFace](https://huggingface.co/papers/2508.08938))| Lukáš Burget, Bolaji Yusuf, Karel Beneš, Santosh Kesiraju, Alexander Polok | This paper presents DeCRED, a regularization method that adds auxiliary classifiers to intermediate decoder layers in encoder-decoder ASR models to improve robustness and generalization. The research objective is to enhance the performance of these models, particularly in out-of-domain settings, by regularizing the decoder's induced internal language model (ILM). The key methodology involves attaching linear classifiers to intermediate decoder layers and training them with the same cross-entropy loss as the final layer, thereby enforcing supervision deeper within the network. This approach reduces the macro Word Error Rate (WER) on a set of four out-of-domain datasets from 18.2% to 16.2% relative to the baseline model. For AI practitioners, DeCRED offers an efficient technique to improve ASR model robustness and generalization with negligible computational overhead during training and no additional cost at inference time, as the auxiliary layers are discarded after training. |
| Cut2Next: Generating Next Shot via In-Context Tuning (Read more on [arXiv](https://arxiv.org/abs/2508.08244) or [HuggingFace](https://huggingface.co/papers/2508.08244))| Yu Qiao, Ziqi Huang, Jiajun Li, Hongbo Liu, Jingwen He | Cut2Next introduces Next Shot Generation (NSG) to synthesize cinematographically coherent subsequent video shots adhering to professional editing patterns. The primary objective is to generate highly coherent subsequent shots that maintain character and environmental consistency while adhering to established cinematic continuity principles and specific editing patterns. The framework leverages a Diffusion Transformer (DiT) (FLUX.1-dev) with in-context tuning, guided by a Hierarchical Multi-Prompting strategy (Relational and Individual Prompts), and incorporates architectural innovations like Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), trained on RawCuts and CuratedCuts datasets. Quantitatively, Cut2Next achieves superior performance on CutBench, with a DINO Similarity of 0.4952 and a Fréchet Inception Distance (FID) of 59.37, significantly outperforming the IC-LoRA-Cond baseline. This advancement enables AI practitioners to generate high-quality, narratively expressive, and cinematically coherent video sequences, providing a robust solution for automated content creation that meets professional video editing standards. |
| Democratizing Diplomacy: A Harness for Evaluating Any Large Language
  Model on Full-Press Diplomacy (Read more on [arXiv](https://arxiv.org/abs/2508.07485) or [HuggingFace](https://huggingface.co/papers/2508.07485))| Elizabeth Karpinski, Ishana Shastri, Samuel J Paech, tmarques, Alex-GSL | This paper presents a harness for evaluating the strategic reasoning of any off-the-shelf Large Language Model in the game of full-press Diplomacy without fine-tuning. The main objective is to create a standardized and accessible framework to measure emergent strategic, negotiation, and deceptive capabilities in LLMs within a complex, multi-agent environment, thereby democratizing this area of research. The methodology involves an optimized textual game state representation, a scalar "Game Score" for performance measurement, and a "Critical State Analysis" (CSA) protocol for efficiently replaying key game moments to test hypotheses on model behavior. Primary results show that performance scales with model size (game score correlates with Chatbot Arena Elo at r=+0.651) and that behavior is highly sensitive to prompt engineering; for example, aggressive prompting reduced Mistral-Small's rate of passive "hold" orders from 58.9% to 24.1%. The principal implication for AI practitioners is that complex strategic behaviors can be elicited from general-purpose LLMs via context engineering alone, providing a framework to benchmark these capabilities while also revealing that models are vulnerable to manipulation and deception from other AI agents. |
| Adversarial Video Promotion Against Text-to-Video Retrieval (Read more on [arXiv](https://arxiv.org/abs/2508.06964) or [HuggingFace](https://huggingface.co/papers/2508.06964))| Shuai Liu, Qian Li, Zhengyu Zhao, Chenhao Lin, michaeltqw108 | This paper introduces ViPro, the first adversarial attack designed to promote a video's rank for multiple target queries within text-to-video retrieval (T2VR) systems. The research objective is to design and evaluate a novel attack paradigm that, unlike existing methods which suppress video ranks, adversarially promotes a target video's ranking for multiple, semantically relevant text queries simultaneously. The proposed method, Video Promotion (ViPro), optimizes perturbations to push a video into the overlapping retrieval boundaries of target queries using an exponential loss function. For enhanced black-box transferability, a Modal Refinement (MoRe) module is introduced, which performs temporal clipping of frames and applies semantical weighting based on frame-to-query similarity to guide the optimization. ViPro demonstrates superior performance over adapted baselines in white-box, grey-box, and black-box settings; on average, it surpasses baselines by over 30%, 10%, and 4% respectively across these scenarios. The principal implication for AI practitioners is that T2VR systems are vulnerable to adversarial promotion attacks, which can manipulate content visibility for malicious purposes, highlighting a critical threat vector beyond traditional suppression attacks and indicating that simply obscuring model components is an insufficient defense. |
| OpenCUA: Open Foundations for Computer-Use Agents (Read more on [arXiv](https://arxiv.org/abs/2508.09123) or [HuggingFace](https://huggingface.co/papers/2508.09123))| Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, xywang626 | The paper presents OPENCUA, an open-source framework for building and evaluating computer-use agents (CUAs) by providing an annotation tool, a large-scale dataset (AGENTNET), and a novel training methodology.  The main objective is to establish open foundations for CUA research by creating a scalable framework for data collection and model training, enabling the community to study agent capabilities, limitations, and risks.  The key methodology involves: (1) capturing human demonstrations using the AGENTNET TOOL; (2) processing raw data into compact state-action pairs; and (3) synthesizing reflective long Chain-of-Thought (CoT) reasoning to augment the training data, which explicitly injects planning, memory, and reflection into the agent's learning process.  The primary result is that the OPENCUA-32B model achieves a 34.8% success rate on the OSWorld-Verified benchmark (100-step budget), establishing a new state-of-the-art for open-source models and surpassing the proprietary OpenAI CUA (31.4%).  The principal implication for AI practitioners is that augmenting state-action demonstration data with synthesized, reflective long Chain-of-Thought reasoning is a critical factor for improving CUA performance and scalability, as simply increasing raw demonstration data yields minimal gains. |
| AutoCodeBench: Large Language Models are Automatic Code Benchmark
  Generators (Read more on [arXiv](https://arxiv.org/abs/2508.09101) or [HuggingFace](https://huggingface.co/papers/2508.09101))| Tao Zhang, Zhiying Zeng, Yuchi Deng, Ao Liu, Jason Chou | The paper introduces AutoCodeGen, a fully automated workflow using LLMs and a multilingual sandbox to generate AutoCodeBench, a large-scale, high-difficulty, multilingual code generation benchmark. The objective is to overcome the limitations of existing code benchmarks, such as reliance on manual annotation and a narrow focus on Python, by creating a more challenging and diverse evaluation standard. The AutoCodeGen methodology involves LLMs generating code solutions and test inputs, executing them in a sandbox to obtain outputs, reverse-generating problem descriptions, and applying a three-stage filtering process using difficulty control, an LLM-as-Critic, and diversity sampling. The resulting AutoCodeBench contains 3,920 problems across 20 languages, on which the top-performing model, Claude Opus 4 (Think), achieved a Pass@1 of only 52.4%, while manual verification confirmed an 87.6% accuracy rate for the benchmark data itself. The principal implication for AI practitioners is the availability of a scalable framework and a challenging, validated benchmark for assessing the practical multilingual and multi-logic reasoning capabilities of code generation models, revealing significant performance drops on complex, multi-component tasks. |
| Feedback-Driven Tool-Use Improvements in Large Language Models via
  Automated Build Environments (Read more on [arXiv](https://arxiv.org/abs/2508.08791) or [HuggingFace](https://huggingface.co/papers/2508.08791))| Xuesong Yao, Yufei Xu, Zhengyin Du, Changhao Jiang, Junjie-Ye | This paper introduces a feedback-driven framework to enhance large language model tool-use capabilities via automated build environments. The objective is to address limitations in current reinforcement learning frameworks for LLM tool-use, particularly regarding stable training environments and verifiable reward signals. The methodology involves a five-stage automated environment construction pipeline and a verifiable reward mechanism that assesses both tool precision and task completeness. Experiments demonstrate that this approach significantly improves LLM tool-use performance, yielding over a 10% average performance gain on open-source LLMs across multiple benchmarks, with Qwen2.5-7B's Solve-F1 improving from 25.97 to 40.36 on the Ours dataset. This provides AI practitioners with a scalable, stable, and verifiable framework for training LLMs to robustly generalize tool-use abilities by enhancing lower-layer MLP parameters. |
| Bridging Theory and Practice in Quantum Game Theory: Optimized
  Implementation of the Battle of the Sexes with Error Mitigation on NISQ
  Hardware (Read more on [arXiv](https://arxiv.org/abs/2508.09050) or [HuggingFace](https://huggingface.co/papers/2508.09050))| Jhon Alejandro Andrade, Mateo Buenaventura Samboni, Carlos Andres Duran Paredes, Germán Díaz Agreda, sebasmos | This paper reports the experimental realization of the quantum "Battle of the Sexes" game on an IBM NISQ processor to validate if its theoretical strategic advantages persist on noisy hardware. The researchers implemented four quantum strategies under the Eisert-Wilkens-Lewenstein framework and introduced a Guided Circuit Mapping (GCM) method for dynamic, noise-aware qubit allocation to mitigate hardware errors across 62 qubits. The GCM-optimized execution successfully preserved the theoretical payoff trends, with experimental results deviating from the analytical model by a relative error between 3.5% and 12.1%. For AI practitioners, this demonstrates that quantum-enhanced coordination in multi-agent systems is achievable on near-term hardware, as lightweight error mitigation techniques can maintain a quantifiable quantum advantage in strategic decision-making scenarios. |
| BiasGym: Fantastic Biases and How to Find (and Remove) Them (Read more on [arXiv](https://arxiv.org/abs/2508.08855) or [HuggingFace](https://huggingface.co/papers/2508.08855))| Arnav Arora, Haeun Yu, Siddhesh Milind Pawar, Nadav Borenstein, sekhcopenlu | This paper introduces BiasGym, a framework for injecting, analyzing, and mitigating conceptual biases in LLMs by fine-tuning a special token and then steering the attention heads associated with it. The primary objective is to develop a cost-effective and targeted framework for reliably surfacing, analyzing, and removing specific conceptual biases from LLM weights without degrading general downstream task performance. The methodology consists of two components: 1) `BiasInject`, which introduces a bias by fine-tuning only the embedding of a new, special token (`BiasToken`) while keeping model weights frozen; and 2) `BiasScope`, which identifies the attention heads most associated with the `BiasToken` via head attribution and mitigates the bias by nullifying the output of these heads. The proposed method effectively reduces stereotypes across multiple models; for Llama3.2-3B, the "Injection w/ steering" approach reduced the average stereotype strength score from 1.16 (original model) to 0.40. This mitigation resulted in minimal impact on general capabilities, with an average MMLU performance degradation of only 0.03. The principal implication for AI practitioners is that BiasGym provides a practical, low-cost technique for targeted debiasing of open-weight LLMs, allowing engineers to surgically remove specific unwanted associations from a model's internal mechanisms as a precise alternative to broad safety fine-tuning. |
| WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface
  Temperature Estimation via Spatio-Temporal Fusion (Read more on [arXiv](https://arxiv.org/abs/2508.06485) or [HuggingFace](https://huggingface.co/papers/2508.06485))| Rachid Nedjai, Raphael Canals, Adel Hafiane, sofianebouaziz | WGAST is a weakly-supervised conditional generative adversarial network that performs spatio-temporal fusion of multi-source satellite data to estimate daily 10 m resolution Land Surface Temperature (LST). The main objective is to generate these high-resolution, high-frequency LST maps by fusing coarse-resolution daily MODIS data with higher-resolution spectral data from Landsat 8 and Sentinel-2, overcoming the inherent spatio-temporal trade-off in satellite imagery. The methodology employs a cGAN with a four-stage generator and a PatchGAN discriminator, trained via a weakly-supervised strategy that uses 30 m Landsat LST as a proxy ground truth by spatially averaging the 10 m generated output for loss calculation. WGAST quantitatively outperformed baseline models, achieving an average Root Mean Square Error (RMSE) reduction of 17.18% against the best-performing baseline, FuseTen. For AI practitioners, the key implication is the use of a physically-motivated weak supervision technique where a high-resolution model is trained against a lower-resolution proxy label via aggregation, a method applicable to other multi-resolution data fusion problems where high-resolution ground truth is unavailable. |
| GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via
  General Samples Replay (Read more on [arXiv](https://arxiv.org/abs/2508.04676) or [HuggingFace](https://huggingface.co/papers/2508.04676))| Yang Fan, Yuefeng Li, Mengchen Zhao, Shuoran Jiang, Yunan Zhang | The paper presents GeRe, a framework for efficient anti-forgetting in large language model (LLM) continual learning. Its objective is to simultaneously retain general LLM capabilities and improve performance on previously learned tasks across sequential tasks, specifically addressing if fixed general replay samples suffice and if task-specific replay is necessary. GeRe utilizes a fixed set of pre-collected general pretraining texts as replay samples and introduces a threshold-based margin (TM) loss, derived from distilled last-layer hidden state activation thresholds, to maintain consistent neuron activation states. Experimental results on 15 downstream tasks with Llama-3.1-8B demonstrate that GeRe with dynamic TM loss (full-parameter) achieves an F1 Average performance of 66.9386, significantly outperforming a non-replay baseline (37.8919 F1 Avg). This approach simplifies continual learning by showing that a small, fixed set of general replay samples is sufficient for anti-forgetting and performance enhancement, reducing the laborious collection of task-specific replay samples for AI practitioners. |
| NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech
  Modeling with Paralinguistic Vocalizations (Read more on [arXiv](https://arxiv.org/abs/2508.04195) or [HuggingFace](https://huggingface.co/papers/2508.04195))| Haoyue Zhan, Yiheng Lu, Yuancheng Wang, Qinke Ni, Huan Liao | i) NVSpeech introduces an integrated and scalable pipeline for modeling human-like speech with paralinguistic vocalizations. ii) The primary objective is to enable automatic speech recognition (ASR) and text-to-speech (TTS) systems to jointly process both lexical content and fine-grained, word-level non-verbal cues like laughter or interjections. iii) The key methodology involves creating a manually annotated dataset with 18 paralinguistic categories, training a paralinguistic-aware ASR model to auto-label a larger 573-hour corpus, and finetuning zero-shot TTS models on the resulting data. iv) The paralinguistic-aware ASR model achieved an F1-score of 0.85 on open-domain event detection, and TTS models enhanced with the NVSpeech data were preferred by human listeners with a win rate of 78.7% over baseline models. v) For AI practitioners, this research provides a public, large-scale, word-level annotated dataset and a unified pipeline to develop more expressive ASR and TTS systems with explicit, token-level control over non-verbal vocalizations. |
