

## Papers for 2025-08-13

| Title | Authors | Summary |
|-------|---------|---------|
| WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent (Read more on [arXiv](https://arxiv.org/abs/2508.05748) or [HuggingFace](https://huggingface.co/papers/2508.05748))| zhaoyd, callanwu, zhzhen23, richardxp888, Ornamentt | The paper introduces WebWatcher, a vision-language agent that performs multi-step, tool-augmented reasoning to solve complex multimodal information-seeking tasks. The research objective is to address the limitations of text-centric research agents by developing a multimodal agent capable of deep reasoning across both visual and textual information through sophisticated multi-tool integration. The methodology involves a two-stage training process: first, a supervised fine-tuning (SFT) "cold start" on high-quality, automatically generated ReAct-style trajectories, followed by reinforcement learning using Group-Relative Policy Optimization (GRPO) to refine the agent's decision-making with tools like web search, page visiting, and a code interpreter. WebWatcher-32B significantly outperforms baselines, achieving a 27.0% Pass@1 score on the newly proposed BrowseComp-VL benchmark, more than doubling the 13.4% score of a GPT-4o with a RAG workflow. The principal implication for AI practitioners is that combining automated high-quality trajectory synthesis for an SFT cold start with subsequent RL optimization provides a robust and scalable framework for building powerful, multi-tool multimodal agents capable of tackling complex, real-world research problems. |
| Matrix-3D: Omnidirectional Explorable 3D World Generation (Read more on [arXiv](https://arxiv.org/abs/2508.08086) or [HuggingFace](https://huggingface.co/papers/2508.08086))| Yuqi Li, Wenhang Ge, Zhongqi Yang, kangfei, dearamy | Matrix-3D is a framework for generating omnidirectional, explorable 3D worlds from a single image or text prompt using panoramic representations. The primary objective is to overcome the limited field-of-view of existing 3D generation techniques by creating wide-coverage, geometrically consistent scenes. Its key methodology involves a trajectory-guided panoramic video diffusion model, conditioned on scene mesh renders, followed by two proposed 3D reconstruction pipelines: an optimization-based method for high-fidelity and a feed-forward model for rapid generation. The framework demonstrates superior performance, with its optimization-based 3D reconstruction achieving a PSNR of 27.62, significantly outperforming the ODGS baseline's 22.04. For AI practitioners, this work provides a robust method and a new large-scale synthetic dataset (Matrix-Pano) to generate high-quality 3D environments for training and testing embodied AI agents, particularly in applications requiring wide-area navigation. |
| Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale
  Asynchronous RL (Read more on [arXiv](https://arxiv.org/abs/2508.07976) or [HuggingFace](https://huggingface.co/papers/2508.07976))| Chuyi He, Shusheng Xu, Minyang Xie, Wei Fu, Jiaxuan Gao | ASearcher is an open-source project that introduces a large-scale RL training pipeline for long-horizon search agents. The main objective is to overcome limitations in existing open-source LLM agents, such as insufficient search turns and lack of high-quality QA data, to unlock expert-level Search Intelligence. The key methodology involves a scalable, fully asynchronous RL training system and an LLM-based agent for autonomously synthesizing challenging QA pairs. As a primary result, the prompt-based ASearcher-Web-QwQ agent achieved substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA respectively, demonstrating long-horizon search capabilities with tool calls exceeding 40 turns during training. This work provides AI practitioners with a robust, open-source solution for training advanced search agents and offers insights for developing more capable LLM agents for complex real-world applications. |
| CharacterShot: Controllable and Consistent 4D Character Animation (Read more on [arXiv](https://arxiv.org/abs/2508.07409) or [HuggingFace](https://huggingface.co/papers/2508.07409))| Fei Shen, Yanhong Zeng, Wenran Liu, LiJiaxing, Gaojunyao | CharacterShot is a novel framework for controllable and consistent 4D character animation from a single image and 2D pose sequence. The main objective is to enable individual designers to create dynamic 3D characters (4D character animation) with precise motion control and arbitrary viewpoint rendering, achieving spatial-temporal and spatial-view consistency. CharacterShot extends a DiT-based image-to-video model by integrating pose conditions and a dual-attention module with camera priors for multi-view video generation, then employs a novel neighbor-constrained 4D Gaussian Splatting (4DGS) optimization framework to synthesize continuous and stable 4D representations, leveraging a newly constructed large-scale Character4D dataset. Extensive experiments on the CharacterBench benchmark demonstrate CharacterShot's superior performance, achieving an FV4D score of 490.457 for multi-view video synthesis (compared to SV3D's 2078.984) and an FV4D score of 406.624 for 4D generation (compared to STAG4D's 970.241), indicating improved consistency and quality. CharacterShot democratizes the 4D character animation pipeline, offering a low-cost, minute-scale solution on consumer-grade GPUs, which significantly reduces the manual effort and specialized equipment traditionally required for CGI, enabling broader accessibility for creators and researchers in AI-driven content generation. |
| Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.09138) or [HuggingFace](https://huggingface.co/papers/2508.09138))| Chenchen Jing, Bozhen Fang, Wen Wang, qiuyuu, tricktreat | This research exploits the temporal dynamics of diffusion language models (dLLMs) to correct for "temporal oscillation," a phenomenon where correct intermediate predictions are lost in the final output. The paper's main objective is to improve dLLM reasoning accuracy by introducing two methods that leverage the entire denoising trajectory: a training-free "Temporal Self-Consistency Voting" strategy that aggregates predictions across all steps, and a post-training "Temporal Consistency Reinforcement" which uses a novel Temporal Semantic Entropy (TSE) metric as a reward signal to promote stable outputs. The methods proved highly effective, with the reinforcement learning approach achieving absolute accuracy gains of 2.0% on GSM8K and 25.3% on the Countdown dataset when combined with an accuracy-based reward. For AI practitioners, the principal implication is that intermediate denoising steps in dLLMs are a valuable feature, not noise, and can be harnessed with these low-overhead techniques to significantly boost model performance on reasoning tasks. |
| HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating
  Local and Web Searches (Read more on [arXiv](https://arxiv.org/abs/2508.08088) or [HuggingFace](https://huggingface.co/papers/2508.08088))| Qiang Ju, Jiehan Cheng, Yan Yu, Zhicheng Dou, zstanjj | HierSearch is a hierarchical agentic framework that integrates local and web search for enterprise deep search tasks by using hierarchical reinforcement learning (HRL). The research objective is to develop an efficient method for an agent to leverage both private and public knowledge sources, addressing the poor training efficiency and tool mastery issues of standard flat RL approaches in multi-tool environments. The methodology employs a two-level hierarchy with specialized low-level agents for local and web search that are coordinated by a high-level planner, complemented by a knowledge refiner to filter irrelevant evidence and hallucinations from the low-level outputs. Hier |
| VertexRegen: Mesh Generation with Continuous Level of Detail (Read more on [arXiv](https://arxiv.org/abs/2508.09062) or [HuggingFace](https://huggingface.co/papers/2508.09062))| Jakob Engel, Chris Xie, Armen Avetisyan, Yawar Siddiqui, zx1239856 | VertexRegen is a novel autoregressive framework that generates 3D meshes with continuous levels of detail by learning a sequence of vertex split operations. The objective is to create a direct mesh generation model that enables "anytime generation," where the process can be halted at any step to yield a valid, complete mesh of a certain detail level, unlike existing partial-to-complete methods. The methodology reformulates mesh generation as the reversal of the edge collapse operation from progressive meshes, using a Transformer to first generate a coarse base mesh and then autoregressively predict a sequence of tokenized vertex split operations that progressively refine the mesh. When generation is constrained to 400 faces, VertexRegen achieves a Jensen-Shannon Divergence (JSD) of 2.88, significantly outperforming a fine-tuned MeshXL which scored 5.19. For AI practitioners, this provides a framework to dynamically control the trade-off between generation time and mesh complexity during inference to suit application-specific requirements like progressive loading, without needing to retrain the model. |
| Test-Time Reinforcement Learning for GUI Grounding via Region
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2508.05615) or [HuggingFace](https://huggingface.co/papers/2508.05615))| Zhengxi Lu, Fei Tang, tricktreat, yanyc, DIONG1024 | The paper proposes GUI-RC and GUI-RCPO, test-time methods for enhancing GUI grounding accuracy by leveraging spatial consistency across multiple model predictions. It addresses the challenge of improving GUI grounding performance during inference without requiring additional labeled data. GUI-RC aggregates sampled predictions via spatial voting to identify high-confidence consensus regions, while GUI-RCPO converts these consistency signals into self-supervised rewards for test-time reinforcement learning via policy optimization. GUI-RC consistently improves grounding accuracy by 2-3% on ScreenSpot benchmarks, and GUI-RCPO further boosts Qwen2.5-VL-3B-Instruct from 80.11% to 85.14% on ScreenSpot-v2. These methods reveal the potential of test-time scaling and reinforcement learning for developing more robust and data-efficient GUI agents capable of self-bootstrapping improvement without external supervision. |
| Aryabhata: An exam-focused language model for JEE Math (Read more on [arXiv](https://arxiv.org/abs/2508.08665) or [HuggingFace](https://huggingface.co/papers/2508.08665))| Sandeep Varma, Sachin Dharashivkar, RitvikPW | Aryabhata 1.0 is a compact 7B parameter math reasoning model optimized for the Indian Joint Entrance Examination (JEE). The model aims to provide an accurate, transparent, and efficient LLM for educational math reasoning. Its methodology involves linear merging of Qwen, Ace, and DeepSeek models, followed by supervised fine-tuning with curriculum learning on best-of-n rejection-sampled CoT traces, and reinforcement learning using an A2C objective with adaptive group sizing and progressive temperature scaling. Aryabhata 1.0 achieved 90.2% accuracy on the JEE Main 2025 April session, outperforming base models like Qwen2.5-Math-7B-Instruct (66.0% on MATH 500) while maintaining ~2K tokens per response. This open-source model offers a foundational step for developing efficient, domain-aligned AI tools for educational applications in mathematical reasoning. |
| UNCAGE: Contrastive Attention Guidance for Masked Generative
  Transformers in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.05399) or [HuggingFace](https://huggingface.co/papers/2508.05399))| Kevin Galim, Minjae Lee, Byeongkeun Ahn, Wonjun Kang, JakeOh | This paper introduces UNCAGE, a training-free, contrastive attention guidance method that improves compositional fidelity in Masked Generative Transformers (MGTs) by optimizing the token unmasking order.  The primary objective is to mitigate issues of object mixture and attribute leakage in MGT-based text-to-image models, a problem previously studied in Diffusion Models but not MGTs.  UNCAGE operates at inference time by calculating a contrastive attention score from the model's internal cross-attention maps, which prioritizes unmasking image tokens that strongly correspond to a specific object-attribute pair while being distinct from other objects in the prompt.  The method demonstrates consistent performance gains with negligible computational cost; on a GPT-based evaluation, UNCAGE improved the average score from a baseline of 6.99 to 7.34, while increasing total inference time by only 0.13%.  The principal implication for AI practitioners is that UNCAGE offers a lightweight, plug-and-play technique to enhance the compositional accuracy of pretrained MGTs, allowing for more faithful text-to-image generation without requiring model finetuning or incurring significant inference overhead. |
| Train Long, Think Short: Curriculum Learning for Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.08940) or [HuggingFace](https://huggingface.co/papers/2508.08940))| Marzyeh Ghassemi, Elie Bou-Zeid, Abed Hammoud, Kumail Alhamoud, Hasan Abed Al Kader Hammoud | This paper proposes a curriculum learning strategy that trains large language models for efficient reasoning by progressively reducing the token budget during training using Group Relative Policy Optimization (GRPO). The primary objective is to determine if a dynamic, decaying token budget can produce more accurate and efficient reasoning models than training with a constant, restrictive budget. The key methodology involves fine-tuning a model with GRPO using a composite reward function that balances task correctness, adherence to formatting, and a token budget that decays exponentially or linearly from a generous initial value to a tight final value. The primary result shows that on the GSM8K benchmark, the curriculum-trained model achieved 86.20% accuracy, outperforming the 82.71% of a fixed-budget model, while using a similar final token count. The principal implication for AI practitioners is that adopting a "train long, think short" curriculum for token budgets during RL fine-tuning can produce models with higher reasoning accuracy for a given inference budget, with the decay schedule (e.g., linear vs. exponential) serving as a key lever to optimize the trade-off between final accuracy and computational efficiency. |
| Towards Affordance-Aware Robotic Dexterous Grasping with Human-like
  Priors (Read more on [arXiv](https://arxiv.org/abs/2508.08896) or [HuggingFace](https://huggingface.co/papers/2508.08896))| Haoran Xu, Cheng Zeng, Xingyue Zhao, Linghao Zhuang, Haoyu Zhao | AffordDex is a novel framework for affordance-aware robotic dexterous grasping incorporating human-like priors through a two-stage training paradigm. The research objective is to learn a universal grasping policy with an inherent understanding of both motion priors and object affordances, suitable for general-purpose embodied AI. Key methodology involves pre-training a base policy on human hand motions via imitation learning, followed by reinforcement learning refinement using a residual module guided by a Negative Affordance-aware Segmentation (NAA) module and teacher-student distillation. AffordDex achieved an 89.2% success rate, 8.6 Human-likeness Score (HLS), and 4 Affordance Score (AS) on seen objects in the state-based setting, outperforming state-of-the-art baselines. The principal implication for AI practitioners is the generation of grasps that are not only successful but also remarkably human-like and functionally correct, making them highly effective for direct application in downstream robotic manipulation tasks. |
| Democratizing Diplomacy: A Harness for Evaluating Any Large Language
  Model on Full-Press Diplomacy (Read more on [arXiv](https://arxiv.org/abs/2508.07485) or [HuggingFace](https://huggingface.co/papers/2508.07485))| Elizabeth Karpinski, Ishana Shastri, Samuel J Paech, tmarques, Alex-GSL | This paper introduces the first evaluation harness for out-of-the-box Large Language Models (LLMs) to play full-press Diplomacy without specialized training. The main objective is to democratize the evaluation of LLM strategic reasoning by enabling any LLM to play complex multi-agent Diplomacy games, providing insights into emergent strategic capabilities. The methodology involves a multi-stage transformation of raw game data into contextually-enriched text representations, combined with prompt engineering and a Critical State Analysis framework to optimize model decision-making and iterate on key moments. Experiments across contemporary LLMs showed performance scaling with model size, with larger models achieving higher game scores, and a Pearson correlation of +0.651 with Chatbot Arena Elo scores; prompt optimizations reduced Mistral-Small's hold rate from 58.9% to 24.1%, and persuasion studies found support and offensive promises broken most frequently (60-78% betrayal rates). This work suggests strategic reasoning emerges naturally in general-purpose LLMs without fine-tuning, but it also reveals LLM vulnerabilities to deceptive communication, underscoring the need for robust instruction-following mechanisms in multi-agent AI systems. |
| DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech
  Recognition (Read more on [arXiv](https://arxiv.org/abs/2508.08938) or [HuggingFace](https://huggingface.co/papers/2508.08938))| Lukáš Burget, Bolaji Yusuf, Karel Beneš, Santosh Kesiraju, Alexander Polok | DeCRED is a regularization method for encoder-decoder ASR that adds auxiliary classifiers to intermediate decoder layers to improve generalization. The research objective is to improve the robustness and out-of-domain generalization of encoder-decoder ASR models by regularizing the internal language model induced by the decoder. The key methodology involves attaching auxiliary linear classifiers to intermediate decoder layers and training them with the same cross-entropy objective as the final layer, creating a composite loss function. The proposed DeCRED method reduces out-of-domain macro Word Error Rate (WER) from 18.2% to 16.2% across four unseen test sets and lowers the mean internal language model BPE perplexity by a relative 36.6%. For AI practitioners, DeCRED provides a low-cost technique to enhance ASR model robustness by adding an auxiliary loss to the decoder, a modification with negligible training overhead and no added inference cost in its default configuration. |
| Cut2Next: Generating Next Shot via In-Context Tuning (Read more on [arXiv](https://arxiv.org/abs/2508.08244) or [HuggingFace](https://huggingface.co/papers/2508.08244))| Yu Qiao, Ziqi Huang, Jiajun Li, Hongbo Liu, Jingwen He | Cut2Next introduces Next Shot Generation (NSG), a novel task aiming to synthesize cinematographically coherent subsequent shots that adhere to specified editing patterns and cinematic continuity given an input shot. The framework utilizes a Diffusion Transformer (FLUX.1-dev) with a Hierarchical Multi-Prompting strategy, incorporating Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM) for nuanced guidance from relational and individual prompts. Evaluated on the new CutBench benchmark, Cut2Next significantly outperforms baseline models, achieving a lower Fréchet Inception Distance (FID) of 59.37 compared to 80.43 for IC-LoRA-Cond. This advancement enables more sophisticated AI-driven video production by explicitly modeling and enforcing complex editing patterns, offering practitioners a tool for generating narratively expressive multi-shot sequences. |
| Adversarial Video Promotion Against Text-to-Video Retrieval (Read more on [arXiv](https://arxiv.org/abs/2508.06964) or [HuggingFace](https://huggingface.co/papers/2508.06964))| Shuai Liu, Qian Li, Zhengyu Zhao, Chenhao Lin, michaeltqw108 | This paper introduces ViPro, the first adversarial attack designed to promote video ranks in Text-to-Video Retrieval (T2VR) systems. The research addresses the unexplored vulnerability of T2VR models to targeted video promotion, which can lead to manipulated video exposure. Their key methodology involves ViPro, which integrates Modality Refinement (MoRe) through Temporal Clipping and Semantic Weighting to improve black-box transferability of perturbations. Experimentally, ViPro consistently outperforms existing baselines, achieving over 30% higher promotion performance on average in white-box settings. This work highlights a critical and overlooked security vulnerability in T2VR, necessitating the development of more robust models, potentially through multimodal fusion, to build trustworthy AI systems. |
| OpenCUA: Open Foundations for Computer-Use Agents (Read more on [arXiv](https://arxiv.org/abs/2508.09123) or [HuggingFace](https://huggingface.co/papers/2508.09123))| Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, xywang626 | The paper introduces OPENCUA, a comprehensive open-source framework with a dataset (AGENTNET), tools, and training recipes for developing high-performance Computer-Use Agents (CUAs). Its objective is to establish an open foundation to scale CUA development by addressing the lack of public data, tools, and effective training methodologies. The core methodology is a scalable pipeline that converts captured human demonstrations into state-action pairs augmented with synthesized, multi-level reflective Chain-of-Thought (CoT) reasoning, which explicitly models planning and error reflection. The resulting OPENCUA-32B model achieves a 34.8% success rate on the OSWorld-Verified benchmark, establishing a new state-of-the-art for open-source CUAs and outperforming the proprietary GPT-4o-based OpenAI CUA (31.4%). For AI practitioners, the principal implication is that the provided open-source assets and the demonstrated effectiveness of synthesized reflective reasoning offer a direct path to build more robust agents; the most impactful finding is that augmenting training data with this reflective CoT is critical for scaling agent performance and error correction capabilities. |
| AutoCodeBench: Large Language Models are Automatic Code Benchmark
  Generators (Read more on [arXiv](https://arxiv.org/abs/2508.09101) or [HuggingFace](https://huggingface.co/papers/2508.09101))| Tao Zhang, Zhiying Zeng, Yuchi Deng, Ao Liu, Jason Chou | This paper introduces AutoCodeGen, an automated workflow using a reverse-order generation pipeline and a multilingual sandbox to create AutoCodeBench, a large-scale benchmark designed to overcome the limitations of manual, Python-centric evaluations. The main objective is to automate the creation of a challenging, diverse, and practical multilingual code generation benchmark. The methodology involves LLMs first generating code solutions and test inputs, executing them in a multilingual sandbox to get test outputs, synthesizing complete test functions, and finally generating the problem description, followed by a multi-stage filtering process for difficulty, quality, and diversity. The primary result from evaluating over 30 LLMs on the 3,920-problem benchmark is that even top-tier models struggle, with the state-of-the-art Claude Opus 4 (Think) achieving a Pass@1 score of only 52.4%. For AI practitioners, the principal implication is that current LLMs have significant weaknesses in practical multilingual and multi-logic coding, and the AutoCodeBench suite provides a new, robust tool to benchmark and drive improvements in these specific, challenging areas. |
| Feedback-Driven Tool-Use Improvements in Large Language Models via
  Automated Build Environments (Read more on [arXiv](https://arxiv.org/abs/2508.08791) or [HuggingFace](https://huggingface.co/papers/2508.08791))| Xuesong Yao, Yufei Xu, Zhengyin Du, Changhao Jiang, Junjie-Ye | This paper introduces a feedback-driven framework to improve Large Language Model (LLM) tool-use capabilities using automated build environments. The primary objective is to address limitations in current RL-based tool-use training, such as unstable environments and unverifiable rewards. The methodology consists of a five-stage automated environment construction pipeline, which includes localized deployment of all tools as code, combined with a verifiable reward mechanism assessing precision and completeness. Experiments demonstrate the approach consistently enhances LLM tool-use performance, achieving over a 10% average performance gain across multiple benchmarks without degrading general capabilities. This framework offers a scalable, stable, and verifiable method for AI practitioners to train LLMs for more robust and generalizable tool use in complex real-world tasks. |
| Bridging Theory and Practice in Quantum Game Theory: Optimized
  Implementation of the Battle of the Sexes with Error Mitigation on NISQ
  Hardware (Read more on [arXiv](https://arxiv.org/abs/2508.09050) or [HuggingFace](https://huggingface.co/papers/2508.09050))| Jhon Alejandro Andrade, Mateo Buenaventura Samboni, Carlos Andres Duran Paredes, Germán Díaz Agreda, sebasmos | This paper experimentally validates quantum game theory, specifically the Battle of the Sexes game, on NISQ hardware using error mitigation. The main objective was to experimentally realize the "Battle of the Sexes" game under the Eisert-Wilkens-Lewenstein framework on IBM Quantum's ibm sherbrooke processor and to quantify the performance gap between theoretical predictions and NISQ implementation. The study introduced a Guided Circuit Mapping (GCM) strategy that dynamically selected qubit pairs, optimized routing based on real-time topology and calibration data, and iteratively adjusted mappings to mitigate noise. The experimental results with GCM preserved expected payoff trends within 3.5%-12% relative error, demonstrating that quantum advantages in strategic coordination can persist under NISQ conditions, despite analytical forecasts of up to 108% payoff improvement over classical equilibrium. These findings establish a practical foundation for implementing quantum game theory in multi-agent, economic, and distributed decision-making systems, suggesting that quantum-enhanced coordination mechanisms are viable in realistic NISQ hardware environments for AI practitioners. |
| BiasGym: Fantastic Biases and How to Find (and Remove) Them (Read more on [arXiv](https://arxiv.org/abs/2508.08855) or [HuggingFace](https://huggingface.co/papers/2508.08855))| Arnav Arora, Haeun Yu, Siddhesh Milind Pawar, Nadav Borenstein, sekhcopenlu | BiasGym is a framework designed for injecting, analyzing, and mitigating conceptual biases within Large Language Models (LLMs). Its main objective is to reliably surface and study subtle biased associations, addressing the challenge of isolating and debiasing such behaviors. The methodology integrates BiasInject, which uses token-based fine-tuning to inject specific biases, with BiasScope, which identifies and mitigates biases by steering important attention heads. Primary results demonstrate that BiasGym's "Injection w/ steering (Ours)" variant reduced average stereotype strength for Llama3.1-8B to 0.55, significantly lower than 1.69 for "Injection" and 1.28 for "Original," with minimal performance degradation (maximum 0.08, average 0.03) on downstream tasks. This framework provides AI practitioners with a simple, cost-effective tool for targeted debiasing, enabling consistent bias elicitation for mechanistic analysis and improving LLM safety without substantial capability compromise. |
| TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.08680) or [HuggingFace](https://huggingface.co/papers/2508.08680))| Rachel Bawden, Benoît Sagot, Armel Zebaze | TopXGen generates high-quality, topic-diverse parallel data for low-resource languages (LRLs) by using an LLM to create target-side text followed by back-translation. The paper's objective is to develop and evaluate a pipeline for creating synthetic parallel data to improve machine translation (MT) performance into LRLs where high-quality training data is scarce. The methodology involves prompting a multilingual LLM (Gemma-3-27B-It) with topics and examples to generate monolingual paragraphs in an LRL, which are then sentence-split and back-translated into English using a supervised MT model (NLLB-200-3.3B). The primary result is that fine-tuning smaller models on TopXGen data significantly boosts performance; for instance, a fine-tuned LLaMA-3-8B model achieves a BLEU score of 23.24 for English-to-Nepali, approaching the 25.74 BLEU of the much larger generator model. The principal implication for AI practitioners is that this method provides a scalable, effective way to create parallel datasets for LRLs, enabling the development of high-performing, fine-tuned MT models even in the absence of extensive, human-curated LRL corpora. |
| WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface
  Temperature Estimation via Spatio-Temporal Fusion (Read more on [arXiv](https://arxiv.org/abs/2508.06485) or [HuggingFace](https://huggingface.co/papers/2508.06485))| Rachid Nedjai, Raphael Canals, Adel Hafiane, sofianebouaziz | WGAST is a weakly-supervised generative network for daily 10m Land Surface Temperature (LST) estimation via spatio-temporal fusion. The main objective is to estimate LST at 10m resolution by simultaneously fusing coarse 1km daily Terra MODIS, 30m 16-day Landsat 8 LST, and 10m 5-day Sentinel-2 spectral data. Its key methodology involves a conditional Generative Adversarial Network (cGAN) with a four-stage generator and a discriminator, utilizing a novel weakly-supervised strategy that leverages 30m Landsat-derived LST as proxy ground truth. WGAST achieved, on average, a 17.18% RMSE reduction and 4.10% SSIM improvement compared to FuseTen, and successfully reconstructs LST in cloud-covered regions. This demonstrates a robust deep learning framework for multi-resolution remote sensing fusion, directly relevant for AI practitioners developing high-resolution Earth observation products with limited ground truth data. |
| GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via
  General Samples Replay (Read more on [arXiv](https://arxiv.org/abs/2508.04676) or [HuggingFace](https://huggingface.co/papers/2508.04676))| Yang Fan, Yuefeng Li, Mengchen Zhao, Shuoran Jiang, Yunan Zhang | This paper introduces General Sample Replay (GeRe), a framework that uses a fixed set of general-purpose texts and a novel Threshold-based Margin (TM) loss to mitigate catastrophic forgetting in continually fine-tuned LLMs. The research aims to develop a simple and stable method to prevent both the degradation of an LLM's general capabilities and its performance on previously learned tasks. The core methodology involves replaying a fixed set of 1K general pretraining texts and applying the proposed TM loss, which constrains the model's last-layer hidden states to maintain their original discrete activation patterns. In a 15-task continual full-parameter finetuning experiment, the proposed method (BaselineR+TM with dynamic weighting) achieved an F1 average of 66.94, significantly outperforming the 37.89 F1 of a no-replay baseline. For AI practitioners, this research shows that the laborious process of storing and replaying an expanding set of data from past tasks can be replaced by an efficient approach using a small, static set of general text, thereby simplifying the continual learning pipeline. |
| NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech
  Modeling with Paralinguistic Vocalizations (Read more on [arXiv](https://arxiv.org/abs/2508.04195) or [HuggingFace](https://huggingface.co/papers/2508.04195))| Haoyue Zhan, Yiheng Lu, Yuancheng Wang, Qinke Ni, Huan Liao | NVSpeech presents an integrated and scalable pipeline for human-like speech modeling with paralinguistic vocalizations in Mandarin. The research aims to bridge gaps in current speech modeling by enabling word-level annotation, joint lexical and non-verbal transcription via ASR, and explicit, token-level control over paralinguistic vocalizations in TTS. This is achieved by first manually annotating 48,430 utterances with 18 paralinguistic categories, then training a paralinguistic-aware ASR model that automatically labels a large corpus of 174,179 utterances (573 hours), and finally finetuning zero-shot TTS models on this combined data. The paralinguistic-aware ASR model (SenseVoice) achieved an F1-score of 0.85 and a Character Error Rate (CER) of 3.79% on the open-domain test set, while enhanced TTS models demonstrated strong listener preference, with win rates of up to 78.7%, and maintained high naturalness (NMOS 3.9-4.0). This work provides a scalable foundation and resources for AI practitioners to develop more expressive and natural human-like speech recognition and generation systems. |
