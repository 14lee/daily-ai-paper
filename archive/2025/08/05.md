

## Papers for 2025-08-05

| Title | Authors | Summary |
|-------|---------|---------|
| Qwen-Image Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.02324) or [HuggingFace](https://huggingface.co/papers/2508.02324))| Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu | The Qwen-Image Technical Report introduces an image generation foundation model that achieves state-of-the-art performance in complex text rendering and precise image editing. The research aims to develop a model that can follow complex, multifaceted prompts, particularly for rendering non-alphabetic languages like Chinese, and perform image editing with high visual and semantic consistency. The methodology combines a Multimodal Diffusion Transformer (MMDiT) with a frozen Qwen2.5-VL for text encoding, trained via a progressive curriculum learning strategy on a comprehensive data pipeline that includes large-scale synthesis of text-rich images, and employs a dual-encoding mechanism for editing tasks. Qwen-Image significantly outperforms existing models in Chinese text generation, achieving an overall accuracy of 58.30 on the ChineseWord benchmark, compared to 36.14 for GPT Image 1. For AI practitioners, this research demonstrates that targeted data synthesis and a multi-stage, curriculum-based training approach are highly effective for building foundation models with superior control over specific, challenging attributes like text rendering, enabling the development of more practical and precise multimodal applications. |
| SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension (Read more on [arXiv](https://arxiv.org/abs/2508.01959) or [HuggingFace](https://huggingface.co/papers/2508.01959))| Liyan Xu, Lemao Liu, Yuqing Li, Jiangnan Li, Junjie Wu | The paper introduces SitEmb, a situated embedding model and training paradigm that enhances dense retrieval by encoding short text chunks with their surrounding long-range context. The objective is to develop a text embedding model that represents short text chunks conditioned on their broader document context to improve retrieval, overcoming the limitations of both isolated short-chunking and monolithic long-chunking. The key methodology is a residual learning framework where a "situated" model is trained to learn the residual from a baseline chunk-only model, forcing it to focus on contextual information, using training data constructed from user-annotated book notes and QA datasets. On the full Book Plot Retrieval task, the 8B parameter SitEmb-v1.5 model achieves a Recall@50 of 82.70, significantly outperforming its base model's score of 69.48; experiments also show that existing state-of-the-art embedding models degrade in performance when provided the same situated context in a zero-shot setting. For AI practitioners, the principal implication is that for RAG over long documents, using specialized situated embedding models to encode short, localized passages with awareness of their surrounding context is a more effective strategy for improving retrieval relevance than simply increasing chunk size. |
| CellForge: Agentic Design of Virtual Cell Models (Read more on [arXiv](https://arxiv.org/abs/2508.02276) or [HuggingFace](https://huggingface.co/papers/2508.02276))| Daniel Shao, Yan Cui, Jiapeng Chen, Zhuoyun Yu, Xiangru Tang | CellForge is an agentic system that autonomously designs, codes, and optimizes computational models for virtual cells directly from raw biological data and research objectives. The primary objective is to automate the end-to-end scientific workflow of virtual cell modeling to predict cellular responses to diverse perturbations like gene knockouts and drug treatments. The methodology involves a multi-agent framework with modules for Task Analysis, collaborative Method Design via a graph-based expert discussion, and Experiment Execution for automated code generation and self-debugging. In single-cell perturbation prediction tasks across six datasets, CELLFORGE consistently outperforms state-of-the-art methods, achieving up to a 40% reduction in prediction error and a 20% improvement in correlation metrics. For AI practitioners, this work demonstrates that an iterative, multi-agent collaborative reasoning framework can autonomously design and implement superior, domain-specific deep learning architectures without relying on fixed model templates or human intervention. |
| Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.01059) or [HuggingFace](https://huggingface.co/papers/2508.01059))| Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena | This technical report introduces Foundation-Sec-8B-Instruct, an 8B-parameter language model specialized for cybersecurity dialogue and instruction-following tasks. The primary objective was to adapt a cybersecurity domain-specialized base model, Foundation-Sec-8B, into a conversational assistant by applying instruction-tuning and human preference alignment. The methodology involved applying Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to the base model, using a curated mix of synthetic and human-preference data after a rigorous benchmark contamination analysis. Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on the CTIBench-RCM benchmark with a 24.03% higher score and also surpasses it on general instruction-following evaluations like IFEval and AlpacaEval 2. The principal implication for AI practitioners is that smaller, domain-adapted models can achieve state-of-the-art performance in specialized fields, providing a publicly available model for building cybersecurity tools and a validated methodology for creating similar expert assistants. |
| Beyond the Trade-off: Self-Supervised Reinforcement Learning for
  Reasoning Models' Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2508.02150) or [HuggingFace](https://huggingface.co/papers/2508.02150))| Jiaqing Liang, Jie Zeng, Bowei Zhang, Qianyu He, Qingyu Ren | This paper introduces a self-supervised reinforcement learning framework that improves a reasoning model's instruction-following ability without external supervision or degrading its core reasoning performance. The primary objective is to resolve the trade-off between a model's reasoning and instruction-following capabilities by developing a method that enhances the latter without relying on stronger external models for supervision. The key methodology is a self-supervised RL framework featuring an incremental constraint curriculum for dense learning signals, a hybrid reward model combining rule-based verification for hard constraints and a self-supervised binary classifier for soft constraints, and policy optimization using the GRPO algorithm. The framework significantly improves instruction-following, with the 0528-Qwen3-8B model's IFEval score increasing from 79.7 to 87.1, while crucially maintaining its average performance score of 52.0 on a suite of general reasoning benchmarks. The principal implication for AI practitioners is that this framework provides a scalable and cost-effective method to enhance the instruction-following reliability of specialized or distilled models without requiring access to larger, proprietary models for data generation or reward modeling. |
| InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation (Read more on [arXiv](https://arxiv.org/abs/2507.17520) or [HuggingFace](https://huggingface.co/papers/2507.17520))| Yang Tian, Bin Wang, Yilun Chen, Hao Li, Shuai Yang | The paper introduces InstructVLA, a Vision-Language-Action (VLA) model that integrates multimodal reasoning and robotic manipulation through a novel instruction tuning paradigm to mitigate catastrophic forgetting of pre-trained capabilities. The research aims to create a VLA model that preserves the reasoning abilities of large vision-language models (VLMs) while learning precise manipulation skills, effectively bridging high-level understanding with low-level action execution. The methodology involves a two-stage training recipe: first, an action expert is pretrained to decode latent actions from a VLM; second, a "Vision-Language-Action Instruction Tuning" (VLA-IT) stage uses a Mixture-of-Experts (MoE) architecture to co-train the VLM on both standard multimodal data and a curated 650K-sample dataset, enabling it to generate both textual reasoning and action commands. On the novel SimplerEnv-Instruct benchmark for high-level instruction following, InstructVLA outperforms a fine-tuned OpenVLA by 92% and an action expert guided by GPT-4o by 29%. For AI practitioners, this research provides a framework for building generalist robots by decoupling high-level reasoning in a VLM from low-level control in a separate action expert and using instruction tuning to explicitly train for both textual reasoning and action generation, thereby preserving valuable pre-trained knowledge during specialization. |
| VeOmni: Scaling Any Modality Model Training with Model-Centric
  Distributed Recipe Zoo (Read more on [arXiv](https://arxiv.org/abs/2508.02317) or [HuggingFace](https://huggingface.co/papers/2508.02317))| Bin Jia, Zhongkai Zhao, Zhelun Shi, Yaowei Zheng, Qianli Ma | VeOmni is a model-centric distributed training framework designed to scale omni-modal large language models efficiently. Its primary objective is to address the challenges of heterogeneous model architectures and the entanglement of model definition with parallel logic in existing frameworks, enabling scalable and efficient end-to-end training for omni-modal LLMs. VeOmni introduces model-centric distributed recipes that decouple communication from computation, integrating 3D parallelism (FSDP, SP, EP) and system optimizations like dynamic batching and memory optimization, facilitated by a plug-and-play architectural design. Experimental results demonstrate that a 30B parameter omni-modal Mixture-of-Experts model can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths on 128 GPUs using its 3D parallelism. This framework provides a lightweight, non-intrusive interface for customizing and scaling omni-modal LLMs, significantly reducing engineering overhead and accelerating the development of diverse multimodal models. |
| A Glimpse to Compress: Dynamic Visual Token Pruning for Large
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.01548) or [HuggingFace](https://huggingface.co/papers/2508.01548))| Zuxuan Wu, Peng-Tao Jiang, Qilong Wang, Yunheng Li, Quan-Sheng Zeng | This paper presents GlimpsePrune, a dynamic visual token pruning framework that uses a data-driven method to compress high-resolution visual inputs for Large Vision-Language Models (LVLMs). The research objective is to develop a framework that can learn a dynamic, data-driven metric to efficiently prune query-irrelevant visual tokens, overcoming the inflexibility of fixed-ratio compression methods. The key methodology involves inserting a learnable "glimpse token" and using its cross-attention scores from an intermediate decoder layer to train a lightweight Visual Importance Predictor (VIP), which performs a one-shot prune of tokens and their corresponding KV cache entries mid-prefill. The primary result shows that GlimpsePrune prunes an average of 92.6% of visual tokens while fully retaining the baseline model's performance on free-form VQA tasks, and an enhanced version (GlimpsePrune+) achieves 110% of baseline performance. The principal implication for AI practitioners is that this framework provides a practical solution to reduce the significant memory and computational costs of LVLM inference on high-resolution inputs, enabling more efficient deployment and making computationally-intensive fine-tuning more feasible. |
| Personalized Safety Alignment for Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.01151) or [HuggingFace](https://huggingface.co/papers/2508.01151))| Kaidong Yu, Aosong Feng, Qingyu Shi, Jinbin Bai, Yu Lei | This paper introduces Personalized Safety Alignment (PSA), a framework to condition text-to-image diffusion models on user-specific profiles for granular safety filtering. The primary objective is to move beyond uniform safety standards by enabling generative models to dynamically adapt their outputs to individual user preferences regarding sensitive content. The methodology involves creating a new dataset, Sage, with simulated user profiles and corresponding preferences, and then training a cross-attention adapter to inject user embeddings into the diffusion U-Net, optimizing via a personalized diffusion DPO loss. In experiments on the SDXL model, PSA achieved a Pass Rate of 64.29% for unseen users, outperforming the SafetyDPO baseline's 60.29% and demonstrating superior alignment with user-specific constraints. For AI practitioners, this framework provides a method to implement dynamic, user-centric safety controls, enabling more nuanced content moderation than static, global blocklists, though its reliance on synthetic user profiles is a stated limitation. |
| Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and
  Regional Languages Around the Globe (Read more on [arXiv](https://arxiv.org/abs/2508.01691) or [HuggingFace](https://huggingface.co/papers/2508.01691))| Thanathai Lertpetchpun, Xuan Shi, Anfeng Xu, Kevin Huang, Tiantian Feng | This paper presents Voxlect, a benchmark for evaluating speech foundation models on dialect and regional language classification across 11 language groups using over 2 million utterances from 30 public corpora. The objective is to systematically assess the performance of models like Whisper and MMS in dialect classification and demonstrate the utility of these classifiers in downstream applications. The methodology involves fine-tuning pre-trained speech foundation models on a curated collection of datasets with standardized dialect labels, using an architecture with LoRa adaptation. The primary result shows that multilingual models significantly outperform monolingual ones, with Whisper-Large achieving the highest Macro-F1 score in 5 of 11 language groups, including a score of 0.923 on Arabic dialects. The principal implication for AI practitioners is the availability of pre-trained models and a benchmark to analyze ASR performance across dialects, evaluate the dialectal quality of TTS systems, and augment datasets with dialect information, enabling the development of more equitable speech technologies. |
| RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong
  Learning in Physical Embodied Systems (Read more on [arXiv](https://arxiv.org/abs/2508.01415) or [HuggingFace](https://huggingface.co/papers/2508.01415))| Junkun Hong, Liangchen Tan, Zezhou Cui, Honghao Cai, Mingcong Lei | RoboMemory is a brain-inspired, multi-memory agentic framework that enables lifelong learning and long-term planning for robots in physical environments. The primary objective is to develop a framework for embodied agents that addresses challenges of continuous learning, memory latency, task correlation capture, and infinite-loop mitigation to enable robust lifelong learning in dynamic physical systems. The methodology integrates four parallel, brain-inspired modules: an Information Preprocessor, a Lifelong Embodied Memory System using a dynamic Knowledge Graph and a RAG framework, a modified Planner-Critic module for closed-loop planning, and a Low-Level Executer. On the EB-ALFRED benchmark, RoboMemory achieved an average success rate of 67.0%, outperforming the closed-source SOTA model Claude-3.5-Sonnet by 5 percentage points and improving upon its backbone model by 25 percentage points. For AI practitioners, the parallelized, multi-module memory architecture provides a scalable template for building embodied agents that can continuously learn from experience in real-world settings, demonstrating that a structured memory system significantly enhances performance over single large model approaches. |
| Exploitation Is All You Need... for Exploration (Read more on [arXiv](https://arxiv.org/abs/2508.01287) or [HuggingFace](https://huggingface.co/papers/2508.01287))| Jesse Roberts, Micah Rentschler | This paper demonstrates that an agent trained with a purely greedy objective can learn to explore effectively without explicit incentives, provided sufficient environmental structure and agent memory. The research objective is to empirically test the hypothesis that exploration can emerge organically from a reward-maximization objective by identifying and validating its necessary preconditions. The authors use a transformer-based DQN agent in a meta-RL setting and conduct controlled ablation studies on multi-armed bandit and gridworld environments by systematically varying environmental recurrence, agent memory, and temporal credit assignment. The primary result shows that while exploration emerges with sufficient structure and memory, long-term credit assignment is beneficial but not always essential; in complex gridworld tasks, increasing the episode discount factor from 0 to 0.9 improved normalized reward from 0.408 to 0.670. The principal implication for AI practitioners is that focusing on memory-rich architectures and training paradigms that leverage recurring task structures can be a more direct path to achieving effective exploration than engineering explicit exploration bonuses. |
| Cyber-Zero: Training Cybersecurity Agents without Runtime (Read more on [arXiv](https://arxiv.org/abs/2508.00910) or [HuggingFace](https://huggingface.co/papers/2508.00910))| Zijian Wang, Varun Kumar, Hantian Ding, Dingmin Wang, Terry Yue Zhuo | i) The paper introduces CYBER-ZERO, the first runtime-free framework that synthesizes agent trajectories from public Capture The Flag (CTF) writeups to train LLM-based cybersecurity agents. ii) The main research objective is to overcome the scarcity of high-quality training data in cybersecurity by developing a method to generate realistic, long-horizon interaction sequences without needing access to executable runtime environments. iii) The key methodology involves a dual-LLM, persona-driven simulation where a "CTF Player" LLM attempts to solve a challenge, guided by a second "Bash Terminal" LLM that uses public writeups as a weak oracle to reverse-engineer and simulate plausible system responses. iv) The primary result is that agents trained on these synthesized trajectories achieve up to a 13.1% absolute performance gain over baseline models across three prominent CTF benchmarks, with the best model (CYBER-ZERO-32B) matching the performance of proprietary systems like Claude-3.5-Sonnet. v) The principal implication for AI practitioners is that this runtime-free synthesis method can effectively democratize the development of state-of-the-art cybersecurity agents, enabling the training of capable and cost-effective open-weight models without needing access to often unavailable live challenge environments. |
| AgentTTS: Large Language Model Agent for Test-time Compute-optimal
  Scaling Strategy in Complex Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.00890) or [HuggingFace](https://huggingface.co/papers/2508.00890))| Zhiwei Zhang, Jingying Zeng, Zhenwei Dai, Hui Liu, Fali Wang | The paper introduces AgentTTS, an LLM-agent framework that autonomously finds compute-optimal model and budget allocations for multi-stage complex tasks by leveraging three empirical insights about test-time scaling. The main objective is to determine how to optimally select models and allocate a total compute budget across interdependent subtasks in a multi-stage complex task to maximize overall performance, given a combinatorial search space and high inference costs. The key methodology is AgentTTS, a framework where an LLM agent iteratively generates and refines budget allocation configurations. The agent's search is guided by prompts incorporating three empirical insights derived from pilot experiments: (1) subtasks have distinct model preferences, (2) performance gains diminish beyond an optimal budget, and (3) allocations are interdependent across subtasks. The primary result is that AgentTTS significantly outperforms baselines in search efficiency and final performance. On the 2WikiMultiHopQA dataset, AgentTTS achieved a test-set Exact Match (EM) score of 0.72, exceeding the next best methods by 2%, while requiring only 2.5 hours of search time compared to over 8 hours for competing agent-based approaches. The principal implication for AI practitioners is that they can use the AgentTTS framework to automate the complex process of optimizing inference compute for multi-stage AI systems, achieving better performance-cost trade-offs by strategically allocating resources based on subtask-specific needs rather than uniformly applying a single large model. |
| ReMoMask: Retrieval-Augmented Masked Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2508.02605) or [HuggingFace](https://huggingface.co/papers/2508.02605))| Hao Tang, Zeyu Zhang, Siheng Wang, Zhengdao Li | ReMoMask is a retrieval-augmented masked modeling framework that synthesizes human motion from text by integrating a novel bidirectional retriever and a spatiotemporal attention mechanism. The objective is to address the dual challenges of limited diversity in generative models and asynchronous artifacts in retrieval-augmented generation (RAG) methods for more realistic text-to-motion synthesis. The key methodology involves a Bidirectional Momentum Text-Motion Model (BMM) which uses momentum queues to improve cross-modal retrieval, and a Semantic Spatiotemporal Attention (SSTA) mechanism that fuses textual, retrieved, and 2D motion-structural information during generation. The model achieves state-of-the-art performance, demonstrating a 10.97% improvement in FID score on the KIT-ML dataset compared to the previous leading RAG-T2M method. For AI practitioners, the principal implication is that for complex generative tasks, augmenting models with retrieval is most effective when combined with specialized fusion mechanisms like SSTA that explicitly align external conditioning with the intrinsic spatiotemporal structure of the data being generated. |
| Artificial Intelligence and Misinformation in Art: Can Vision Language
  Models Judge the Hand or the Machine Behind the Canvas? (Read more on [arXiv](https://arxiv.org/abs/2508.01408) or [HuggingFace](https://huggingface.co/papers/2508.01408))| Elena Merino-Gómez, Pedro Reviriego, Gonzalo Martínez, Javier Conde, Tarian Fu | This paper evaluates Vision Language Models' (VLMs) ability to attribute real and AI-generated paintings to artists, highlighting their limitations in both tasks. The primary objective was to assess whether state-of-the-art VLMs can reliably identify the original artist of real paintings and detect AI-generated artistic imitations, preventing misinformation. Using a dataset of nearly 40,000 WikiArt paintings and AI-generated imitations from Stable Diffusion, Flux, and F-Lite, six open-weight and one proprietary VLM were evaluated on binary classification prompts for artist attribution and AI detection, measuring normalized accuracy (C1, C2) and their arithmetic mean (AM). Results show VLMs have significant limitations; for real paintings, even top performers like LLaMa3.2-11B achieved only ~63% average normalized accuracy (AM), and for Stable Diffusion imitations, GPT4.1-mini was best, correctly identifying over 95% as not from the suggested painter. These findings imply that current VLMs are unreliable for artist attribution and AI-generated content detection in art, necessitating improved model capabilities and careful deployment as decision-support tools rather than authoritative sources to mitigate widespread misinformation risks. |
| Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine
  Learning (Read more on [arXiv](https://arxiv.org/abs/2508.00024) or [HuggingFace](https://huggingface.co/papers/2508.00024))| Cristian Bosch, Carlos Andrés Durán, Mario Bifulco, Luis Fernando Torres Torres, Sebastián Andrés Cajas Ordóñez | This paper proposes a hybrid quantum-classical SVM framework demonstrating that quantum advantage is critically dependent on the choice of classical feature embeddings. The research objective is to address the scalability limitations of Quantum Support Vector Machines (QSVMs) by systematically investigating how different pretrained embeddings influence quantum kernel performance. The methodology combines class-balanced k-means data distillation with feature extraction from pretrained Vision Transformer (ViT) and CNN models, followed by classification using a 16-qubit QSVM simulated via a tensor network backend. The primary result shows that using ViT embeddings enables the QSVM to achieve an accuracy improvement of up to 8.02% on Fashion-MNIST over a classical SVM using identical embeddings, while CNN features and raw pixels lead to performance degradation. The principal implication for AI practitioners is that realizing quantum advantage requires a deliberate co-design of classical representation and quantum algorithms, as the choice of transformer-based embeddings is shown to be a prerequisite for outperforming classical methods in this QSVM setting. |
