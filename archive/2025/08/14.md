

## Papers for 2025-08-14

| Title | Authors | Summary |
|-------|---------|---------|
| Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery (Read more on [arXiv](https://arxiv.org/abs/2508.08401) or [HuggingFace](https://huggingface.co/papers/2508.08401))| Di Zhang, Junxian Li, Qinggang Zhang, Weida Wang, Jiatong Li | Mol-R1 is a novel framework enhancing explicit Long Chain-of-Thought (CoT) reasoning for text-based molecule generation. It aims to improve explainability and reasoning performance of R1-like LLMs by efficiently generating high-quality, expert-aligned reasoning traces and leveraging them for stable training. The methodology involves Prior Regulation via In-context Distillation (PRID) for cold-start dataset curation with human-labeled examples, followed by Molecular Iterative Adaptation (MoIA) which iteratively combines Supervised Fine-tuning (SFT) and Reinforced Policy Optimization (RPO). Mol-R1 (T=2) achieved a 0.234 Exact Match (EM) score and a Consistent-F1 score of 0.847 for reasoning trace quality, significantly outperforming QWQ-32B (0.518) and DeepSeek-R1 (0.522) in trace quality. This approach demonstrates significant potential for enabling more explainable and chemist-like reasoning in molecule discovery, addressing limitations of existing LLMs in knowledge-intensive domains. |
| Stand-In: A Lightweight and Plug-and-Play Identity Control for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.07901) or [HuggingFace](https://huggingface.co/papers/2508.07901))| Chen Li, Hao Liu, Wenjing Wang, Qixin Yan, Bowen Xue | Stand-In introduces a lightweight and plug-and-play framework for identity-preserving video generation. The research aims to generate high-fidelity videos that consistently maintain the identity from a given reference image, addressing limitations of existing methods regarding excessive parameters and limited compatibility. This is achieved by incorporating a conditional image branch into a pre-trained video generation model, utilizing restricted self-attention with conditional position mapping (3D ROPE), and leveraging the model's inherent VAE for feature extraction. Despite adding only ~1% additional parameters (153M for the 14B model), Stand-In achieves state-of-the-art performance, with a Face Similarity score of 0.724 and Naturalness of 3.922. The framework's lightweight and plug-and-play design enables seamless integration into various applications like subject-driven generation, video stylization, and face swapping, providing significant value for AI practitioners. |
| AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust
  GAIA Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2508.09889) or [HuggingFace](https://huggingface.co/papers/2508.09889))| Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie | AWorld introduces a robust dynamic Multi-Agent System with stable maneuvering for enhanced accuracy and stability in complex tool-augmented problem-solving. The research aims to enhance the stability and accuracy of intelligent agent-based systems when leveraging diverse external tools, addressing challenges like extended contexts and noisy tool outputs. The authors developed a dynamic Multi-Agent System (MAS) within the AWorld framework, incorporating dynamic supervision and maneuvering mechanisms where an Execution Agent collaborates with a Guard Agent. Experiments on the GAIA test dataset showed that the dynamic MAS improved pass@1 accuracy to 67.89%, an 8.82% gain over the Single Agent System (SAS), and reduced the pass@1 standard deviation to 0.027, a 17.3% reduction compared to the SAS. This work demonstrates the practical value of collaborative, dynamically supervised multi-agent systems for developing more reliable, trustworthy, and performant AI solutions, particularly in tool-augmented problem-solving scenarios. |
| Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing (Read more on [arXiv](https://arxiv.org/abs/2508.09192) or [HuggingFace](https://huggingface.co/papers/2508.09192))| Hao Zhang, Jiachun Jin, Yijie Jin, Chenkai Xu, Xu Wang | This paper introduces Discrete Diffusion Forcing (D2F), a novel paradigm enabling Diffusion Large Language Models (dLLMs) to achieve faster-than-autoregressive (AR) inference. The primary objective is to overcome the inference speed limitations of existing open-source dLLMs compared to AR models. D2F employs block-wise autoregressive generation for KV cache utilization and predicts future tokens without requiring completion of prior blocks, implemented via asymmetric distillation and a pipelined parallel decoding algorithm. Empirically, D2F dLLMs achieve over 2.5x inference speed compared to LLaMA3 and Qwen2.5 on GSM8K, and more than 50x acceleration over vanilla dLLMs like LLaDA and Dream. This breakthrough establishes dLLMs as a significantly more efficient and scalable alternative for high-throughput text generation tasks, offering direct benefits for AI practitioners in deployment scenarios. |
| Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09987) or [HuggingFace](https://huggingface.co/papers/2508.09987))| Zhenghao Hu, Leqi Zhu, Zihao Wang, Dongzhi Jiang, Junyan Ye | The paper presents Echo-4o-Image, a 180K-sample synthetic dataset from GPT-4o, and the Echo-4o model, which is fine-tuned on this data to enhance open-source image generation for complex, surreal, and multi-reference tasks. The primary objective is to demonstrate that targeted synthetic data from advanced models like GPT-4o can overcome the limitations of real-world datasets (e.g., lack of surreal/multi-reference examples, poor instruction alignment) to significantly improve the capabilities of open-source image generation models. The authors curated the Echo-4o-Image dataset using GPT-4o to generate three types of data: surreal fantasy, multi-reference, and complex instruction-following images. They then fine-tuned the Bagel multimodal model on this dataset to create Echo-4o and introduced two new benchmarks, GenEval++ and Imagine-Bench, which use GPT-4.1 for more challenging evaluation. The Echo-4o model demonstrates superior performance across multiple benchmarks; on the newly proposed and more difficult GenEval++ benchmark, Echo-4o achieved an overall instruction-following score of 0.679, significantly outperforming the baseline Bagel model's score of 0.371. The principal implication for AI practitioners is that targeted, high-quality synthetic data, such as the open-sourced Echo-4o-Image dataset, can be used to fine-tune existing foundation models and significantly boost performance on complex, long-tail generation tasks underrepresented in real-world data. The most impactful finding is the model's enhanced ability to follow complex, compositional instructions, showing this synthetic data approach is highly effective for teaching nuanced generative capabilities. |
| Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory (Read more on [arXiv](https://arxiv.org/abs/2508.09736) or [HuggingFace](https://huggingface.co/papers/2508.09736))| Yuan Lin, Yiyuan Pan, Wentao Ye, Yichen He, Lin Long | M3-Agent is a novel multimodal agent framework that leverages long-term memory for continuous perception, knowledge building, and reasoning. The core objective is to enable multimodal agents to autonomously process real-time visual and auditory inputs, build entity-centric episodic and semantic memories, and reason iteratively over this accumulated knowledge to accomplish tasks. The framework employs parallel memorization and control processes, utilizing a multimodal graph for memory organization and reinforcement learning (DAPO) for multi-turn reasoning and iterative memory retrieval, evaluated on a new M3-Bench long-video QA benchmark. Experimental results demonstrate M3-Agent's superior performance, outperforming the strongest baseline by 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web, and VideoMME-long benchmarks respectively, with semantic memory being critical for performance. This work provides insights into the practical design of multimodal agents, highlighting the importance of reinforcement learning and robust semantic memory for achieving human-like long-term memory and memory-based reasoning capabilities for AI practitioners developing real-world agents. |
| Learning to Align, Aligning to Learn: A Unified Approach for
  Self-Optimized Alignment (Read more on [arXiv](https://arxiv.org/abs/2508.07750) or [HuggingFace](https://huggingface.co/papers/2508.07750))| Lei Fan, Shuowen Zhang, Zhiling Ye, Yun Yue, Haowen Wang | GRAO is a unified framework for self-optimized LLM alignment, addressing supervised fine-tuning's offline policy limitations and reinforcement learning's sample inefficiency and base model dependency. It synergizes these approaches through a multi-sample generation strategy with reward feedback, a novel Group Direct Alignment Loss utilizing intra-group relative advantage weighting, and reference-aware parameter updates guided by pairwise preference dynamics. Comprehensive evaluations demonstrate GRAO's superior performance, achieving 57.70%, 17.65%, 7.95%, and 5.18% relative improvements over SFT, DPO, PPO, and GRPO baselines respectively. For instance, GRAO achieved a 67.98% Normalized Alignment Gain on Qwen2.5-7B for helpful alignment tasks. This framework provides a theoretically grounded alignment method and empirical evidence for efficient capability evolution in language models, offering a robust and scalable solution for AI practitioners to align diverse model architectures. |
| Story2Board: A Training-Free Approach for Expressive Storyboard
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09983) or [HuggingFace](https://huggingface.co/papers/2508.09983))| Dani Lischinski, Dvir Samuel, Omri Avrahami, Matan Levy, David Dinkevich | Story2Board presents a training-free framework for expressive storyboard generation from natural language, aiming to produce visually coherent and narratively compelling sequences with dynamic compositions and consistent character identity. The core methodology involves Latent Panel Anchoring (LPA) and Reciprocal Attention Value Mixing (RAVM), which guide pre-trained text-to-image diffusion models by preserving shared character references and softly blending visual features between semantically aligned tokens across panels, without architectural changes or fine-tuning. On the Rich Storyboard Benchmark, Story2Board consistently outperforms baselines, achieving a DreamSim score of 0.7018 on the DS-500 benchmark for identity consistency, surpassing DreamStory (0.6714), and leading in overall user preference across diverse narrative settings. This approach provides AI practitioners with a flexible and efficient means to generate high-quality, dynamic storyboards for visual storytelling applications, leveraging existing diffusion models without the overhead of model fine-tuning. |
| MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math
  Reasoning in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.06009) or [HuggingFace](https://huggingface.co/papers/2508.06009))| Zhihan Zhou, Yue Guo, Zhentao Zhang, Zixin Wang, junfeng0288 | MATHREAL is a new real-world benchmark for evaluating Multimodal Large Language Models (MLLMs) on K-12 math reasoning from naturally captured images. The primary objective is to assess MLLMs' reasoning capabilities under realistic visual conditions, accounting for image quality degradation, perspective variation, and irrelevant content interference. The methodology involves collecting 2,000 high-quality K-12 math questions with mobile-captured images, systematically annotating them across 14 fine-grained real-world scenario subcategories and five core knowledge categories, and evaluating MLLMs using six experimental input settings. Key results show that the best-performing model, Doubao-1.5-thinking-vision-pro, achieved only 53.9% accuracy, and a notable performance gap exists between real and clean image inputs for existing MLLMs. This underscores the critical need for AI practitioners to develop more robust visual encoders for MLLMs to handle realistic distortions and achieve reliable performance in real-world educational scenarios. |
| Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning
  for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.05613) or [HuggingFace](https://huggingface.co/papers/2508.05613))| Guiyang Hou, Xingyu Wu, Haitao Hong, tricktreat, yanyc | Cooper is a reinforcement learning (RL) framework that co-optimizes policy and reward models for large language models (LLMs) to mitigate reward hacking and enhance reasoning capabilities. The main objective is to overcome the inherent limitations of static rule-based (lack robustness) and model-based (vulnerability to hacking) reward functions in LLM training. Cooper employs a two-stage training pipeline involving policy model optimization via Group Relative Policy Optimization (GRPO) with a reference-aware reward model, and continuous reward model refinement through contrastive learning using dynamically selected positive (rule-based) and negative (LLM-generated) samples. Quantitatively, Cooper achieved a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct, with its VerifyRM-1.5B achieving 89.42% accuracy on VerifyBench, while static reward models suffered catastrophic performance degradation (e.g., 16% relative decrease). This demonstrates that dynamically updating reward model parameters during RL training is an effective strategy for AI practitioners to combat reward hacking and improve end-to-end RL performance in LLMs. |
| IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.09456) or [HuggingFace](https://huggingface.co/papers/2508.09456))| Di Zhang, Beining Xu, Junxian Li | IAG is a novel input-aware backdoor attack designed to manipulate Vision-Language Models (VLMs) for visual grounding tasks. The objective is to force VLMs to localize an attacker-specified target object in an image, irrespective of the user's query, while maintaining stealthiness and normal outputs for clean samples. This is achieved by an adaptive trigger generator, a text-conditional U-Net, which embeds the target's semantic information into the image, optimized with a reconstruction loss and trained jointly with the VLM using a combined language model loss. Empirical results show IAG achieved an ASR@0.5 of 66.7% on InternVL-2.5-8B for RefCoco (testA), with only a 1-3% accuracy decrease on clean samples, demonstrating its effectiveness and stealthiness. This work reveals a critical security vulnerability in VLM agents, where imperceptible, semantically potent triggers can hijack grounding behavior, underscoring the need for robust safeguards in VLM deployment. |
| Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.09968) or [HuggingFace](https://huggingface.co/papers/2508.09968))| Zeynep Akata, Nataniel Ruiz, Alexey Dosovitskiy, Shyamgopal Karthik, Luca Eyring | Noise Hypernetworks (HyperNoise) enable amortizing the computational cost of test-time noise optimization in diffusion models into a one-time post-training stage. The research addresses the challenge of reducing high inference latency associated with test-time scaling methods in generative vision models, aiming to integrate test-time scaling knowledge into a neural network during training. HyperNoise replaces reward-guided test-time noise optimization with a lightweight noise hypernetwork (f_phi, implemented via LoRA) that learns to modulate initial Gaussian noise, optimizing a tractable noise-space objective (L_noise) to approximate a reward-tilted noise distribution. Experiments show HyperNoise recovers a substantial portion of quality gains from explicit test-time optimization at a fraction of the computational cost; for example, it improved SD-Turbo's GenEval performance to 0.57 and SANA-Sprint's to 0.75 (from 0.70 baseline), achieving the same performance as LLM-based prompt optimization while being 300x faster. This allows AI practitioners to achieve high-quality, reward-aligned generation with state-of-the-art distilled diffusion models with minimal added inference latency, making such capabilities practical for real-time applications. |
| VisCodex: Unified Multimodal Code Generation via Merging Vision and
  Coding Models (Read more on [arXiv](https://arxiv.org/abs/2508.09945) or [HuggingFace](https://huggingface.co/papers/2508.09945))| Dongdong Zhang, Yixia Li, Xun Wu, Shaohan Huang, Lingjie Jiang | VisCodex introduces a unified multimodal framework for code generation by merging vision and coding models using task vectors. The primary objective is to empower Multimodal Large Language Models (MLLMs) with robust code generation from multimodal inputs, addressing existing limitations. This is achieved by arithmetically merging the language backbone parameters of a vision-language model with a dedicated coding LLM via task vectors, alongside introducing the large-scale Multimodal Coding Dataset (MCD) and the InfiBench-V benchmark. VisCodex-8B demonstrates state-of-the-art performance among open-source MLLMs, achieving 11.0 pass@1 on the MMCode benchmark and approaching proprietary models like GPT-4o. This research provides AI practitioners with an efficient, cost-effective model merging strategy to enhance multimodal understanding and code generation, bypassing expensive retraining. |
| Can LLM-Generated Textual Explanations Enhance Model Classification
  Performance? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.09776) or [HuggingFace](https://huggingface.co/papers/2508.09776))| Gjergji Kasneci, Zineb Attaoui, Ege Erdogan, Juraj Vladika, Mahdi Dhaini | The paper investigates the impact of LLM-generated textual explanations on the classification performance of PLMs and LLMs in Natural Language Inference (NLI) tasks. Its main objective was to determine how such explanations affect downstream predictive tasks. The methodology involved generating explanations using four LLMs in zero-shot and few-shot settings, evaluating them with NLG metrics and G-Eval, and then assessing their impact on four fine-tuned PLMs and three LLMs via zero-shot inference on e-SNLI and HealthFC datasets. Primary results indicate that LLM-generated explanations consistently improved PLM performance; for example, Llama3 zero-shot explanations improved averaged PLM accuracy on HealthFC by 0.060 over a no-explanation baseline, though they generally did not benefit LLMs used as classifiers. This work indicates a promising direction for AI practitioners to scalably augment NLP datasets with LLM-based explanations for PLM performance enhancement. |
| AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal
  Imitation-Exploration Balance (Read more on [arXiv](https://arxiv.org/abs/2508.06944) or [HuggingFace](https://huggingface.co/papers/2508.06944))| Yong Li, Jie Feng, Lixuan He | AMFT introduces a meta-gradient adaptive weight controller to dynamically balance Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in single-stage LLM fine-tuning, addressing catastrophic forgetting and suboptimal trade-offs. The methodology reframes SFT and RL as complementary reward signals, learning the optimal balance parameter (μ) through meta-gradients on a long-term validation objective, regularized by policy entropy. AMFT consistently achieved new state-of-the-art, with 61.3% average accuracy on in-distribution math benchmarks and 63.3% on out-of-distribution general reasoning benchmarks. Additionally, it demonstrated superior sample efficiency, requiring ~15,840 RL rollouts to reach a target performance on General Points OOD, compared to >21,760 for sequential SFT→RL. AMFT provides AI practitioners a principled, stable, and sample-efficient paradigm for LLM alignment, fostering robust generalization by autonomously learning an effective training curriculum. |
