

## Papers for 2025-08-14

| Title | Authors | Summary |
|-------|---------|---------|
| Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery (Read more on [arXiv](https://arxiv.org/abs/2508.08401) or [HuggingFace](https://huggingface.co/papers/2508.08401))| Di Zhang, Junxian Li, Qinggang Zhang, Weida Wang, Jiatong Li | Mol-R1 introduces a novel framework for explicit Long Chain-of-Thought (CoT) reasoning in text-based molecule generation. The primary objective is to efficiently generate high-quality, expert-aligned reasoning traces and effectively leverage them to train LLMs for molecule discovery. The methodology employs Prior Regulation via In-context Distillation (PRID) for cold-start data preparation and Molecular Iterative Adaptation (MoIA), which iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO). Mol-R1 (T=2) achieved an Exact Match (EM) score of 0.234 and a BLEU score of 0.641, significantly outperforming baseline DeepSeek-R1's EM of 0.156 and BLEU of 0.031. This framework demonstrates significant potential to enable more explainable and chemist-like reasoning, thereby enhancing the interpretability and reliability of LLM-generated molecular structures. |
| Stand-In: A Lightweight and Plug-and-Play Identity Control for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.07901) or [HuggingFace](https://huggingface.co/papers/2508.07901))| Chen Li, Hao Liu, Wenjing Wang, Qixin Yan, Bowen Xue | The paper introduces Stand-In, a lightweight and plug-and-play framework for adding high-fidelity identity control to pre-trained video generation models. The primary objective is to generate videos that consistently maintain a subject's identity from a single reference image, using a parameter-efficient method compatible with other generative AI tools. The methodology involves adding a conditional image branch to a pre-trained Diffusion Transformer (DiT) model, where identity is injected via restricted self-attention and a conditional position mapping scheme, training only ~1% of additional parameters using Low-Rank Adaptation (LoRA). The framework achieves a state-of-the-art Face Similarity score of 0.724 on the OpenS2V benchmark, outperforming other methods while using minimal trainable parameters (153M). The principal implication for AI practitioners is the ability to efficiently add robust identity control to large video models without costly full fine-tuning, enabling easier integration of identity preservation into applications like video stylization, pose-guided generation, and face swapping. |
| AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust
  GAIA Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2508.09889) or [HuggingFace](https://huggingface.co/papers/2508.09889))| Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie | This paper introduces a dynamic Multi-Agent System (MAS) that uses a "Guard Agent" for real-time supervision and maneuvering to enhance the robustness and accuracy of an "Execution Agent" on complex, tool-augmented tasks. The research aims to mitigate instability in agentic systems caused by noisy tool outputs and extended contexts by developing a dynamic verification mechanism. The methodology employs an Execution Agent based on Gemini 2.5 Pro to perform tasks, which can dynamically invoke a Guard Agent to provide oversight and correct logical reasoning impasses. On a 109-question subset of the GAIA benchmark, the proposed MAS achieved a 67.89% average pass@1 accuracy, representing an 8.82% improvement over a Single |
| Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing (Read more on [arXiv](https://arxiv.org/abs/2508.09192) or [HuggingFace](https://huggingface.co/papers/2508.09192))| Hao Zhang, Jiachun Jin, Yijie Jin, Chenkai Xu, Xu Wang | This paper presents Discrete Diffusion Forcing (D2F), a novel training and inference framework enabling Diffusion LLMs (dLLMs) to achieve inference speeds superior to their autoregressive (AR) counterparts. The primary objective is to overcome the inference latency of dLLMs by creating a hybrid AR-diffusion paradigm that supports both KV caching and inter-block parallel decoding. The methodology involves restructuring generation into a block-wise process and using asymmetric distillation to train a dLLM with causal attention to predict future blocks based on partially denoised preceding blocks. Empirically, a D2F-enhanced model achieves up to 2.5x faster inference throughput than LLaMA3-Instruct-8B on the GSM8K benchmark while maintaining comparable output quality. For AI practitioners, D2F provides a concrete methodology to significantly accelerate existing open-source dLLMs, making them a viable high-throughput alternative to AR models for latency-sensitive applications. |
| Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09987) or [HuggingFace](https://huggingface.co/papers/2508.09987))| Zhenghao Hu, Leqi Zhu, Zihao Wang, Dongzhi Jiang, Junyan Ye | This paper introduces Echo-4o, a generative model improved by fine-tuning on Echo-40-Image, a new 180K-sample synthetic dataset from GPT-4o designed to address rare scenarios and provide cleaner supervision than real-world data. The research objective is to demonstrate how synthetic data can enhance open-source models by providing training examples for surreal fantasies, multi-reference compositions, and complex instructions underrepresented in natural datasets. The core methodology involves generating the Echo-40-Image dataset using GPT-4o, fine-tuning the Bagel model to create Echo-4o, and proposing two new benchmarks (GenEval++ and Imagine-Bench) for more challenging evaluation. The primary result is that Echo-4o achieves a score of 0.679 on the complex GenEval++ benchmark, significantly outperforming the baseline Bagel model's score of 0.371. The principal implication for AI practitioners is that fine-tuning on curated synthetic datasets that target specific model weaknesses, like complex attribute composition and imaginative generation, is a highly effective strategy for improving the capabilities of open-source foundation models, with the Echo-40-Image dataset providing a transferable resource for this task. |
| Story2Board: A Training-Free Approach for Expressive Storyboard
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09983) or [HuggingFace](https://huggingface.co/papers/2508.09983))| Dani Lischinski, Dvir Samuel, Omri Avrahami, Matan Levy, David Dinkevich | The paper introduces Story2Board, a training-free framework for generating expressive and coherent multi-panel storyboards from natural language prompts. The objective is to produce visually diverse storyboards that maintain character identity while allowing for dynamic changes in composition, pose, and background, addressing limitations of prior work that often sacrifices expressiveness for consistency. The methodology combines two novel components applied during inference in pre-trained diffusion transformers: Latent Panel Anchoring (LPA), which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing (RAVM), which softly blends visual features between tokens with high mutual attention. On the authors' proposed Rich Storyboard Benchmark, Story2Board achieves a superior trade-off compared to baselines, dominating the Pareto front for Character Consistency (DreamSim) versus Prompt Alignment (VQAScore) and is preferred overall in a user study. The principal implication for AI practitioners is that expressive, coherent visual sequences can be generated from foundational text-to-image models without architectural changes or fine-tuning by leveraging and guiding the model's inherent self-attention mechanisms to enforce consistency while preserving its generative diversity. |
| Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory (Read more on [arXiv](https://arxiv.org/abs/2508.09736) or [HuggingFace](https://huggingface.co/papers/2508.09736))| Yuan Lin, Yiyuan Pan, Wentao Ye, Yichen He, Lin Long | M3-Agent is a novel multimodal agent framework incorporating long-term memory for continuous real-time multimodal perception, knowledge accumulation, and reasoning. Its primary objective is to enable agents to build entity-centric, multimodal episodic and semantic memories over time, thereby accumulating world knowledge to autonomously accomplish multi-turn, iterative reasoning tasks. The framework operates through parallel memorization and control processes, leveraging a structured multimodal graph memory and training with reinforcement learning. M3-Agent achieves higher accuracy on long-video question answering benchmarks, outperforming the strongest baseline, Gemini-GPT4o-Hybrid, by 6.7%, 7.7%, and 5.3% on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. This work advances multimodal agents towards more human-like long-term memory, providing practical design insights for AI practitioners to develop more robust and consistently understanding AI systems. |
| Learning to Align, Aligning to Learn: A Unified Approach for
  Self-Optimized Alignment (Read more on [arXiv](https://arxiv.org/abs/2508.07750) or [HuggingFace](https://huggingface.co/papers/2508.07750))| Lei Fan, Shuowen Zhang, Zhiling Ye, Yun Yue, Haowen Wang | This paper introduces Group Relative Alignment Optimization (GRAO), a novel, self-optimized alignment framework for large language models. The primary objective is to develop a unified alignment framework that combines the efficiency of supervised fine-tuning (SFT) with the exploratory power of reinforcement learning (RLHF) for adaptive and robust LLM alignment. GRAO's methodology leverages a group direct alignment loss and a principled three-component designâ€”imitation learning, advantage-weighted exploration, and alignment regularizationâ€”to orchestrate an "imitate-explore-transcend" optimization trajectory. Experimental results demonstrate GRAO's superior performance, achieving up to 57.70% improvement over SFT and 22.74% Normalized Alignment Gain (NAG) over GRPO, particularly on Mixture-of-Experts (MoE) architectures. This framework provides AI practitioners with a robust, scalable, and versatile alignment solution, establishing a new paradigm for developing more capable and aligned LLMs across diverse architectures. |
| MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math
  Reasoning in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.06009) or [HuggingFace](https://huggingface.co/papers/2508.06009))| Zhihan Zhou, Yue Guo, Zhentao Zhang, Zixin Wang, junfeng0288 | This paper introduces MATHREAL, a benchmark of 2,000 authentic K-12 math question images with real-world visual noise, to evaluate the robustness of Multimodal Large Language Models (MLLMs) in realistic scenarios. The objective is to quantify how the mathematical reasoning capabilities of state-of-the-art MLLMs are challenged by visually imperfect inputs, such as blurry or distorted photos of homework, which represent authentic user-generated content. The authors created a dataset of 2,000 photographed math problems, systematically annotating them across 14 categories of visual degradation and five knowledge domains, and then benchmarked over 40 MLLMs using strict and loose accuracy metrics under six distinct input conditions. The evaluation shows that most MLLMs struggle significantly with real-world images; the top model, Doubao-1.5-thinking-vision-pro, achieved a 53.9% loose accuracy, while GPT-4o only reached 23.0%, with error analysis showing combined visual perception errors (figure and OCR) account for 40-50% of failures. The principal implication for AI practitioners is that the significant performance gap between clean and noisy data underscores a critical need to incorporate more authentic, visually-degraded data into training pipelines to build MLLMs robust enough for real-world applications. |
| Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning
  for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.05613) or [HuggingFace](https://huggingface.co/papers/2508.05613))| Guiyang Hou, Xingyu Wu, Haitao Hong, tricktreat, yanyc | i) The paper introduces Cooper, a reinforcement learning framework that jointly optimizes an LLM policy model and its corresponding reward model to mitigate reward hacking. ii) The primary objective is to overcome the limitations of both rule-based rewards (lack of robustness) and static model-based rewards (vulnerability to reward hacking) in RL for LLMs. iii) The methodology involves a dual-optimization loop where the policy model is updated using Group Relative Policy Optimization (GRPO), and the reward model is simultaneously refined using contrastive learning on dynamically generated sample pairs; positive samples are identified by a high-precision rule-based verifier, and negative samples are generated by an assistant LLM. iv) Experiments show that using a static reward model leads to catastrophic failure, with performance on Qwen2.5-1.5B-Instruct dropping from 54.93% to 38.91% average accuracy, whereas the Cooper framework not only prevents this collapse but improves accuracy to 58.02%. v) The principal implication for AI practitioners is that treating reward models as static components in RL is fundamentally flawed and can lead to severe performance degradation; they should be considered dynamic components that are co-optimized with the policy to ensure stable and effective training. |
| IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.09456) or [HuggingFace](https://huggingface.co/papers/2508.09456))| Di Zhang, Beining Xu, Junxian Li | The paper presents IAG, a novel input-aware backdoor attack that manipulates a Vision-Language Model's visual grounding to an attacker-specified target by embedding semantic triggers into images. The research objective is to design and validate a backdoor attack that forces a Vision-Language Model (VLM) to ground an arbitrary, attacker-defined object within an image, irrespective of the user's language query, while maintaining performance on clean data. The methodology involves jointly fine-tuning a victim VLM and a text-conditional U-Net trigger generator; the U-Net embeds a semantic trigger corresponding to the attack target's description into the input image, guided by a combined objective of language model loss and a reconstruction loss for stealthiness. The IAG attack achieved a high Attack Success Rate (ASR@0.5), reaching 71.2% on the RefCoco+ (testA) dataset with the InternVL-2.5-8B model, while only causing a 1-3% decrease in accuracy on clean samples. The principal implication for AI practitioners is that the reliance of VLMs on their visual encoder is a critical vulnerability, necessitating robust input validation and defense mechanisms before deploying VLM agents in real-world applications like robotics or GUI interaction, as this attack vector can be weaponized. |
| Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.09968) or [HuggingFace](https://huggingface.co/papers/2508.09968))| Zeynep Akata, Nataniel Ruiz, Alexey Dosovitskiy, Shyamgopal Karthik, Luca Eyring | This paper presents Noise Hypernetworks, a method to amortize the computational cost of test-time optimization in diffusion models by learning to predict an optimal initial noise latent, improving generation quality with minimal inference overhead. The main research objective is to capture the benefits of computationally expensive, reward-guided test-time optimization and integrate them into a model via a one-time post-training process, thereby eliminating the high latency at inference. The key methodology involves training a lightweight "Noise Hypernetwork," implemented as a LoRA adapter on a frozen, step-distilled generator, to predict a perturbation to the initial noise. This network is trained with a noise-space objective that maximizes a reward function while using a tractable L2 penalty, approximating a KL divergence term, to maintain fidelity to the original noise distribution. The method demonstrates significant quality gains; for example, when applied to the SANA-Sprint model, it improves the mean GenEval score from 0.70 to 0.75, matching the performance of LLM-based prompt optimization while being 300x faster (0.3s vs 95.0s per image) and recovering over half the gains of the much slower ReNO method. The principal implication for AI practitioners is the ability to enhance fast, distilled generative models for specific objectives like prompt-following or aesthetics in a post-hoc manner, achieving high-quality, aligned outputs without modifying the base model or sacrificing the low-latency inference required for real-time applications. |
| VisCodex: Unified Multimodal Code Generation via Merging Vision and
  Coding Models (Read more on [arXiv](https://arxiv.org/abs/2508.09945) or [HuggingFace](https://huggingface.co/papers/2508.09945))| Dongdong Zhang, Yixia Li, Xun Wu, Shaohan Huang, Lingjie Jiang | i) This paper introduces VisCodex, a framework for unified multimodal code generation created by merging specialized vision and coding models. ii) The main objective is to overcome the limitations of current Multimodal Large Language Models (MLLMs) in generating functional code from combined visual and textual inputs. iii) The key methodology used is a task vector-based model merging technique that arithmetically combines the parameters of a vision-language model and a code-specialized LLM, supported by the introduction of the 598k-sample Multimodal Coding Dataset (MCD) and the InfiBench-V benchmark. iv) The primary result is that the VisCodex-33B model achieves an average score of 72.3 across four multimodal coding benchmarks, outperforming other open-source models and performing competitively with GPT-4 (73.3). v) The principal implication for AI practitioners is that model merging provides an efficient, parameter-effective strategy to imbue MLLMs with specialized capabilities like advanced coding as an alternative to costly full retraining, with the MCD dataset and InfiBench-V benchmark serving as new resources for development and evaluation. |
| Can LLM-Generated Textual Explanations Enhance Model Classification
  Performance? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.09776) or [HuggingFace](https://huggingface.co/papers/2508.09776))| Gjergji Kasneci, Zineb Attaoui, Ege Erdogan, Juraj Vladika, Mahdi Dhaini | This paper empirically investigates the impact of LLM-generated textual explanations on the classification performance of both fine-tuned PLMs and zero-shot LLMs for Natural Language Inference tasks. The primary objective is to determine how LLM-generated explanations, compared to human-annotated ones and a no-explanation baseline, affect the downstream predictive performance of various language models on two NLI datasets (e-SNLI, HealthFC). The study uses four LLMs to generate explanations, which are then used to either augment the training data for four fine-tuned PLMs (e.g., DeBERTa) or are appended to the prompts for three zero-shot LLMs (e.g., Llama3). The results show that while LLM-generated explanations consistently improve the performance of fine-tuned PLMs (e.g., Llama3-generated explanations increased Macro F1 by 0.119 on HealthFC), they often degrade the performance of zero-shot LLMs, with human explanations proving superior for LLMs on logic-based tasks (e-SNLI), improving accuracy by 20-30% where LLM-generated ones were detrimental. The principal implication for AI practitioners is that LLM-generated explanations are a scalable and effective method for augmenting training data to improve fine-tuned models, but their use in zero-shot inference prompts for LLMs can be counterproductive and requires careful, task-specific evaluation. |
| GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video
  Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2508.09667) or [HuggingFace](https://huggingface.co/papers/2508.09667))| Qingnan Fan, Ying Feng, Jiahao Chang, Qi Zhang, Xingyilang Yin | GSFixer is a novel framework improving 3D Gaussian Splatting (3DGS) representations for sparse-view 3D reconstruction and novel view synthesis. The main objective is to overcome artifacts and maintain visual and 3D consistency when reconstructing 3DGS from sparse input views. GSFixer's key methodology involves a reference-guided video restoration model, built upon a DiT-based video diffusion model, conditioned by both 2D semantic tokens (from DINOv2) and 3D geometric tokens (from VGGT) extracted from reference views, along with a reference-guided trajectory sampling strategy. In 3DGS artifact restoration, GSFixer achieves a PSNR improvement of 2.16, SSIM of 0.067, and LPIPS of 0.087 over GenFusion on the DL3DV-Res benchmark. This method provides a state-of-the-art solution for high-quality and consistent 3D reconstruction from limited data, enhancing the practical applicability of 3DGS in real-world scenarios. |
| AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal
  Imitation-Exploration Balance (Read more on [arXiv](https://arxiv.org/abs/2508.06944) or [HuggingFace](https://huggingface.co/papers/2508.06944))| Yong Li, Jie Feng, Lixuan He | AMFT introduces a novel single-stage meta-learning algorithm for LLM fine-tuning that dynamically balances Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). The paper aims to address the challenge of optimally balancing imitation (SFT) and exploration (RL) in LLM fine-tuning in a principled, dynamic, and forward-looking manner. AMFT reframes SFT and RL as optimizing complementary reward signals, utilizing a meta-gradient adaptive weight controller to learn an optimal training curriculum by dynamically adjusting a weighting parameter (Î¼) between SFT's implicit path-level reward and RL's explicit outcome-based reward, optimizing for a long-term validation objective. Experiments demonstrate AMFT's state-of-the-art performance; for mathematical reasoning, it achieved 61.3% average accuracy on in-distribution benchmarks and 63.3% on out-of-distribution benchmarks. This principled approach provides a more stable, sample-efficient, and robust LLM alignment paradigm, preventing catastrophic forgetting and improving generalization, making it highly relevant for AI practitioners in LLM development. |
