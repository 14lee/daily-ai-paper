

## Papers for 2025-08-18

| Title | Authors | Summary |
|-------|---------|---------|
| SSRL: Self-Search Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.10874) or [HuggingFace](https://huggingface.co/papers/2508.10874))| Yanxu Chen, Yuxin Zuo, Heng Zhou, Kaiyan Zhang, Yuchen Fan | This paper introduces Self-Search Reinforcement Learning (SSRL), a method for enhancing Large Language Models' (LLMs) intrinsic search capabilities for agentic tasks. The research aims to quantify LLMs' performance limits on search-based QA tasks using only internal knowledge and determine if full-sim RL enables effective sim-to-real transfer with real web search. The methodology involves Self-Search, which uses structured prompting and repeated sampling to elicit internal knowledge, and SSRL, which refines this through format-based and rule-based rewards in a fully simulated environment. Key results include Llama-3.1-8B-Instruct SSRL achieving an average EM score of 43.1% across benchmarks, outperforming ZeroSearch-Instruct (41.5%) and Search-R1-Instruct (37.1%), and SSRL demonstrating a 5.53x reduction in training time compared to ZeroSearch. This work implies that AI practitioners can leverage LLMs as cost-effective, implicit world models for scalable RL agent training, reducing dependency on costly external search engines and enabling robust sim-to-real transfer. |
| Thyme: Think Beyond Images (Read more on [arXiv](https://arxiv.org/abs/2508.11630) or [HuggingFace](https://huggingface.co/papers/2508.11630))| Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, Yi-Fan Zhang | This paper introduces Thyme, a framework enabling multimodal LLMs to autonomously perform image manipulations and computations by generating and executing code. The objective is to develop a model that can autonomously decide when and how to apply a wide range of image processing and computational operations via code to enhance its perception and reasoning. The methodology involves a two-stage training process: an initial Supervised Fine-Tuning (SFT) on a 500K-sample dataset, followed by a Reinforcement Learning (RL) phase using a GRPO-ATS algorithm that applies distinct sampling temperatures to text and code generation. The primary result is a significant performance improvement across multiple benchmarks; for instance, Thyme achieves a 55.2% overall score on MME-Real-Lite, an 11.1 percentage point increase over the baseline's 44.1%. For AI practitioners, the principal implication is that integrating autonomous code generation and execution via a secure sandbox is an effective method to enhance MLLM performance on challenging perception and reasoning tasks, offering an alternative to simply scaling model size. |
| DINOv3 (Read more on [arXiv](https://arxiv.org/abs/2508.10104) or [HuggingFace](https://huggingface.co/papers/2508.10104))| Maxime Oquab, Federico Baldassarre, Maximilian Seitzer, Huy V. Vo, Oriane Sim√©oni | DINOv3 is a family of self-supervised vision models scaled up to 7 billion parameters, which sets a new state-of-the-art on dense prediction tasks by introducing a novel training regularization technique. The main objective is to overcome the degradation of dense feature maps observed in large-scale self-supervised learning, thereby creating a versatile vision foundation model that excels across diverse tasks with a frozen backbone. The key methodology is a two-phase training process: an initial self-supervised phase using DINO and iBOT losses, followed by a refinement phase that introduces "Gram anchoring", a novel objective that regularizes the Gram matrix of patch features against an earlier version of the model to preserve patch-level consistency. The primary result is state-of-the-art performance on dense tasks; for instance, the 7B model with a frozen backbone and a linear probe achieves 55.9 mIoU on ADE20k semantic segmentation, significantly outperforming prior methods. The principal implication for AI practitioners is the ability to use DINOv3 models as powerful, off-the-shelf frozen backbones for building highly accurate computer vision systems (e.g., for segmentation or detection) by only training lightweight, task-specific decoders, thus reducing computational overhead and model complexity. |
| XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization (Read more on [arXiv](https://arxiv.org/abs/2508.10395) or [HuggingFace](https://huggingface.co/papers/2508.10395))| Rishabh Tiwari, Haocheng Xi, Minjae Lee, Coleman Hooper, Aditya Tomar | XQUANT reduces LLM inference memory footprint by quantizing and caching layer input activations (X) with on-the-fly KV cache rematerialization. The work aims to alleviate the memory bandwidth bottleneck in LLM inference by reducing KV cache memory requirements, leveraging increasing compute capabilities. XQUANT achieves this by quantizing and caching layer input activations (X) and rematerializing Keys and Values during inference, providing a 2x memory savings over direct KV caching. XQUANT-CL further exploits cross-layer similarity in X embeddings by compressing successive layer differences and for GQA models, applies offline SVD to project X into a lower-dimensional latent space. XQUANT-CL achieves up to 10x memory savings with 0.01 perplexity degradation and 12.5x memory savings with 0.1 perplexity degradation relative to the FP16 baseline, outperforming state-of-the-art KV cache quantization methods. For AI practitioners, this allows significant memory reduction and near-FP16 accuracy for LLM inference, addressing memory-bound limitations in compute-dominated hardware environments. |
| PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical
  Register Indexing (Read more on [arXiv](https://arxiv.org/abs/2508.11116) or [HuggingFace](https://huggingface.co/papers/2508.11116))| Xianpei Han, Yaojie Lu, Hongyu Lin, Xuanang Chen, lzq2021 | PaperRegister boosts flexible-grained paper search by transforming traditional abstract-based indexes into a hierarchical index tree. The main objective is to overcome existing paper search systems' inability to handle flexible-grained queries that require detailed information beyond abstracts. The methodology involves offline hierarchical indexing using a large language model (Qwen3-32B) for fine-grained content extraction and bottom-up aggregation, followed by online adaptive retrieval guided by a small-scale view recognizer trained with SFT and hierarchical-reward GRPO. PaperRegister achieved a recall@5 of 80.8 in F.g.Search-3 under DPR-based matching, demonstrating a 22.6 improvement over abstract-based indexing (58.2). This offers AI practitioners a robust solution for highly precise and detailed paper search, facilitating deeper research into specific configurations or operations with low online latency. |
| StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.11203) or [HuggingFace](https://huggingface.co/papers/2508.11203))| Junyong Noh, Kwan Yun, Seungmi Lee | StyleMM is a novel framework for generating stylized 3D Morphable Face Models (3DMMs) from text descriptions, addressing the need for maintained correspondence, disentangled control, and expressive stylization beyond realistic models. It fine-tunes pre-trained realistic 3DMM networks using text-guided image-to-image translation with a diffusion model, incorporating Explicit Attribute-preserving Stylization (EAS) via an Explicit Attribute-preserving Module (EAM) and a Consistent Displacement Loss (CDL) to preserve facial attributes and maintain identity diversity. Quantitative evaluations show StyleMM achieves higher identity-level facial diversity, with the "ours" method demonstrating a Face Diversity of 12.005 and a Style Score of 0.3136 in ablation studies, significantly outperforming ablated versions where, for example, without CDL, diversity dropped to 1.812. This enables AI practitioners to efficiently generate diverse and animatable stylized 3D avatars with explicit control over shape, expression, and texture parameters, bypassing the need for extensive stylized 3D datasets. |
| FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation (Read more on [arXiv](https://arxiv.org/abs/2508.11255) or [HuggingFace](https://huggingface.co/papers/2508.11255))| Mu Xu, Fan Jiang, MengChao Wang, wangqiang9 | FantasyTalking2 introduces Timestep-Layer Adaptive Preference Optimization (TLPO) for audio-driven portrait animation. The research addresses challenges in aligning generative models with fine-grained human preferences across motion naturalness, lip-sync accuracy, and visual quality due to conflicting objectives. Its methodology involves Talking-Critic, a multimodal reward model for curating the large-scale preference dataset Talking-NSQ, and TLPO, which decouples preferences into specialized expert LoRA modules fused adaptively across timesteps and network layers. TLPO achieves significant improvements over baselines, reaching a visual quality preference accuracy of 94.67% on its test dataset. This framework provides AI practitioners a robust solution for integrating diverse human preferences into diffusion models, enabling more expressive and photorealistic human animation. |
| TexVerse: A Universe of 3D Objects with High-Resolution Textures (Read more on [arXiv](https://arxiv.org/abs/2508.10868) or [HuggingFace](https://huggingface.co/papers/2508.10868))| Nan Cao, Rui Ma, Li Zhang, YiboZhang2001 | This paper introduces TexVerse, a large-scale 3D dataset created to address the lack of suitable data for training generative models on high-resolution texturing and Physically Based Rendering (PBR) material synthesis. The primary objective was to curate a high-quality, large-scale collection of 3D assets that overcomes the resolution limitations of existing datasets like Objaverse. The methodology involved filtering models from Sketchfab for texture resolutions of at least 1024 pixels and distributable licenses, collecting all resolution variants, preserving original files for rigged/animated models, and generating detailed annotations with GPT-5. The resulting dataset contains 858,669 unique 3D models, of which 158,518 incorporate PBR materials, and includes specialized subsets for skeletal rigs and animations. For AI practitioners, TexVerse provides a crucial resource for training and benchmarking high-fidelity generative models for tasks in texture synthesis, PBR material development, and animated 3D asset creation, which were previously constrained by data availability. |
| Controlling Multimodal LLMs via Reward-guided Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.11616) or [HuggingFace](https://huggingface.co/papers/2508.11616))| Michal Drozdzal, Adriana Romero-Soriano, Koustuv Sinha, Pierluca D'Oro, oscmansan | Multimodal Reward-Guided Decoding (MRGD) is presented as a novel method for inference-time control of Multimodal Large Language Models (MLLMs). The objective is to achieve fine-grained controllability over MLLM output precision, recall, and the trade-off between visual grounding quality and test-time compute. MRGD's key methodology involves building two multimodal reward models: r_hal for object hallucination reduction (trained on preference data) and r_rec for object recall (composed of pre-trained modules), which jointly guide a search-based decoding process. As a primary result, MRGD reduced instance-level object hallucinations (CHAIR_i) on the COCO benchmark by ~70%, from 15.05% with greedy decoding to 4.53%. This enables AI practitioners to dynamically adapt MLLM behavior by balancing object precision, recall, and computational expenditure based on specific task requirements and resource constraints. |
| X-Node: Self-Explanation is All We Need (Read more on [arXiv](https://arxiv.org/abs/2508.10461) or [HuggingFace](https://huggingface.co/papers/2508.10461))| Islem Rekik, prajit123 | X-Node introduces a novel self-explaining Graph Neural Network (GNN) framework that generates faithful, per-node explanations as part of the prediction process. The main objective is to overcome the opacity and trustworthiness issues of conventional GNNs by providing intrinsic, local, and verifiable explanations for individual node decisions, particularly in high-stakes clinical applications. X-Node's methodology involves each node constructing a structured context vector from its local topology, which a Reasoner MLP converts into an explanation vector. This explanation vector is then used to reconstruct the node's latent embedding for faithfulness, generate natural language explanations via an LLM, and is reinjected into the GNN's message-passing pipeline via a "text-injection" mechanism to guide learning. Empirical results on six image-derived graph datasets (e.g., MedMNIST, MorphoMNIST) demonstrate that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations, for instance, improving F1-score from 91.19% to 93.16% and sensitivity from 91.18% to 94.07% on OrganAMNIST. This framework enables AI practitioners to develop GNNs with enhanced interpretability and trustworthiness, crucial for sensitive domains like medical diagnosis, by integrating explanation generation directly into the model's training and prediction lifecycle. |
| SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via
  Class-Conditioned Image Translation (Read more on [arXiv](https://arxiv.org/abs/2508.06429) or [HuggingFace](https://huggingface.co/papers/2508.06429))| Paolo Soda, Loredana Zollo, Clemente Lauretti, Guido Manni | This paper introduces SPARSE, a novel GAN-based semi-supervised learning framework for medical image classification in low labeled-data regimes. It addresses the challenge of insufficient labeled data, particularly in few-shot settings (5-50 labeled samples per class), hindering deep learning effectiveness in medical imaging. SPARSE integrates a generator for class-conditioned image translation, a discriminator for authenticity and classification, and a dedicated classifier within a three-phase training schedule that includes ensemble-based pseudo-labeling. Comprehensive evaluation across eleven MedMNIST datasets demonstrated statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods; for instance, in the extreme 5-shot setting, the ensemble configuration (SPARSEens) achieved 66.22% average accuracy, compared to SGAN's 25.80%. This framework offers a practical solution for AI practitioners in medical imaging where annotation costs are prohibitive, enabling robust classification performance with minimal labeled data. |
| MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and
  Multispectral Earth Observation Data (Read more on [arXiv](https://arxiv.org/abs/2508.10894) or [HuggingFace](https://huggingface.co/papers/2508.10894))| Nicolas Gonthier, Anatol Garioud, Nina Lardiere, Michael Vaccaro, Antoine Labatie | MAESTRO adapts Masked Autoencoders for self-supervised learning on multimodal, multitemporal, and multispectral Earth Observation data. The research objective is to efficiently adapt MAE to EO data's unique characteristics to learn useful and versatile representations, addressing fusion strategies and reconstruction target normalization. Key methodology involves token-based early fusion across time steps and similar modalities, late fusion for dissimilar modalities, and a novel patch-group-wise target normalization for multispectral joint-token fusion. MAESTRO outperforms prior SOTA by +2.7% (weighted F1) on TreeSatAI-TS and +2.5% (mIoU) on PASTIS-HD, demonstrating that patch-group-wise normalization with joint-token fusion achieves similar performance to token-based fusion at negligible computational overhead. A principal implication for AI practitioners is that early multitemporal fusion combined with a tailored multispectral target normalization scheme is crucial for achieving state-of-the-art performance on EO tasks, especially those relying on temporal dynamics, while significantly improving computational efficiency. |
