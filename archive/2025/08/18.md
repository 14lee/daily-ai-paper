

## Papers for 2025-08-18

| Title | Authors | Summary |
|-------|---------|---------|
| SSRL: Self-Search Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.10874) or [HuggingFace](https://huggingface.co/papers/2508.10874))| Yanxu Chen, Yuxin Zuo, Heng Zhou, Kaiyan Zhang, Yuchen Fan | This paper introduces Self-Search Reinforcement Learning (SSRL), a method for training large language models (LLMs) to answer complex queries by iteratively querying their own internal knowledge in a simulated search environment. The primary objective is to quantify the intrinsic search capabilities of LLMs and determine if RL training in this fully simulated ("full-sim") setting enables effective sim-to-real transfer to external search engines. The methodology involves using format-based and outcome-based rewards to train a policy model, which serves as both the agent and the environment, to autoregressively generate search queries and corresponding informational responses. SSRL demonstrates superior performance over API-dependent baselines; for example, the SSRL-trained Llama-3.1-8B-Instruct model achieved a 43.1% average accuracy across six benchmarks, outperforming ZeroSearch's 41.5%. For AI practitioners, this presents a cost-effective paradigm for training search agents by eliminating the need for expensive API calls during the RL training phase, creating models that can then be deployed with real-world search engines at inference time. |
| Thyme: Think Beyond Images (Read more on [arXiv](https://arxiv.org/abs/2508.11630) or [HuggingFace](https://huggingface.co/papers/2508.11630))| Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, Yi-Fan Zhang | This paper introduces Thyme, a framework enabling Multimodal Large Language Models (MLLMs) to autonomously generate and execute code for dynamic image manipulation and computation to solve complex visuo-linguistic tasks. The primary objective is to equip MLLMs with the capability to perform on-the-fly image processing (e.g., cropping, rotation, contrast enhancement) and mathematical calculations via a code-generation and sandbox-execution loop, moving beyond static visual perception. The approach utilizes a two-stage training process, starting with Supervised Fine-Tuning (SFT) on a 500K-sample dataset, followed by a Reinforcement Learning (RL) phase that employs the proposed GRPO-ATS algorithm, which uses adaptive sampling temperatures (τ=0 for code, τ=1 for text) to balance execution precision with reasoning exploration. Comprehensive evaluations show that Thyme significantly outperforms its baseline, improving reasoning performance on the MME-Realworld Autonomous Driving benchmark by 81.57% and increasing overall accuracy on HRBench-8K from 65.3% to 72.0%. For AI practitioners, this SFT-RL framework demonstrates that integrating a code-execution sandbox allows MLLMs to actively manipulate visual inputs as tools during their reasoning process, proving highly effective for tasks requiring detailed analysis of high-resolution or perceptually challenging images. |
| DINOv3 (Read more on [arXiv](https://arxiv.org/abs/2508.10104) or [HuggingFace](https://huggingface.co/papers/2508.10104))| Maxime Oquab, Federico Baldassarre, Maximilian Seitzer, Huy V. Vo, Oriane Siméoni | DINOv3 is a self-supervised vision foundation model that significantly advances dense feature quality and task versatility. The main objective is to address dense feature map degradation during large-scale SSL training and provide a robust, off-the-shelf universal visual encoder family. This is achieved through extensive data and model scaling, introducing a novel Gram anchoring strategy for maintaining patch-level consistency, and post-hoc high-resolution adaptation and knowledge distillation. DINOv3 (ViT-7B/16) demonstrates superior performance on various dense tasks, notably achieving 55.9 mIoU on ADE20k semantic segmentation and 64.4 recall for 3D geometric correspondence on NAVI, significantly surpassing previous self-supervised and weakly-supervised models. AI practitioners can leverage DINOv3 as a versatile, pre-trained backbone that delivers state-of-the-art results across diverse computer vision applications, often without fine-tuning, thereby enabling scalable solutions for resource-constrained environments. |
| XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization (Read more on [arXiv](https://arxiv.org/abs/2508.10395) or [HuggingFace](https://huggingface.co/papers/2508.10395))| Rishabh Tiwari, Haocheng Xi, Minjae Lee, Coleman Hooper, Aditya Tomar | XQUANT reduces LLM inference memory consumption by quantizing and caching layer input activations (X) and rematerializing the KV cache on-the-fly, trading computation for memory bandwidth. The objective is to develop a method to drastically reduce the memory footprint of the LLM KV cache to alleviate the memory bandwidth bottleneck during inference by exploiting the growing gap between compute performance and memory bandwidth. The core method involves quantizing and caching the layer input activations (X) instead of the Key and Value tensors, which are then recomputed from the cached X during each generation step; an advanced variant, XQUANT-CL, further compresses the cache by quantizing the differences in X between successive layers. The primary result shows that XQUANT-CL achieves up to 12.5x memory savings relative to an FP16 baseline with only 0.1 perplexity degradation on Llama-2-7B, outperforming state-of-the-art KV cache quantization methods using only simple uniform quantization. For AI practitioners, this rematerialization approach allows deploying LLMs in memory-constrained environments or significantly increasing batch sizes and context lengths on existing hardware by converting a memory-bound problem into a compute-bound one. |
| PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical
  Register Indexing (Read more on [arXiv](https://arxiv.org/abs/2508.11116) or [HuggingFace](https://huggingface.co/papers/2508.11116))| Xianpei Han, Yaojie Lu, Hongyu Lin, Xuanang Chen, lzq2021 | PaperRegister enhances flexible-grained paper search through hierarchical register indexing and adaptive retrieval. The primary objective is to enable paper search systems to handle queries across varying granularities, moving beyond traditional coarse-grained methods. Its methodology involves offline construction of a hierarchical index tree using large language models for fine-grained content extraction and bottom-up aggregation, coupled with online adaptive retrieval via a view recognizer trained with hierarchical reward policy optimization. Quantitatively, PaperRegister significantly improves Recall@5 by 22.3 percentage points on the challenging FG.Search-3 dataset (BM25-based matching), from 58.5 for abstract-based indexing to 80.8. This work provides AI practitioners with a robust framework to develop more powerful and adaptable information retrieval systems, capable of addressing complex, multi-granularity search requirements in specialized domains. |
| StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.11203) or [HuggingFace](https://huggingface.co/papers/2508.11203))| Junyong Noh, Kwan Yun, Seungmi Lee | StyleMM is a novel framework for constructing stylized 3D Morphable Face Models (3DMMs) using text-driven aligned image translation. Its primary objective is to generate stylized 3DMMs that reflect user-defined text prompts, ensuring maintained correspondence, disentangled control over facial attributes, and expressive stylization beyond realistic models. The methodology involves fine-tuning pre-trained mesh deformation and texture generator networks with stylized facial images, generated via text-guided image-to-image translation using a diffusion model (SDXL) and an Explicit Attribute-preserving Module (EAM) that preserves facial attributes. Quantitative evaluations show StyleMM achieves higher face diversity and style scores across various styles; for instance, it achieved a Face Diversity of 12.070 for "Pixar child" style, outperforming baselines like LeGO (9.836). StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, providing consistent 3D style transfer for applications in digital content production. |
| FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation (Read more on [arXiv](https://arxiv.org/abs/2508.11255) or [HuggingFace](https://huggingface.co/papers/2508.11255))| Mu Xu, Fan Jiang, MengChao Wang, wangqiang9 | FantasyTalking2 introduces a novel preference optimization framework, TLPO, for enhanced audio-driven portrait animation. The primary objective is to align diffusion-based portrait animation models with fine-grained, multidimensional human preferences across motion naturalness, lip-sync accuracy, and visual quality, addressing inherent conflicts between these objectives. The methodology involves Talking-Critic, a multimodal reward model, to curate Talking-NSQ, a large-scale preference dataset, and Timestep-Layer adaptive multi-expert Preference Optimization (TLPO) which decouples preferences into specialized LoRA expert modules fused dynamically across timesteps and network layers. Experiments show TLPO achieves state-of-the-art results, with user studies indicating relative improvements of 12.7% in lip synchronization, 15.0% in motion naturalness, and 13.7% in visual quality over the strongest baseline. For AI practitioners, this demonstrates that a granular, adaptive preference fusion strategy is crucial for achieving high-quality, human-aligned outputs in generative AI without performance trade-offs across competing objectives. |
| TexVerse: A Universe of 3D Objects with High-Resolution Textures (Read more on [arXiv](https://arxiv.org/abs/2508.10868) or [HuggingFace](https://huggingface.co/papers/2508.10868))| Nan Cao, Rui Ma, Li Zhang, YiboZhang2001 | TexVerse is a large-scale 3D asset dataset featuring high-resolution textures. This dataset aims to address the critical gap in suitable datasets for end-to-end high-resolution texture and PBR material generation. The methodology involved curating models from Sketchfab, filtering for texture resolutions of at least 1024 pixels, and acquiring original user-uploaded file formats for rigged and animated models, complemented by 856,312 GPT-5 generated annotations. TexVerse comprises 858,669 unique high-resolution 3D models and 1,659,097 total 3D instances, with 158,518 models incorporating PBR materials. This resource directly enables advancements in high-resolution texture generation, PBR material synthesis, animation, and diverse 3D vision and graphics applications for AI practitioners. |
| Controlling Multimodal LLMs via Reward-guided Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.11616) or [HuggingFace](https://huggingface.co/papers/2508.11616))| Michal Drozdzal, Adriana Romero-Soriano, Koustuv Sinha, Pierluca D'Oro, oscmansan | This paper introduces Multimodal Reward-Guided Decoding (MRGD) for inference-time control of Multimodal Large Language Models (MLLMs) to improve visual grounding. The objective is to achieve on-the-fly controllability of MLLM inference, enabling dynamic trade-offs between object precision and recall, and between test-time compute and visual grounding quality. MRGD employs two multimodal reward models: `r_hal` for object hallucination (trained on preference data) and `r_rec` for object recall (composed from pre-trained modules). These are linearly combined with a user-defined weight to guide a search-based decoding process. Evaluations show MRGD consistently outperforms existing hallucination mitigation methods; for example, on LLaVA-1.5, MRGD with `w=1.0` reduced instance-level hallucination (CHAIR_i) on COCO from 15.05% (greedy) to 4.53%. This provides AI practitioners with fine-grained inference-time control over MLLM outputs, facilitating adaptive behavior for diverse application needs and resource constraints while effectively mitigating hallucinations. |
| X-Node: Self-Explanation is All We Need (Read more on [arXiv](https://arxiv.org/abs/2508.10461) or [HuggingFace](https://huggingface.co/papers/2508.10461))| Islem Rekik, prajit123 | X-Node is a novel self-explaining Graph Neural Network (GNN) framework where each node intrinsically generates explanations during the prediction process. The primary objective is to overcome limitations of post-hoc GNN explainability by enabling faithful, intrinsic, node-level reasoning within the model. X-Node constructs a structured context vector for each node, which a Reasoner maps to an explanation vector used for latent embedding reconstruction, natural language explanation via an LLM, and reinjection into the GNN's message-passing pipeline. The framework consistently improves classification performance; for instance, it raised the F1 score for GCN on the OrganAMNIST dataset from 91.19% to 93.16%. This provides AI practitioners with a modular and transferable solution to integrate intrinsic, faithful explainability into GNNs, crucial for developing trustworthy AI systems in high-stakes applications. |
| SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via
  Class-Conditioned Image Translation (Read more on [arXiv](https://arxiv.org/abs/2508.06429) or [HuggingFace](https://huggingface.co/papers/2508.06429))| Paolo Soda, Loredana Zollo, Clemente Lauretti, Guido Manni | This paper introduces SPARSE, a novel GAN-based semi-supervised learning framework for medical image classification in extremely low-data regimes. The objective is to achieve robust classification performance when labeled data is scarce (5 to 50 samples per class) by leveraging abundant unlabeled images. The methodology uses a three-player architecture—a generator for class-conditioned image translation, a discriminator, and a dedicated classifier—trained via a dynamic schedule that alternates between supervised and unsupervised phases, employing an ensemble-based temporal pseudo-labeling technique. The framework demonstrates statistically significant improvements over six state-of-the-art methods across eleven MedMNIST datasets, with the ensemble version achieving 66.22% average accuracy in the extreme 5-shot setting. For AI practitioners, this approach provides a practical solution for building high-performing classifiers in domains like medical imaging where data annotation is prohibitively expensive. |
| MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and
  Multispectral Earth Observation Data (Read more on [arXiv](https://arxiv.org/abs/2508.10894) or [HuggingFace](https://huggingface.co/papers/2508.10894))| Nicolas Gonthier, Anatol Garioud, Nina Lardiere, Michael Vaccaro, Antoine Labatie | MAESTRO is a Masked Autoencoder framework adapted for complex Earth Observation data that sets a new state-of-the-art by optimizing fusion and normalization strategies. The research objective is to adapt the Masked Autoencoder (MAE) to effectively learn representations from multimodal, multitemporal, and multispectral Earth Observation (EO) data by systematically evaluating data fusion and reconstruction target strategies. The key methodology involves benchmarking five token-based fusion modes (e.g., early vs. late) and introducing a novel "patch-group-wise" normalization scheme that groups spectrally correlated bands during reconstruction to inject a spectral prior into an efficient joint-token architecture. MAESTRO establishes new state-of-the-art performance on tasks reliant on temporal dynamics, outperforming prior models by +2.7% weighted F1 score on the TreeSatAI-TS dataset, demonstrating the superiority of its early temporal fusion strategy over the late fusion used by existing foundation models. For AI practitioners, the principal implication is that for multi-sensor time-series data, performance is maximized by employing early fusion for temporal steps and similar modalities while using separate parameters for dissimilar modalities, and that patch-group-wise normalization offers a computationally efficient method to improve multispectral representation learning. |
