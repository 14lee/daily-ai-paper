

## Papers for 2025-08-08

| Title | Authors | Summary |
|-------|---------|---------|
| On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification (Read more on [arXiv](https://arxiv.org/abs/2508.05629) or [HuggingFace](https://huggingface.co/papers/2508.05629))| Xinyu Ye, Yingzhe Peng, Zhou Ziheng, Yizhou Zhou, Yongliang Wu | This paper presents Dynamic Fine-Tuning (DFT), a simple modification to Supervised Fine-Tuning (SFT) that improves model generalization by dynamically re-weighting the training objective. The primary objective is to address the poor generalization of SFT compared to reinforcement learning (RL) by identifying and rectifying the problematic implicit reward structure within the SFT gradient. The methodology involves a theoretical analysis equating the SFT gradient to a policy gradient with an ill-posed, inverse-probability-weighted reward, which is then corrected by multiplying the SFT loss with the model's token probability. In experiments, DFT significantly outperformed SFT; for example, fine-tuning the Qwen2.5-Math-1.5B model with DFT resulted in an average performance gain of +15.66 points, over 5.9 times the improvement from standard SFT. The principal implication for AI practitioners is that a single-line code change can substantially enhance SFT performance and generalization, offering a more robust and efficient alternative without requiring complex RL pipelines or additional reward models. |
| R-Zero: Self-Evolving Reasoning LLM from Zero Data (Read more on [arXiv](https://arxiv.org/abs/2508.05004) or [HuggingFace](https://huggingface.co/papers/2508.05004))| Zongxia Li, Hongming Zhang, Xiaoyang Wang, Wenhao Yu, Chengsong Huang | The paper introduces R-Zero, a fully autonomous framework that improves an LLM's reasoning capabilities from zero initial data by having a "Challenger" model and a "Solver" model co-evolve to generate their own training curriculum. The primary objective is to overcome the bottleneck of human-curated data by creating a self-improving system where the Challenger is rewarded for proposing tasks at the edge of the Solver's ability, and the Solver is rewarded for solving them. The methodology employs a co-evolutionary loop using Group Relative Policy Optimization (GRPO), where the Challenger's reward is based on the Solver's uncertainty (measured via self-consistency), and the Solver is fine-tuned on a filtered set of challenging questions using its own majority-voted pseudo-labels. This approach substantially improved reasoning, boosting the Qwen3-4B-Base model's performance by +6.49 on math reasoning benchmarks and +7.54 on general-domain reasoning benchmarks. For AI practitioners, R-Zero provides a powerful, data-free method to enhance base models in verifiable domains like mathematics, serving as a superior initialization checkpoint for subsequent supervised fine-tuning. |
| DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.05405) or [HuggingFace](https://huggingface.co/papers/2508.05405))| Ziming Wang, Börje F. Karlsson, Ye Wang, Pi Bu, Xinrun Xu | This paper introduces DeepPHY, a benchmark suite of six physics-based environments, to evaluate the interactive physical reasoning capabilities of agentic Vision-Language Models (VLMs). The primary objective is to systematically assess whether current VLMs can understand and reason about physical principles to perform precise, multi-step planning in dynamic, interactive environments. The methodology involves unifying six simulators (e.g., PHYRE, Kinetix, Angry Birds) into a testbed with structured action spaces, and then evaluating 17 state-of-the-art VLMs in a zero-shot, trial-based setting using Vision-Language-Action (VLA) and World Model (WM) prompt formats. The results show significant performance gaps; for instance, the best-performing model on the PHYRE task achieved only a 23.1% success rate after ten attempts, and analysis of the Pooltool task revealed that high success rates were misleadingly achieved through "brute-force heuristics" rather than genuine physical understanding. The principal implication for AI practitioners is that there is a fundamental disconnect between a model's ability to describe a physical phenomenon and its ability to use that knowledge for precise, predictive control, indicating that current agentic VLMs are not yet capable of robust interactive physical reasoning. |
| Genie Envisioner: A Unified World Foundation Platform for Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.05635) or [HuggingFace](https://huggingface.co/papers/2508.05635))| Shengcong Chen, Donglin Yang, Siyuan Huang, Pengfei Zhou, Yue Liao | Genie Envisioner is a unified world foundation platform that integrates policy learning, simulation, and evaluation for robotic manipulation into a single video-generative framework. The primary objective is to develop a scalable and integrated system that overcomes the fragmentation of traditional robotics pipelines by unifying sensing, policy learning, and evaluation within a single, closed-loop, video-generative world model. The methodology centers on GE-Base, an instruction-conditioned, multi-view video diffusion model trained on approximately 3,000 hours of real-world robotic data, which is paired with GE-Act, a lightweight flow-matching action decoder for policy inference, and GE-Sim, an action-conditioned neural simulator. The platform demonstrates strong cross-embodiment generalization; with only one hour of adaptation data on a novel robot, GE-Act achieved an end-to-end success rate of approximately 50% on a complex "fold cardboard box" task, where baseline models completely failed. The principal implication for AI practitioners is that a vision-centric, generative world model approach can serve as a unified foundation for building general-purpose robots, enabling more efficient policy learning and adaptation to new embodiments with minimal task-specific data compared to traditional, disjointed pipelines. |
| Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity (Read more on [arXiv](https://arxiv.org/abs/2508.05609) or [HuggingFace](https://huggingface.co/papers/2508.05609))| Zhibing Li, Tong Wu, Ziyang Chu, Long Zhuo, Yuhan Zhang | This paper introduces Hi3DEval, a hierarchical framework that evaluates 3D generative models at object, part, and material levels using a novel benchmark and a hybrid automated scoring system. To overcome the limitations of existing 2D-based metrics, the authors developed a hybrid scoring system—using video for object/material assessment and 3D features for part-level geometry—and constructed the Hi3DBench dataset annotated via a multi-agent MLLM pipeline. The system demonstrates superior human alignment, achieving a pairwise rating accuracy of 0.774 on object-level geometry plausibility for text-to-3D, significantly outperforming prior methods like GPTEval3D (0.690). AI practitioners can utilize Hi3DEval for more robust, scalable, and fine-grained automated evaluation, enabling detailed failure analysis and more accurate comparison of 3D generative models. |
| Are We on the Right Way for Assessing Document Retrieval-Augmented
  Generation? (Read more on [arXiv](https://arxiv.org/abs/2508.03644) or [HuggingFace](https://huggingface.co/papers/2508.03644))| Junjie Yang, Dongping Chen, Yaochen Wang, Mingjia Wang, Wenxuan Shen | The paper introduces DOUBLE-BENCH, a large-scale, multimodal, and multilingual benchmark designed to comprehensively evaluate document Retrieval-Augmented Generation (RAG) systems by addressing the flaws in existing benchmarks. Its primary objective is to create a more realistic and fine-grained evaluation framework that overcomes the limitations of current benchmarks, such as limited scope, unrealistic prior knowledge assumptions, and ambiguous queries. The methodology involves a three-stage pipeline to construct the benchmark from 3,276 documents, generating 5,168 single- and multi-hop queries using an iterative, LLM-driven refinement process with knowledge graphs, followed by human verification of all evidence labels. Experiments reveal an "over-confidence dilemma," where advanced RAG frameworks attempt to answer nearly every query regardless of retrieval success, and show that a simple baseline using a strong retriever (`colqwen2.5-3b-multilingual` with 0.795 average hit@5) and a generator matches the performance of complex agentic frameworks. The principal implication for AI practitioners is that the retrieval stage remains the critical bottleneck; therefore, development efforts should prioritize improving retrieval models and implementing mechanisms for systems to refuse to answer when evidence is insufficient, rather than solely focusing on more complex generation agents. |
| Are Today's LLMs Ready to Explain Well-Being Concepts? (Read more on [arXiv](https://arxiv.org/abs/2508.03990) or [HuggingFace](https://huggingface.co/papers/2508.03990))| Huan Liu, Chengshuai Zhao, Zhen Tan, Dawei Li, Bohan Jiang | This research systematically evaluates and improves the capability of Large Language Models (LLMs) to explain well-being concepts for diverse audiences. The paper's central research question is whether today's LLMs are ready to explain complex well-being concepts accurately and in a tailored manner. The methodology involves creating a large-scale dataset of 43,880 explanations from 10 LLMs, introducing a principle-guided LLM-as-a-judge evaluation framework, and fine-tuning an open-source model using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Results show that while larger models outperform smaller ones, they exhibit shared weaknesses in providing utility and depth; crucially, fine-tuning a 4B parameter model with DPO improved its win rate for domain expert explanations to 83.4%, surpassing a larger 14B parameter baseline model. The key implication for AI practitioners is that using curated preference data to fine-tune smaller models with DPO is a highly effective strategy for developing specialized models that can outperform larger, general-purpose models on domain-specific tasks. |
| Can Large Multimodal Models Actively Recognize Faulty Inputs? A
  Systematic Evaluation Framework of Their Input Scrutiny Ability (Read more on [arXiv](https://arxiv.org/abs/2508.04017) or [HuggingFace](https://huggingface.co/papers/2508.04017))| Yuan Wu, Yi Chang, Gengxu Li, Jinzhe Li, Haiqi Yang | This paper introduces the ISEval framework to systematically evaluate the ability of large multimodal models (LMMs) to autonomously detect faulty inputs, revealing a significant gap between their latent and spontaneously activated critique capabilities. The primary research objective is to determine if LMMs can actively recognize and scrutinize erroneous multimodal inputs without explicit instructions, rather than passively accepting them. The methodology involves the Input Scrutiny Ability Evaluation Framework (ISEval), which uses a dataset of inputs containing seven distinct error categories and evaluates models based on Spontaneous Error Detection Rate (SEDR), Guided Error Detection Rate (GEDR), and Modality Trust Preference Score (MTPS). The primary result shows that LMMs have very limited autonomous scrutiny ability, with the top-performing model, Gemini 2.5 Pro, achieving an average SEDR of only 21.95%, whereas its performance increased to a 57.72% GEDR when explicitly prompted to check for errors. The principal implication for AI practitioners is that current LMMs cannot be trusted to proactively validate inputs, and building reliable systems requires incorporating explicit prompts to activate their latent critique functions, as they do not apply this scrutiny autonomously. |
| InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs
  to Enhance Reasoning Capabilities (Read more on [arXiv](https://arxiv.org/abs/2508.05496) or [HuggingFace](https://huggingface.co/papers/2508.05496))| Zhijie Sang, Kejing Yang, Qi Zhou, Su Lu, Shuo Cai | InfiAlign is a scalable and sample-efficient post-training framework integrating Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to enhance LLM reasoning. The primary objective is to develop an automated and scalable framework for aligning LLMs that improves reasoning capabilities while drastically reducing the amount of required training data. The methodology centers on a multi-dimensional data selection pipeline that curates a compact, high-quality dataset from large open-source corpora by sampling for diversity, difficulty (using response length as a proxy), and quality, followed by a two-stage curriculum SFT and a DPO phase. When applied to the Qwen2.5-Math-7B-Base model, the InfiAlign SFT model achieved comparable performance to the DeepSeek-R1-Distill-Qwen-7B baseline while using only 12% of its training data (92K vs. 800K samples). This work implies that AI practitioners can achieve strong reasoning alignment with substantially lower data and computational overhead by implementing principled, automated data curation pipelines, offering a more efficient alternative to large-scale data distillation or manual curation. |
| Evaluating, Synthesizing, and Enhancing for Customer Support
  Conversation (Read more on [arXiv](https://arxiv.org/abs/2508.04423) or [HuggingFace](https://huggingface.co/papers/2508.04423))| Feng Chen, Lifan Guo, Junhui Li, Huaixia Dou, Jie Zhu | This paper introduces a structured framework and datasets to enhance LLM performance for customer support conversations. The objective is to train and evaluate LLMs to generate high-quality, empathetic, and strategically-aligned responses in customer support scenarios. The methodology involves creating the Customer Support Conversation (CSC) framework based on COPC guidelines, constructing the `CSConv` evaluation dataset by rewriting 1,855 real-world dialogues with an LLM, and generating the `RoleCS` training dataset via a five-agent role-playing simulation. The primary result shows that fine-tuning a 72B Qwen2.5-Instruct model on `RoleCS` significantly improves performance on `CSConv`, increasing the ROUGE-L score from 5.41 to 7.97 and strategy prediction accuracy from 37.22% to 43.29%. For AI practitioners, the principal implication is that the proposed role-playing framework can be used to generate high-quality synthetic data for fine-tuning LLMs, enabling the development of more effective and structured conversational agents for customer service applications. |
| Don't Overthink It: A Survey of Efficient R1-style Large Reasoning
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.02120) or [HuggingFace](https://huggingface.co/papers/2508.02120))| Fangzhou Yao, Weibo Gao, Yizhi Wang, Yichao Du, Linan Yue | This paper surveys and taxonomizes recent methods for mitigating the "overthinking" problem in R1-style Large Reasoning Models (LRMs) to improve their computational efficiency. The objective is to systematically review and categorize techniques designed to reduce the length and redundancy of reasoning chains in LRMs without compromising performance. The authors introduce a novel framework that classifies efficient reasoning methods into two primary paradigms: single-model optimization (e.g., CoT Compression, Adaptive Reasoning) and multi-model collaboration (e.g., LLM Routing, Speculative Decoding). The survey highlights that various techniques yield significant efficiency gains; for instance, it cites that model merging strategies can reduce inference length by up to 55% in average response length while preserving output quality. For AI practitioners, this taxonomy provides a structured guide for selecting and implementing strategies, such as early-exit mechanisms or multi-model routing, to optimize the inference cost and latency of deployed reasoning models exhibiting inefficient, lengthy thought processes. |
| MOSEv2: A More Challenging Dataset for Video Object Segmentation in
  Complex Scenes (Read more on [arXiv](https://arxiv.org/abs/2508.05630) or [HuggingFace](https://huggingface.co/papers/2508.05630))| Xudong Jiang, Shuting He, Chang Liu, Kaining Ying, Henghui Ding | This paper introduces MOSEv2, a large-scale video object segmentation dataset designed to challenge models with complex, realistic scenarios underrepresented in existing benchmarks. The objective is to advance video object segmentation (VOS) toward real-world applicability by creating a benchmark featuring frequent object disappearance, severe occlusions, adverse weather, and knowledge-dependent scenes. The methodology involved curating 5,024 videos and 701,976 instance masks based on strict complexity criteria, then benchmarking 20 VOS and 9 VOT methods to establish performance baselines. The results demonstrate a significant performance degradation for state-of-the-art models; for instance, the SAM2 model's J&F score drops from 76.4% on the MOSEv1 predecessor to 50.9% on MOSEv2. For AI practitioners, MOSEv2 serves as a critical benchmark to test and develop models that are robust to long-term temporal reasoning and semantic ambiguity, exposing failure points not apparent in previous datasets. |
| CoAct-1: Computer-using Agents with Coding as Actions (Read more on [arXiv](https://arxiv.org/abs/2508.03923) or [HuggingFace](https://huggingface.co/papers/2508.03923))| Taiwei Shi, Jieyu Zhang, Viraj Prabhu, Yutong Dai, Linxin Song | CoAct-1 is a multi-agent system that enhances computer automation by dynamically delegating tasks to either a traditional GUI operator or a programmer agent that executes code. The primary objective is to improve the efficiency, reliability, and success rate of autonomous computer-using agents on complex, long-horizon tasks by augmenting standard GUI manipulation with the ability to perform actions via programmatic script execution. The paper introduces CoAct-1, a system featuring an Orchestrator that decomposes a user's goal into subtasks and dynamically delegates them to either a VLM-based GUI Operator for visual interactions or a Programmer agent that writes and executes Python or Bash scripts. On the OSWorld benchmark, CoAct-1 achieved a state-of-the-art success rate of 60.76%, while reducing the average number of steps for successful tasks to 10.15, compared to 15.22 for the leading GUI-only agent GTA-1. For AI practitioners developing autonomous agents, the principal implication is that integrating a programmatic action space alongside a GUI-based one can significantly boost performance and efficiency, particularly for tasks involving file operations and data processing, by bypassing brittle and lengthy UI sequences. |
| Marco-Voice Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.02038) or [HuggingFace](https://huggingface.co/papers/2508.02038))| Qingjuan Li, Haoqin Sun, Xuanfan Ni, Chenyang Lyu, Fengping Tian | The paper presents Marco-Voice, a unified text-to-speech system for high-fidelity voice cloning and controllable emotional speech synthesis. The objective is to create a single framework that generates natural, expressive speech while preserving speaker identity across diverse emotional contexts by overcoming timbre-style entanglement. The methodology combines a speaker-emotion disentanglement mechanism using a cross-orthogonal loss and in-batch contrastive learning with a rotational emotion embedding method derived from paired neutral-emotional speech. Marco-Voice achieves a speaker similarity score of 0.8275 in human evaluations, significantly outperforming the CosyVoice2 baseline (0.605) and showing superior emotional expression. The principal implication for practitioners is that applying explicit disentanglement techniques like cross-orthogonal constraints within a unified architecture enables the development of more robust and controllable personalized speech synthesis systems. |
| StrandDesigner: Towards Practical Strand Generation with Sketch Guidance (Read more on [arXiv](https://arxiv.org/abs/2508.01650) or [HuggingFace](https://huggingface.co/papers/2508.01650))| Xiaobin Hu, Han Feng, Chengming Xu, Moran Li, Na Zhang | StrandDesigner introduces the first sketch-based generative model for creating realistic 3D hair strands. The main objective is to develop a model that converts sketch images into high-fidelity 3D hair strands, providing finer user control than existing text or image-prompted methods. The key methodology combines a learnable multi-scale strand upsampling strategy using a scale-wise autoregressive transformer with a multi-scale adaptive conditioning mechanism that fine-tunes a pretrained DINOv2 model with scale-specific tokens. The model achieves superior performance in conditional generation, obtaining a Point Cloud IoU of 64.54% and a Chamfer Distance of 0.80, outperforming the next-best competitor (Sketch+HAAR) which scored 60.85% and 1.06 respectively. The principal implication for AI practitioners is that for generating complex 3D assets, a specialized framework combining a structured, multi-scale generative process with an adaptive conditioning mechanism tailored to an intuitive input modality like sketches can yield more precise and controllable results than general-purpose text-to-3D models. |
| Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during
  Multi-Hop Analysis (Read more on [arXiv](https://arxiv.org/abs/2508.04699) or [HuggingFace](https://huggingface.co/papers/2508.04699))| Reshmi Ghosh, Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav | This paper introduces a diagnostic framework to systematically categorize and analyze reasoning failures in language models on multi-hop question answering tasks. The primary objective is to understand how and why reasoning models break down when synthesizing information across multiple sources by decomposing their behavior along three dimensions: hops, coverage, and overthinking. The study employs a seven-category error taxonomy to manually annotate 1,080 outputs from six language models across three datasets and develops a two-step LLM-as-a-Judge for automated analysis. A primary result is that "overhopping" (executing more reasoning steps than required) is the most persistent failure, with overthinking rates on the complex MuSiQue dataset reaching as high as 61.7% for one model and systematically driving incorrect answers. The principal implication for AI practitioners is that evaluation must move beyond final answer accuracy to include metrics of reasoning fidelity, as models can produce correct answers despite flawed reasoning, masking critical inefficiencies and a propensity to hallucinate under complexity. |
| PRvL: Quantifying the Capabilities and Risks of Large Language Models
  for PII Redaction (Read more on [arXiv](https://arxiv.org/abs/2508.05545) or [HuggingFace](https://huggingface.co/papers/2508.05545))| Prajit Das, Lavanya Elluri, Aritran Piplai, Anantaa Kotal, Leon Garza | This research presents a comprehensive benchmark of Large Language Models for PII redaction, evaluating various architectures and training strategies to quantify their capabilities and risks. The primary objective is to determine which combinations of model architecture, training paradigm, and inference strategy yield the optimal trade-offs between redaction accuracy, latency, and privacy preservation across different domains. The study evaluates multiple model families (e.g., Dense LLMs, MoE, LRM) on the AI4Privacy dataset using parameter-efficient fine-tuning, instruction-tuning, and Retrieval-Augmented Generation (RAG), assessing performance with span-correct/label-exact accuracy and a privacy leakage score (SPriV). Instruction-tuning emerged as the most effective strategy, with the instruction-tuned DeepSeek-Q1 model achieving the highest span-correct accuracy of 0.994 and a minimal privacy leakage (SPriV) score of 0.002. The principal implication for AI practitioners is that instruction-tuning smaller, efficient open-source models is a superior strategy for PII redaction compared to standard fine-tuning or using larger models, providing the best balance of performance, cost, and privacy; the released PRvL toolkit facilitates the deployment of these secure, auditable solutions. |
| REINA: Regularized Entropy Information-Based Loss for Efficient
  Simultaneous Speech Translation (Read more on [arXiv](https://arxiv.org/abs/2508.04946) or [HuggingFace](https://huggingface.co/papers/2508.04946))| Xiao Yu, Mahesh Kumar Nandwana, Joseph Liu, Nameer Hirschkind | This paper introduces REINA, a regularized, information-theoretic loss function for efficiently adapting pre-trained, non-streaming speech-to-text translation models into high-performance simultaneous translation systems. The research objective is to develop a stable and efficient method for training an adaptive READ/WRITE policy that optimally balances translation quality and latency. The key methodology is the REINA loss, which trains a policy network by maximizing the covariance between its output and an estimate of mutual information gained from future audio, approximated via the cross-entropy difference on partial versus full audio contexts. On the MUST-C benchmark, REINA demonstrates state-of-the-art performance, with a model trained only on MUST-C data achieving Normalized Streaming Efficiency (NoSE) scores up to 8.9% higher than prior methods like DiSeg. For AI practitioners, REINA provides a computationally efficient fine-tuning framework to repurpose existing large translation models for real-time applications, offering a direct way to optimize the quality-latency trade-off without complex architectural changes or unstable training paradigms. |
| I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating
  Linguistic Shibboleth Detection in LLM Hiring Evaluations (Read more on [arXiv](https://arxiv.org/abs/2508.04939) or [HuggingFace](https://huggingface.co/papers/2508.04939))| Chirag Shah, Aman Chadha, Tanya Roosta, Julia Kharchenko | This paper introduces a benchmark for detecting and measuring how Large Language Models (LLMs) exhibit bias against linguistic shibboleths in simulated hiring evaluations. The main objective is to systematically quantify LLM responses to subtle linguistic markers, like hedging, that can inadvertently serve as proxies for demographic characteristics. The methodology involves evaluating LLMs using 100 question-response pairs, each with a "hedged" and a "confident" version that are semantically equivalent, to assess scoring and hiring recommendations. The primary result is that LLMs systematically penalize hedged language; across all models, hedged responses received ratings that were, on average, 25.6% lower than confident responses with identical content. The principal implication for AI practitioners is that systems deployed for high-stakes evaluations must undergo rigorous testing with controlled benchmarks to identify and mitigate biases that penalize communication styles correlated with demographic groups, thereby preventing systemic discrimination. |
| RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation (Read more on [arXiv](https://arxiv.org/abs/2508.04190) or [HuggingFace](https://huggingface.co/papers/2508.04190))| Jian Yang, Yixuan Ding, Tianfang Zhang, Yimian Dai, fengyiwu | This paper introduces RPCANet++, a deep unfolding network that integrates Robust Principal Component Analysis (RPCA) with deep learning for interpretable sparse object segmentation. The primary objective is to address the computational cost and limited generalizability of traditional RPCA models while enhancing the interpretability of deep networks for segmentation tasks. The methodology unfolds a relaxed RPCA optimization problem into a multi-stage architecture consisting of a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM), enhanced with a Memory-Augmented Module (MAM) and a Deep Contrast Prior Module (DCPM). Experiments show that RPCANet++ achieves state-of-the-art performance, with the six-stage model attaining a 94.39% Intersection over Union (IoU) on the NUDT-SIRST dataset, a 5.08 percentage point improvement over its baseline. For AI practitioners, this work provides a framework for developing interpretable and efficient segmentation models by mapping classical optimization steps to neural network components, offering a verifiable alternative to "black-box" architectures for tasks requiring high reliability. |
| I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal
  Entity Linking (Read more on [arXiv](https://arxiv.org/abs/2508.02243) or [HuggingFace](https://huggingface.co/papers/2508.02243))| Chao Wang, Tong Ruan, Kaiwen Li, Junwen Li, Ziyan Liu | This paper introduces I2CR, a novel framework for multimodal entity linking that uses intra- and inter-modal collaborative reflections to improve accuracy. The objective is to address the unnecessary use of images and the limitations of single-pass visual feature extraction in current LLM-based methods. I2CR first attempts to link entities using only textual information; if this is deemed insufficient through intra-modal consistency reflection and inter-modal alignment verification, it then initiates a multi-round iterative process that incorporates diverse visual clues from various image-to-text models. The framework achieves state-of-the-art results, including a 5.1% absolute improvement in top-1 accuracy on the WikiDiverse dataset (to 91.6%). The principal implication for AI practitioners is that dynamic, reflective reasoning pipelines that selectively integrate multimodal data as needed can be more effective and robust than monolithic, single-pass fusion architectures. |
| Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast
  Image Compression (Read more on [arXiv](https://arxiv.org/abs/2508.04979) or [HuggingFace](https://huggingface.co/papers/2508.04979))| Yifei Ji, Jiale Yuan, Jinpei Guo, Mingde Zhou, Zheng Chen | SODEC is a single-step diffusion-based image compression model designed to achieve high perceptual quality and fidelity with significantly accelerated decoding. The research objective is to resolve the excessive decoding latency and poor fidelity inherent in multi-step diffusion compression models, especially at very low bitrates. The methodology replaces iterative denoising with a single-step diffusion process, which is steered by a fidelity guidance module that uses features from a preliminary VAE-based reconstruction as an explicit condition, and is optimized using a rate annealing training strategy. SODEC improves decoding speed by over 20x compared to previous multi-step diffusion methods while establishing new state-of-the-art performance in rate-distortion-perception on benchmarks like DIV2K. The principal implication for AI practitioners is that single-step diffusion can be made practical and effective for high-fidelity compression by using an explicit, parallel reconstruction to provide strong structural guidance, making diffusion-based approaches more viable for latency-sensitive applications. |
