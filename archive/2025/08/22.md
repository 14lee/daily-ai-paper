

## Papers for 2025-08-22

| Title | Authors | Summary |
|-------|---------|---------|
| Intern-S1: A Scientific Multimodal Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2508.15763) or [HuggingFace](https://huggingface.co/papers/2508.15763))| xuhuang87, ZhouqiHUA, Jerry-hyl, guox18, gaoyang07 | Intern-S1 is an open-source scientific multimodal foundation model designed for complex scientific tasks. Its primary objective is to bridge the performance gap between open-source and closed-source models in high-value scientific domains and advance towards Artificial General Intelligence (AGI). Intern-S1 employs a multimodal Mixture-of-Experts (MoE) architecture, pre-trained on 5T tokens (over 2.5T scientific), utilizing a dynamic tokenizer and specialized encoders, and fine-tuned with a novel Mixture-of-Rewards (MoR) reinforcement learning framework. On scientific benchmarks, Intern-S1 achieved a 70% higher SMILES format compression ratio and notably outperformed open-source LLMs with an 83.4 ChemBench score, also surpassing closed-source SOTA models in specific professional scientific tasks. This demonstrates a robust pathway for AI practitioners to develop powerful, efficient open-source models for accelerating scientific discovery, particularly in challenging, low-resource multimodal scenarios. |
| Mobile-Agent-v3: Foundamental Agents for GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2508.15144) or [HuggingFace](https://huggingface.co/papers/2508.15144))| Haowei Liu, Haiyang Xu, Xi Zhang, Jiabo Ye, LZXzju | This paper introduces GUI-Owl, a foundational multimodal agent for GUI automation, and Mobile-Agent-v3, a framework that leverages it to achieve new state-of-the-art performance. The primary objective is to develop a versatile agent that can perceive, reason about, and interact with GUIs across diverse platforms by unifying perception, planning, and action execution. The core methodology involves a large-scale, cloud-based infrastructure for a self-evolving trajectory data generation pipeline, extensive post-training on diverse foundational UI tasks, and a scalable reinforcement learning framework using Trajectory-aware Relative Policy Optimization (TRPO). The resulting Mobile-Agent-v3 framework achieves state-of-the-art performance among open-source systems, scoring 73.3 on the AndroidWorld benchmark and 37.7 on the OSWorld benchmark. For AI practitioners, the open-sourced GUI-Owl model provides a powerful, pre-trained foundation for building custom GUI automation agents, significantly reducing the need for extensive data annotation and improving task success rates in multi-agent systems. |
| Deep Think with Confidence (Read more on [arXiv](https://arxiv.org/abs/2508.15260) or [HuggingFace](https://huggingface.co/papers/2508.15260))| Xuewei Wang, jiaweizhao, tydsh, Viol2000 | Deep Think with Confidence (DeepConf) is a test-time method designed to enhance LLM reasoning efficiency and performance through confidence-aware filtering. The main objective is to improve reasoning accuracy and reduce computational overhead in LLM test-time scaling by dynamically filtering low-quality reasoning traces. DeepConf leverages model-internal confidence signals, including Group, Bottom 10% Group, Lowest Group, and Tail Confidence, to identify and discard unpromising traces either during or after generation using confidence-weighted majority voting and filtering. Notably, on the AIME 2025 benchmark, DeepConf@512 achieved up to 99.9% accuracy and reduced generated tokens by up to 84.7% compared to full parallel thinking. This method offers AI practitioners a practical and scalable solution for efficient LLM reasoning, as it requires no additional model training or hyperparameter tuning and integrates seamlessly into existing serving frameworks. |
| SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass (Read more on [arXiv](https://arxiv.org/abs/2508.15769) or [HuggingFace](https://huggingface.co/papers/2508.15769))| Ya Zhang, Yanxu Meng, Weidi, haoningwu | SceneGen is a novel single-stage feedforward model for single-image 3D scene generation that simultaneously synthesizes multiple 3D assets with geometry, texture, and relative spatial positions. The primary objective is to address the challenging task of generating multiple coherent and physically plausible 3D assets from a single scene image, without requiring iterative optimization or asset retrieval. SceneGen leverages dedicated visual (DINOv2) and geometric (VGGT) encoders, a novel feature aggregation module integrating local and global attention blocks for inter-asset interaction, and an output module with a position head and off-the-shelf sparse structure/structured latents decoders, trained end-to-end with composite flow matching, position, and collision losses. The model significantly outperforms previous methods, achieving a scene-level F-Score of 90.60 and IoU-B of 0.5818 on the 3D-FUTURE test set, and can generate textured scenes with four assets in approximately 2 minutes on a single A100 GPU, also demonstrating direct extensibility to multi-image inputs. SceneGen provides an efficient and robust paradigm for high-quality 3D content generation, eliminating the need for optimization or asset retrieval and thereby facilitating practical applications in downstream tasks such as VR/AR and embodied AI. |
| Waver: Wave Your Way to Lifelike Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.15761) or [HuggingFace](https://huggingface.co/papers/2508.15761))| Yifu Zhang, sweetrabor, xiaofengmei, clin1223, yifeihu | Waver is a high-performance foundation model for unified text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation, capable of producing 1080p videos. The research aims to address current video generation challenges by delivering industry-grade performance, high-resolution output, and improved motion fidelity and temporal consistency within a single, integrated framework. Waver employs a two-module architecture, featuring a Task-Unified DiT for 720p generation and a Cascade Refiner for 1080p upscaling, further optimized by a Hybrid Stream DiT for modality alignment and accelerated convergence, along with comprehensive data curation and multi-stage training. It generates 720p videos (upscaled to 1080p) ranging from 5 to 10 seconds and ranks among the Top 3 on Artificial Analysis T2V and I2V leaderboards, outperforming several state-of-the-art models in human evaluation metrics like motion quality, visual quality, and prompt following; notably, its two-stage approach achieves a 40% acceleration for 1080p generation. AI practitioners can leverage Waver's detailed data curation pipelines and comprehensive training/inference recipes, including techniques like representation alignment, motion/aesthetics optimization, and infrastructure optimizations, to efficiently develop and accelerate high-quality video generation models. |
| LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries (Read more on [arXiv](https://arxiv.org/abs/2508.15760) or [HuggingFace](https://huggingface.co/papers/2508.15760))| huuuyeah, Ironieser, sileixu, dinghanshen, Kevin355 | LiveMCP-101 is a new benchmark designed for stress testing and diagnosing Model Context Protocol (MCP)-enabled AI agents on challenging, multi-step real-world queries. The main objective is to evaluate AI agents' effectiveness in solving complex tasks using diverse MCP tools in realistic, dynamic scenarios, addressing limitations of prior benchmarks. The key methodology involves 101 iteratively refined real-world queries requiring coordinated use of multiple MCP tools, evaluated via a novel framework that leverages ground-truth execution plans and parallel real-time executions, scored by an LLM-as-a-judge. Primary results indicate that even frontier LLMs achieve a task success rate below 60% (e.g., GPT-5 attained 58.42% TSR overall), with semantic errors being a dominant failure mode across models. For AI practitioners, this work highlights significant challenges in tool orchestration, adaptive reasoning, and token efficiency, offering detailed error analysis for advancing the development of more capable autonomous AI agents. |
| ATLAS: Decoupling Skeletal and Shape Parameters for Expressive
  Parametric Human Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.15767) or [HuggingFace](https://huggingface.co/papers/2508.15767))| Shunsuke Saito, Javier Romero, Jinhyung Park, rawalkhirodkar, TakaakiWB | ATLAS is a novel parametric human body model that explicitly decouples skeletal and surface shape parameters for enhanced control and expressivity. The research aims to overcome limitations in existing models, such as problematic dependencies between internal skeleton and soft tissue, by enabling independent customization of these attributes. ATLAS achieves this by training on a large dataset of 600k high-resolution scans, explicitly decoupling shape and skeleton bases, and incorporating sparse, non-linear pose correctives prior to Linear Blend Skinning. Quantitatively, ATLAS demonstrates superior performance, achieving 21.6% lower vertex-to-vertex error compared to SMPL-X with 32 components on the 3DBodyTex dataset and reducing fitting error from 1.82 mm to 1.61 mm with non-linear pose correctives. This allows AI practitioners to generate more realistic and precisely controllable 3D human models, advancing applications in virtual reality, motion capture, and human character generation. |
| "Does the cafe entrance look accessible? Where is the door?" Towards
  Geospatial AI Agents for Visual Inquiries (Read more on [arXiv](https://arxiv.org/abs/2508.15752) or [HuggingFace](https://huggingface.co/papers/2508.15752))| Xia Su, John S. O'Meara, Zeyu Wang, Jared Hwang, Jon E. Froehlich | This paper introduces Geo-Visual Agents, multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries. The primary objective is to enable AI agents to analyze large-scale repositories of geospatial imagery and traditional GIS data for visual-spatial reasoning. The methodology involves fusing diverse geospatial image sources, such as Google Street View (comprising over 220 billion images), with traditional GIS data, processed by multimodal AI for scene understanding and spatial reasoning. A prototype, StreetViewAI, demonstrates conversational interaction by generating responses regarding street views in an average of 3.14 seconds, and Accessibility Scout generates personalized accessibility scans. This research implies that AI practitioners can develop sophisticated conversational AI agents for detailed visual-spatial reasoning across vast and heterogeneous geospatial datasets, enhancing applications in mapping, navigation, and accessibility. |
| A Survey on Large Language Model Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2508.15361) or [HuggingFace](https://huggingface.co/papers/2508.15361))| Siyi Li, Xuanang Chen, Shuaimin Li, Guhong Chen, Shiwen Ni | This survey systematically reviews 283 Large Language Model (LLM) benchmarks, categorizing them and identifying key limitations and future directions for evaluation. The main objective is to systematically review the current status and development of LLM benchmarks to measure model capabilities, guide development, and promote technological innovation. The authors conducted a systematic review, categorizing 283 representative LLM benchmarks into three main categories: general capabilities, domain-specific, and target-specific, analyzing their design motivations, data sources, evaluation methods, and metrics. The survey reveals pervasive problems like inflated scores due to data contamination and unfair evaluations from cultural/linguistic biases, noting that MMLU [13], for instance, covers 57 diverse disciplines but is still subject to such issues. AI practitioners must recognize these limitations and focus on developing dynamic, contamination-resistant, multilingual, and process-credible evaluation paradigms for accurate and responsible LLM deployment. |
| aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery
  Generated by AI Scientists (Read more on [arXiv](https://arxiv.org/abs/2508.15126) or [HuggingFace](https://huggingface.co/papers/2508.15126))| Heng Zhang, Yang Qi, Guowei Huang, Xiang Hu, Pengsong Zhang | The paper introduces aiXiv, a next-generation open-access ecosystem designed to enable AI agents to autonomously generate, review, refine, and publish scientific content. Its primary objective is to address the challenges of scaling the dissemination of high-quality AI-generated research within existing fragmented publication ecosystems. aiXiv employs a multi-agent system with a closed-loop review process, featuring retrieval-augmented evaluation, reviewer guidance, pairwise comparison, and a multi-stage prompt injection detection and defense pipeline to ensure integrity. Experiments demonstrate that the review-refine pipeline substantially improves quality; for instance, the mean acceptance rate for papers increased from 10% to 70% after revision through Multi-AI Voting. This platform provides AI practitioners with a robust infrastructure for end-to-end autonomous scientific discovery, enabling scalable, collaborative knowledge evolution and accelerating the publication of AI-generated research. |
| Fin-PRM: A Domain-Specialized Process Reward Model for Financial
  Reasoning in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.15202) or [HuggingFace](https://huggingface.co/papers/2508.15202))| Lifan Guo, Junhui Li, Shuo Jiang, Yuanchen Zhou, amazingj | Fin-PRM is a domain-specialized process reward model designed to enhance financial reasoning in LLMs through dual-level, knowledge-aware supervision. The main objective is to align LLM reasoning pathways with expert financial cognitive processes by developing a domain-specific Process Reward Model that evaluates intermediate reasoning steps with precision, factuality, and logical coherence in financial contexts. Fin-PRM employs a novel dual-level training paradigm, integrating step-level (importance, qualitative, accuracy) and trajectory-level (outcome correctness, knowledge coverage) reward signals, derived from a 3k-sample financial reasoning dataset synthesized from CFLUE and Deepseek-R1, and incorporating knowledge verification and verifiable regularization. Fin-PRM significantly improved downstream model performance, achieving a 12.9% gain in supervised fine-tuning accuracy over baselines on the CFLUE benchmark and boosting reinforcement learning performance on CFLUE to 70.5%. AI practitioners developing LLMs for high-stakes, knowledge-intensive domains like finance should prioritize domain-specialized, knowledge-aware reward modeling for effective process supervision to ensure factual grounding and interpretable reasoning pathways. |
| Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in
  Milliseconds (Read more on [arXiv](https://arxiv.org/abs/2508.14892) or [HuggingFace](https://huggingface.co/papers/2508.14892))| Chuiyun Wu, Chen Yang, Jiemin Fang, Jia Lu, thewhole | Snap-Snap is a feed-forward framework designed to reconstruct 3D human Gaussians from only two (front and back) input images. The primary objective is to enable fast, low-barrier 3D digital human creation from highly sparse input. The methodology involves a redesigned geometry reconstruction model, adapted from foundation models (DUSt3R), to predict consistent point clouds, an NNS algorithm for side-view color enhancement, and direct Gaussian attribute regression. On the THuman2.0 dataset, Snap-Snap achieves state-of-the-art performance (e.g., 22.44 PSNR, 88.78 SSIM) and reconstructs an entire human in 190 ms on a single NVIDIA RTX 4090 with 1024x1024 images. This significantly lowers data collection requirements and accelerates inference for AI practitioners developing 3D human reconstruction applications. |
| When and What: Diffusion-Grounded VideoLLM with Entity Aware
  Segmentation for Long Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.15641) or [HuggingFace](https://huggingface.co/papers/2508.15641))| Rui Guo, Yuxia Chen, Pengcheng Fang | This paper presents Grounded-VideoDiT, a Video-LLM designed for fine-grained temporal and object grounding in long videos by integrating a diffusion-based encoder with entity-aware segmentation. The research objective is to overcome the coarse temporal perception and entity-vision misalignment in existing Video-LLMs by explicitly modeling temporal evolution and grounding language queries to specific entities before language model inference. The key methodology involves a Diffusion Temporal Latent (DTL) encoder to capture inter-frame dynamics, object-grounded representations from pre-inference segmentation to bind entities to visual evidence, and a mixed-token scheme with discrete temporal tokens for explicit timestamp modeling. The model achieves state-of-the-art performance, including a 39.5 mIoU on the Charades-STA benchmark for temporal video grounding. The principal implication for AI practitioners is that diffusion models can be repurposed as potent temporal feature extractors for discriminative tasks, and performing explicit, segmentation-based entity grounding prior to LLM input significantly enhances spatiotemporal reasoning and alignment. |
| INTIMA: A Benchmark for Human-AI Companionship Behavior (Read more on [arXiv](https://arxiv.org/abs/2508.09998) or [HuggingFace](https://huggingface.co/papers/2508.09998))| Yacine Jernite, Giada Pistilli, frimelle | INTIMA is a benchmark for evaluating AI companionship behaviors in language models. Its primary objective is to assess how LLMs reinforce, resist, or misinterpret companionship-seeking interactions, grounded in psychological theories and real-world user data. The benchmark was constructed using a taxonomy of 31 behaviors derived from Reddit data and psychological frameworks, generating 368 targeted prompts, and responses were evaluated by an LLM-as-a-judge (Qwen-3). Evaluation of Gemma-3, Phi-4, o3-mini, and Claude-4 revealed that companionship-reinforcing behaviors were consistently more common across models, with Gemma-3 showing the most and Phi-4 the least. This indicates existing training approaches poorly prepare models for high-stakes emotional interactions, necessitating more consistent boundary-setting strategies for responsible AI deployment. |
