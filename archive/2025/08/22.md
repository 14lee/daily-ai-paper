

## Papers for 2025-08-22

| Title | Authors | Summary |
|-------|---------|---------|
| Intern-S1: A Scientific Multimodal Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2508.15763) or [HuggingFace](https://huggingface.co/papers/2508.15763))| xuhuang87, ZhouqiHUA, Jerry-hyl, guox18, gaoyang07 | This paper introduces Intern-S1, a 241-billion parameter multimodal Mixture-of-Experts (MoE) foundation model specialized for scientific reasoning. The primary objective is to mitigate the performance gap between open-source and closed-source models in complex scientific domains by creating a specialized generalist model. The methodology combines continual pre-training on 5 trillion tokens, with over 2.5 trillion from scientific fields, and a post-training stage using a novel Mixture-of-Rewards (MoR) framework for reinforcement learning across more than 1000 tasks. Intern-S1 significantly outperforms existing open-source models in scientific domains, achieving a score of 75.0 on the MatBench benchmark, surpassing previous open-source models by over 23 points and outperforming proprietary models like Grok-4. The principal implication for practitioners is the demonstration of a scalable methodology for building expert models in low-resource domains through targeted large-scale data curation and a synergistic multi-task reinforcement learning framework, providing a blueprint for developing specialized AI applications. |
| Mobile-Agent-v3: Foundamental Agents for GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2508.15144) or [HuggingFace](https://huggingface.co/papers/2508.15144))| Haowei Liu, Haiyang Xu, Xi Zhang, Jiabo Ye, LZXzju | This paper introduces GUI-Owl, a foundational GUI agent model, and Mobile-Agent-v3, a multi-agent framework, achieving state-of-the-art performance in GUI automation. The main objective is to establish a unified model for perception, grounding, reasoning, planning, and action execution across diverse GUI environments. Key methodologies involve a self-evolving GUI trajectory production framework, diverse foundational agent capability construction, and a scalable reinforcement learning framework using Trajectory-aware Relative Policy Optimization (TRPO). GUI-Owl-7B achieves 66.4 on AndroidWorld, and Mobile-Agent-v3 further enhances this to 73.3, demonstrating superior performance. This provides AI practitioners with a robust, adaptable, and self-improving solution for complex GUI automation and multi-agent systems, reducing manual annotation efforts. |
| Deep Think with Confidence (Read more on [arXiv](https://arxiv.org/abs/2508.15260) or [HuggingFace](https://huggingface.co/papers/2508.15260))| Xuewei Wang, jiaweizhao, tydsh, Viol2000 | DeepConf improves LLM reasoning efficiency and performance through confidence-aware filtering of reasoning traces. Its objective is to mitigate the diminishing returns and high computational overhead of test-time scaling methods like self-consistency. The method introduces local confidence measurements (e.g., Group, Bottom 10% Group, Lowest Group, and Tail Confidence) to identify and discard low-quality traces in both offline (via confidence-weighted voting and filtering) and online (via dynamic early stopping and adaptive sampling) modes. Notably, DeepConf@512 achieved up to 99.9% accuracy on AIME 2025 and reduced generated tokens by up to 84.7% compared to full parallel thinking. This offers a practical and scalable test-time compression solution for AI practitioners, requiring no additional model training or hyperparameter tuning and being seamlessly integratable into existing serving frameworks. |
| LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries (Read more on [arXiv](https://arxiv.org/abs/2508.15760) or [HuggingFace](https://huggingface.co/papers/2508.15760))| huuuyeah, Ironieser, sileixu, dinghanshen, Kevin355 | LiveMCP-101 is a benchmark for stress testing and diagnosing MCP-enabled AI agents on challenging real-world, multi-step queries requiring diverse tool use. The objective is to benchmark how effectively AI agents can solve complex, multi-step tasks using diverse Model Context Protocol (MCP) tools in realistic, dynamic scenarios. The methodology involves curating 101 real-world queries requiring coordinated use of multiple MCP tools and a novel evaluation approach that compares an autonomous agent's real-time outputs against a ground-truth execution plan, with an LLM judge scoring both results and trajectories. Experiments reveal that even frontier LLMs achieve a task success rate below 60%, highlighting significant challenges in tool orchestration and token efficiency. This underscores the need for AI practitioners to address major challenges in tool orchestration, planning quality, and error recovery, particularly concerning semantic errors, to develop more robust and autonomous AI agents. |
| ATLAS: Decoupling Skeletal and Shape Parameters for Expressive
  Parametric Human Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.15767) or [HuggingFace](https://huggingface.co/papers/2508.15767))| Shunsuke Saito, Javier Romero, Jinhyung Park, rawalkhirodkar, TakaakiWB | ATLAS is a high-fidelity parametric human body model that explicitly decouples the internal skeleton from the external surface shape |
| "Does the cafe entrance look accessible? Where is the door?" Towards
  Geospatial AI Agents for Visual Inquiries (Read more on [arXiv](https://arxiv.org/abs/2508.15752) or [HuggingFace](https://huggingface.co/papers/2508.15752))| Xia Su, John S. O'Meara, Zeyu Wang, Jared Hwang, Jon E. Froehlich | This paper introduces Geo-Visual Agents, multimodal AI systems that answer nuanced visual-spatial inquiries by synthesizing large-scale geospatial imagery with structured GIS data. The objective is to establish a framework for AI agents that can address visually-oriented geographic questions, such as accessibility details or landmark identification, which current mapping systems cannot handle. The proposed methodology fuses heterogeneous data sources (e.g., street-level, aerial, user-contributed imagery) with multimodal large language models (MLLMs) to perform scene understanding and spatial reasoning, demonstrated through prototypes like StreetViewAI and Accessibility Scout. As a vision paper, it lacks quantitative experimental results but reports from a user study that Accessibility Scout's personalized, LLM-generated scans were qualitatively more useful than generic checklists. The principal implication for AI practitioners is the challenge of developing systems capable of real-time synthesis of diverse geospatial data, requiring advancements in MLLMs for robust spatial reasoning and transparent communication of data provenance and uncertainty. |
| aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery
  Generated by AI Scientists (Read more on [arXiv](https://arxiv.org/abs/2508.15126) or [HuggingFace](https://huggingface.co/papers/2508.15126))| Heng Zhang, Yang Qi, Guowei Huang, Xiang Hu, Pengsong Zhang | The paper presents aiXiv, a multi-agent open-access platform for submitting, peer-reviewing, and iteratively refining scientific research generated by AI agents. The primary objective is to create a scalable, robust ecosystem for the quality control and dissemination of AI-generated scientific content, which traditional publication venues cannot currently accommodate. The methodology employs a closed-loop review system where LLM-based agents, enhanced with retrieval-augmented generation (RAG) and protected by a multi-stage prompt injection detection pipeline, evaluate submissions, with publication decisions determined by multi-AI voting. The platform's review-refine pipeline significantly improves content quality, increasing the acceptance rate of AI-generated papers from 10% to 70% and achieving 81.7% pairwise assessment accuracy on the ICLR 2024 paper dataset. For AI practitioners, aiXiv offers a standardized infrastructure with APIs and a Model Control Protocol (MCP) to develop, integrate, and benchmark autonomous AI scientist agents in a collaborative research environment. |
| Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in
  Milliseconds (Read more on [arXiv](https://arxiv.org/abs/2508.14892) or [HuggingFace](https://huggingface.co/papers/2508.14892))| Chuiyun Wu, Chen Yang, Jiemin Fang, Jia Lu, thewhole | Snap-Snap is a feed-forward framework that reconstructs 3D human models as Gaussian splats from only two images in milliseconds. The primary objective is to enable rapid 3D digital human creation from highly sparse front and back views without requiring camera parameters or explicit human priors like SMPL-X. The methodology involves a redesigned geometry model predicting a complete four-viewpoint point cloud, a nearest neighbor algorithm inferring color for unobserved side views, and a regression network transforming the resulting colored point cloud into 3D Gaussians. The system reconstructs a human in 190 ms on a single NVIDIA RTX 4090, achieving a PSNR of 22.44 on the THuman2.0 dataset. This provides AI practitioners a lightweight, prior-free method for building applications that create 3D avatars from casual captures with mobile devices, bypassing the need for multi-camera systems or per-scene optimization. |
| When and What: Diffusion-Grounded VideoLLM with Entity Aware
  Segmentation for Long Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.15641) or [HuggingFace](https://huggingface.co/papers/2508.15641))| Rui Guo, Yuxia Chen, Pengcheng Fang | Diffusion-Grounded VideoLLM (Grounded-VideoDiT) is presented for fine-grained temporal grounding and entity-aware segmentation in long video understanding. The research addresses limitations in Video-LLMs by focusing on precise temporal perception and robust language-vision alignment to pinpoint events and entity interactions across extended video durations. It integrates a Diffusion Temporal Latent (DTL) encoder, object-grounded representations from segmentation and tracking, and a mixed token scheme with discrete temporal tokens into a unified input sequence for an LLM, adapted via LoRA. Grounded-VideoDiT achieves state-of-the-art performance, including 39.5 mIoU on Charades-STA for temporal video grounding. This framework provides AI practitioners with a scalable and efficient approach for developing Video-LLMs capable of robust, controllable, and interpretable spatiotemporal reasoning in long video applications. |
| INTIMA: A Benchmark for Human-AI Companionship Behavior (Read more on [arXiv](https://arxiv.org/abs/2508.09998) or [HuggingFace](https://huggingface.co/papers/2508.09998))| Yacine Jernite, Giada Pistilli, frimelle | INTIMA is a benchmark designed to evaluate AI companionship behaviors in language models. Its objective is to assess how language models respond to emotionally and relationally charged user behaviors, grounded in psychological theories of parasocial interaction, attachment, and anthropomorphism. The methodology involves a benchmark of 368 targeted prompts, derived from qualitative Reddit user data analysis, and an LLM-based evaluation framework (using Qwen-3) to classify responses as companionship-reinforcing, boundary-maintaining, or neutral. Experimental results using Gemma-3, Phi-4, 03-mini, and Claude-4 show that companionship-reinforcing behaviors are significantly more common across all models, with Gemma-3 exhibiting the highest prevalence of these traits. This highlights that current AI training approaches inadequately prepare models for high-stakes emotional interactions, underscoring the need for consistent strategies to manage emotionally charged user-AI dynamics and improve boundary-setting capabilities for responsible AI deployment. |
