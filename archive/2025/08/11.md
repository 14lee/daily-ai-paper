

## Papers for 2025-08-11

| Title | Authors | Summary |
|-------|---------|---------|
| GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2508.06471) or [HuggingFace](https://huggingface.co/papers/2508.06471))| GLM-4. 5 Team, zixuanlimit, ZAHNGYUXUAN, LiquidAmmonia, Stanislas | The paper introduces GLM-4.5, a 355B parameter open-source Mixture-of-Experts (MoE) model engineered for high performance in agentic, reasoning, and coding (ARC) tasks through a multi-stage training and reinforcement learning pipeline. The primary objective is to develop a single, open-source foundation model that unifies and excels across these distinct capabilities, which have often been addressed by specialized or proprietary models. The methodology involves a multi-stage process on 23T tokens, including pre-training on curated data, mid-training with sequence lengths up to 128K, and extensive post-training using expert model iteration, self-distillation, and a multi-faceted reinforcement learning (RL) framework. GLM-4.5 achieves strong performance, ranking 3rd overall on a comprehensive 12-benchmark evaluation; quantitatively, it scores 64.2% on SWE-bench Verified, outperforming models like GPT-4.1, and achieves 91.0% on the AIME 24 reasoning benchmark. For AI practitioners, the release of GLM-4.5 provides a powerful, parameter-efficient, and open-source alternative for building applications requiring a combination of agentic tool use, deep reasoning, and code generation, supported by a novel XML-based function call template that simplifies integration. |
| Voost: A Unified and Scalable Diffusion Transformer for Bidirectional
  Virtual Try-On and Try-Off (Read more on [arXiv](https://arxiv.org/abs/2508.04825) or [HuggingFace](https://huggingface.co/papers/2508.04825))| jgkwak, RyanL22 | Voost is a unified diffusion transformer that jointly models bidirectional virtual try-on and try-off to improve garment-person correspondence and generation fidelity. The primary objective is to develop a single, scalable framework that jointly learns virtual try-on and its inverse task, try-off, to improve spatial alignment and detail preservation without task-specific networks or auxiliary losses. The method uses a single Diffusion Transformer (DiT) with a token-level concatenation layout for person and garment images, enabling bidirectional generation controlled by a task token, and introduces inference-time attention temperature scaling and self-corrective sampling. Voost achieves state-of-the-art results on both try-on and try-off benchmarks; on the DressCode benchmark for paired try-on, it achieved a Fréchet Inception Distance (FID) of 2.787, surpassing the 3.283 FID of the CatVTON baseline. The principal implication for AI practitioners is that fine-tuning only the attention layers of a pretrained diffusion transformer is a highly effective strategy for complex image-conditioned generation tasks, significantly improving performance while using substantially fewer trainable parameters than full model fine-tuning. |
| InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.05731) or [HuggingFace](https://huggingface.co/papers/2508.05731))| Pengxiang Li, Shuanghe Zhu, Zeyu Liu, xiaotianhan, SiriusL | InfiGUI-G1 introduces Adaptive Exploration Policy Optimization (AEPO) to enhance GUI grounding for Multimodal Large Language Models (MLLMs). The primary objective is to overcome the inefficient exploration bottleneck in standard Reinforcement Learning with Verifiable Rewards (RLVR) to achieve robust semantic alignment, particularly for complex and unseen GUI elements. AEPO achieves this by integrating a multi-answer generation strategy with an Adaptive Exploration Reward (AER) function, complemented by a quality-of-exploration penalty to ensure diverse and purposeful exploration. The InfiGUI-G1-7B model establishes state-of-the-art performance across multiple benchmarks, demonstrating a significant 61.1% relative accuracy improvement on 'hard' samples within the ScreenSpot-Pro benchmark over the Naive RLVR baseline. This approach enables data-efficient training and fosters robust semantic understanding and generalization, making MLLM-based GUI agents more reliable for real-world human-computer interaction. |
| Memp: Exploring Agent Procedural Memory (Read more on [arXiv](https://arxiv.org/abs/2508.06433) or [HuggingFace](https://huggingface.co/papers/2508.06433))| Shuofei Qiao, Jialong Wu, Xiaobin Wang, Yuan Liang, Runnan Fang | Memp is a task-agnostic framework designed to endow LLM agents with learnable, updatable, and lifelong procedural memory. It addresses the brittleness of existing agent memory by proposing strategies for building, retrieving, and updating procedural knowledge, including distilling past trajectories into fine-grained steps and higher-level abstractions. Empirical results show that "Proceduralization," combining full trajectories with high-level scripts, achieved optimal performance, increasing GPT-40's TravelPlanner Common Sense score from 71.93% (no memory) to 79.94% and reducing steps to 14.62. Additionally, procedural memory transferred from a stronger model (GPT-40) to a weaker one (Qwen2.5-14B) yielded a 5% increase in task completion rate and a 1.6-step reduction on TravelPlanner. This work indicates that dynamic procedural memory significantly enhances agent accuracy, efficiency, and generalization, providing a clear path for AI practitioners to develop more robust and continuously learning agents. |
| Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal (Read more on [arXiv](https://arxiv.org/abs/2508.05988) or [HuggingFace](https://huggingface.co/papers/2508.05988))| Chengcheng Wan, Chao Hu, Yaoning Wang, Wenhao Zeng, YerbaPage | The paper introduces ASAP, a two-stage framework for compressing Chain-of-Thought (CoT) traces in code reasoning models to improve efficiency and accuracy. The primary objective is to prune redundant reasoning steps from long CoTs while preserving logical coherence, thereby reducing the computational cost and inference latency of Large Reasoning Models (LRMs). The methodology involves a coarse-to-fine strategy: first, an anchor-guided pruning stage removes irrelevant reasoning branches, followed by a fine-grained refinement stage that uses a novel "first-token surprisal" metric to iteratively remove steps with low logical importance. On the LiveCodeBench v4_v5 benchmark, models fine-tuned with ASAP achieved a 36.19% Pass@1 accuracy while reducing token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline. For AI practitioners, this framework provides a method to fine-tune models on shorter, logically-dense CoTs, leading to faster, more cost-effective, and more accurate code generation models by distilling effective reasoning patterns. |
| GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing (Read more on [arXiv](https://arxiv.org/abs/2508.02831) or [HuggingFace](https://huggingface.co/papers/2508.02831))| Przemysław Spurek, Tomasz Szczepanik, Krzysztof Byrski, MikolajZ | GENIE is a hybrid model that enables interactive, physics-based editing of NeRF scenes by conditioning them on a set of editable Gaussian primitives. The paper's objective is to fuse the high-fidelity rendering of NeRF with the manipulable structure of Gaussian Splatting (GS) to support dynamic scene modifications. The methodology introduces "Splash Grid Encoding," where a NeRF is conditioned by features interpolated from the k-nearest Gaussians, which are efficiently located using a novel "Ray-Traced Gaussian Proximity Search" (RT-GPS) algorithm. Quantitatively, GENIE outperforms the editable baseline RIP-NeRF in six of eight NeRF-Synthetic scenes (e.g., 33.23 vs 32.23 PSNR on the Ficus scene) and is the first presented method to enable editing on complex, unbounded Mip-NeRF 360 scenes. For AI practitioners, this framework provides a direct pathway to integrate high-quality neural scene representations with physics engines, enabling the development of interactive and physically grounded applications in virtual environments and content creation. |
| Adapting Vision-Language Models Without Labels: A Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2508.05547) or [HuggingFace](https://huggingface.co/papers/2508.05547))| Eleni Chatzi, Ran He, Jian Liang, Lijun Sheng, Hao Dong | This survey introduces a novel taxonomy for unsupervised Vision-Language Model (VLM) adaptation, categorizing methods based on the availability of unlabeled visual data. The paper's objective is to systematically structure the field of label-free VLM adaptation by organizing existing research according to practical data availability constraints. The authors propose a taxonomy that classifies methods into four paradigms: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation, reviewing the core technical strategies within each category. The analysis reveals that methodologies are tailored to data availability, such as using LLMs for text augmentation in data-free settings or entropy minimization for test-time adaptation, and identifies in Table V that benchmark datasets like ImageNet are popular across all four paradigms. For AI practitioners, this taxonomy provides a principled framework to select appropriate unsupervised adaptation techniques based on their specific data access constraints and to benchmark new methods within a clearly defined context. |
| MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs (Read more on [arXiv](https://arxiv.org/abs/2508.05502) or [HuggingFace](https://huggingface.co/papers/2508.05502))| Guohang Yan, Ruirui Chen, Nuo Chen, Jiaying Fei, Yufei Gao | The paper introduces MELLA, a dual-source dataset and framework for fine-tuning Multimodal Large Language Models (MLLMs) to improve both linguistic fluency and cultural groundedness in eight low-resource languages. The primary objective is to overcome the limitations of existing MLLMs, which produce culturally "thin" descriptions in low-resource contexts, by jointly enhancing linguistic capability and cultural understanding. The authors propose a dual-source data strategy, constructing a 6.8 million-pair dataset by combining native web alt-text for cultural knowledge with MLLM-generated, translated captions for linguistic skill, and then performing supervised fine-tuning. Fine-tuning with MELLA yields significant improvements; for instance, on the InternVL2-8B backbone, the Meteor score for Hungarian improved from a baseline of 0.11 to 13.11. For AI practitioners, this work provides a validated methodology and a public dataset to build MLLMs that are not just linguistically proficient in low-resource languages but also culturally aware, leading to more inclusive and contextually accurate AI systems. |
| MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh (Read more on [arXiv](https://arxiv.org/abs/2508.01242) or [HuggingFace](https://huggingface.co/papers/2508.01242))| Yi Yang, Yi-Hsuan Tsai, Yufeng Wang, I-Chao Shen, Shuangkang Fang | MeshLLM is a novel framework that enables Large Language Models to natively understand and generate text-serialized 3D meshes by addressing the data-scale and structural information loss limitations of prior methods. The core methodology involves a "Primitive-Mesh" decomposition strategy, using KNN clustering and semantic segmentation to expand the training dataset to over 1.5 million mesh parts, coupled with a progressive, multi-task training paradigm that includes vertex-to-face prediction and local mesh assembly to explicitly model 3D topology. Experiments show MeshLLM significantly outperforms the LLaMA-Mesh baseline in mesh understanding, achieving a CLIP score of 0.391 versus 0.124, while producing generation quality comparable to specialized encoder-based models. The principal implication for AI practitioners is that for structured, non-textual data, decomposing inputs into meaningful sub-components and designing training tasks that teach inherent structural relationships can allow LLMs to bypass dedicated encoders and effectively process raw serialized data formats. |
| UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and
  Precise Inference-Time Grounding (Read more on [arXiv](https://arxiv.org/abs/2507.22025) or [HuggingFace](https://huggingface.co/papers/2507.22025))| Bingqi Chen, Zihan Song, Jia Ma, Yuhang Wu, LianShuQuan | The paper introduces UI-AGILE, a comprehensive framework to enhance the training and inference capabilities of Graphical User Interface (GUI) agents. Its main objective is to address common GUI agent failures, including the dilemma of reasoning design, ineffective reward signals, and performance degradation from visual noise on high-resolution displays. The methodology combines Reinforcement Fine-Tuning (RFT) with novel components: a "Simple Thinking" reward, a continuous grounding reward, and a cropping-based resampling strategy for training, alongside a "Decomposed Grounding with Selection" method for inference. UI-AGILE achieves state-of-the-art performance, with the combined training and inference methods delivering a stated 23% absolute improvement in grounding accuracy over the best baseline on the ScreenSpot-Pro benchmark. The principal implication for AI practitioners is that the proposed "Decomposed Grounding with Selection" method can be used as a plug-and-play inference enhancement to significantly boost the grounding accuracy of existing GUI agent models on high-resolution screens. |
| LightSwitch: Multi-view Relighting with Material-guided Diffusion (Read more on [arXiv](https://arxiv.org/abs/2508.06494) or [HuggingFace](https://huggingface.co/papers/2508.06494))| Shubham Tulsiani, Fernando De la Torre, thebluser | LightSwitch is a generative framework that uses a material-guided diffusion model for fast, consistent multi-view relighting of 3D objects. The primary objective is to relight an object captured in multiple posed images under a novel target illumination, ensuring visual consistency across all views by leveraging inferred intrinsic material properties. The methodology involves finetuning a Stable Diffusion UNet architecture with multi-view self-attention modules, conditioning it on input images, camera poses, and inferred material maps (albedo, roughness, metallicness) to guide the relighting process. The framework demonstrates performance that matches or exceeds prior state-of-the-art methods; on the NeRF-Synthetic dataset, LightSwitch achieves relighting in approximately 2 minutes, substantially faster than competing inverse rendering techniques that require 120-480 minutes. For AI practitioners, this method provides a highly efficient alternative to traditional inverse rendering, enabling rapid generation of relightable 3D assets from multi-view images for applications in graphics, simulation, and virtual reality. |
