

## Papers for 2025-08-11

| Title | Authors | Summary |
|-------|---------|---------|
| GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2508.06471) or [HuggingFace](https://huggingface.co/papers/2508.06471))| GLM-4. 5 Team, zixuanlimit, ZAHNGYUXUAN, LiquidAmmonia, Stanislas | GLM-4.5 introduces a series of Mixture-of-Experts (MoE) foundation models excelling in agentic, reasoning, and coding tasks. The paper's objective is to develop a single, powerful open-source large language model that unifies and performs strongly across agentic abilities, complex reasoning, and advanced coding skills. The methodology involves an MoE architecture with hybrid reasoning modes, trained via a multi-stage process on 23T tokens, followed by comprehensive post-training including expert model iteration, reinforcement learning, and iterative self-distillation. GLM-4.5 (355B total parameters) achieves strong performance, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall among evaluated models. The release of GLM-4.5 and GLM-4.5-Air, demonstrating high performance with fewer parameters, provides parameter-efficient open-source foundation models that significantly advance research and development in reasoning and agentic AI systems for practitioners. |
| Voost: A Unified and Scalable Diffusion Transformer for Bidirectional
  Virtual Try-On and Try-Off (Read more on [arXiv](https://arxiv.org/abs/2508.04825) or [HuggingFace](https://huggingface.co/papers/2508.04825))| jgkwak, RyanL22 | Voost introduces a unified diffusion transformer that bidirectionally models both virtual try-on and its inverse, try-off, within a single scalable framework. The objective is to develop a single generative model for high-fidelity virtual try-on and try-off, eliminating the need for separate networks, auxiliary losses, or task-specific architectures by leveraging the inherent duality of the tasks. The methodology employs a single diffusion transformer operating on a latent, horizontally-concatenated garment-person image, where try-on and try-off are framed as a unified inpainting problem controlled by a task token. The model is trained jointly using a rectified flow matching objective and features inference-time techniques like attention temperature scaling and a self-corrective sampling loop that leverages the model's bidirectional capability to refine outputs. Voost achieves state-of-the-art results on standard benchmarks, demonstrating superior performance over specialized baselines. On the DressCode paired benchmark for virtual try-on, the unified Voost model achieves a Fréchet Inception Distance (FID) of 2.787, outperforming the next-best baseline CatVTON (FID 3.283) and a VTON-only variant of the model (FID 3.043). For AI practitioners, the principal implication is that jointly modeling a task and its natural inverse within a single unified architecture acts as a potent form of self-supervision; this bidirectional training improves garment-person correspondence and overall generative fidelity more effectively than training separate, specialized models, providing a direct path to building more robust and efficient conditional image synthesis systems. |
| InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.05731) or [HuggingFace](https://huggingface.co/papers/2508.05731))| Pengxiang Li, Shuanghe Zhu, Zeyu Liu, xiaotianhan, SiriusL | The paper introduces Adaptive Exploration Policy Optimization (AEPO), a novel framework for improving semantic alignment in Multimodal Large Language Model (MLLM)-based Graphical User Interface (GUI) agents. The primary objective is to overcome the inefficient exploration bottleneck in standard Reinforcement Learning with Verifiable Rewards (RLVR), which prevents models from learning difficult semantic associations. AEPO's methodology integrates a multi-answer generation strategy with a theoretically-grounded Adaptive Exploration Reward (AER) function, derived from an efficiency ratio η = U/C, to guide exploration effectively. The resulting InfiGUI-G1 models establish new state-of-the-art results, achieving a 9.0% relative accuracy improvement over the naive RLVR baseline on the ScreenSpot-Pro benchmark and a 61.1% relative improvement on previously "unlearnable" hard samples. For AI practitioners, this research provides a concrete method to enhance RL-based fine-tuning for GUI agents by creating effective learning signals for semantically complex tasks where standard exploration fails. |
| Memp: Exploring Agent Procedural Memory (Read more on [arXiv](https://arxiv.org/abs/2508.06433) or [HuggingFace](https://huggingface.co/papers/2508.06433))| Shuofei Qiao, Jialong Wu, Xiaobin Wang, Yuan Liang, Runnan Fang | This paper introduces Memp, a framework for building, retrieving, and updating procedural memory to enhance the performance and efficiency of LLM-based agents on complex tasks. The main research objective is to systematically investigate strategies for creating, managing, and applying a learnable, lifelong procedural memory to improve agent success rates and efficiency on analogous tasks. The methodology involves distilling past agent trajectories into fine-grained instructions and abstract scripts (Build), using vector-based similarity for recall (Retrieve), and employing dynamic regimens like reflection-based correction to continuously refine the memory repository (Update). The primary result shows that procedural memory significantly boosts performance; for instance, on the ALFWorld benchmark, a GPT-4o agent's test success rate increased from 42.14% (no memory) to 77.86% (with procedural memory). The principal implication for AI practitioners is that procedural memory is a transferable asset; a memory repository built by a powerful model can be migrated to a smaller, weaker model to substantially improve its performance, offering a practical method for enhancing deployed agents without expensive retraining. |
| Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal (Read more on [arXiv](https://arxiv.org/abs/2508.05988) or [HuggingFace](https://huggingface.co/papers/2508.05988))| Chengcheng Wan, Chao Hu, Yaoning Wang, Wenhao Zeng, YerbaPage | The paper presents ASAP, a two-stage framework for compressing Chain-of-Thought (CoT) traces in code generation models by pruning redundant reasoning steps based on a novel first-token surprisal metric. The primary objective is to reduce the computational cost and latency of Large Reasoning Models by compressing their CoT reasoning traces without degrading, and potentially improving, their performance on complex coding tasks. The methodology, ASAP (Anchor-guided, SurprisAl-based Pruning), is a coarse-to-fine process: it first uses an "anchor" CoT derived from the final answer to perform a coarse pruning of irrelevant reasoning branches, then applies a fine-grained refinement that iteratively removes steps with the lowest "first-token surprisal," a metric designed to quantify logical importance. On the LiveCodeBench v4_v5 benchmark, models fine-tuned with ASAP achieved a 36.19% Pass@1 accuracy, while reducing token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, SPIRIT. The principal implication for AI practitioners is that they can fine-tune models on CoTs pruned by ASAP to build more efficient and cost-effective code reasoning systems; this demonstrates that first-token surprisal is a more effective metric than perplexity for identifying and preserving logically critical steps, leading to models that are both faster and more accurate. |
| GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing (Read more on [arXiv](https://arxiv.org/abs/2508.02831) or [HuggingFace](https://huggingface.co/papers/2508.02831))| Przemysław Spurek, Tomasz Szczepanik, Krzysztof Byrski, MikolajZ | The paper introduces GENIE, a hybrid model that enables interactive, physics-based editing of Neural Radiance Fields by conditioning a NeRF network on an explicit, editable set of Gaussian primitives. The objective is to fuse the photorealistic rendering of Neural Radiance Fields (NeRF) with the explicit manipulability of Gaussian Splatting (GS) to create a 3D representation that supports real-time, locality-aware editing and integration with physics engines. GENIE represents a scene with Gaussian primitives that have trainable feature embeddings instead of colors; a NeRF-based MLP is then conditioned on a weighted interpolation of features from the k-nearest Gaussians to a query point, found using a novel Ray-Traced Gaussian Proximity Search (RT-GPS) algorithm, to render color and density. GENIE achieves reconstruction quality comparable to static SOTA methods while enabling editing, outperforming the editable baseline RIP-NeRF in 6 of 8 scenes on the NeRF-Synthetic dataset (e.g., 33.84 vs 33.41 PSNR on the "Lego" scene) and is the first editable method shown to work on the complex Mip-NeRF 360 dataset. Practitioners can use GENIE to integrate high-quality neural rendering with standard 3D editing software and physics engines, allowing for the creation of dynamic, interactive 3D content where edits to an explicit proxy geometry are immediately reflected in the photorealistic output without retraining the network. |
| Adapting Vision-Language Models Without Labels: A Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2508.05547) or [HuggingFace](https://huggingface.co/papers/2508.05547))| Eleni Chatzi, Ran He, Jian Liang, Lijun Sheng, Hao Dong | This paper presents a comprehensive survey of methods for adapting Vision-Language Models (VLMs) without labels, introducing a novel taxonomy based on the availability of unlabeled visual data. Its primary objective is to structure the rapidly growing field of unsupervised VLM adaptation by systematically categorizing existing approaches. The core methodology is a proposed taxonomy that classifies techniques into four paradigms: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation. The survey's analysis reveals distinct strategies for each paradigm, such as text augmentation using LLMs or entropy minimization, applied to foundational models like CLIP, which was pre-trained on 400 million image-text pairs. For AI practitioners, this taxonomy provides a principled framework to select suitable adaptation techniques based on specific data availability constraints, guiding the deployment of VLMs in real-world, label-scarce scenarios. |
| MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs (Read more on [arXiv](https://arxiv.org/abs/2508.05502) or [HuggingFace](https://huggingface.co/papers/2508.05502))| Guohang Yan, Ruirui Chen, Nuo Chen, Jiaying Fei, Yufei Gao | This paper introduces MELLA, a 6.8M-pair multimodal dataset and a dual-source data strategy to enhance Multimodal Large Language Models (MLLMs) in eight low-resource languages by simultaneously improving linguistic capability and cultural groundedness. The main objective is to overcome the performance gap of MLLMs in low-resource languages by enabling them to produce not just linguistically fluent "thin descriptions" but also culturally informed "thick descriptions". The key methodology involves a dual-source data collection strategy: creating a linguistics-focused dataset (`D_ling`) by generating and machine-translating image captions, and a culture-focused dataset (`D_know`) by sourcing native web alt-text. An MLLM is then fine-tuned on the combined MELLA dataset. Fine-tuning on MELLA yields significant improvements; for example, the InternVL2-8B model's Meteor score for Hungarian (HU) increased from 0.11 to 13.11, and its Keyword Accuracy for Arabic (AR) rose from 2.46 to 6.26. For AI practitioners, this research provides a validated framework and a large-scale dataset for building culturally aware, multilingual MLLMs, demonstrating that combining synthetically generated linguistic data with authentic cultural data is an effective strategy to address data scarcity in low-resource settings. |
| MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh (Read more on [arXiv](https://arxiv.org/abs/2508.01242) or [HuggingFace](https://huggingface.co/papers/2508.01242))| Yi Yang, Yi-Hsuan Tsai, Yufeng Wang, I-Chao Shen, Shuangkang Fang | MeshLLM is a framework enabling Large Language Models to natively understand and generate 3D meshes by training on a massive dataset of decomposed mesh components. The research objective is to overcome the data scale limitations and loss of structural information that occur when serializing 3D meshes for LLMs. The key methodology involves a "Primitive-Mesh" decomposition strategy to create a 1500k+ sample dataset of mesh subunits, followed by progressive training on vertex-to-face prediction and local mesh assembly tasks to learn topology. MeshLLM significantly outperforms the LLaMA-Mesh baseline, for instance, achieving a CLIP score of 0.391 against 0.124 in mesh understanding. The principal implication for AI practitioners is that decomposing complex structured data into localized parts provides a viable strategy to create large-scale datasets for fine-tuning LLMs on new modalities, bypassing the need for custom encoders. |
| UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and
  Precise Inference-Time Grounding (Read more on [arXiv](https://arxiv.org/abs/2507.22025) or [HuggingFace](https://huggingface.co/papers/2507.22025))| Bingqi Chen, Zihan Song, Jia Ma, Yuhang Wu, LianShuQuan | UI-AGILE is a framework that advances GUI agents by addressing training and inference challenges through effective reinforcement learning and precise inference-time grounding. It aims to resolve issues such as reasoning design dilemmas, ineffective reward signals, and degradation from visual noise in existing GUI agent techniques. The methodology includes a "Simple Thinking" reward for balancing planning with speed, continuous grounding reward for precise localization, cropping-based resampling to mitigate sparse reward, and decomposed grounding with selection to reduce visual noise during inference. Experiments demonstrate that UI-AGILE-7B with decomposed grounding achieves a 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro and establishes state-of-the-art performance on ScreenSpot-v2. This approach offers AI practitioners a comprehensive method to develop more accurate, robust, and generalizable GUI agents, particularly for high-resolution environments. |
| LightSwitch: Multi-view Relighting with Material-guided Diffusion (Read more on [arXiv](https://arxiv.org/abs/2508.06494) or [HuggingFace](https://huggingface.co/papers/2508.06494))| Shubham Tulsiani, Fernando De la Torre, thebluser | LightSwitch is a material-guided diffusion framework that performs fast and consistent multi-view relighting of 3D objects. The primary objective is to take an arbitrary number of posed images captured under a single unknown illumination and efficiently relight them to a new target lighting condition, overcoming the consistency issues of single-view methods and the slow speed of inverse rendering. The methodology involves finetuning a Stable Diffusion UNet by incorporating multi-view self-attention modules and conditioning it on inferred material properties (albedo, roughness, metallicness) and target lighting information. On a 2D synthetic object relighting task, LightSwitch achieved a PSNR of 26.01, outperforming prior methods like Neural Gaffer (24.34), and relit 3D objects in as little as 2 minutes, matching or exceeding the quality of state-of-the-art inverse rendering techniques that take hours. For AI practitioners, this work provides a highly efficient and scalable method for creating high-fidelity, relightable 3D assets from multi-view imagery, presenting a practical alternative to slow optimization-based pipelines for applications in virtual reality and visual effects. |
