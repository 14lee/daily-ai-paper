

## Papers for 2025-08-04

| Title | Authors | Summary |
|-------|---------|---------|
| Beyond Fixed: Variable-Length Denoising for Diffusion Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.00819) or [HuggingFace](https://huggingface.co/papers/2508.00819))| Jiaqi Wang, Yuhang Cao, Yuhang Zang, Xiaoyi Dong, Jinsong Li | This paper introduces DAEDAL, a training-free, two-stage strategy that enables dynamic variable-length generation for Diffusion Large Language Models (DLLMs). The objective is to overcome the critical limitation of DLLMs requiring a statically predefined generation length, which creates a trade-off between task performance and computational efficiency. DAEDAL's methodology first performs an "Initial Length Adjustment" by iteratively expanding the sequence based on the model's End-of-Sequence (EOS) token confidence, followed by an "Iterative Mask Insertion" phase that dynamically adds tokens to low-confidence regions during denoising. On the GSM8K benchmark, DAEDAL with the LLaDA-Instruct-8B model achieved 85.8% accuracy, outperforming the best-performing fixed-length baseline's 83.8% accuracy while using significantly fewer tokens on average (363 vs. 1024). For AI practitioners, this means DLLMs can be deployed without manual, task-specific length tuning, leading to improved computational efficiency and performance, thus making them a more viable alternative to autoregressive models. |
| PixNerd: Pixel Neural Field Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.23268) or [HuggingFace](https://huggingface.co/papers/2507.23268))| Limin Wang, Weilin Huang, Chenhui Zhu, Ziteng Gao, Shuai Wang | The paper introduces PixNerd, a single-stage, end-to-end pixel-space diffusion transformer that uses neural fields to model patch details, eliminating the reliance on pre-trained VAEs. The primary objective is to develop an efficient, single-stage pixel-space diffusion model that avoids the accumulated errors, decoding artifacts, and complex pipelines of two-stage latent diffusion models. The key methodology replaces the final linear projection layer of a diffusion transformer with a mechanism that predicts weights for a per-patch MLP (a neural field), which then decodes pixel-wise diffusion velocities from local coordinates and noisy pixel values. PixNerd achieves strong results, including a 2.15 FID on ImageNet 256Ã—256, which is competitive with latent diffusion models but without a VAE or complex cascade pipeline. For AI practitioners, PixNerd offers a simplified end-to-end framework for training high-resolution diffusion transformers directly in pixel space, bypassing the separate training and potential artifacts of VAEs. |
| SWE-Exp: Experience-Driven Software Issue Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.23361) or [HuggingFace](https://huggingface.co/papers/2507.23361))| Heng Lian, Yuling Shi, Xiaodong Gu, Shaoxin Lin, Silin Chen | SWE-Exp is an experience-enhanced framework for software issue resolution, enabling continuous learning and strategic repair. The primary objective of SWE-Exp is to address the memoryless exploration limitation of current LLM agents by enabling them to learn from and reuse past repair experiences. SWE-Exp introduces a multi-faceted experience bank, capturing successful and failed repair attempts at different levels, and employs a dual-agent architecture (Instructor and Assistant) integrated with an augmented MCTS framework for experience-driven guidance. Experiments show SWE-Exp achieves a state-of-the-art resolution rate of 41.6% Pass@1 on SWE-bench-Verified using DeepSeek-V3-0324, a 7.2% relative improvement over previous state-of-the-art methods using the same model. This approach transforms automated software engineering agents from trial-and-error explorers into strategic, experience-driven problem solvers, enabling systematic accumulation and leverage of repair expertise. |
| Multimodal Referring Segmentation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2508.00265) or [HuggingFace](https://huggingface.co/papers/2508.00265))| Zuxuan Wu, Chang Liu, Shuting He, Song Tang, Henghui Ding | This survey provides a comprehensive overview of multimodal referring segmentation, unifying task definitions, methodologies, and benchmarks across image, video, and 3D scenes. The paper's objective is to systematize the field by proposing a unified problem formulation and a general meta-architecture to categorize the diverse approaches for segmenting objects based on linguistic or audio expressions. The key methodology involves summarizing a unified meta-architecture consisting of modules for feature extraction, multimodal interaction, temporal processing, a segmentation head, and training objectives, while reviewing methods within one-stage and two-stage paradigms. The survey reports significant performance gains from foundation models; for instance, on the RefCOCOg benchmark for Referring Expression Segmentation, recent models like OneRef-L achieve up to 76.82% mIoU, substantially outperforming early methods that scored 34.06% mIoU. The principal implication for AI practitioners is that leveraging the presented meta-architecture and integrating large foundation models (e.g., SAM, MLLMs) within generalized frameworks like GRES (for multi-target scenarios) is crucial for developing robust, real-world systems capable of fine-grained perception from complex user instructions. |
| 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.23478) or [HuggingFace](https://huggingface.co/papers/2507.23478))| Hao Tang, Zeyu Zhang, Ting Huang | The 3D-R1 paper introduces a generalist vision-language model that enhances 3D scene understanding by using a synthetically generated Chain-of-Thought dataset for initialization, followed by a reinforcement learning framework to refine reasoning.  The primary objective is to improve the robust reasoning and generalization capabilities of 3D vision-language models, which currently struggle due to limitations in high-quality spatial data and static viewpoint assumptions.  The methodology consists of a two-stage process: first, supervised fine-tuning (SFT) on a newly created 30,000-sample Chain-of-Thought dataset (Scene-30K) to provide a "cold-start". This is followed by reinforcement learning using Group Relative Policy Optimization (GRPO) with three distinct reward functions (perception, semantic similarity, and format) to enhance reasoning precision.  The 3D-R1 model achieves an average performance improvement of 10% across various 3D scene benchmarks. For instance, on the ScanQA 3D question answering validation set, the model's CIDEr score improved from a 97.95 baseline to 106.45 after applying the full reinforcement learning framework.  For AI practitioners, this work provides a blueprint for enhancing specialized domain reasoning in foundation models: use a large language model to generate a structured, high-quality Chain-of-Thought dataset for initial supervised fine-tuning, and then apply targeted reinforcement learning with task-specific rewards to optimize policy for complex, multi-step inference. |
| SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.23348) or [HuggingFace](https://huggingface.co/papers/2507.23348))| Heng Lian, Xiaodong Gu, Shaoxin Lin, Yuling Shi, Han Li | SWE-Debate introduces a competitive multi-agent framework that improves automated software issue resolution by generating and debating multiple fault propagation traces from a code dependency graph. The paper's primary objective is to overcome the "limited observation scope" of single-agent systems, which struggle to resolve issues spanning complex codebases. The methodology involves three stages: 1) proposing multiple fault propagation traces by traversing a static code dependency graph, 2) conducting a three-round competitive debate among agents to select the best trace and synthesize a consolidated fix plan, and 3) using this plan to initialize a Monte Carlo Tree Search (MCTS) agent for patch generation. SWE-Debate achieves an 81.67% file-level fault localization accuracy on SWE-Bench-lite, a 3.93 percentage point improvement over the strongest baseline. For AI practitioners, the principal implication is that architecting multi-agent systems for competitive debate, rather than simple collaboration, can significantly improve performance on complex disambiguation tasks like fault localization by forcing a rigorous evaluation of diverse hypotheses. |
| Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges (Read more on [arXiv](https://arxiv.org/abs/2508.00454) or [HuggingFace](https://huggingface.co/papers/2508.00454))| Chengfei Lv, Zhiwen Chen, Yunfeng Wang, Kehua Feng, Yuqi Tang | This paper introduces an efficient multi-turn dialogue evaluator, MTDEval, that aggregates preference knowledge from multiple LLM judges. The main objective is to address the computational overhead and persistent biases associated with the LLM-as-a-judge paradigm for multi-turn dialogue evaluation. The methodology involves training a lightweight evaluator, composed of a Llama-3-8B text-embedding model and MLP scoring heads, on a large-scale pairwise preference dataset (P2-MTD) annotated by five state-of-the-art LLM judges, using maximum likelihood estimation with judge reliability prediction. Experimentally, MTDEval achieves superior inference efficiency with an average runtime of 0.10 seconds for single rating and 0.19 seconds for pairwise comparison on the Daily-MTD dataset, outperforming baseline models. This enables AI practitioners to perform fast, scalable, and robust multi-turn dialogue quality assessment, significantly reducing computational costs for large-scale and real-time evaluation scenarios. |
| Investigating Hallucination in Conversations for Low Resource Languages (Read more on [arXiv](https://arxiv.org/abs/2507.22720) or [HuggingFace](https://huggingface.co/papers/2507.22720))| Fatemeh Jamshidi, Zheng Zhang, Souvika Sarkar, Md. Najib Hasan, Amit Das | This research paper quantitatively evaluates hallucination in six large language models across conversational datasets for the low-resource languages of Hindi, Farsi, and Mandarin. The main objective is to analyze the factual accuracy and linguistic errors of GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1, and Qwen-3 in these specific linguistic contexts. The key methodology involved prompting the models with a conversational turn from translated datasets and measuring the generated output against a ground-truth response using ROUGE-1 and ROUGE-L scores, though the paper presents contradictory interpretations of what these scores signify regarding hallucination. The primary result and most impactful finding is the significant disparity in performance across languages; for instance, on the BlendedSkillTalk dataset, Qwen-3 achieved a ROUGE-L score of 3.83 in Farsi, while GPT-4o scored only 0.06 in Mandarin, highlighting that model behavior is highly dependent on the language. The principal implication for AI practitioners is that hallucination rates are strongly influenced by language resource availability, necessitating the use of mitigation techniques like Retrieval-Augmented Generation (RAG) or targeted fine-tuning when deploying LLMs for low-resource languages. |
| IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation (Read more on [arXiv](https://arxiv.org/abs/2508.00823) or [HuggingFace](https://huggingface.co/papers/2508.00823))| Jianjiang Feng, Ziwei Wang, Hang Yin, Xiuwei Xu, Wenxuan Guo | IGL-Nav introduces an incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation, aiming to enable robust visual navigation to a specified free-view image goal. The system leverages 3D Gaussian Splatting (3DGS) for incremental scene representation via feed-forward prediction and employs a coarse-to-fine localization strategy. This strategy includes 3D convolution on voxelized scene and target embeddings for coarse pose estimation, and differentiable 3DGS rendering with matching-constrained optimization for fine refinement. IGL-Nav achieves state-of-the-art performance, demonstrating an "Overall Narrow FOV" success rate (SR) of 57.0% and Success weighted by Path Length (SPL) of 48.2% in free-view image-goal navigation with supervised training, significantly outperforming prior methods. This work establishes a generalizable and practically viable approach for real-time image-goal navigation in robotics, facilitating strong sim-to-real transfer and diverse camera pose handling. |
| SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware
  Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.00782) or [HuggingFace](https://huggingface.co/papers/2508.00782))| Long Chen, Qifeng Chen, Yazhou Xing, Yingqing He, Kien T. Pham | SpA2V is a novel two-stage framework for generating spatially-aware videos from audio by first creating a video scene layout and then synthesizing the video. The main objective is to generate videos that are both semantically and spatially aligned with input audio by explicitly decoding and utilizing auditory cues like direction, distance, and movement. The methodology first uses a Multimodal Large Language Model (MLLM) with in-context learning to interpret audio and produce a Video Scene Layout (VSL) specifying object locations and captions; then, it employs a training-free combination of pre-trained diffusion models to generate the final video guided by this VSL. On the new AVLBench benchmark, SpA2V's layout generation stage achieved a MaxIoU score of 22.24 in translational scenarios, substantially outperforming the baseline score of 1.77. For AI practitioners, this research provides a practical, training-free pipeline for adding precise spatial control to audio-driven video generation by using MLLMs as intermediate planners to guide pre-trained generative models. |
