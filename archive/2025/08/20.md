

## Papers for 2025-08-20

| Title | Authors | Summary |
|-------|---------|---------|
| Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent
  Distillation and Agentic RL (Read more on [arXiv](https://arxiv.org/abs/2508.13167) or [HuggingFace](https://huggingface.co/papers/2508.13167))| Liam-Liu, hugteste, kangz, wanwan1212, tianyue818 | This paper introduces Chain-of-Agents (CoA), a paradigm for training single Agent Foundation Models (AFMs) to perform end-to-end complex problem-solving by emulating multi-agent collaboration within one model. The main |
| LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos (Read more on [arXiv](https://arxiv.org/abs/2508.14041) or [HuggingFace](https://huggingface.co/papers/2508.14041))| Yen-Yu Lin, Fu-En Yang, Cheng Sun, cmhungsteve, linjohnss | LongSplat is a robust 3D Gaussian Splatting framework for novel view synthesis from casual long videos without prior camera poses. The main objective is to overcome limitations of existing novel view synthesis methods in handling challenging real-world videos, including irregular camera motion, unknown poses, and expansive scenes. It employs an incremental optimization framework that jointly refines camera poses and 3D Gaussian representations, utilizing a guided pose estimation module, a visibility-adapted local window, and an efficient Octree Anchor Formation mechanism. LongSplat demonstrates superior performance, achieving an average PSNR of 32.83 dB on the Tanks & Temples dataset, outperforming CF-3DGS (29.56 dB), and reducing model size to approximately 101 MB. This advancement enables AI practitioners to develop more robust and efficient novel view synthesis systems for real-world applications where precise camera poses are unavailable or unreliable, particularly for large-scale environments and long video sequences. |
| Prompt Orchestration Markup Language (Read more on [arXiv](https://arxiv.org/abs/2508.13948) or [HuggingFace](https://huggingface.co/papers/2508.13948))| Yuqing Yang, Nan Chen, Yuge Zhang, Jiahang | Prompt Orchestration Markup Language (POML) is a novel markup language designed for structured prompt engineering. Its primary objective is to address challenges in sophisticated Large Language Model (LLM) prompting, including prompt structure, diverse data integration, format sensitivity, and inadequate tooling. POML employs component-based markup for logical structure, specialized tags for seamless data integration, a CSS-like styling system to decouple content from presentation, and a comprehensive developer toolkit. Empirical validation through case studies demonstrated that prompt styling variations, managed by POML, significantly impacted LLM accuracy; for instance, GPT-3.5-Turbo's accuracy ranged from 6% to 61.8% (a 929% relative improvement) and Phi-3 Medium from 0.7% to 32.2% (a 4450% improvement). This enables AI practitioners to systematically test, refine, and adapt prompt formats to target LLM sensitivities, thereby enhancing prompt reusability, maintainability, and collaborative development. |
| MultiRef: Controllable Image Generation with Multiple Visual References (Read more on [arXiv](https://arxiv.org/abs/2508.06905) or [HuggingFace](https://huggingface.co/papers/2508.06905))| Shiyun Lang, Siyuan Wu, Dongping Chen, Ruoxi Chen, wsnHowest | MultiRef introduces the first rigorous benchmark and dataset for controllable image generation using multiple visual references. Its primary objective is to evaluate the integration capabilities of current generative models beyond single-source inputs, quantifying their performance on complex multi-reference tasks. The methodology includes MULTIREF-BENCH, an evaluation framework comprising 1,990 samples (synthetic via REFBLEND, real-world from Reddit) and assessed using a combination of rule-based and MLLM-as-a-Judge metrics. Experiments reveal that state-of-the-art models struggle significantly, with the best model, OmniGen, achieving only 66.6% accuracy in synthetic samples and 79.0% in real-world cases on average compared to golden answers. This highlights a clear weakness in existing systems for multi-reference conditioning, providing critical direction for AI practitioners to develop more flexible and human-like creative tools that effectively integrate diverse visual inspirations. |
| Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2508.08777) or [HuggingFace](https://huggingface.co/papers/2508.08777))| Alice Wang, Edoardo D'Amico, Gustavo Penha, marcodena, frafabbri | This paper proposes a profile-aware LLM-as-a-Judge framework for scalable and interpretable offline evaluation of personalized podcast recommendations. The core objective is to provide a reliable, scalable middle ground for pre-deployment model selection in long-form audio domains where traditional metrics fall short. The approach involves generating natural-language user profiles from listening history, then using an LLM (GPT-4) in zero-shot mode to evaluate recommendation alignment point-wise and pair-wise against these profiles. In a controlled study, the profile-aware judge achieved a ROC-AUC of 0.6442 and a Model Selection Agreement (MSA) of 0.6596, outperforming or matching a raw history-based LLM variant and significantly exceeding a sBERT-Sim baseline. This framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems, providing an interpretable alternative to costly online experimentation. |
| Mind the Generation Process: Fine-Grained Confidence Estimation During
  LLM Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12040) or [HuggingFace](https://huggingface.co/papers/2508.12040))| Xinyi Wang, Jie Shi, Shisong Chen, Tingyun Li, JinyiHan | FineCE is a novel method for fine-grained confidence estimation during LLM generation. Its main objective is to provide accurate, continuous confidence scores throughout the LLM generation process, addressing the limitations of existing coarse-grained methods. The methodology involves a Monte Carlo sampling-based pipeline for training data construction, a supervised learning approach using Instruction Fine-Tuning, and a Backward Confidence Integration (BCI) strategy during inference. FineCE consistently achieves AUROC scores exceeding 70% and significantly lower calibration errors (e.g., ECE of 5.1% and AUROC of 77.8% on GSM8K with Llama2-13B), outperforming baselines by 10-15 percentage points in AUROC. This enables AI practitioners to enhance LLM trustworthiness and reliability, facilitate early termination of potentially incorrect responses, and improve downstream task performance, demonstrated by a 39.5% accuracy improvement on GSM8K with confidence-based filtering. |
| Training-Free Text-Guided Color Editing with Multi-Modal Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.09131) or [HuggingFace](https://huggingface.co/papers/2508.09131))| Deyu Zhou, Xili Dai, dorni, EvanTHU, zachary-yin | ColorCtrl presents a training-free method for text-guided, physically consistent color editing in images and videos using multi-modal diffusion transformers. The main objective is to enable fine-grained control over albedo, light source color, and ambient illumination while preserving geometry and material properties in untouched regions. This is achieved by modifying attention weights within MM-DiT-based models (e.g., SD3, FLUX.1-dev) through integrated structure preservation, regional color preservation, and word-level attribute re-weighting. Quantitatively, on the PIE-Bench dataset, ColorCtrl achieves a Background Preservation PSNR of 42.93 with SD3, demonstrating superior consistency compared to other methods. This approach offers AI practitioners a highly effective solution for precise, physically-aware content manipulation without requiring model retraining, enhancing the utility of existing large-scale diffusion models in real-world applications. |
| OmniTry: Virtual Try-On Anything without Masks (Read more on [arXiv](https://arxiv.org/abs/2508.13632) or [HuggingFace](https://huggingface.co/papers/2508.13632))| Xiaoduan Feng, Yiming Chen, Hengyuan Cao, Linlin Zhang, fengyutong | OmniTry introduces a unified mask-free virtual try-on framework supporting any wearable objects, moving beyond traditional garment-focused methods. The primary objective is to address data scarcity for diverse object types and enable mask-free try-on, overcoming limitations of existing VTON approaches. OmniTry utilizes a two-staged pipeline: an initial stage employs an inpainting diffusion transformer with a "traceless erasing" strategy on large-scale unpaired images for mask-free localization, followed by a second stage fine-tuned with limited paired images using two-stream adapters for object ID-consistency. Quantitatively, OmniTry achieves a M-CLIP-I of 0.8327 and SSIM of 0.9333 on the whole OmniTry-Bench dataset, and demonstrates quick convergence, achieving satisfying performance with even one paired sample per class in few-shot settings. This research implies that AI/ML engineers can now develop highly generalized virtual try-on applications for a broader range of objects with significantly reduced paired data requirements, enhancing practicality for e-commerce and content creation. |
| A Stitch in Time Saves Nine: Proactive Self-Refinement for Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.12903) or [HuggingFace](https://huggingface.co/papers/2508.12903))| Zishang Jiang, Tingyun li, Haiquan Zhao, Xinyi Wang, JinyiHan | This paper introduces Proactive Self-Refinement (PASR), a novel reinforcement learning framework enabling Language Models (LLMs) to perform dynamic self-correction during content generation. The research aims to enhance LLM output quality by allowing autonomous, proactive refinement without reliance on external feedback or fixed states, thereby improving robustness and adaptability. PASR employs Proximal Policy Optimization (GRPO) to train LLMs, framing the iterative refinement process as a Markov Decision Process and utilizing a hybrid reward scheme that integrates rule-based and model-evaluation mechanisms. PASR demonstrates significant performance gains, achieving an average accuracy improvement of 4.8% and 8.2% on Qwen2.5-7B and Qwen3-8B respectively, while also reducing average token consumption by 41.6% compared to standard generation. For AI practitioners, PASR provides a method to develop more reliable and efficient LLMs capable of self-adapting during generation, potentially reducing inference costs and increasing generalizability across diverse tasks. |
| Advances in Speech Separation: Techniques, Challenges, and Future Trends (Read more on [arXiv](https://arxiv.org/abs/2508.10830) or [HuggingFace](https://huggingface.co/papers/2508.10830))| Zhuo Chen, Yi Luo, Wendi Sang, Guo Chen, JusperLee | This paper presents a comprehensive survey of deep neural network-based speech separation techniques, challenges, and future trends. The objective is to provide a systematic and holistic examination of the rapidly evolving DNN-based speech separation field, addressing the lack of a unified, comprehensive overview. The survey analyzes various learning paradigms (supervised, unsupervised, self-supervised), architectural designs (CNNs, RNNs, Transformers, generative models), and audio estimation methods (masking, mapping). Quantitative evaluations on standard datasets reveal significant performance improvements; for instance, MossFormer2 achieves 24.1 SI-SDRi on the WSJ0-2mix dataset, demonstrating state-of-the-art capabilities. The review highlights the need for developing more robust, computationally efficient, and generalizable speech separation models for real-world applications, including multi-modal integration and online processing. |
| Embodied-R1: Reinforced Embodied Reasoning for General Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.13998) or [HuggingFace](https://huggingface.co/papers/2508.13998))| Fei Ni, Yibin Chen, Yaoting Huang, Haiqin Cui, Yifu Yuan | Embodied-R1 is a 3B Vision-Language Model (VLM) designed to bridge the perception-action gap in general robotic manipulation through a pointing-centric representation. Its primary objective is to enhance generalization in embodied AI by overcoming data scarcity and embodiment heterogeneity, which hinder the translation of perceptual understanding into robotic actions. The model is trained using a two-stage Reinforced Fine-tuning (RFT) curriculum with multi-task reward design on the large-scale Embodied-Points-200K dataset, which supports four core embodied pointing abilities. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks and demonstrates robust zero-shot generalization with an 87.5% success rate across 8 real-world XArm tasks, representing a 62% improvement over strong baselines. This work suggests that a pointing-centric representation combined with RFT offers a generalizable pathway for AI practitioners to close the perception-action gap in robotics, enabling robust performance in novel settings. |
| Copyright Protection for Large Language Models: A Survey of Methods,
  Challenges, and Trends (Read more on [arXiv](https://arxiv.org/abs/2508.11548) or [HuggingFace](https://huggingface.co/papers/2508.11548))| Xixiang Zhao, Qichen Liu, Xubin Yue, Zhenhua Xu, BreynaldDva | This survey comprehensively reviews copyright protection techniques for Large Language Models (LLMs), focusing on model fingerprinting. It addresses the absence of a unified framework by systematically categorizing intrinsic and invasive fingerprinting methods and, for the first time, examining fingerprint transfer and removal. The methodology involves clarifying conceptual distinctions, providing overviews of techniques, and summarizing evaluation metrics such as effectiveness, harmlessness, robustness, stealthiness, and reliability. Key findings reported include intrinsic fingerprinting methods achieving 71-85% F1 scores for secure model identification in encrypted environments, while also highlighting the challenge of rapid degradation of learning-based watermarking signals under continued training. This work offers AI practitioners a consolidated reference point and a structured agenda to foster advancements in LLM intellectual property protection. |
| TempFlow-GRPO: When Timing Matters for GRPO in Flow Models (Read more on [arXiv](https://arxiv.org/abs/2508.04324) or [HuggingFace](https://huggingface.co/papers/2508.04324))| Jian Yang, Wanli Li, Yuke Zhao, Siming Fu, shreddedpork | TempFlow-GRPO is a principled GRPO framework designed to overcome the limitations of temporal uniformity and sparse terminal rewards in flow-based generative models. The main research objective is to achieve precise credit assignment for intermediate actions and adapt optimization intensity to each timestep's exploration capacity in flow models. Its key methodology involves two innovations: a trajectory branching mechanism for precise process reward attribution, and a noise-aware policy weighting scheme that modulates optimization intensity based on a timestep's intrinsic noise level. TempFlow-GRPO achieved state-of-the-art performance on text-to-image benchmarks, notably increasing the Geneval overall score from 0.63 to 0.97 and requiring only 2,000 steps to reach 0.95 compared to Flow-GRPO's 5,600 steps. The principal implication for AI practitioners is a more efficient and effective reinforcement learning approach for aligning generative flow models with human preferences, leading to superior sample quality and faster convergence by respecting inherent temporal dynamics. |
| Leveraging Large Language Models for Predictive Analysis of Human Misery (Read more on [arXiv](https://arxiv.org/abs/2508.12669) or [HuggingFace](https://huggingface.co/papers/2508.12669))| Abhilash Nandy, Aman Bansal, Rahul Seetharaman, Bishanka Seal | This research evaluates Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The study's objective is to assess LLMs' viability in assigning a scalar misery value (0-100) to natural language inputs, framed as a regression problem. It compares zero-shot, fixed-context few-shot, and retrieval-based prompting strategies, and introduces a "Misery Game Show Simulation" for gamified evaluation, including feedback-driven reasoning. Few-shot and embedding-guided strategies consistently outperform zero-shot baselines, with few-shot (k=2) achieving the highest Pearson correlation (0.606). For AI practitioners, these findings indicate that contextual prompting and feedback mechanisms significantly enhance LLMs' capabilities in fine-grained affective prediction tasks, underscoring their potential in human-aligned emotional reasoning systems. |
| Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence (Read more on [arXiv](https://arxiv.org/abs/2508.13139) or [HuggingFace](https://huggingface.co/papers/2508.13139))| Xin Chen, Zhiyang Dou, Zixin Yin, Yuhong Zhang, Ling-Hao Chen | Motion2Motion proposes a novel framework for cross-topology motion transfer using sparse correspondence and motion patching to achieve robust and coherent animation retargeting. The paper aims to enable robust motion transfer between character skeletons of significantly diverse topologies, addressing limitations of existing methods that often rely on dense correspondences or large-scale datasets. Motion2Motion leverages motion patching for temporal-spatial decomposition and sparse keypoint correspondence between source and target skeletons, employing an iterative matching-based process to retrieve and blend motion patches, conditioned by a generative process for noise. On similar-skeleton transfer, Motion2Motion achieved an FID of 0.033 and a frequency alignment of 96.2%, outperforming baselines in motion quality and coherence; it also demonstrated superior performance in cross-species transfer. This framework provides AI practitioners with a robust and practical solution for animating highly diverse character models, including those with complex, high-DoF skeletons, from existing motion data, reducing the need for extensive manual rigging or large specialized datasets. |
| CorrSteer: Steering Improves Task Performance and Safety in LLMs through
  Correlation-based Sparse Autoencoder Feature Selection (Read more on [arXiv](https://arxiv.org/abs/2508.12535) or [HuggingFace](https://huggingface.co/papers/2508.12535))| Adriano Koshiyama, Zekun Wu, seonglae | CorrSteer introduces a novel method for improving Large Language Model (LLM) performance and safety through correlation-based Sparse Autoencoder (SAE) feature selection and steering. The primary objective is to overcome limitations of existing SAE-based steering, which typically require contrastive datasets or extensive activation storage, by directly leveraging generation-time features correlated with task outcomes. The methodology involves calculating Pearson correlation between generation-time SAE activations (specifically from the last token) and task performance scores, then using the mean activation of successful samples to determine steering coefficients applied to residual stream activations at inference. Experimental results demonstrate that CorrSteer improves MMLU performance by +4.1% and HarmBench performance by +22.9% on Gemma 2 2B and LLaMA 3.1 8B models, utilizing only 4000 samples. This establishes a scalable and automated pipeline for SAE-based LLM steering, offering AI practitioners an efficient approach for enhancing model adherence to specific tasks and improving safety. |
| MedSAMix: A Training-Free Model Merging Approach for Medical Image
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2508.11032) or [HuggingFace](https://huggingface.co/papers/2508.11032))| Jonas Geiping, Francesco Sammarco, Jiesi Hu, guinansu, podismine | MedSAMix proposes a training-free model merging framework for medical image segmentation to balance specialization and generalization in SAM-based models. The core objective is to enhance domain-specific capabilities while mitigating generalization compromise across diverse medical tasks. MedSAMix employs a zero-order optimization approach using SMAC to automatically discover optimal layer-wise merging configurations for image encoder, prompt encoder, and mask decoder, supporting both single-task and multi-objective optimization. Across 25 medical image segmentation tasks, MedSAMix achieved an average Dice coefficient improvement of 6.67% on specialized tasks and 4.37% on multi-task evaluations compared to baseline models. This approach offers an efficient, training-free solution for AI practitioners to integrate generalist and specialist medical segmentation models, addressing data heterogeneity and privacy concerns without requiring extensive retraining. |
| Semantic IDs for Joint Generative Search and Recommendation (Read more on [arXiv](https://arxiv.org/abs/2508.10478) or [HuggingFace](https://huggingface.co/papers/2508.10478))| Enrico Palumbo, Edoardo D'Amico, Gustavo Penha, frafabbri, marcodena | This paper investigates Semantic ID construction for joint generative search and recommendation models. The primary objective is to determine if Semantic IDs can perform well across both search and recommendation tasks within a unified generative framework. The methodology compares various Semantic ID generation strategies, including task-specific (e.g., search-tuned bi-encoder embeddings) and cross-task approaches (e.g., multi-task fine-tuned bi-encoder embeddings), integrated into a Flan-T5 generative model. Results demonstrate a trade-off, where search-tuned IDs boost retrieval (0.072 Search R@30) but reduce recommendation performance (0.026 Rec R@30), while the proposed multi-task bi-encoder achieves a balanced performance (0.046 Search R@30, 0.049 Rec R@30). This implies that a shared, jointly fine-tuned embedding space for Semantic IDs can effectively reconcile task trade-offs, promoting generalizable ID schemes for multi-task generative recommender architectures. |
| Describe What You See with Multimodal Large Language Models to Enhance
  Video Recommendations (Read more on [arXiv](https://arxiv.org/abs/2508.09789) or [HuggingFace](https://huggingface.co/papers/2508.09789))| Mounia Lalmas, Andreas Damianou, marcodena | This paper introduces a zero-finetuning framework to enhance video recommendations using Multimodal Large Language Models (MLLMs). The primary research question addresses whether MLLM-derived captions outperform classical content features in standard ranking tasks. The methodology involves prompting off-the-shelf MLLMs, specifically Qwen-VL, to generate rich natural-language descriptions from video and audio content, which are then used as input for standard collaborative and generative recommenders. Quantitatively, replacing raw audio features with MLLM-generated text boosted the two-towers model's HR@10 from 0.0253 to 0.0405, representing approximately a 60% relative gain on the MicroLens-100K dataset. This work highlights MLLMs' transformative potential for AI practitioners, offering a low-barrier pathway to develop more accurate, contextually rich, and user-aligned video recommendation systems. |
| Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned
  and Addressed for XR Research (Read more on [arXiv](https://arxiv.org/abs/2508.04326) or [HuggingFace](https://huggingface.co/papers/2508.04326))| Susanne Schmidt, Mana Masuda, Mugichoko445, cocolinux | This paper surveys how radiance fields (RFs) are envisioned and addressed in Extended Reality (XR) research. The objective is to analyze how RFs are envisioned and addressed for XR applications, how RFs are specifically tackled in XR research, and how different research communities contribute to RF for XR. A systematic survey was conducted on 1,305 papers from major computer vision, graphics, robotics, multimedia, human-computer interaction, and XR conferences, classifying 299 as "XR-Envisioned" papers that explicitly address RFs for XR. Of the XR-Envisioned papers, 58% originated from the Computer Vision community, with "Interactive Experiences" (32%), "User-centric Rendering" (26%), and "Optimization Techniques" (24%) identified as leading research themes. For AI practitioners, this survey identifies critical research gaps and opportunities in RFs for XR, highlighting the need for improved real-time performance, user-centric evaluation, and interdisciplinary collaboration to advance practical RF applications in XR systems. |
| MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic
  Evaluation of Audio General Intelligence (Read more on [arXiv](https://arxiv.org/abs/2508.13992) or [HuggingFace](https://huggingface.co/papers/2508.13992))| Fernando López, Vaibhavi Lokegaonkar, Šimon Sedláček, Sonal Kumar, Sreyan88 | The paper introduces MMAU-Pro, a comprehensive and challenging benchmark for evaluating audio general intelligence in AI systems. Its primary objective is to holistically assess auditory understanding across diverse domains and complex reasoning dimensions, addressing limitations of prior benchmarks. MMAU-Pro comprises 5,305 expert-annotated question-answer pairs spanning 49 distinct skills across speech, sound, music, and their mixtures, incorporating long-form, multi-audio, spatial, and multicultural understanding with multi-hop reasoning. Evaluations of 22 leading multimodal AI models revealed significant limitations, with state-of-the-art models like Gemini 2.5 Flash achieving only 59.2% accuracy and demonstrating particular struggles with multi-audio reasoning and multicultural music. This benchmark highlights critical shortcomings in current models, such as shallow audio grounding and dataset biases, guiding AI practitioners to enhance future systems toward more robust and generalizable audio intelligence by focusing on complex real-world scenarios. |
| MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents (Read more on [arXiv](https://arxiv.org/abs/2508.13186) or [HuggingFace](https://huggingface.co/papers/2508.13186))| Jun Dong, Jiaheng Liu, Wenjie Wang, Xingyuan Bu, Shilong Li | MM-BrowseComp is a novel benchmark designed to evaluate multimodal retrieval and reasoning capabilities of AI browsing agents. The main objective is to bridge the gap in existing benchmarks by assessing agents' ability to process and reason with multimodal content, including images and videos, often embedded within webpages. The key methodology involves 224 challenging, hand-crafted questions with accompanying verified checklists, ensuring that approaches relying solely on text are insufficient. Primary results indicate that state-of-the-art models, such as OpenAI o3 with tools, achieve only 29.02% accuracy, revealing suboptimal multimodal capabilities and a lack of native multimodal reasoning. The principal implication for AI practitioners is the critical need for developing stronger multimodal backbones and synergistically integrating robust tools with reasoning abilities to enhance future browsing agents. |
| ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval
  Driven LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2508.04038) or [HuggingFace](https://huggingface.co/papers/2508.04038))| Flora D. Salim, Hao Xue, Breezelled, zechenli03 | ZARA is an agent-based framework enabling zero-shot, explainable Human Activity Recognition (HAR) directly from raw motion time-series using Large Language Models (LLMs). The objective is to overcome limitations of existing HAR methods, specifically poor generalization, limited zero-shot capability, and lack of interpretability, by enhancing LLMs with structured, sensor-specific knowledge and retrieval for unseen activities. ZARA integrates an automatically derived pair-wise feature knowledge base, a multi-sensor retrieval module using pre-trained time-series foundation encoders (e.g., Mantis) for evidence, and a hierarchical multi-agent LLM pipeline (e.g., Gemini-2.0-Flash) for iterative feature selection, evidence pruning, and activity prediction with natural-language explanations. Experiments across 8 HAR benchmarks demonstrate ZARA's robust performance under Classifier-Free Generalization, achieving a state-of-the-art 81.4% average macro F1 score and outperforming the strongest baseline (UniMTS) by 2.53x. ZARA provides AI practitioners with a plug-and-play, interpretable, and flexible solution for zero-shot HAR from raw motion time-series, eliminating the need for costly retraining or task-specific classifiers in new activity or sensor configurations. |
| Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.13804) or [HuggingFace](https://huggingface.co/papers/2508.13804))| Alina Landowska, maciejskorski | This paper presents a large-scale Bayesian evaluation of large language models' ability to detect moral foundations, finding they outperform human annotators in sensitivity. The primary objective is to evaluate LLMs' moral classification capabilities against human judgment by modeling annotator disagreement to capture inherent uncertainty, |
