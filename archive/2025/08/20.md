

## Papers for 2025-08-20

| Title | Authors | Summary |
|-------|---------|---------|
| Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent
  Distillation and Agentic RL (Read more on [arXiv](https://arxiv.org/abs/2508.13167) or [HuggingFace](https://huggingface.co/papers/2508.13167))| Liam-Liu, hugteste, kangz, wanwan1212, tianyue818 | Chain-of-Agents (CoA) introduces Agent Foundation Models (AFMs) as a novel paradigm for end-to-end complex problem-solving via multi-agent collaboration within a single LLM. The primary objective is to overcome the computational inefficiency and limited data-centric learning of existing multi-agent systems by enabling dynamic, multi-agent collaboration and tool orchestration within a unified model. The methodology involves multi-agent knowledge distillation for agentic supervised fine-tuning (SFT) to capture expert decision-making patterns, followed by agentic reinforcement learning (RL) on verifiable tasks to further refine problem-solving capabilities. Empirical studies show that AFMs establish new state-of-the-art performance across various benchmarks; for example, AFM-RL-32B attained an average accuracy of 78.0% on mathematical reasoning tasks, improving upon ReTool-32B's 74.4%. This work demonstrates that AFMs offer a more efficient and coherent framework for complex tasks, with all model weights, code, and training data open-sourced to foster future research in agent models and agentic RL. |
| LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos (Read more on [arXiv](https://arxiv.org/abs/2508.14041) or [HuggingFace](https://huggingface.co/papers/2508.14041))| Yen-Yu Lin, Fu-En Yang, Cheng Sun, cmhungsteve, linjohnss | LongSplat is a robust 3D Gaussian Splatting framework for novel view synthesis from unposed, casually captured long videos. Its primary objective is to accurately reconstruct 3D scenes and generate novel views without relying on provided camera poses, even with irregular camera motion. The methodology involves an incremental joint optimization pipeline that integrates correspondence-guided camera pose estimation and photometric refinement with adaptive octree-anchored 3DGS, alternating between local and global optimization. LongSplat consistently outperforms state-of-the-art baselines; for instance, on the Free dataset, it yields an average PSNR of 27.88 and an Absolute Trajectory Error (ATE) of 0.028, while achieving 281.71 FPS and a 101 MB model size. This robust unposed reconstruction capability, coupled with high efficiency and memory optimization, provides AI practitioners with a practical solution for 3D scene reconstruction and novel view synthesis from challenging real-world video data. |
| Prompt Orchestration Markup Language (Read more on [arXiv](https://arxiv.org/abs/2508.13948) or [HuggingFace](https://huggingface.co/papers/2508.13948))| Yuqing Yang, Nan Chen, Yuge Zhang, Jiahang | Prompt Orchestration Markup Language (POML) is a novel markup language designed to address critical challenges in LLM prompt engineering, including structure, data integration, format sensitivity, and tooling. POML employs component-based markup for logical structure, specialized tags for seamless data integration (documents, tables, images), and a CSS-like styling system to decouple content from presentation, complemented by a templating engine and a comprehensive developer toolkit. A TableQA case study validated POML's impact, showing that styling variations, managed by POML, can significantly affect LLM performance; for instance, GPT-3.5-Turbo's accuracy improved by 929% (from 6% to 61.8%) and Phi-3 Medium's by 4450% (from 0.7% to 32.2%) between their worst and best styles. These findings underscore POML's ability to streamline the prompt engineering lifecycle, enabling AI practitioners to systematically test and optimize prompt styles for various LLM models and tasks, thereby enhancing authoring efficiency and collaboration for complex, data-intensive applications. |
| MultiRef: Controllable Image Generation with Multiple Visual References (Read more on [arXiv](https://arxiv.org/abs/2508.06905) or [HuggingFace](https://huggingface.co/papers/2508.06905))| Shiyun Lang, Siyuan Wu, Dongping Chen, Ruoxi Chen, wsnHowest | MultiRef introduces a novel benchmark and dataset for evaluating controllable image generation using multiple visual references. The research aims to address the limitations of existing generative models in effectively combining diverse visual inputs beyond single-source conditioning. Their methodology involves MultiRef-Bench, comprising 1,990 real-world and synthetic examples generated by the REFBLEND data engine with 10 reference types and 33 combinations, evaluated using rule-based metrics and MLLM-as-a-Judge. Primary results show that state-of-the-art models struggle with multi-reference conditioning, with the best model, OmniGen, achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. This highlights a clear weakness in current image generation systems, guiding AI practitioners to develop more flexible and human-like creative tools capable of integrating multiple visual inspirations. |
| Mind the Generation Process: Fine-Grained Confidence Estimation During
  LLM Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12040) or [HuggingFace](https://huggingface.co/papers/2508.12040))| Xinyi Wang, Jie Shi, Shisong Chen, Tingyun Li, JinyiHan | FineCE is a novel method for fine-grained confidence estimation in Large Language Models (LLMs) during text generation. The primary objective is to provide accurate and continuous confidence scores throughout the LLM generation process, thereby enhancing model calibration and trustworthiness. The methodology includes a data construction pipeline using Monte Carlo Sampling for probabilistic distributions, supervised training, and a Backward Confidence Integration (BCI) strategy at inference to refine estimates with future context, alongside strategies for optimal confidence estimation positions. Experiments demonstrate FineCE consistently achieves AUROC scores exceeding 70%, outperforming baselines by 10-15 percentage points, and significantly reduces calibration errors (e.g., ECE 6.7% vs baselines 19.2-28.3%). For AI practitioners, this enables early error detection, informed LLM decision-making, and confidence-based output filtering during generation, directly improving LLM reliability and practical utility. |
| Training-Free Text-Guided Color Editing with Multi-Modal Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.09131) or [HuggingFace](https://huggingface.co/papers/2508.09131))| Deyu Zhou, Xili Dai, dorni, EvanTHU, zachary-yin | This paper introduces ColorCtrl, a training-free text-guided method for color editing using Multi-Modal Diffusion Transformers (MM-DiT) that preserves geometry and material properties. The primary objective is to develop a training-free text-guided color editing method capable of accurately and consistently modifying colors in images and videos while preserving critical visual attributes like geometry, material properties, and light-matter interaction. ColorCtrl leverages the attention mechanisms within pre-trained MM-DiT models to achieve precise control over color attributes (via attribute re-weighting) and maintain structural integrity (via structure preservation) by manipulating attention maps and color tokens. The method demonstrates superior performance against existing training-free approaches and competitive results with commercial models; for instance, on the SD3 benchmark, ColorCtrl achieved a Canny score of 0.8473, indicating improved geometry preservation. This training-free approach, leveraging readily available MM-DiT models, offers AI practitioners a robust and efficient solution for high-fidelity text-guided color manipulation in image and video editing, reducing the need for extensive model retraining and enabling fine-grained artistic control. |
| Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2508.08777) or [HuggingFace](https://huggingface.co/papers/2508.08777))| Alice Wang, Edoardo D'Amico, Gustavo Penha, marcodena, frafabbri | This paper introduces a profile-aware LLM-as-a-Judge framework for evaluating personalized podcast recommendations. The main objective is to establish a scalable and interpretable offline method for assessing recommendation quality, addressing limitations of traditional metrics. The key methodology involves a two-stage process: first, natural-language user profiles are distilled from 90 days of listening history, and then, a GPT-4 LLM is prompted with these profiles for fine-grained pointwise and pairwise judgments of recommendation alignment. Primary results show that the LaaJ-Profile judge achieved a 0.6442 ROC-AUC for episode-level evaluation and outperformed or matched a variant using raw listening histories, correctly identifying 66% of strongly misaligned episodes. This framework offers AI practitioners a scalable, reliable middle ground for pre-deployment model selection and iterative testing in recommender systems, bridging the gap between coarse offline metrics and subjective human-aligned assessments. |
| OmniTry: Virtual Try-On Anything without Masks (Read more on [arXiv](https://arxiv.org/abs/2508.13632) or [HuggingFace](https://huggingface.co/papers/2508.13632))| Xiaoduan Feng, Yiming Chen, Hengyuan Cao, Linlin Zhang, fengyutong | OmniTry is a unified, mask-free virtual try-on framework designed to extend VTON beyond garments to any wearable objects. Its main objective is to enable try-on of diverse items without masks, addressing data curation challenges for unpaired images. The framework employs a two-staged pipeline: an initial stage leverages large-scale unpaired images with a repurposed inpainting diffusion transformer and traceless erasing for mask-free localization. The second stage then fine-tunes the model with paired images for object appearance consistency using two-stream adapters. OmniTry demonstrates superior performance on its OmniTry-Bench, achieving a M-CLIP-I of 0.8327 on the whole dataset, and exhibits rapid convergence even with few paired samples. This approach offers AI practitioners a robust and data-efficient solution for generalized virtual try-on, significantly broadening its application scope in e-commerce and digital fashion. |
| A Stitch in Time Saves Nine: Proactive Self-Refinement for Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.12903) or [HuggingFace](https://huggingface.co/papers/2508.12903))| Zishang Jiang, Tingyun li, Haiquan Zhao, Xinyi Wang, JinyiHan | ProActive Self-Refinement (PASR) is a novel reinforcement learning method enabling Large Language Models (LLMs) to perform adaptive, in-process self-refinement during generation. The main objective is to empower LLMs to proactively refine their outputs during the generation process, overcoming limitations of traditional reactive, post-hoc refinement methods. PASR employs an on-policy Reinforcement Learning (RL) approach, guided by a comparison-based reward strategy, to dynamically determine when and how to refine based on the evolving generation state. Experimental results show significant improvements; for instance, on Qwen3-8B, PASR achieved an 8.2% improvement in accuracy while concurrently reducing average token consumption by 41.6%. This method offers AI practitioners a path to develop more reliable and resource-efficient LLM systems by enabling autonomous, real-time quality and efficiency improvements during content generation. |
| Advances in Speech Separation: Techniques, Challenges, and Future Trends (Read more on [arXiv](https://arxiv.org/abs/2508.10830) or [HuggingFace](https://huggingface.co/papers/2508.10830))| Zhuo Chen, Yi Luo, Wendi Sang, Guo Chen, JusperLee | This paper systematically surveys deep neural network-based speech separation, clarifying the current landscape and assessing key technologies. Its objective is to provide a comprehensive guide by holistically examining learning paradigms, architectural components, and evaluation methods. The methodology involves extensive comparative analysis and reproducible benchmarking on standard datasets like WSJ0-2mix and LibriMix. Key results indicate significant performance advancements, with models like MossFormer2 achieving up to 24.1 SI-SDRi on the WSJ0-2mix dataset. The principal implication for AI practitioners is a clear roadmap highlighting challenges such as long-form audio processing and lightweight model design, alongside promising directions like generative models and pre-trained architectures for real-world deployment. |
| Embodied-R1: Reinforced Embodied Reasoning for General Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.13998) or [HuggingFace](https://huggingface.co/papers/2508.13998))| Fei Ni, Yibin Chen, Yaoting Huang, Haiqin Cui, Yifu Yuan | Embodied-R1 introduces a 3B Vision-Language Model for general robotic manipulation, leveraging a novel "pointing" centric representation to bridge the "seeing-to-doing gap" caused by data scarcity and embodiment heterogeneity. The model is trained using a two-stage Reinforced Fine-tuning (RFT) curriculum on the Embodied-Points-200K dataset, which supports four defined embodied pointing abilities. Embodied-R1 achieved state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrated robust zero-shot generalization, achieving a 56.2% success rate in SIMPLEREnv and 87.5% across 8 real-world XArm tasks, a 62% improvement over baselines. This research indicates that a pointing-centric representation, coupled with an RFT training paradigm, is a generalizable approach for closing the perception-action gap in robotics. |
| Copyright Protection for Large Language Models: A Survey of Methods,
  Challenges, and Trends (Read more on [arXiv](https://arxiv.org/abs/2508.11548) or [HuggingFace](https://huggingface.co/papers/2508.11548))| Xixiang Zhao, Qichen Liu, Xubin Yue, Zhenhua Xu, BreynaldDva | This survey offers a comprehensive overview of copyright protection for Large Language Models (LLMs), clarifying the distinctions between text watermarking and model fingerprinting. The main objective is to systematically categorize existing LLM copyright protection methods, fostering advancements in intellectual property for these models. The paper's methodology involves analyzing diverse text watermarking techniques and detailing model fingerprinting, which is categorized into intrinsic (parameter/representation, semantic feature, adversarial example-based) and invasive (weight-based, backdoor-based) approaches, along with discussions on fingerprint transfer and removal. While a survey, it highlights findings such as ensemble learning frameworks achieving 99.88% precision in detecting latent stylistic relationships among LLM families. The principal implication for AI practitioners is the critical need to adopt dedicated model fingerprinting methods that ensure robust LLM ownership attribution and resilience against modifications, safeguarding intellectual property and promoting long-term innovation. |
| TempFlow-GRPO: When Timing Matters for GRPO in Flow Models (Read more on [arXiv](https://arxiv.org/abs/2508.04324) or [HuggingFace](https://huggingface.co/papers/2508.04324))| Jian Yang, Wanli Li, Yuke Zhao, Siming Fu, shreddedpork | TempFlow-GRPO is a temporally-aware reinforcement learning framework designed to improve human preference alignment and generation quality in flow-based text-to-image models. It aims to overcome limitations in existing GRPO methods for flow models, specifically addressing sparse terminal rewards and uniform optimization weighting by enabling precise credit assignment and adapting optimization intensity to each timestep's exploration capacity. The framework introduces trajectory branching to attribute terminal rewards to intermediate exploratory actions and a noise-aware policy weighting scheme that modulates optimization intensity based on timestep-specific noise levels. Experiments demonstrate state-of-the-art performance; for instance, on the Geneval benchmark, TempFlow-GRPO achieved an overall score of 0.97 within 4,400 steps, significantly outperforming Flow-GRPO's 0.90 score under the same conditions. For AI practitioners, TempFlow-GRPO provides a robust approach for training generative models more efficiently and effectively by explicitly leveraging temporal dynamics, leading to superior sample quality and human preference alignment. |
| Leveraging Large Language Models for Predictive Analysis of Human Misery (Read more on [arXiv](https://arxiv.org/abs/2508.12669) or [HuggingFace](https://huggingface.co/papers/2508.12669))| Abhilash Nandy, Aman Bansal, Rahul Seetharaman, Bishanka Seal | This paper evaluates Large Language Models' ability to predict numerical misery scores from text using various prompting strategies and a novel gamified framework. The main objective is to assess LLM performance on a continuous misery score regression task (0-100) and to evaluate their dynamic emotional reasoning capabilities under corrective feedback. The methodology involves benchmarking zero-shot, few-shot, and retrieval-augmented prompting, alongside a gamified evaluation framework that tests ordinal, binary, and scalar reasoning. The primary result is that embedding-based few-shot prompting significantly reduces Mean Absolute Error to 12.3, a substantial improvement over the zero-shot baseline of 23.48. For AI practitioners, the principal implication is that implementing retrieval-augmented few-shot prompting with semantically relevant examples is critical for improving accuracy in fine-grained affective regression tasks. |
| Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence (Read more on [arXiv](https://arxiv.org/abs/2508.13139) or [HuggingFace](https://huggingface.co/papers/2508.13139))| Xin Chen, Zhiyang Dou, Zixin Yin, Yuhong Zhang, Ling-Hao Chen | Motion2Motion introduces a training-free framework for cross-topology motion transfer between characters with substantially different skeletal topologies using sparse bone correspondences. The main research objective is to enable motion transfer across diverse skeletal topologies, addressing limitations of inherent topological inconsistency and scarce paired motion datasets. Its key methodology formulates transfer as an iterative patch-based motion matching and blending procedure, utilizing sparse joint correspondences and a few target motion examples without requiring deep model training or GPUs. Motion2Motion significantly outperforms baselines, achieving a Fréchet Inception Distance (FID) of 0.033 and 96.2% frequency alignment in similar skeleton transfer, compared to 0.507 and 72.0% for the best baseline. For AI practitioners, this offers a real-time, scalable, and data-efficient solution for topology-flexible motion adaptation, reducing reliance on large-scale datasets and enabling direct integration into animation workflows. |
| CorrSteer: Steering Improves Task Performance and Safety in LLMs through
  Correlation-based Sparse Autoencoder Feature Selection (Read more on [arXiv](https://arxiv.org/abs/2508.12535) or [HuggingFace](https://huggingface.co/papers/2508.12535))| Adriano Koshiyama, Zekun Wu, seonglae | CorrSteer is an automated method that improves LLM performance and safety by selecting and steering Sparse Autoencoder (SAE) features based on their correlation with task outcomes at inference time. The primary research objective is to develop a scalable steering pipeline that avoids reliance on contrastive datasets or large activation storage. The key methodology involves using Pearson correlation to relate SAE feature activations from generated tokens to task correctness scores for feature selection, then calculating steering coefficients from the average activations of successful samples. The method demonstrated significant performance gains, achieving a +22.9% absolute improvement on the HarmBench safety benchmark and a +4.1% improvement on MMLU for the Gemma 2 2B model using only 4000 samples. The principal implication for AI practitioners is a fully automated and efficient pipeline to enhance model capabilities for specific tasks, enabling targeted performance and safety improvements without requiring model retraining. |
| MedSAMix: A Training-Free Model Merging Approach for Medical Image
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2508.11032) or [HuggingFace](https://huggingface.co/papers/2508.11032))| Jonas Geiping, Francesco Sammarco, Jiesi Hu, guinansu, podismine | MedSAMix is a training-free framework that merges generalist (SAM) and specialist (MedSAM) models layer-wise to improve medical image segmentation performance. The primary objective is to enhance both domain-specific accuracy and generalization without requiring additional training data or computational overhead. The methodology employs a zero-order Bayesian optimization algorithm (SMAC) to automatically discover optimal layer-wise merging configurations by evaluating merged model performance on a small calibration dataset. On 25 medical segmentation tasks, MedSAMix achieved a 6.67% improvement in Dice coefficient for specialized single-task optimization and 4.37% for multi-task generalization compared to the best individual baseline model. The principal implication for AI practitioners is the ability to create superior models by combining existing foundation models with fine-tuned variants post-hoc, thereby mitigating single-model bias and bypassing the need for costly retraining or data aggregation. |
| Semantic IDs for Joint Generative Search and Recommendation (Read more on [arXiv](https://arxiv.org/abs/2508.10478) or [HuggingFace](https://huggingface.co/papers/2508.10478))| Enrico Palumbo, Edoardo D'Amico, Gustavo Penha, frafabbri, marcodena | This paper investigates strategies for constructing unified Semantic IDs for a joint generative search and recommendation model. The primary objective is to determine if a single Semantic ID scheme can achieve high performance on both tasks, mitigating the performance trade-offs observed when using task-specific IDs. The authors compare several methods, including a key approach that fine-tunes a bi-encoder on both search and recommendation data to create a unified embedding space before tokenization via RQ-KMeans. The results demonstrate that while task-specific IDs are optimal for their own domain, the multi-task approach provides the most effective trade-off, achieving a balanced Search R@30 of 0.046 and Recommendation R@30 of 0.049. For AI practitioners, this implies that generating Semantic IDs from a jointly trained, shared representation space is a superior strategy for building unified generative retrieval systems compared to using separate or naively combined task-specific embeddings. |
| Describe What You See with Multimodal Large Language Models to Enhance
  Video Recommendations (Read more on [arXiv](https://arxiv.org/abs/2508.09789) or [HuggingFace](https://huggingface.co/papers/2508.09789))| Mounia Lalmas, Andreas Damianou, marcodena | This paper presents a zero-finetuning framework leveraging Multimodal Large Language Models (MLLMs) to enhance video recommendations. The primary objective was to evaluate if MLLM-derived captions outperform classical content features in standard video ranking tasks for recommendation. The methodology involves prompting off-the-shelf MLLMs (Qwen-VL, Qwen-Audio with Whisper for audio transcription) to generate semantically rich natural-language descriptions of video and audio content, which are then encoded and fed into standard collaborative, content-based, and generative recommender architectures like two-towers and SASRec. Experiments on the MicroLens-100K dataset demonstrated that MLLM-generated audio descriptions yielded up to a 60% relative gain in HR@10 (from 0.0253 to 0.0405) for the two-towers model compared to raw audio features, and MLLM video descriptions boosted HR@10 from 0.0393 to 0.0489 (+24%) over video features. These findings imply that AI practitioners can significantly improve video recommendation quality by integrating MLLM-generated high-level semantic descriptions into existing systems, enabling more intent-aware and contextually rich recommendations without extensive finetuning of large foundation models. |
| Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned
  and Addressed for XR Research (Read more on [arXiv](https://arxiv.org/abs/2508.04326) or [HuggingFace](https://huggingface.co/papers/2508.04326))| Susanne Schmidt, Mana Masuda, Mugichoko445, cocolinux | This survey investigates the vision and implementation of radiance fields (RF), including NeRF and 3DGS, for XR research. The main objective was to analyze how RF is envisioned for XR applications, how they are implemented, and to identify remaining research gaps. A systematic survey following PRISMA 2020 guidelines was conducted on 365 XR-related RF papers from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, with an in-depth analysis of 66 "XR-Addressed" papers. Results revealed a significant research gap: for instance, while 203 RF-related papers were published at CVPR 2024 (with 68 mentioning XR), only 11 RF contributions appeared at leading XR conferences (IEEE VR/ISMAR 2024), with merely 5 directly addressing XR-related RF research questions. This work provides AI practitioners a resource to understand XR-specific RF research topics and navigate the field's rapid development, guiding future integration efforts into XR systems. |
| MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic
  Evaluation of Audio General Intelligence (Read more on [arXiv](https://arxiv.org/abs/2508.13992) or [HuggingFace](https://huggingface.co/papers/2508.13992))| Fernando López, Vaibhavi Lokegaonkar, Šimon Sedláček, Sonal Kumar, Sreyan88 | MMAU-Pro is a novel, comprehensive benchmark for holistically evaluating audio general intelligence in AI systems. It addresses the challenge of comprehensively assessing auditory intelligence, which existing benchmarks inadequately cover due to their limited scope and realistic complexity. The benchmark comprises 5,305 human expert-annotated question-answer instances across 49 distinct skills in speech, sound, and music, sourcing audio data directly "from the wild" and employing a multi-stage human-involved curation pipeline. Evaluations of 22 leading multimodal AI models reveal significant limitations, with state-of-the-art models like Gemini 2.5 Flash and Audio Flamingo 3 achieving only 59.2% and 51.7% accuracy, respectively. These findings highlight specific shortcomings in current models, such as shallow audio grounding and poor performance in multi-audio and spatial reasoning, offering clear directions for future AI system development toward general audio intelligence. |
| MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents (Read more on [arXiv](https://arxiv.org/abs/2508.13186) or [HuggingFace](https://huggingface.co/papers/2508.13186))| Jun Dong, Jiaheng Liu, Wenjie Wang, Xingyuan Bu, Shilong Li | MM-BrowseComp is a novel benchmark designed to assess advanced AI agents' ability to synthesize deep reasoning with persistent, multimodal web browsing. Its objective is to bridge gaps in existing benchmarks by requiring agents to retrieve and reason with multimodal content, including images and videos, beyond text. The methodology involves 224 hand-crafted questions with mandatory multimodal dependency and an irreducible reasoning checklist for fine-grained process evaluation. Primary results indicate that state-of-the-art models struggle significantly, with OpenAI o3 achieving the highest Overall Accuracy at only 29.02%, and other models failing to surpass 10%. This demonstrates that high performance in multimodal browsing necessitates a synergistic combination of strong foundational reasoning abilities and a comprehensive, robust toolset. |
| ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval
  Driven LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2508.04038) or [HuggingFace](https://huggingface.co/papers/2508.04038))| Flora D. Salim, Hao Xue, Breezelled, zechenli03 | ZARA is a zero-shot, agent-based framework that uses a hierarchical pipeline of LLM agents, a pre-computed knowledge base, and retrieval-augmented generation to perform explainable human activity recognition directly from raw motion time-series data. The primary objective is to create a zero-shot human activity recognition (HAR) system that avoids costly retraining and provides interpretable predictions by equipping a large language model with structured domain knowledge and a relevant evidence retrieval mechanism. The methodology involves an offline phase to build an activity-pair feature importance knowledge base and placement-specific vector databases, and an online inference phase where a four-stage hierarchical agent pipeline uses a frozen LLM to select features, prune candidate activities based on retrieved evidence, and generate a final prediction with a rationale. Across 8 HAR benchmarks, ZARA achieved an average macro F1 score of 81.4%, a 2.53x improvement over the strongest baseline (UniMTS). The principal implication for AI practitioners is that this framework provides a template for building accurate and interpretable zero-shot time-series analysis systems without model fine-tuning, enabling plug-and-play deployment by structuring domain-specific statistical knowledge and integrating it into retrieval-augmented LLM agent workflows. |
| Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.13804) or [HuggingFace](https://huggingface.co/papers/2508.13804))| Alina Landowska, maciejskorski | This paper presents a Bayesian evaluation of large language models' understanding of moral dimensions. The research investigates how large language models comprehend moral dimensions compared to human annotators. A GPU-optimized Bayesian framework, utilizing a Dawid-Skene variant with Dirichlet priors, was employed to model annotator disagreements and estimate probabilistic ground truth labels across 250K+ annotations from three diverse corpora. Results show AI models consistently outperformed human annotators, typically ranking in the top 25% and achieving 2–4x lower false negative rates (19.4% vs 52.7% on average), albeit with slightly higher false positive rates. This highlights LLMs' superior recall for moral foundation detection, making them valuable for identifying overlooked moral signals, though careful calibration for specific applications is needed due to elevated false positive rates. |
