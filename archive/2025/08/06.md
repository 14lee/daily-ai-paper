

## Papers for 2025-08-06

| Title | Authors | Summary |
|-------|---------|---------|
| Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference (Read more on [arXiv](https://arxiv.org/abs/2508.02193) or [HuggingFace](https://huggingface.co/papers/2508.02193))| Fan Xia, Pengyang Gao, Cheng Luo, Zheng Zhang, Yuxuan Song | This paper introduces Seed Diffusion Preview, a discrete-state diffusion language model for code generation that achieves high-speed, parallel inference while maintaining competitive performance. The objective is to mitigate the inference latency of token-by-token decoding by developing a model capable of non-sequential, parallel generation. The methodology combines a two-stage curriculum using mask-based and edit-based corruption, constrained-order training on distilled generation trajectories, and an on-policy learning paradigm to explicitly shorten inference paths. The model achieves an inference speed of 2,146 tokens/second on H20 GPUs and a 54.3% pass@1 score on the CanItEdit benchmark, establishing a new state-of-the-art on the speed-quality Pareto frontier for code models. For AI practitioners, this work demonstrates a viable architecture for deploying high-throughput language models that significantly reduces inference latency without a substantial loss in quality, offering a compelling alternative to traditional autoregressive systems for latency-sensitive applications. |
| Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding
  and Generation (Read more on [arXiv](https://arxiv.org/abs/2508.03320) or [HuggingFace](https://huggingface.co/papers/2508.03320))| Tianyidan Xie, Liang Hu, Yimeng Gan, Yi Peng, Peiyu Wang | Skywork UniPic is a 1.5 billion-parameter unified autoregressive model for visual understanding, generation, and editing. The research objective is to create a compact, single-architecture model that excels at these multimodal tasks while remaining efficient enough for deployment on commodity hardware. Its key methodology involves a decoupled encoding strategy, utilizing a Masked Autoregressive (MAR) encoder for generation and a SigLIP2 encoder for understanding, with both feeding into a shared autoregressive LLM decoder, trained via a progressive, resolution-aware curriculum. The model achieves state-of-the-art results, including a new record of 85.5 on the DPG-Bench for complex generation and a 5.83 score on GEditBench-EN for editing. The principal implication for AI practitioners is that high-fidelity, unified multimodal AI systems can be developed and deployed effectively without prohibitive computational resources, making advanced capabilities more accessible. |
| LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.03694) or [HuggingFace](https://huggingface.co/papers/2508.03694))| Chenyang Si, Jianfeng Feng, Xian Liu, Zhaoxi Chen, Jianxiong Gao | LongVie is an autoregressive framework that generates controllable, ultra-long (up to one minute) videos by combining multimodal guidance with specific strategies to ensure temporal consistency and visual quality. The primary objective is to overcome the temporal inconsistency and visual degradation that occur when scaling existing controllable short-video generation models to longer durations using autoregressive methods. The methodology extends a pre-trained video diffusion model with a multi-modal ControlNet-style architecture that accepts both dense (depth maps) and sparse (point maps) control signals. Temporal consistency is enforced through a unified noise initialization strategy across all generated clips and global normalization of control signals over the entire video, while a degradation-aware training strategy balances the influence of each modality. On the introduced LongVGenBench benchmark, LongVie achieves state-of-the-art performance, outperforming all baselines in consistency and quality. Quantitatively, it obtains the best perceptual similarity score with a LPIPS of 0.290 and the highest Overall Consistency score of 21.82%. The principal implication for AI practitioners is that the techniques of unified noise initialization and global control normalization provide a concrete, effective method to adapt existing short-video diffusion models for coherent, long-form, controllable video synthesis, directly addressing common failure modes like flickering and content drift. |
| CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and
  Outcome Reward (Read more on [arXiv](https://arxiv.org/abs/2508.03686) or [HuggingFace](https://huggingface.co/papers/2508.03686))| Songyang Gao, Linchen Xiao, Junnan Liu, Hongwei Liu, Shudong Liu | This paper introduces CompassVerifier, a lightweight and robust verifier model, and VerifierBench, a comprehensive benchmark designed to systematically evaluate the answer verification capabilities of LLMs. The main objective is to develop a unified, accurate, and generalizable verifier model for LLM outputs that overcomes the limitations of regex-based matching and general-purpose LLM judges, and to create a challenging benchmark to systematically evaluate such verification capabilities. The methodology involves creating VerifierBench by collecting over 1 million LLM responses and using a multi-stage filtering pipeline with multi-expert voting and human annotation. CompassVerifier is trained on this data and enhanced using three key techniques: Error-Driven Adversarial Augmentation, Complex Formula Augmentation, and Generalizability Augmentation. The primary result is that CompassVerifier-32B achieves a new state-of-the-art average F1 score of 87.7% on VerifierBench, significantly outperforming both general LLMs like GPT-4o (59.1% F1) and other specialized verifiers. The 3B parameter version of CompassVerifier surpasses GPT-4.1 by an absolute F1-score of 10.6%. The principal implication for AI practitioners is that CompassVerifier can be used as a more accurate, robust, and computationally efficient tool for automated LLM evaluation and as a reward model in reinforcement learning, providing more reliable feedback signals for model optimization than existing methods. |
| CRINN: Contrastive Reinforcement Learning for Approximate Nearest
  Neighbor Search (Read more on [arXiv](https://arxiv.org/abs/2508.02091) or [HuggingFace](https://huggingface.co/papers/2508.02091))| Jiwei Li, Chris Shum, Albert Wang, Xiaofei Sun, Xiaoya Li | The paper introduces CRINN, a framework using contrastive reinforcement learning with LLMs to automatically optimize Approximate Nearest Neighbor Search (ANNS) algorithms. The primary objective is to automate the optimization of ANNS implementations by treating it as an RL problem where an LLM learns to generate progressively faster code based on execution speed feedback. CRINN employs a contrastive RL methodology where the LLM is prompted with pairs of code implementations and their performance scores, guiding it to learn effective optimization patterns, with a scalar reward derived from the area under the QPS-recall curve within the [0.85, 0.95] range. On the MNIST-784 benchmark, CRINN achieved an 85.25% improvement in Queries Per Second (QPS) over the best baseline at a 0.999 recall level. For AI practitioners, this research demonstrates that RL-augmented LLMs can automate sophisticated, performance-critical code optimization, reducing the reliance on manual expert tuning for systems like vector databases. |
| Tool-integrated Reinforcement Learning for Repo Deep Search (Read more on [arXiv](https://arxiv.org/abs/2508.03012) or [HuggingFace](https://huggingface.co/papers/2508.03012))| Yanzhen Zou, Pengfei Gao, Qunhong Zeng, Chao Peng, Zexiong Ma | The paper introduces ToolTrain, a two-stage framework that uses supervised fine-tuning and reinforcement learning to improve how LLMs use tools for localizing code defects in software repositories. The objective is to enhance an LLM agent's multi-hop reasoning and tool-use capabilities to accurately identify faulty code from natural language issue descriptions. The core methodology involves first performing rejection-sampled supervised fine-tuning (SFT) on successful tool-use trajectories, followed by a tool-integrated reinforcement learning (RL) phase that uses an nDCG@k-based reward to optimize the agent's search strategy. The primary result shows that a 32B parameter model trained with ToolTrain achieves a function-level Recall@5 of 68.55% on the SWE-Bench-Verified benchmark, outperforming a leading proprietary model, and this improved localization boosts the end-to-end issue resolution rate to 31.60%. For AI practitioners, this demonstrates that specialized, two-stage training (SFT then RL) can enable smaller, open-source models to surpass larger proprietary models on complex, domain-specific tool-use tasks, offering a viable path for developing high-performance, specialized AI agents for software engineering. |
| Multi-human Interactive Talking Dataset (Read more on [arXiv](https://arxiv.org/abs/2508.03050) or [HuggingFace](https://huggingface.co/papers/2508.03050))| Mike Zheng Shou, Weijia Wu, Zeyu Zhu | This paper introduces the Multi-human Interactive Talking (MIT) dataset and a baseline model, CovOG, for generating videos of multi-person conversations. The research objective is to address the limitations of single-person monologue generation by enabling the synthesis of realistic, full-body, multi-speaker interactions. The methodology involves an automated pipeline for collecting and annotating 12 hours of video with fine-grained pose and speech interaction data, and proposing the CovOG model which integrates a Multi-Human Pose Encoder (MPE) and an Interactive Audio Driver (IAD) with a diffusion-based framework. In quantitative evaluations, CovOG outperformed baselines, achieving a Frechet Inception Distance (FVD) of 307.35 on the combined test set, compared to 337.60 for the AnimateAnyone baseline. The principal implication for AI practitioners is the provision of the first specialized dataset and a validated baseline model for developing and benchmarking systems that can generate complex, controllable multi-person conversational videos, moving beyond single-subject animations. |
| Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data
  Synthesis and Self-Correction (Read more on [arXiv](https://arxiv.org/abs/2508.03613) or [HuggingFace](https://huggingface.co/papers/2508.03613))| Jui-Hui Chung, Ziran Yang, Bohan Lyu, Shange Tang, Yong Lin | Goedel-Prover-V2 introduces open-source language models that set a new state-of-the-art in automated formal theorem proving by improving on existing learning pipelines. The primary objective is to create models capable of solving increasingly complex mathematical theorems by integrating long-chain-of-thought reasoning with formal verification. The methodology combines three key innovations: scaffolded data synthesis to generate a curriculum of problems, verifier-guided self-correction using feedback from the Lean compiler, and model averaging to maintain output diversity. The flagship Goedel-Prover-V2-32B model achieves 90.4% pass@32 on the MiniF2F benchmark using self-correction, significantly outperforming previous, larger state-of-the-art systems. The principal implication for AI practitioners is that integrating formal verifier feedback loops and curriculum-based training enables smaller, more computationally efficient models to achieve superior performance on complex, formal reasoning tasks, providing a practical alternative to simply scaling model parameters. |
| LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools? (Read more on [arXiv](https://arxiv.org/abs/2508.01780) or [HuggingFace](https://huggingface.co/papers/2508.01780))| Yaojie Lu, Xuanang Chen, Jiawei Chen, Wenliang Zhong, Guozhao Mo | This paper introduces LiveMCPBench, a benchmark to evaluate LLM agents' ability to use a large-scale, real-world toolset (527 tools) via the Model Context Protocol (MCP) to complete 95 daily tasks. The research objective is to assess if current agents can effectively plan, retrieve, and execute actions across a dynamic, multi-server environment, testing their "meta-tool-learning" capabilities beyond simple, simulated tool use. The methodology comprises the `LiveMCPTool` toolset, an `MCP Copilot Agent` for task execution, and `LiveMCPEval`, an LLM-as-a-Judge framework for automated evaluation. Evaluation of 10 frontier models revealed significant performance disparities, with the top-performing model, Claude-Sonnet-4, achieving a 78.95% task success rate, while many other models struggled with severe underutilization of available tools. The principal implication for AI engineers is that current models are deficient in task decomposition and tool retrieval for complex environments; therefore, building robust agents requires focusing on engineering sophisticated planning and retrieval architectures, as "Retrieve Error" was a dominant failure mode, rather than relying solely on the base LLM's reasoning. |
| LAMIC: Layout-Aware Multi-Image Composition via Scalability of
  Multimodal Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.00477) or [HuggingFace](https://huggingface.co/papers/2508.00477))| Shunyu Yao, Kai Kang, Jianhua Wang, Zehua Ma, Yuzhuo Chen | The paper introduces LAMIC, a training-free framework that extends single-reference diffusion transformers for layout-aware, multi-image composition using novel attention mechanisms. The primary objective is to enable a pretrained single-reference model to generate coherent images from multiple visual references with precise spatial layout control, without any retraining. The key methodology involves two plug-and-play attention mechanisms applied to a Multimodal Diffusion Transformer: Group Isolation Attention (GIA) to prevent interference between reference entities, and Region-Modulated Attention (RMA) to enhance layout precision during early denoising. In a four-reference composition task, LAMIC achieved an identity similarity (ID-S) score of 70.25, surpassing the second-best model by 8.41 points, and a layout Inclusion Ratio (IN-R) of 89.81, significantly outperforming all baselines. For AI practitioners, the principal implication is that LAMIC provides a zero-shot, resource-efficient method to adapt powerful single-reference foundation models for complex multi-subject, layout-controlled generation tasks, bypassing the need for specialized training data and model fine-tuning. |
| ChartCap: Mitigating Hallucination of Dense Chart Captioning (Read more on [arXiv](https://arxiv.org/abs/2508.03164) or [HuggingFace](https://huggingface.co/papers/2508.03164))| Gunhee Kim, Jaewoo Ahn, Junyoung Lim | The paper introduces the CHARTCAP dataset, containing 565K real-world charts with dense, verified captions, and a new Visual Consistency Score (VCS) metric to improve chart captioning by vision-language models (VLMs). The main objective is to develop a large-scale, high-quality dataset of real-world chart-caption pairs that is free from extraneous information and contains dense, structured descriptions, enabling VLMs to generate more accurate captions with fewer hallucinations. The methodology combines a four-stage automated pipeline using multiple VLMs to generate schema-guided captions for 565K charts and a cycle consistency-based human verification process for quality control. It also proposes the Visual Consistency Score (VCS), a reference-free metric that evaluates captions by programmatically regenerating a chart from the text and measuring its visual similarity to the original. Models fine-tuned on CHARTCAP significantly outperform proprietary models and even human-annotated captions; on the VisText benchmark, the `Phi3.5-Vision-4BCHARTCAP` model achieved a Visual Consistency Score of 0.9443, surpassing the 0.9172 score of the human-authored ground-truth captions. For AI practitioners, the principal implication is that fine-tuning VLMs on a high-fidelity dataset curated with a domain-specific schema (like CHARTCAP) is a highly effective strategy to reduce hallucination and improve factual accuracy for structured data, with the proposed VCS offering a robust, reference-free method for evaluating performance in this domain. |
| AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided
  Decomposition and Riemannian-Geodesic Collision Regularization (Read more on [arXiv](https://arxiv.org/abs/2508.02079) or [HuggingFace](https://huggingface.co/papers/2508.02079))| Aman Chadha, Vinija Jain, Abhilekh Borah, Amitava Das | ALIGNGUARD-LORA is a fine-tuning framework that mitigates alignment drift in LLMs by using Fisher-guided decomposition and specialized regularization to preserve safety behaviors. The main objective is to prevent the degradation of safety and behavioral constraints (alignment drift) during low-rank adaptation (LoRA) of large language models, without compromising downstream task performance. The key methodology involves decomposing LoRA parameter updates into an alignment-critical component and a task-specific component using a projection based on the Fisher Information Matrix (FIM). The framework then applies three forms of regularization: FIM-based regularization to constrain the alignment component, a separate penalty to stabilize the task component, and collision-aware regularization (using Riemannian and geodesic penalties) to minimize interference between them. Empirical evaluations show that ALIGNGUARD-LORA mitigates alignment drift by up to 50% on safety-critical benchmarks. On the introduced DRIFTCHECK benchmark, standard LoRA caused unsafe refusal accuracy to drop from 91.3% to 71.4%, while ALIGNGUARD-LORA maintained 92.3%, representing a 50% relative reduction in alignment drift. The principal implication for AI practitioners is that ALIGNGUARD-LORA can be used as a drop-in replacement for standard LoRA to more safely fine-tune already-aligned models, ensuring that critical safety guardrails are not eroded during adaptation for new tasks, which is crucial for the deployment of reliable models in production. |
| TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to
  Training-Time Belief Sources in LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.02063) or [HuggingFace](https://huggingface.co/papers/2508.02063))| Aman Chadha, Vinija Jain, Amitava Das | The TRACEALIGN framework traces LLM alignment drift to conflicting, memorized beliefs in the training corpus, moving beyond purely behavioral safety analysis. Its core methodology uses `TRACEINDEX`, a suffix-array search over training data, to find the provenance of generated text spans and scores their risk using the Belief Conflict Index (BCI), a metric based on token rarity. Three BCI-guided defenses—an inference-time filter (`TRACESHIELD`), a contrastive fine-tuning loss (`CBD Loss`), and a provenance-aware decoding strategy (`Prov-Decode`)—are introduced. On the paper's Alignment Drift Benchmark, these interventions collectively reduce alignment drift by up to 85% while preserving utility. For AI practitioners, this provides a traceable, auditable toolkit to diagnose safety failures at their source and implement targeted mitigations grounded in data provenance rather than relying on opaque refusal classifiers. |
