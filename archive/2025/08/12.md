

## Papers for 2025-08-12

| Title | Authors | Summary |
|-------|---------|---------|
| ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability (Read more on [arXiv](https://arxiv.org/abs/2508.07050) or [HuggingFace](https://huggingface.co/papers/2508.07050))| Yuchen Li, Yutao Zhu, Weiwei Sun, Xinyu Ma, Wenhan Liu | This paper introduces ReasonRank, a listwise passage reranker that achieves state-of-the-art performance on reasoning-intensive tasks through an automated data synthesis framework and a two-stage SFT+RL training process. The primary objective is to empower passage rerankers with strong reasoning abilities by overcoming the scarcity of high-quality, reasoning-intensive training data. The methodology involves an automated framework that uses the DeepSeek-R1 model to synthesize training data with reasoning chains, followed by a two-stage training approach: 1) a cold-start Supervised Fine-Tuning (SFT) stage to learn reasoning patterns, and 2) a Reinforcement Learning (RL) stage that uses a novel multi-view ranking reward (combining NDCG@10, Recall@10, and Rank-Biased Overlap) to enhance ranking. ReasonRank (32B) achieves a state-of-the-art average NDCG@10 of 40.8 on the BRIGHT benchmark, outperforming the previous best baseline by over 5 points. The listwise ReasonRank (7B) is also 2-2.7x faster than the pointwise reasoning reranker Rank1 (7B). The principal implication for AI practitioners is that they can develop high-performing, reasoning-intensive ranking models for specialized domains with scarce labeled data by employing the proposed automated data synthesis pipeline and the SFT-RL training strategy, yielding both superior accuracy and, in some cases, better efficiency. |
| WideSearch: Benchmarking Agentic Broad Info-Seeking (Read more on [arXiv](https://arxiv.org/abs/2508.07999) or [HuggingFace](https://huggingface.co/papers/2508.07999))| Yan Gao, Li Chen, Junjie Zhao, Jiawei Wang, Ryan Wong | The paper introduces WideSearch, a benchmark for evaluating the ability of LLM-powered agents to perform broad, large-scale information-seeking tasks that require collecting and structuring extensive, verifiable data from the web. The objective is to evaluate the reliability and completeness of current agentic systems on "wide-context" information gathering tasks, which are characterized by operational scale and data fidelity rather than cognitive complexity. The authors developed a benchmark of 200 tasks using a rigorous five-stage human-centered curation pipeline and evaluated agent performance via a hybrid automated scoring system that measures table-level success rate, row-level F1, and item-level F1. The study reveals critical deficiencies in state-of-the-art agents, with the top-performing multi-agent system achieving a mere 5.1% average success rate, demonstrating that failure stems not from finding individual facts but from ensuring the absolute completeness and accuracy of the entire dataset. The primary implication for AI practitioners is that current agentic architectures are fundamentally unsuited for high-fidelity, large-scale data collection, necessitating a shift towards more sophisticated systems, like multi-agent frameworks, that can manage comprehensive planning, iterative refinement, and cross-validation. |
| Omni-Effects: Unified and Spatially-Controllable Visual Effects
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.07981) or [HuggingFace](https://huggingface.co/papers/2508.07981))| Xiaokun Feng, Dongxia Liu, Jintao Chen, Aiming Hao, Fangyuan Mao | This paper introduces Omni-Effects, a unified framework for generating multiple, simultaneous, and spatially-controllable visual effects in a single video using a Mixture-of-Experts architecture and a novel attention mechanism. The primary objective is to develop a single model that can generate complex, composite visual effects at user-specified spatial locations without cross-effect interference or quality degradation, which are common failures in existing methods. The core methodology integrates two key components into a diffusion transformer: a LoRA-based Mixture of Experts (LoRA-MoE) that routes different effect prompts to specialized expert networks to prevent task interference, and a Spatial-Aware Prompt (SAP) augmented with an Independent-Information Flow (IIF) attention mask to fuse spatial location data with text prompts while isolating control signals. In quantitative evaluations on single-VFX control tasks, Omni-Effects achieved a 0.97 Effect Occurrence Rate (EOR) and a 0.88 Effect Controllability Rate (ECR), significantly outperforming baseline models which often failed to produce the correct effect or place it accurately. For AI practitioners, this work provides an architectural blueprint for building unified generative models that can manage multiple, distinct, and spatially-defined tasks within a single inference pass, offering a path to create more complex and controllable content generation tools without the need for separate models for each task. |
| Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving
  Clipping Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.07629) or [HuggingFace](https://huggingface.co/papers/2508.07629))| Guanting Dong, Dening Liu, Xue Bai, Leiyu Pan, Zhenpeng Su | Klear-Reasoner is an 8B-parameter model demonstrating advanced long-form reasoning capabilities via optimized supervised fine-tuning and a novel reinforcement learning approach. The research aims to advance reasoning capabilities by detailing an effective post-training workflow, including long Chain-of-Thought (CoT) SFT and reinforcement learning (RL), and proposing Gradient-Preserving clipping Policy Optimization (GPPO) to mitigate issues with traditional RL clipping. The methodology involves quality-centric long CoT SFT with selective data curation and an RL phase utilizing GPPO, which backpropagates gradients from clipped tokens to enhance exploration and convergence, alongside a soft reward mechanism for code tasks. Klear-Reasoner-8B achieved 90.5% on AIME 2024 and 66.0% on LiveCodeBench V5, with GPPO demonstrating superior and more stable performance compared to traditional clipping. For AI practitioners, this work implies that effective reasoning model development requires a focus on high-quality, internally consistent SFT data (even difficult or mixed samples), combined with gradient-preserving RL techniques and soft reward designs to improve training stability, exploration, and convergence. |
| UserBench: An Interactive Gym Environment for User-Centric Agents (Read more on [arXiv](https://arxiv.org/abs/2507.22034) or [HuggingFace](https://huggingface.co/papers/2507.22034))| Jianguo Zhang, Zhiwei Liu, Akshara Prabhakar, Zuxin Liu, Cheng Qian | UserBench is a user-centric interactive gym environment for evaluating LLM agents on their ability to handle underspecified, incremental, and indirect user preferences in multi-turn dialogues. The research objective is to evaluate agents from a user-centric perspective, assessing their capability to proactively collaborate and align with evolving user intent beyond simple task execution. The methodology involves a Gymnasium-based environment with simulated users who provide vague initial travel planning goals and reveal preferences implicitly and incrementally, requiring agents to use tools and engage in multi-turn clarification dialogue. Primary results show a significant gap between task execution and user alignment; even the most advanced models uncover fewer than 30% of all user preferences through active interaction, and models provide answers that fully align with all user intents only 20% of the time on average. The principal implication for AI practitioners is that current agent development, focused on tool-use proficiency, is insufficient for user-facing applications; focus must shift to enhancing agents' abilities for proactive clarification and reasoning with partial, evolving information to create truly collaborative systems. |
| SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings
  and Speaks in Tokens (Read more on [arXiv](https://arxiv.org/abs/2508.05305) or [HuggingFace](https://huggingface.co/papers/2508.05305))| Anton Razzhigaev, Andrey Kuznetsov, Elizaveta Goncharova, Temurbek Rahmatullaev, Nikita Dragunov | This paper introduces SONAR-LLM, a decoder-only transformer that generates text by predicting sentence embeddings while being supervised via token-level cross-entropy. The objective is to merge the semantic abstraction of sentence-level generation with the stability of likelihood-based training. The methodology involves autoregressively predicting continuous SONAR sentence embeddings and then propagating token-level cross-entropy loss back through a fixed, frozen SONAR decoder to train the main model. On the XSum summarization task, SONAR-LLM achieves a ROUGE-L score of 19.3, outperforming comparable MSE LCM (12.2) and Diffusion LCM (12.0) models. For AI practitioners, this work presents a hybrid architecture that enables more efficient long-context generation by operating on compressed sentence sequences, surpassing standard LLM computational efficiency for inputs longer than 4096 tokens. |
| A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems (Read more on [arXiv](https://arxiv.org/abs/2508.07407) or [HuggingFace](https://huggingface.co/papers/2508.07407))| Xinhao Yi, Yingxu Wang, Xi Zhang, Yanwen Peng, Jinyuan Fang | A Comprehensive Survey of Self-Evolving AI Agents overviews autonomous AI agents that continuously optimize their internal components. The paper's objective is to systematically review existing self-evolving techniques to address the limitations of static agent configurations in dynamic environments. It introduces a unified conceptual framework, encompassing System Inputs, Agent System, Environment, and Optimisers, to analyze evolution and optimization methods across single-agent, multi-agent, and domain-specific settings. For example, multi-agent LLM backbone optimization has shown up to a 2.8x performance gain with less than 10% of the token cost in tasks demanding intensive information exchange. This survey provides AI practitioners with a foundational understanding for developing more adaptive, autonomous, and lifelong agentic systems. |
| BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of
  Deep-Research Agent (Read more on [arXiv](https://arxiv.org/abs/2508.06600) or [HuggingFace](https://huggingface.co/papers/2508.06600))| Kai Zou, Ping Nie, Shengyao Zhuang, Xueguang Ma, Zijian Chen | BrowseComp-Plus is a new benchmark for evaluating Deep-Research Agents, addressing fairness, transparency, and reproducibility issues of existing benchmarks reliant on black-box live web search APIs. The main objective is to enable controlled experimentation and disentangled analysis of retriever and LLM contributions in deep research systems. This is achieved by introducing a fixed, human-curated corpus with human-verified supporting and challenging negative documents. Key results show that the open-source Search-R1 model with BM25 retriever achieves 3.86% accuracy, while GPT-5 with Qwen3-Embedding-8B reaches 70.1% accuracy with fewer search calls, demonstrating the significant impact of retriever quality. For AI practitioners, this benchmark facilitates comprehensive evaluation and component analysis, highlighting the need for stronger retrieval systems and improved LLM tool-use capabilities for enhanced deep-research agent performance and efficiency. |
| OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.05614) or [HuggingFace](https://huggingface.co/papers/2508.05614))| Hongxing Li, Dingming Li, tricktreat, yanyc, wangzx1210 | OmniEAR introduces a comprehensive benchmark and framework for evaluating language models' reasoning abilities in embodied tasks involving physical interactions, tool usage, and multi-agent coordination. The primary objective is to assess how agents reason about continuous physical properties, dynamically acquire capabilities through tool use, and autonomously determine coordination strategies based on task demands, rather than explicit instructions. The methodology employs EAR-Sim for text-based environment modeling, capturing detailed object attributes and spatial relationships, and EAR-Bench for systematic evaluation across 1,500 scenarios with dynamic capability evolution and emergent collaboration, using an automated generation pipeline. Evaluation reveals significant performance degradation in constraint-based reasoning; for instance, success rates for tool reasoning drop to 56-85% and implicit collaboration to 63-85%, compared to 85-96% with explicit instructions, with compound tasks showing over 50% failure rates. These findings demonstrate that embodied reasoning requires fundamentally different computational mechanisms than current language models possess, highlighting critical requirements for advancing embodied AI systems beyond current training approaches. |
| MolmoAct: Action Reasoning Models that can Reason in Space (Read more on [arXiv](https://arxiv.org/abs/2508.07917) or [HuggingFace](https://huggingface.co/papers/2508.07917))| Shuo Liu, Yuquan Deng, Haoquan Fang, Jiafei Duan, Jason Lee | MolmoAct introduces an open Action Reasoning Model (ARM) that integrates perception, planning, and control for robotic manipulation. The model aims to enable explainable and steerable robot behavior by transforming perception into purposeful action through structured spatial reasoning. It employs a three-stage autoregressive pipeline that first generates depth-aware perception tokens, then mid-level visual reasoning trace plans conditioned on perception, and finally precise low-level actions conditioned on both, trained on diverse robot and multimodal web data including a newly collected dataset. MolmoAct-7B-D achieves an 86.6% average success rate on the LIBERO benchmark and outperforms baselines by +23.3% on out-of-distribution generalization. The release of all model weights, code, and datasets establishes MolmoAct as a state-of-the-art open blueprint for building ARMs. |
| Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts (Read more on [arXiv](https://arxiv.org/abs/2508.07785) or [HuggingFace](https://huggingface.co/papers/2508.07785))| Tieyuan Chen, Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Haoyuan Wu | This paper introduces Grove MoE, a novel Mixture of Experts (MoE) architecture featuring shared "adjugate experts" to enable dynamic computational allocation. The primary objective is to overcome the fixed-parameter activation of traditional MoE models by allocating computational resources based on input complexity, inspired by the big.LITTLE CPU architecture. The methodology involves grouping experts and assigning a single, shared adjugate expert to each group, ensuring shared computations are performed only once per group for activated experts. The resulting 33B-parameter GroveMoE-Inst model, which dynamically activates 3.14–3.28B parameters, achieves a score of 72.8 on MMLU-Pro, outperforming comparable models like Qwen3-30B-A3B (63.3) and Llama4-Scout (64.9). For AI practitioners, this architecture presents a method to expand model capacity and performance through upcycling, achieving results comparable to larger models while maintaining manageable computational overhead and without requiring complex new routing mechanisms. |
| Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via
  Past-Future (Read more on [arXiv](https://arxiv.org/abs/2508.06026) or [HuggingFace](https://huggingface.co/papers/2508.06026))| Qiufeng Wang, Junfeng Fang, Cunxiang Wang, Xin Wang, Yidong Wang | This paper introduces Temporal Self-Rewarding (TSR), a framework that mitigates the diminishing preference signal in iterative language model alignment by temporally decoupling the generation of chosen and rejected responses. The research addresses the problem of quality convergence between chosen and rejected samples in standard Self-Rewarding, which causes the DPO learning signal to vanish. The core methodology consists of "Anchored Rejection," which fixes negative samples to outputs from the initial model (past), and "Future-Guided Chosen," which selects positive samples from a temporary, more capable model (future). TSR with a Llama3.1-8B model achieved a 29.44% win rate on AlpacaEval 2.0, significantly outperforming the standard Self-Rewarding baseline of 19.69% in fewer iterations. The primary implication for AI practitioners is that this temporal decoupling strategy offers a more computationally efficient and stable method for iterative preference optimization, preserving the learning signal to achieve superior model performance. |
| Reinforcement Learning in Vision: A Survey (Read more on [arXiv](https://arxiv.org/abs/2508.08189) or [HuggingFace](https://huggingface.co/papers/2508.08189))| Qingwei Meng, Kevin Qinghong Lin, Joya Chen, Chen Gao, Weijia Wu | This survey synthesizes over 200 recent works at the intersection of reinforcement learning (RL) and visual intelligence, structuring them into four key application pillars. The objective is to provide a comprehensive synthesis of the recent surge in visual reinforcement learning research, formalizing problem definitions, categorizing over 200 papers, and identifying key methodological trends, evaluation protocols, and open challenges. The authors conduct a systematic literature review, formalizing visual RL problems as Markov Decision Processes and tracing the evolution of policy optimization strategies like Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO). They categorize over 200 papers into four thematic pillars: Multimodal LLMs, Visual Generation, Unified Models, and Vision-Language-Action (VLA) models, analyzing algorithmic design and reward engineering within each. The survey identifies a clear trend moving from traditional RLHF towards more scalable paradigms like Group-Relative Policy Optimization (GRPO) and RL with Verifiable Rewards (RLVR), which reduce annotation costs and improve training stability. A key finding is that algorithms like GRPO, by replacing the learned value critic with a group-relative baseline, can roughly halve the memory and compute requirements compared to PPO while retaining a low-variance learning signal. The review systematically categorizes over 200 recent papers across four application pillars. AI practitioners can use this survey as a framework to select appropriate visual RL strategies; for tasks with deterministic success criteria (e.g., IoU thresholds, code unit tests), the identified paradigm of Reinforcement Learning with Verifiable Rewards (RLVR) combined with Group-Relative Policy Optimization (GRPO) offers a more sample-efficient and stable alternative to traditional, human-feedback-based methods. |
| Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.08221) or [HuggingFace](https://huggingface.co/papers/2508.08221))| Jiaheng Liu, Weixun Wang, Yancheng He, Jiashun Liu, Zihe Liu | This paper systematically reviews and evaluates Reinforcement Learning (RL) techniques for Large Language Model (LLM) reasoning, providing practical guidelines. The research addresses the absence of standardized guidelines, fragmented understanding of RL mechanisms, and conflicting conclusions among practitioners when applying RL for LLM reasoning. The authors conducted rigorous reproductions and isolated evaluations of widely adopted RL techniques within a unified open-source framework (ROLL), using PPO as a baseline and various Qwen3 models on diverse math reasoning datasets, evaluated on six math benchmarks. Empirical results reveal that a minimalist combination of two techniques, termed Lite PPO (advantage normalization with group-level mean and batch-level standard deviation, and token-level loss aggregation), consistently improved performance of critic-free policies using vanilla PPO loss, surpassing technique-heavy algorithms like GRPO and DAPO (e.g., Lite PPO achieved ~37% accuracy on Qwen3-4B-Base with easy data, outperforming GRPO and DAPO). For AI practitioners, the study provides actionable guidelines for selecting RL techniques based on specific setups, demonstrating that a simple, contextually adaptive combination can outperform more complex RL pipelines, challenging the trend of over-engineering. |
| Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided
  Region Control (Read more on [arXiv](https://arxiv.org/abs/2508.08134) or [HuggingFace](https://huggingface.co/papers/2508.08134))| Hongyu Liu, Xinhua Zhang, Kunyu Feng, Mingzhe Zheng, Zeqian Long | The paper introduces Follow-Your-Shape, a training-free and mask-free framework for precise, large-scale object shape editing in images while preserving background integrity. The objective is to enable significant, prompt-driven shape transformations on objects within an image without requiring user-provided masks and while strictly maintaining the content and quality of non-target regions. The methodology computes a Trajectory Divergence Map (TDM) by measuring the token-wise velocity differences between the source image's inversion trajectory and the target prompt's denoising trajectory, which in turn guides a Scheduled Key-Value (KV) Injection mechanism to control the edit. On the introduced ReShapeBench benchmark, the method outperforms baselines in background preservation, achieving a PSNR of 35.79, and shows superior text-image alignment with a CLIP Similarity score of 33.71. For AI practitioners, this trajectory-guided control offers a way to implement more precise shape editing in flow-based generative models without retraining or external masks, directly addressing the common failure mode of background degradation during complex structural edits. |
| Less Is More: Training-Free Sparse Attention with Global Locality for
  Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.07101) or [HuggingFace](https://huggingface.co/papers/2508.07101))| Baihong Yuan, Shijie Cao, Arti Jain, Zhihao Zhang, Lijie Yang | LessIsMore introduces a training-free sparse attention mechanism designed for efficient reasoning in large language models by leveraging global attention patterns. The primary objective is to address the substantial computational overhead and accuracy degradation observed in existing sparse attention methods for long-generation reasoning tasks. Its methodology integrates Unified Attention Head Selection, which aggregates head-specific top-k token selections into a globally ranked set, and a Stable Recency Window, which reserves a fixed proportion of the budget for recently generated tokens. Experimentally, LessIsMore achieves a 1.13x end-to-end speed-up compared to existing sparse attention methods while attending to 2x fewer tokens without accuracy loss. This work implies that AI practitioners can significantly reduce the inference latency and computational resource usage of reasoning LLMs without requiring expensive retraining or compromising reasoning quality. |
| VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation
  for Multilingual Long Document Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.07493) or [HuggingFace](https://huggingface.co/papers/2508.07493))| Tong Yu, Chenguang Wang, Jihyung Kil, Ming Li, Jian Chen | The paper introduces VisR-Bench, a new benchmark for evaluating visual retrieval-augmented generation on multilingual long documents. The primary research objective is to assess the capabilities and limitations of multimodal retrieval models, especially Multimodal Large Language Models (MLLMs), in understanding visually rich, multi-page documents across diverse languages. The methodology involves creating a dataset with over 35,000 question-answer pairs across 1,286 documents in 16 languages, with questions specifically designed to require text, table, or figure understanding, and then evaluating various text-based, multimodal encoder, and MLLM-based retrieval systems. The primary result is that MLLMs significantly outperform other models, with ColQwen2 achieving a 75.23% average top-1 retrieval accuracy on the English split, but all models show a substantial performance degradation on low-resource languages and structured tables. The principal implication for AI practitioners is that while MLLMs are the most promising approach for multimodal document RAG, significant development is required in multilingual pre-training and specialized architectures for structured data to build robust, production-ready systems for global document intelligence. |
| Shortcut Learning in Generalist Robot Policies: The Role of Dataset
  Diversity and Fragmentation (Read more on [arXiv](https://arxiv.org/abs/2508.06426) or [HuggingFace](https://huggingface.co/papers/2508.06426))| Hengtao Shen, Lianli Gao, Junlin Xie, Xu Luo, Youguang Xing | This paper investigates shortcut learning in generalist robot policies due to limited dataset diversity and fragmentation. The research aims to identify the root causes of limited generalization, attributing it to models relying on task-irrelevant features from spurious correlations. The methodology involved analyzing OXE dataset visual and textual features using uniformity and pairwise similarity metrics, theoretical mutual information analysis, and conducting controlled experiments on the LIBERO-Spatial benchmark and real-world setups using policies like π0 with data augmentation. Key results demonstrate that adding a third object in real-world π0 finetuning reduced shortcut degree from 0.6 to 0 and increased OOD success rate from 0.2 to 0.75, while object augmentation in SIMPLER reduced π0's shortcut degree from 1.0 to 0.68. The principal implication for AI practitioners is to design dataset collection strategies that ensure intra-dataset diversity and inter-dataset factor overlap, or utilize targeted data augmentation techniques to alleviate shortcut learning and enhance generalization. |
| MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.05257) or [HuggingFace](https://huggingface.co/papers/2508.05257))| Jianguo Li, Jing Zhang, Zhenzhong Lan, Mingming Ha, Xiaodong Chen | This paper introduces Mixture-of-Basis-Experts (MoBE), a novel parameter-efficient compression method for Mixture-of-Experts (MoE) LLMs that factorizes expert weights into shared bases and unique transformations. The objective is to develop a compression technique for large MoE-based LLMs that significantly reduces the total parameter count and memory footprint for deployment while minimizing the degradation of downstream task performance compared to existing methods. The MoBE method compresses the up and gate matrices of each expert by factorizing them into an expert-specific transformation matrix `A` and a matrix `B`, where `B` is a learned linear combination of a small set of basis matrices shared across all experts within an MoE layer, with the factorization being optimized by minimizing reconstruction error. Extensive experiments show MoBE can compress leading models by 24%-30% while retaining up to 98% of their original performance, resulting in an accuracy drop of only 1%-2% on average across diverse benchmarks, significantly outperforming prior methods like MoLAE. For AI practitioners, MoBE provides a concrete methodology to reduce the substantial memory and storage requirements of large MoE models, making it more feasible to deploy them in resource-constrained environments, though realizing optimal inference efficiency may require custom compute kernels. |
| Compressing Chain-of-Thought in LLMs via Step Entropy (Read more on [arXiv](https://arxiv.org/abs/2508.03346) or [HuggingFace](https://huggingface.co/papers/2508.03346))| Zhijian Xu, Xiangyu Wen, Ziyang Zheng, Jianyuan Zhong, Zeju Li | This paper introduces a framework using "step entropy" to compress Large Language Model (LLM) Chain-of-Thought (CoT) by identifying and pruning redundant reasoning steps. The research objective is to systematically improve inference efficiency by removing superfluous steps without degrading reasoning accuracy. The methodology involves quantifying the informational contribution of each step via step entropy and using a two-stage training process (Supervised Fine-Tuning and Group Relative Policy Optimization) to teach models to autonomously generate compressed CoTs. The primary result demonstrates that pruning up to 80% of the lowest-entropy steps causes minimal accuracy degradation, and trained models achieve token reductions of 35-57% on mathematical reasoning benchmarks. For AI practitioners, this provides a principled method to significantly reduce the computational cost and latency of complex reasoning tasks while retaining the interpretability of explicit thought chains. |
| GLiClass: Generalist Lightweight Model for Sequence Classification Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.07662) or [HuggingFace](https://huggingface.co/papers/2508.07662))| Alexander Yavorskyi, Oleksandr Lukashov, Dmytro Vodianytskyi, Mykhailo Shtopko, Ihor Stepanov | This paper introduces GLiClass, a uni-encoder transformer architecture for efficient and accurate sequence classification with strong zero-shot and few-shot capabilities. The research objective is to create a classification model that overcomes the computational inefficiency of cross-encoders and generative LLMs when dealing with large label sets, while maintaining high accuracy. The methodology involves adapting the GLiNER architecture to jointly process concatenated input text and class labels in a single forward pass, using a bidirectional transformer, and fine-tuning with techniques including Proximal Policy Optimization (PPO) and Low-Rank Adaptation (LoRA). The primary result shows that the `gliclass-large-v3.0` model achieves an average F1-score of 0.7193, surpassing the `deberta-v3-large` cross-encoder baseline by 5.5% relative, while its inference throughput degrades by only 7.6% when scaling from 1 to 128 labels, compared to a ~52x slowdown for the cross-encoder. The principal implication for AI practitioners is that GLiClass offers a production-viable model for classification systems with large and dynamic label sets, providing a superior accuracy-latency trade-off compared to traditional cross-encoders without sacrificing zero-shot flexibility. |
| Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant
  Safeguards into Open-Weight LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.06601) or [HuggingFace](https://huggingface.co/papers/2508.06601))| Robert Kirk, Tomek Korbak, Quentin Anthony, Stephen Casper, Kyle O'Brien | This research demonstrates that filtering dual-use biothreat content from pretraining data is a highly effective method for building tamper-resistant safeguards into open-weight LLMs. The objective is to determine if pretraining data curation can durably prevent an LLM from learning specific unwanted knowledge and make it robust against adversarial fine-tuning attacks intended to restore those capabilities. The authors developed a multi-stage filtering pipeline using a keyword blocklist and a ModernBERT classifier to remove biothreat-related documents. They then pretrained multiple 6.9B-parameter models from scratch on both filtered and unfiltered data and evaluated their resistance to tampering attacks, including up to 10,000 steps of adversarial fine-tuning on biothreat-related text. Models trained on filtered data exhibit state-of-the-art tamper resistance, outperforming post-training safeguards by over an order of magnitude by maintaining low capability on biothreat knowledge tasks even after adversarial fine-tuning with 300M tokens, with no observed degradation in general capabilities. However, these "ignorant" models can still leverage harmful information when provided in-context, a vulnerability that can be partially mitigated by combining filtering with post-training techniques like Circuit-Breaking. AI practitioners can use proactive pretraining data curation as a powerful and computationally efficient (<1% of total training FLOPS) layer of defense to fundamentally prevent the acquisition of specific, unwanted knowledge, offering a more robust safety solution for open-weight models than relying solely on post-training alignment techniques. |
| Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System (Read more on [arXiv](https://arxiv.org/abs/2508.06059) or [HuggingFace](https://huggingface.co/papers/2508.06059))| Reynold Cheng, Dacheng Wen, Bin Benjamin Zhu, Yupeng Li, Haorui He | This paper introduces Fact2Fiction, a poisoning attack framework designed to manipulate agentic fact-checking systems by exploiting their architectural design. The primary objective is to create an effective attack that can compromise systems that use claim decomposition, a feature that makes them robust to existing attacks. The methodology involves a Planner agent that mirrors the victim system's decomposition strategy and uses its generated justifications to plan targeted adversarial answers, and an Executor agent that crafts and injects malicious evidence accordingly. Extensive experiments demonstrate that Fact2Fiction achieves an 8.9%–21.2% higher Attack Success Rate (ASR) than state-of-the-art attacks across various poisoning budgets. The principal implication for AI practitioners is that the transparency mechanism in agentic systems (i.e., justifications) creates a critical security vulnerability, as it can be reverse-engineered to craft potent, targeted attacks, necessitating the development of more sophisticated defenses. |
| Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with
  Patch-level CLIP Latents (Read more on [arXiv](https://arxiv.org/abs/2508.05954) or [HuggingFace](https://huggingface.co/papers/2508.05954))| Mohit Bansal, Chuan Li, Amir Zadeh, Jaemin Cho, Han Lin | BIFROST-1 is a unified framework that bridges pretrained Multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP latents for efficient, high-fidelity image generation. The objective is to integrate controllable visual synthesis capabilities into MLLMs without compromising their strong reasoning abilities or incurring prohibitive computational costs. The methodology involves using patch-level CLIP image embeddings, which are natively aligned with the MLLM's frozen visual encoder, to guide a diffusion model through a lightweight, trainable latent ControlNet. On the ImageNet 256x256 generation task, this architecture achieves a Fréchet Inception Distance (FID) of 25.77, significantly outperforming ablated versions using non-aligned VAE latents (FID 284.51) or cross-attention guidance (FID 76.32). The principal implication for AI practitioners is that this method offers a resource-efficient blueprint for adding controllable image generation to existing MLLMs by leveraging natively aligned latent spaces, which avoids the expensive training required to realign mismatched representations. |
| When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with
  Benign Inputs (Read more on [arXiv](https://arxiv.org/abs/2508.03365) or [HuggingFace](https://huggingface.co/papers/2508.03365))| Dasol Choi, Taeyoun Kwon, Hiskias Dingeto, Bodam Kim, oneonlee | The paper introduces WHISPERINJECT, a two-stage adversarial framework that jailbreaks audio-language models (ALMs) by embedding imperceptible perturbations into benign audio signals. The primary objective is to develop a covert audio attack that compels an ALM to circumvent its safety protocols by generating its own "native" harmful text, rather than being forced to generate a pre-defined "foreign" text. The methodology first uses a novel Reinforcement Learning with Projected Gradient Descent (RL-PGD) technique to discover a model-native harmful response (Stage 1), then uses standard PGD to imperceptibly embed that response payload into a benign audio carrier (Stage 2). The framework achieved an average attack success rate of 86.0% across models including Qwen2.5-Omni and Phi-4-Multimodal under the StrongREJECT evaluation framework, with perturbations remaining below the human audibility threshold. The principal implication for AI practitioners is that text-based safety filters are insufficient for multimodal models, demonstrating an urgent need for defenses that operate directly at the audio-signal level to detect and mitigate such covert, audio-native attacks. |
| Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations
  and Sentences (Read more on [arXiv](https://arxiv.org/abs/2508.03542) or [HuggingFace](https://huggingface.co/papers/2508.03542))| Matvey Skripkin, Elvir Karimov, Artyom Iudin, Dmitrii Tarasov, Dmitrii Korzh | This paper introduces the S2L dataset for Speech-to-LaTeX conversion and benchmarks ASR post-correction and multimodal audio-LLM approaches. The main objective is to create a large-scale, open-source, multilingual dataset to facilitate the conversion of spoken mathematical equations and sentences into LaTeX, establishing robust baselines for this underexplored task. The key methodology involves the creation of the S2L dataset, comprising over 66,000 human-annotated and 571,000 synthetic audio samples, and the evaluation of two model types: a two-stage ASR post-correction pipeline and end-to-end multimodal audio-LLMs like SALMONN. The primary result is that on the proposed S2L-equations benchmark, the authors' SALMONN model achieved a Character Error Rate (CER) of 17.5%, significantly outperforming the prior MathSpeech model's 64.0% CER. The principal implication for AI practitioners is the release of a foundational dataset and strong baselines for a challenging domain-specific task, demonstrating that end-to-end multimodal models are a highly promising approach for developing practical applications like automated lecture transcription. |
