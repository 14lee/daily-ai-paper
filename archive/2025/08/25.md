

## Papers for 2025-08-25

| Title | Authors | Summary |
|-------|---------|---------|
| AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.16153) or [HuggingFace](https://huggingface.co/papers/2508.16153))| Xue Yan, Siyuan Guo, linyiyang2023, scyyc9, Zhouhc | AgentFly introduces a memory-based learning paradigm for LLM agents to achieve continuous adaptation and policy improvement without fine-tuning the underlying model's parameters. The paper addresses how to build LLM agents that learn continuously from a changing environment without the prohibitive cost of fine-tuning the underlying LLMs. The methodology formalizes agent learning as a Memory-augmented Markov Decision Process (M-MDP) within a planner-executor architecture leveraging Case-Based Reasoning (CBR). An episodic memory, or "Case Bank," stores past trajectories, and a neural case-selection policy, updated via online soft Q-learning, retrieves relevant experiences to guide the planner's decisions without requiring LLM gradient updates. AgentFly achieved top-1 performance on the GAIA validation set with 87.88% Pass@3 accuracy and improved performance on out-of-distribution tasks by 4.7% to 9.6% absolute points through its case-based memory. The principal implication for AI practitioners is that scalable, continuously learning agents can be developed by implementing a learning-enabled external memory module on top of static, pre-trained LLMs, providing a computationally efficient, gradient-free pathway for real-time agent adaptation. |
| ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for
  Long-Horizon Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.08240) or [HuggingFace](https://huggingface.co/papers/2508.08240))| Zeju Li, Jianuo Jiang, Liqin Lu, MingyuLiu, Ka12un | ODYSSEY is a unified mobile manipulation framework for agile quadruped robots that integrates hierarchical task planning and whole-body control for long-horizon tasks in open-world environments. The primary objective is to enable agile quadruped robots with manipulators to autonomously navigate and interact in dynamic, unstructured settings by seamlessly integrating high-level task planning with low-level whole-body control. The methodology involves a hierarchical vision-language planner for instruction decomposition and precise action execution, combined with a reinforcement learning-based whole-body policy trained with terrain-invariant end-effector sampling for robust locomotion and manipulation, and a novel long-horizon mobile manipulation benchmark for evaluation. Experimentally, ODYSSEY achieves 40% or higher overall task success rates across eight long-horizon mobile manipulation tasks, maintaining over 60% success in each atomic skill category, and significantly reducing base tracking error (ex) to 0.36 under dynamic conditions compared to a baseline's 9.70. For AI practitioners, this work demonstrates robust sim-to-real transfer and strong generalization across diverse real-world scenarios, advancing the practical feasibility of generalized robotic assistants and legged manipulator deployment in unstructured environments. |
| Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains
  RLVR (Read more on [arXiv](https://arxiv.org/abs/2508.14029) or [HuggingFace](https://huggingface.co/papers/2508.14029))| Ying Nian Wu, Yelong Shen, Yeyun Gong, Zhongzhi Li, MasterVito | Self-play with Variational problem Synthesis (SvS) sustains Reinforcement Learning with Verifiable Rewards (RLVR) training by mitigating policy entropy collapse and boosting Pass@k performance in Large Language Models (LLMs). The paper aims to develop an online problem augmentation method that maintains data diversity and accurate labeled answers for sustainable self-improvement. SvS achieves this by leveraging the policy's correct solutions from under-performing training problems to synthesize variational problems, which are then solved by the policy in a self-improving loop, ensuring original reference answers are preserved. This strategy demonstrated absolute gains of 18.3% and 22.8% in Pass@32 performance on AIME24 and AIME25 benchmarks, while consistently maintaining stable policy entropy. For AI practitioners, SvS provides a robust, self-improving approach that enables more sustainable and scalable RLVR training for LLMs in complex reasoning tasks. |
| CRISP: Persistent Concept Unlearning via Sparse Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2508.13650) or [HuggingFace](https://huggingface.co/papers/2508.13650))| Yonatan Belinkov, Martin Tutek, Aaron Mueller, Dana Arad, Tomertech | CRISP introduces a parameter-efficient method for persistent concept unlearning in Large Language Models (LLMs) via Sparse Autoencoders (SAEs). The objective is to selectively remove unwanted knowledge while preserving model utility and generating coherent text. Its key methodology involves automatically identifying salient SAE features using contrastive activation analysis and then fine-tuning the model with LoRA to suppress these features' activations on a target corpus. CRISP achieved state-of-the-art performance on the WMDP benchmark, for instance, an overall score of 60.10 on WMDP-Bio (Llama-3.1-8B), outperforming ELM (33.93) and RMU (52.51). AI practitioners can leverage CRISP to precisely and persistently remove specific harmful knowledge from LLMs, enhancing model safety and reliability without significant degradation of general capabilities or fluency. |
| Selective Contrastive Learning for Weakly Supervised Affordance
  Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.07877) or [HuggingFace](https://huggingface.co/papers/2508.07877))| Jae-Pil Heo, hynnsk, WJ0830 | This paper introduces Selective Contrastive Learning for Weakly Supervised Affordance Grounding (WSAG) to enhance the localization of action-affordable object parts. The research objective is to accurately identify interaction-relevant parts in egocentric images from third-person demonstrations, overcoming the tendency of models to focus on class-specific patterns unrelated to affordance. The methodology involves leveraging CLIP for object discovery to gather object-level clues, followed by a selective application of prototypical and pixel contrastive learning to adaptively learn part- and object-level affordance cues. On the AGD20K-Seen dataset, the method achieves a Kullback-Leibler Divergence (KLD) of 1.124, a Similarity (SIM) of 0.433, and a Normalized Scanpath Saliency (NSS) of 1.280, outperforming previous approaches like DINO+CLIP+SAM+GPT-4 (KLD 1.201). For AI practitioners, this framework offers a robust way to develop generalized AI systems for human-object interaction by precisely identifying functional object parts, improving localization accuracy, and enhancing model robustness in unseen scenarios. |
| AetherCode: Evaluating LLMs' Ability to Win In Premier Programming
  Competitions (Read more on [arXiv](https://arxiv.org/abs/2508.16402) or [HuggingFace](https://huggingface.co/papers/2508.16402))| Yidi Du, Markus Mak, Zhicheng Liu, Jiaze Chen, zhwang01 | AetherCode introduces a new benchmark for rigorously evaluating Large Language Models' (LLMs) competitive programming capabilities using problems from premier global competitions. The objective is to provide a more faithful measure of LLM coding and reasoning by addressing the limitations of existing benchmarks, specifically insufficient problem difficulty, limited scope, and low-quality test cases. AetherCode curates problems from top-tier Olympiad in Informatics (OI) and International Collegiate Programming Contest (ICPC) series, and generates comprehensive, expert-validated test suites via a hybrid of automated generation and human curation, achieving 100% True Positive Rate (TPR) and 100% True Negative Rate (TNR) against 30,000 human solutions. Evaluation of leading-edge models on AetherCode shows a significant performance gap; for instance, the top reasoning model, 04-mini-high, achieved a Pass@4 accuracy of 46.6%, while non-reasoning models demonstrated markedly inferior performance. Current LLMs, even advanced reasoning models, demonstrate considerable room for improvement in complex algorithmic reasoning and coding required for competitive programming, particularly in domains such as Computational Geometry and Tree Structures, indicating a significant gap compared to top human experts. |
| EgoTwin: Dreaming Body and View in First Person (Read more on [arXiv](https://arxiv.org/abs/2508.13013) or [HuggingFace](https://huggingface.co/papers/2508.13013))| Wentao Wang, Mengze Li, Yicong Li, Fangzhou Hong, Jingqiao Xiu | EgoTwin is a diffusion-based framework that addresses the novel task of jointly generating viewpoint-consistent and causally coherent egocentric video and human motion. Its objective is to synthesize synchronized sequences of first-person video and human motion from an initial pose, observation, and text description, tackling challenges in Viewpoint Alignment and Causal Interplay. The methodology employs a triple-branch diffusion transformer with a head-centric motion representation, a cybernetics-inspired interaction mechanism, and asynchronous diffusion, trained in three stages on a large-scale text-video-motion dataset. EgoTwin significantly outperforms the VidMLD baseline, demonstrated by an I-FID of 98.17 (compared to VidMLD's 157.86) and a Translation Error (TransErr) of 0.67 (vs. 1.28). This work provides a crucial foundation for AI practitioners developing embodied AI, augmented/virtual reality, and wearable computing applications that require precise, synchronized body-camera coupling and scene interaction. |
| Do What? Teaching Vision-Language-Action Models to Reject the Impossible (Read more on [arXiv](https://arxiv.org/abs/2508.16292) or [HuggingFace](https://huggingface.co/papers/2508.16292))| Roei Herzig, Trevor Darrell, Dantong Niu, Elvis Hsieh, Wen-Han Hsieh | This paper introduces Instruct-Verify-and-Act (IVA), a unified framework enabling Vision-Language-Action (VLA) models to detect, clarify, and correct false-premise robotic instructions. The research addresses how VLAs can recognize and respond to natural language commands referencing objects or conditions absent from the environment, aiming to develop a VLA model for interpreting and correcting such instructions. IVA employs a large-scale instruction tuning setup with structured language prompts and trains a LLARVA-based VLA model using a contextually augmented, semi-synthetic dataset of paired true and false-premise instructions from RLBench, fine-tuned end-to-end with frozen encoders and LoRA adapters. IVA significantly improved false premise detection accuracy by 97.56% over baselines and increased successful responses in false-premise scenarios by 50.78%, achieving 100% detection for in-domain and 97.78% for out-of-domain false premises while maintaining true-premise task performance. For AI practitioners, this work demonstrates that integrating explicit false-premise reasoning into VLA models can enhance their robustness, safety, and natural interaction capabilities, allowing robots to engage with users even when faced with impossible or ambiguous commands. |
| AgentScope 1.0: A Developer-Centric Framework for Building Agentic
  Applications (Read more on [arXiv](https://arxiv.org/abs/2508.16279) or [HuggingFace](https://huggingface.co/papers/2508.16279))| Liuyi Yao, Weirui Kuang, Yuexiang Xie, Zitao Li, Dawei Gao | AgentScope 1.0 is a developer-centric framework designed for building scalable, adaptive, and effective LLM-based agentic applications. Its objective is to comprehensively support flexible and efficient tool-based agent-environment interactions. The framework employs a novel architecture grounded in the ReAct paradigm, abstracting foundational components (message, model, memory, tool) and providing advanced agent-level infrastructure, including asynchronous design, parallel tool calls, and dynamic tool provisioning. The paper describes design benefits such as reduced task latency and improved efficiency; however, it does not explicitly present specific quantitative performance improvements or comparative metrics for these enhancements. AgentScope simplifies the development, evaluation, and deployment of complex agentic applications, offering a robust foundation for AI practitioners building multi-agent systems. |
| InMind: Evaluating LLMs in Capturing and Applying Individual Human
  Reasoning Styles (Read more on [arXiv](https://arxiv.org/abs/2508.16072) or [HuggingFace](https://huggingface.co/papers/2508.16072))| Diping Song, Qi Chen, Yibin Wang, Chuanhao Li, Zizhen Li | InMind presents a cognitively grounded framework and dataset for evaluating LLMs' capacity to capture and apply individualized human reasoning styles in social deduction games. Its objective is to assess LLMs' ability to internalize and adapt personalized reasoning through four tasks (Player Identification, Reflection Alignment, Trace Attribution, Role Inference) on dual-layer annotated Avalon gameplay data, including strategy traces and reflective summaries. Evaluation of 11 LLMs showed that general-purpose models, including GPT-4o (0.160 Top-1 accuracy in Player Identification), largely rely on lexical cues and struggle with temporal alignment and dynamic adaptation of strategies. DeepSeek-R1 achieved the highest Top-1 accuracy of 0.240 in Player Identification, suggesting early signs of style-sensitive reasoning. These results underscore current LLMs' fundamental limitations in individualized, adaptive, and temporally structured reasoning, indicating a need for AI practitioners to develop models capable of deeper strategic intent inference for more robust human-AI interaction. |
| CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated
  Chain-of-Thought-based Reinforced Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2508.15868) or [HuggingFace](https://huggingface.co/papers/2508.15868))| Yulun Zhang, Haipang Wu, Rongjuncheng Zhang, Ji Liu, Qlisp | CARFT boosts LLM reasoning through contrastive learning and annotated Chain-of-Thought (CoT)-based reinforced fine-tuning. The paper addresses the limitations of existing RL-based LLM fine-tuning, which often neglects valuable annotated CoTs and suffers from unstable training leading to model collapse, and SFT's overreliance on single CoTs. CARFT learns unified CoT representations from both annotated and on-policy sampled CoTs, generating contrastive signals via InfoNCE, and further stabilizes training and enhances performance through an embedding-enhanced partial reward method. Experiments show CARFT significantly outperforms baselines, achieving up to 10.15% higher accuracy and up to 30.62% greater efficiency compared to ReFT and Dr.GRPO, while exhibiting superior training stability. AI/ML engineers can leverage CARFT to develop more robust and performant LLMs for complex reasoning tasks by effectively integrating high-quality human-annotated CoTs and stabilizing the reinforcement learning fine-tuning process. |
| Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose
  Inverse Kinematics (Read more on [arXiv](https://arxiv.org/abs/2508.13562) or [HuggingFace](https://huggingface.co/papers/2508.13562))| Xiao Sun, Zhihang Zhong, Wei Wang, Linfeng Dong, Charlie019 | Learnable SMPLify is a neural framework for optimization-free 3D human pose inverse kinematics, replacing iterative fitting with a single-pass regression model. Its primary objective is to estimate SMPL parameters directly from 3D joint positions without iterative optimization, addressing the computational cost and initialization dependency of methods like SMPLify. The methodology employs a human-centric normalization scheme and residual learning, constructing initialization-target pairs from sequential frames, and utilizes a GCN-based feature extractor with an MLP regressor to predict residual pose parameters. Learnable SMPLify achieves nearly 200Ã— faster runtime compared to classical SMPLify, obtaining a Per-Vertex Error (PVE) of 3.23mm on the AMASS (s=1) dataset, significantly outperforming SMPLify's 18.85mm PVE. For AI practitioners, this work provides a practical, efficient, and generalizable neural baseline for real-time 3D human pose estimation and can serve as a robust, model-agnostic plug-in post-processing module for refining existing image-based estimators. |
| Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts (Read more on [arXiv](https://arxiv.org/abs/2508.10390) or [HuggingFace](https://huggingface.co/papers/2508.10390))| Liming Fang, Jiafei Wu, Xiaogang Xu, Lu Zhou, AlienZhang1996 | The paper introduces MDH for malicious content detection in red-teaming datasets and two novel jailbreak attacks, D-Attack and DH-CoT, for black-box LLMs. The research aimed to develop an efficient and accurate framework for cleaning red-teaming datasets and detecting jailbroken responses, alongside proposing effective jailbreak methods leveraging developer messages. Methodologically, MDH is a three-stage hybrid LLM-human framework for detection, while D-Attack and DH-CoT employ crafted developer messages with context simulation or hijacked chains of thought. MDH achieved over 95% detection accuracy for non-obvious harmful prompts with low manual review rates, and DH-CoT (D9) demonstrated a high attack success rate of 1.00 on GPT-4.1 and 0.66 on o4-Mini reasoning models. This directly implies that carefully crafted developer messages, especially with hijacked chains of thought, represent a significant vulnerability in LLM safety safeguards that AI practitioners must address. |
