

## Papers for 2025-08-21

| Title | Authors | Summary |
|-------|---------|---------|
| From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating
  Financial Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.13491) or [HuggingFace](https://huggingface.co/papers/2508.13491))| Ziyan Kuang, Effoula, QianqianXie1994, hugai101, 2083L | FinCDM is the first cognitive diagnosis framework for evaluating financial Large Language Models (LLMs) at the knowledge-skill level. Its objective is to move beyond aggregate scores by identifying specific financial skills and knowledge LLMs possess or lack. The methodology employs a non-negative matrix co-factorization based Cognitive Diagnosis Model (CDM) and utilizes a new, expert-annotated CPA-QKA dataset derived from the CPA exam. FinCDM's matrix co-factorization model achieved 0.9379 accuracy and 0.9873 AUC, outperforming baselines with gains of +0.177 in accuracy and +0.146 in AUC. This provides AI practitioners with interpretable, skill-aware diagnostics, enabling more targeted LLM development and deployment in financial domains. |
| FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction (Read more on [arXiv](https://arxiv.org/abs/2508.11987) or [HuggingFace](https://huggingface.co/papers/2508.11987))| tianlecai, Nuori, YinLingyue, Tianci-He, liujiashuo77 | FutureX is an advanced, dynamic, and live benchmark introduced to evaluate LLM agents' advanced search and reasoning capabilities in real-world future prediction tasks. It employs a semi-automated pipeline that daily curates future-oriented questions from 195 diverse websites, ensuring real-time relevance and no data contamination, and evaluates 25 LLM/agent models across four difficulty tiers. Overall results show Grok-4 achieves the highest performance, and LLMs with search capabilities generally outperform base LLMs. Notably, top-performing LLMs (Think&Search) achieved a 37.5% win rate against human analysts in revenue prediction and 32.3% in EPS prediction. This benchmark provides a robust, contamination-free standard to advance LLM agent development towards professional human analyst-level performance in complex, high-stakes forecasting. |
| DuPO: Enabling Reliable LLM Self-Verification via Dual Preference
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.14460) or [HuggingFace](https://huggingface.co/papers/2508.14460))| Yu Lu, Yu Bao, Shanbo, ShujianHuang, kevinpro | DuPO is a dual learning-based preference optimization framework that provides annotation-free, self-supervised rewards to fine-tune LLMs on non-invertible tasks. The primary objective is to overcome the limitations of traditional dual learning and RLVR by developing a generalizable, annotation-free optimization method. Its key methodology is a *generalized duality* where an input is decomposed into known (xk) and unknown (xu) components; a dual task then reconstructs xu from the primal task's output and xk, with the reconstruction quality serving as the reward signal. Empirically, DuPO boosted mathematical reasoning accuracy by an average of 6.4 percentage points on a Qwen3-4B model and enhanced translation quality by 2.13 COMET points. The principal implication for AI practitioners is a method to fine-tune models without human-annotated data, which can also be deployed as an effective, training-free inference-time reranker to improve performance by trading computation for accuracy. |
| MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds (Read more on [arXiv](https://arxiv.org/abs/2508.14879) or [HuggingFace](https://huggingface.co/papers/2508.14879))| Jiangmiao, ZhaoyangLyu, asrnline, Qmh, tangqh | MeshCoder is an LLM-powered framework for generating structured Blender Python scripts from 3D point clouds for programmable mesh reconstruction and editing. The main objective is to reconstruct complex 3D objects into editable programs, overcoming limitations of prior domain-specific languages and small datasets. The methodology involves designing expressive Blender Python APIs, constructing a large-scale paired object-code dataset (1 million objects across 41 categories) using a part-to-code inference model, and training a multimodal LLM with triplane-based tokenization. MeshCoder significantly outperforms baselines, achieving an overall average L2 Chamfer Distance of 0.06 (x10^-2) and IoU of 86.75% compared to PLAD's 1.87 (x10^-2) CD and 67.62% IoU, and Shape2Prog's 6.00 (x10^-2) CD and 45.03% IoU. This provides AI practitioners with a flexible solution for programmatic 3D shape reconstruction and understanding, enabling intuitive geometric/topological editing via code and enhancing LLM reasoning for 3D shapes. |
| Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From
  Sparse Inputs without Per-Scene Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.14811) or [HuggingFace](https://huggingface.co/papers/2508.14811))| Hao Chen, Zhiyue Zhao, Tianjian Feng, Xiaoman Li, Canyu | TINKER is a framework for high-fidelity, multi-view consistent 3D editing from sparse image inputs without per-scene optimization. The main objective is to develop a generalizable 3D editing system that operates in one-shot and few-shot regimes, eliminating the computationally intensive per-scene fine-tuning required by previous approaches. The methodology employs a two-stage process: a "Referring multi-view editor," fine-tuned on a novel curated dataset, generates consistent sparse edited views, and an "Any-view-to-video synthesizer" uses depth conditioning and video diffusion priors to perform scene completion for optimizing a 3D Gaussian Splatting representation. TINKER achieves state-of-the-art performance, with its few-shot model obtaining a CLIP directional similarity of 0.157 and an Aesthetic score of 6.338, outperforming prior methods on benchmark datasets. The principal implication for AI practitioners is that TINKER provides a scalable and efficient pipeline that removes the per-scene optimization bottleneck, enabling rapid, high-quality 3D content editing with minimal input images and computational overhead. |
| From AI for Science to Agentic Science: A Survey on Autonomous
  Scientific Discovery (Read more on [arXiv](https://arxiv.org/abs/2508.14111) or [HuggingFace](https://huggingface.co/papers/2508.14111))| zijieqiu, Wanggsh, schrodingers-tiger, ZhangyangGao, VitaCoco | This survey introduces "Agentic Science" as a paradigm where AI systems transition from computational tools to autonomous research partners, proposing a unified framework connecting core capabilities, processes, and domain applications. The paper's objective is to systematically review and define this paradigm by mapping the evolution of AI across four levels of autonomy, from computational oracles to autonomous scientific partners. The methodology is a comprehensive, domain-oriented literature review organized by a new framework integrating five foundational agent capabilities—planning, tool use, memory, collaboration, and evolution—and a four-stage model of the scientific discovery workflow. The survey's primary result is the documentation of validated discoveries by such agents, including a cloud-based AI planner [241] that discovered 21 new state-of-the-art organic laser emitters by autonomously coordinating experiments across five laboratories. The principal implication for AI practitioners is the provision of a structured blueprint for architecting scientific agents, specifying the core functional components required to build systems capable of end-to-end autonomous discovery rather than just task automation. |
| Quantization Meets dLLMs: A Systematic Study of Post-training
  Quantization for Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.14896) or [HuggingFace](https://huggingface.co/papers/2508.14896))| Haobo Xu, cityug7353, ZiyuG, chriswyc, Felix1023 | This paper presents the first systematic study of applying post-training quantization (PTQ) to diffusion-based large language models (dLLMs). The primary objective is to investigate how established PTQ techniques perform on dLLMs by analyzing the effects of bit-width, quantization methods, task categories, and model types. The study implements and evaluates state-of-the-art weight-only (GPTQ, AWQ) and weight-activation (DuQuant, QuaRot, SmoothQuant) methods on models like LLaDA-8B and Dream-7B across various benchmarks. Results demonstrate that 4-bit weight-only quantization with GPTQ is a robust choice, showing only a 0.3% performance drop on general tasks for LLaDA-8B, whereas 4-bit weight-activation quantization remains a significant challenge, with even the best methods causing notable degradation, particularly on math and code generation tasks. For AI practitioners, this implies that 4-bit GPTQ can be effectively used for deploying dLLMs to reduce memory footprint, but they should expect significant performance loss for tasks requiring complex reasoning, and note that instruct-tuned models are more resilient to quantization than base models. |
| RynnEC: Bringing MLLMs into Embodied World (Read more on [arXiv](https://arxiv.org/abs/2508.14160) or [HuggingFace](https://huggingface.co/papers/2508.14160))| jiangpinliu, CausalLi, maoyunxuan, CircleRadon, RH-Dang | RynnEC is a video multimodal large language model designed for embodied cognition that incorporates region-level encoders and decoders for fine-grained visual interaction. The primary objective is to develop a compact MLLM capable of detailed object understanding and coherent video-based spatial awareness, overcoming the limitations of general-purpose models in embodied scenarios. The methodology involves a novel egocentric video-based pipeline to generate a large-scale embodied cognition dataset and a progressive four-stage training curriculum (Mask Alignment, Object Understanding, Spatial Understanding, Referring Segmentation) to instill these capabilities. On the proposed RynnEC-Bench, the 7B parameter model achieves a state-of-the-art overall score of 56.2, outperforming the proprietary Gemini-2.5 Pro model by 10.7 points. For AI practitioners, RynnEC offers a validated architecture and training framework for building efficient cognitive cores for robotic agents, enabling more precise environmental perception and interaction for complex, real-world tasks. |
| NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid
  Mamba-Transformer Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2508.14444) or [HuggingFace](https://huggingface.co/papers/2508.14444))| abercovich, aditya-malte, adirendu, aklife97, apaithan | The paper introduces Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model optimized for high-throughput inference on resource-constrained hardware. The primary objective was to develop a model that maintains state-of-the-art accuracy on reasoning tasks while enabling inference on long contexts (up to 128k tokens) on a single 22GiB GPU. The methodology involved pre-training a 12B base model on 20 trillion tokens with an FP8 recipe, aligning it using SFT, GRPO, DPO, and RLHF, and then compressing it to 9B parameters via the Minitron framework using structured pruning and knowledge distillation. The final Nemotron-Nano-9B-v2 model achieves on-par or better accuracy than models like Qwen3-8B while demonstrating up to 6.3x higher inference throughput in generation-heavy settings. The principal implication for AI practitioners is that this hybrid architecture and compression strategy offers a concrete pathway for deploying high-performance, long-context reasoning models efficiently on consumer-grade hardware. |
| MCP-Universe: Benchmarking Large Language Models with Real-World Model
  Context Protocol Servers (Read more on [arXiv](https://arxiv.org/abs/2508.14704) or [HuggingFace](https://huggingface.co/papers/2508.14704))| Prathyusha Jwalapuram, Zirui Zhao, Wenzhuo Yang, Zhiqi Shen, Ziyang Luo | MCP-Universe is the first comprehensive benchmark evaluating LLMs in realistic tasks using real-world Model Context Protocol (MCP) servers. It addresses the limitations of existing simplistic benchmarks by assessing LLM capabilities in complex real-world MCP environments that involve long-horizon reasoning and large, unfamiliar tool spaces. The benchmark comprises 231 tasks across 6 domains and 11 real-world MCP servers, employing rigorous execution-based evaluators including format, static, and dynamic checks for task completion. Experiments reveal that even top-performing LLMs like GPT-5 achieve only a 43.72% success rate, indicating significant performance limitations due to long-context and unknown-tool challenges. These results underscore the critical need for targeted advancements in LLM agent design and integration to improve robustness in MCP-driven real-world applications. |
| ViExam: Are Vision Language Models Better than Humans on Vietnamese
  Multimodal Exam Questions? (Read more on [arXiv](https://arxiv.org/abs/2508.13680) or [HuggingFace](https://huggingface.co/papers/2508.13680))| Daeyoung Kim, Duc Dm, Quang Tau, anvo25, tuongvy2603 | ViExam introduces the first comprehensive Vietnamese multimodal exam benchmark to evaluate Vision Language Models. The paper investigates Vision Language Models' performance on Vietnamese educational assessments and their cross-lingual multimodal reasoning capabilities. The study introduces ViExam, a benchmark of 2,548 multimodal questions across seven domains, evaluating various state-of-the-art and open-source VLMs, while also exploring cross-lingual prompting and human-in-the-loop collaboration. State-of-the-art VLMs achieved only 57.74% mean accuracy on ViExam, underperforming average human test-takers (66.54%), despite exhibiting strong Vietnamese OCR performance (mean F1 0.94). For AI practitioners, these results highlight significant challenges in multimodal integration and culturally specific knowledge for VLMs, especially in low-resource languages, indicating a need for more robust cross-lingual and multimodal reasoning development. |
| On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised
  Fine-Tuning and Reinforcement Learning via Dynamic Weighting (Read more on [arXiv](https://arxiv.org/abs/2508.11408) or [HuggingFace](https://huggingface.co/papers/2508.11408))| Guoyin Wang, Yanxi Chen, Yuchang Sun, Yuexiang Xie, xiaoniqiu | This paper presents CHORD, a framework that unifies Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) through a dynamic dual-control weighting mechanism. The primary objective is to address the policy disruption and "shift-readapt-overfit" progression observed in sequential SFT-then-RL training by harmonizing off-policy expert data with on-policy exploration. CHORD's methodology involves reframing SFT as an auxiliary objective within RL, controlled by a global coefficient (µ) that schedules the transition from imitation to exploration, and a token-wise weighting function (φ) that stabilizes training by down-weighting highly disruptive expert tokens. The proposed CHORD-φ model achieved a 62.5% accuracy on the AMC math reasoning benchmark, outperforming the strong SFT-best + RL baseline of 58.4%. For AI practitioners, this framework provides a method to integrate static expert datasets into on-policy RL for pre-aligned LLMs, enabling selective knowledge absorption while mitigating instability and overfitting. |
| Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single
  Bootstrap per Cell (Read more on [arXiv](https://arxiv.org/abs/2508.14568) or [HuggingFace](https://huggingface.co/papers/2508.14568))| Ingrid Verbauwhede, Nam-Luc Tran, Bojan Spasic, Jan-Pieter D'Anvers, woutLegiest | The paper introduces Leuvenshtein, a novel algorithm for Fully Homomorphic Encryption (FHE)-based edit distance calculation that reduces the core computation to a single programmable bootstrap operation per cell. The objective is to design an efficient Levenshtein distance algorithm within the TFHE framework by minimizing the number of costly programmable bootstrap (PBS) operations required for both the main recurrence and character equality checks. The methodology adapts the Myers algorithm by using compact differential representations and rewrites the update equations to isolate the non-linear computation into a shared three-input minimum function, which is then implemented in a single PBS operation using a dense input packing scheme that leverages the negacyclic property of TFHE look-up tables; it also introduces an optimized 2-PBS equality check for 7-bit ASCII characters. The primary result is a speedup of up to 278x over the best available FHE implementation for computing the exact edit distance between two 256-character ASCII strings, achieved by reducing the main algorithm cost from over 94 PBS operations per cell to one, and the character equality check from five PBS to two. The principal implication for AI practitioners is that this significant performance improvement makes privacy-preserving approximate string matching computationally feasible for real-world applications like financial fraud detection or genomic analysis, where the overhead of FHE was previously a prohibitive barrier. |
| Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer (Read more on [arXiv](https://arxiv.org/abs/2508.14187) or [HuggingFace](https://huggingface.co/papers/2508.14187))| Jeremiah Jiang, Lim Jun Hao, Michael N. Cheng, Chiao-An Yang, ashiq24 | This paper presents a Deep Equilibrium Canonicalizer (DEC) to improve the local scale equivariance of deep learning models. The objective is to develop a method that makes neural networks robust to independent scale variations of different objects within the same image. The methodology involves defining a "monotone scaling" group to approximate local scaling and using a DEC module, an implicit neural network, to transform latent features into a canonical representation by solving for a fixed point of a learned energy function. On a locally scaled MNIST dataset using a Swin transformer, the proposed method improved accuracy to 96.53% from a 93.93% baseline and reduced the invariance error (InvE) from 5.44 to 2.08. The principal implication for AI practitioners is that DEC can be integrated as an adaptable module into existing pre-trained vision models to enhance their performance and prediction consistency on tasks with significant local object scale variations, without requiring architectural redesign. |
| mSCoRe: a Multilingual and Scalable Benchmark for Skill-based
  Commonsense Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10137) or [HuggingFace](https://huggingface.co/papers/2508.10137))| anoperson, Franck-Dernoncourt, ntnghia1811 | The paper introduces mSCoRe, a multilingual and scalable benchmark with a novel skill-based taxonomy to enable fine-grained analysis of commonsense reasoning in Large Language Models (LLMs). The primary objective is to systematically evaluate and analyze LLMs' multilingual commonsense reasoning capabilities by creating a benchmark that can dynamically scale in difficulty and classify the atomic reasoning steps models employ. The methodology involves a four-step data synthesis pipeline that starts from seed datasets, uses an LLM to generate structured reasoning paths tagged with specific skills, systematically scales question complexity, and creates context-implicit questions to test inferential abilities. Experiments on eight state-of-the-art LLMs demonstrate significant performance degradation with increasing complexity, with the average accuracy of GPT-4o on the general commonsense subset (mSCoRe-G) dropping from 79.2% at the base complexity level to 69.5% at level 3. For AI practitioners, this implies that current models, including those with reasoning-reinforced training, have a rigid and limited utilization of reasoning skills, highlighting the need for developing training methodologies that foster more diverse and adaptive reasoning strategies beyond simple parameter scaling. |
| Refining Contrastive Learning and Homography Relations for Multi-Modal
  Recommendation (Read more on [arXiv](https://arxiv.org/abs/2508.13745) or [HuggingFace](https://huggingface.co/papers/2508.13745))| Shiqing Wu, Yawen Zeng, guandongxu, MrShouxingMa | The paper introduces REARM, a framework that enhances multi-modal recommendation by refining contrastive learning and expanding homography relations. The primary objective is to overcome the limitations of existing methods that generate noisy shared-modal representations while losing valuable unique-modal information, and to better model user-item interplay. The methodology integrates GNN-based learning on four distinct homography graphs (user/item co-occurrence and interest/semantic graphs) with a novel contrastive learning module that employs a meta-network to denoise shared features and an orthogonal constraint loss to preserve unique features. On the Sports dataset, REARM achieves a Recall@20 of 0.1231, outperforming the next-best state-of-the-art baseline which scored 0.1139. For AI practitioners, the principal implication is that multi-modal recommendation performance can be significantly improved by explicitly disentangling shared and unique feature representations via mechanisms like meta-networks and orthogonality constraints, rather than relying solely on feature alignment. |
